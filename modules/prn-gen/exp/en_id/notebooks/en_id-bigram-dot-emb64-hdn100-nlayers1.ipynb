{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/notebooks/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [100, 125, 150, 175, 200]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369, 165, 398, 227, 577, 394, 107, 275, 416, 659, 1]\n",
      "tensor([[369],\n",
      "        [165],\n",
      "        [398],\n",
      "        [227],\n",
      "        [577],\n",
      "        [394],\n",
      "        [107],\n",
      "        [275],\n",
      "        [416],\n",
      "        [659],\n",
      "        [  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7fbe7c190910> ([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "valid grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "test grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {seconds}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 95240 parameters\n",
      "Decoder has a total number of 89340 parameters\n",
      "Total number of all parameters is 184580\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = .001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5, patience=1)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 46.319865465164185s (- 37m 49.673407793045044s) (1 2.0%). train avg loss: 1.4441, val avg loss: 1.3124\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 37.977932929992676s (- 39m 11.470390319824219s) (2 4.0%). train avg loss: 0.5834, val avg loss: 0.9359\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 30.160990953445435s (- 39m 12.522191603978627s) (3 6.0%). train avg loss: 0.4569, val avg loss: 0.9254\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 3m 22.428678512573242s (- 38m 47.929802894592285s) (4 8.0%). train avg loss: 0.4254, val avg loss: 1.0141\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 4m 8.084734439849854s (- 37m 12.762609958648682s) (5 10.0%). train avg loss: 0.3773, val avg loss: 0.9433\n",
      "Training for epoch 6 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 6 finished in 4m 57.103442907333374s (- 36m 18.758581320445046s) (6 12.0%). train avg loss: 0.3142, val avg loss: 0.8039\n",
      "Training for epoch 7 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 7 finished in 5m 45.661449670791626s (- 35m 23.348905120577s) (7 14.0%). train avg loss: 0.3007, val avg loss: 0.7911\n",
      "Training for epoch 8 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 8 finished in 6m 34.9268741607666s (- 34m 33.36608934402466s) (8 16.0%). train avg loss: 0.2965, val avg loss: 0.7698\n",
      "Training for epoch 9 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 9 finished in 7m 24.482535362243652s (- 33m 44.8648833168877s) (9 18.0%). train avg loss: 0.2673, val avg loss: 0.7525\n",
      "Training for epoch 10 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 10 finished in 8m 9.593559503555298s (- 32m 38.37423801422119s) (10 20.0%). train avg loss: 0.2687, val avg loss: 0.7693\n",
      "Training for epoch 11 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 11 finished in 8m 53.671199321746826s (- 31m 32.10697941346598s) (11 22.0%). train avg loss: 0.253, val avg loss: 0.7279\n",
      "Training for epoch 12 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 12 finished in 9m 38.05236554145813s (- 30m 30.499157547950745s) (12 24.0%). train avg loss: 0.2596, val avg loss: 0.711\n",
      "Training for epoch 13 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 13 finished in 10m 23.90636420249939s (- 29m 35.733498114806025s) (13 26.0%). train avg loss: 0.2526, val avg loss: 0.7437\n",
      "Training for epoch 14 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 14 finished in 11m 12.167033195495605s (- 28m 48.42951393127396s) (14 28.0%). train avg loss: 0.2325, val avg loss: 0.696\n",
      "Training for epoch 15 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 15 finished in 12m 0.2829773426055908s (- 28m 0.6602804660797119s) (15 30.0%). train avg loss: 0.2319, val avg loss: 0.753\n",
      "Training for epoch 16 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 16 finished in 12m 45.14789605140686s (- 27m 5.939279109239578s) (16 32.0%). train avg loss: 0.2618, val avg loss: 0.7001\n",
      "Training for epoch 17 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 17 finished in 13m 29.550439834594727s (- 26m 11.480265561272063s) (17 34.0%). train avg loss: 0.2133, val avg loss: 0.693\n",
      "Training for epoch 18 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 18 finished in 14m 20.11583423614502s (- 25m 29.09481641981347s) (18 36.0%). train avg loss: 0.2074, val avg loss: 0.6762\n",
      "Training for epoch 19 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 19 finished in 15m 9.279268503189087s (- 24m 43.560911768361166s) (19 38.0%). train avg loss: 0.2047, val avg loss: 0.6905\n",
      "Training for epoch 20 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 20 finished in 15m 57.4865927696228s (- 23m 56.229889154434204s) (20 40.0%). train avg loss: 0.1972, val avg loss: 0.6569\n",
      "Training for epoch 21 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 21 finished in 16m 44.91857981681824s (- 23m 7.744705461320336s) (21 42.0%). train avg loss: 0.1956, val avg loss: 0.6505\n",
      "Training for epoch 22 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 22 finished in 17m 32.33561062812805s (- 22m 19.33623170852661s) (22 44.0%). train avg loss: 0.1957, val avg loss: 0.628\n",
      "Training for epoch 23 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 23 finished in 18m 17.083266735076904s (- 21m 27.880356602046504s) (23 46.0%). train avg loss: 0.1925, val avg loss: 0.6537\n",
      "Training for epoch 24 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 24 finished in 19m 7.597166299819946s (- 20m 43.23026349147176s) (24 48.0%). train avg loss: 0.1934, val avg loss: 0.6592\n",
      "Training for epoch 25 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 25 finished in 19m 57.23949694633484s (- 19m 57.23949694633484s) (25 50.0%). train avg loss: 0.1814, val avg loss: 0.6253\n",
      "Training for epoch 26 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 26 finished in 20m 48.747645139694214s (- 19m 12.690133975102071s) (26 52.0%). train avg loss: 0.1762, val avg loss: 0.6478\n",
      "Training for epoch 27 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 27 finished in 21m 39.20137548446655s (- 18m 26.727097634915935s) (27 54.0%). train avg loss: 0.1773, val avg loss: 0.6326\n",
      "Training for epoch 28 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 28 finished in 22m 27.790502786636353s (- 17m 38.978252189499926s) (28 56.0%). train avg loss: 0.1707, val avg loss: 0.6167\n",
      "Training for epoch 29 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 29 finished in 23m 12.565738201141357s (- 16m 48.409672490481626s) (29 58.0%). train avg loss: 0.1685, val avg loss: 0.62\n",
      "Training for epoch 30 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 30 finished in 23m 59.86287021636963s (- 15m 59.90858014424657s) (30 60.0%). train avg loss: 0.1695, val avg loss: 0.6269\n",
      "Training for epoch 31 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 31 finished in 24m 46.600746870040894s (- 15m 11.142393242928392s) (31 62.0%). train avg loss: 0.1665, val avg loss: 0.6312\n",
      "Training for epoch 32 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 32 finished in 25m 34.216163635253906s (- 14m 22.996592044830322s) (32 64.0%). train avg loss: 0.1637, val avg loss: 0.6156\n",
      "Training for epoch 33 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 33 finished in 26m 24.396507024765015s (- 13m 36.20426119457579s) (33 66.0%). train avg loss: 0.165, val avg loss: 0.6392\n",
      "Training for epoch 34 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 34 finished in 27m 13.533282279968262s (- 12m 48.72154460233787s) (34 68.0%). train avg loss: 0.1635, val avg loss: 0.6369\n",
      "Training for epoch 35 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 35 finished in 27m 57.55863857269287s (- 11m 58.95370224544013s) (35 70.0%). train avg loss: 0.1632, val avg loss: 0.6344\n",
      "Training for epoch 36 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 36 finished in 28m 41.92695379257202s (- 11m 9.638259808222756s) (36 72.0%). train avg loss: 0.1617, val avg loss: 0.6412\n",
      "Training for epoch 37 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 37 finished in 29m 27.12305760383606s (- 10m 20.881074293239635s) (37 74.0%). train avg loss: 0.1624, val avg loss: 0.6262\n",
      "Training for epoch 38 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 38 finished in 30m 14.979848146438599s (- 9m 33.15153099361214s) (38 76.0%). train avg loss: 0.1624, val avg loss: 0.6271\n",
      "Training for epoch 39 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 39 finished in 31m 3.017050266265869s (- 8m 45.46634751099782s) (39 78.0%). train avg loss: 0.1642, val avg loss: 0.6277\n",
      "Training for epoch 40 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 40 finished in 31m 49.295854330062866s (- 7m 57.32396358251572s) (40 80.0%). train avg loss: 0.1614, val avg loss: 0.6246\n",
      "Training for epoch 41 has started (lr=1.953125e-06). Found 1916 batch(es).\n",
      "Epoch 41 finished in 32m 35.61811184883118s (- 7m 9.28202455218252s) (41 82.0%). train avg loss: 0.1608, val avg loss: 0.6251\n",
      "Training for epoch 42 has started (lr=1.953125e-06). Found 1916 batch(es).\n",
      "Epoch 42 finished in 33m 22.30120611190796s (- 6m 21.39070592607777s) (42 84.0%). train avg loss: 0.1627, val avg loss: 0.6284\n",
      "Early stopping after 42 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 50\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-2, min_value+.075, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 14.799665966185701%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    ' '.join(output_phonemes) if output_phonemes else [\"<EOS>\"],\n",
    "    arpabet_phoneme_sequence if output_phonemes else arpabet_phoneme_sequence+\" <EOS>\"\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> podolski\n",
      "= ['P', 'AX', 'D', 'AO', 'W', 'L', 'S', 'K', 'IY']\n",
      "< P AO D AO L S K IY ['P', 'AO', 'D', 'AO', 'L', 'S', 'K', 'IY']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbcd7c28c70>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVwAAAG/CAYAAAAUxm8AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAZiElEQVR4nO3dfXBUhb3/8c+awIKYXQEJJpcFUsqVhwDSBG0AKwqmv4hcnE6pOEhTqTNNG57M2GK0D2gLizOtY1tq2lCHwjAQ2l/LQ2cEDFWCDqYlgVSKDg+FMVGgGRjdDel1geTcP+4196YkwEmy38Mu79fMmXbXXfczU/vu6UnY43McxxEAIO5u8noAANwoCC4AGCG4AGCE4AKAEYILAEYILgAYIbgAYITgAoARggsARgguABhJiuC+/PLLysrKUp8+fZSTk6M333zT60md2rt3r2bNmqXMzEz5fD5t3brV60lXFA6HNWnSJKWlpSk9PV0PP/ywjhw54vWsTpWVlWn8+PEKBAIKBALKy8vTjh07vJ51zcLhsHw+n5YuXer1lE4tX75cPp+v3XH77bd7PeuKPvzwQz322GMaOHCgbr75Zt15552qra0135Hwwd28ebOWLl2qZ599VgcPHtQ999yjgoIC1dfXez2tQ83NzZowYYJWr17t9ZRrUlVVpeLiYlVXV6uyslKXLl1Sfn6+mpubvZ7WoSFDhmjVqlWqqalRTU2N7r//fs2ePVuHDx/2etpV7d+/X+Xl5Ro/frzXU65q7NixOn36dNtx6NAhryd16qOPPtKUKVPUq1cv7dixQ++++65+8pOf6NZbb7Uf4yS4u+66yykqKmr33KhRo5ynn37ao0XXTpKzZcsWr2e40tjY6EhyqqqqvJ5yzfr37+/8+te/9nrGFTU1NTkjR450KisrnXvvvddZsmSJ15M69YMf/MCZMGGC1zOu2bJly5ypU6d6PcNxHMdJ6DPcCxcuqLa2Vvn5+e2ez8/P1759+zxaldwikYgkacCAAR4vubqWlhZVVFSoublZeXl5Xs+5ouLiYs2cOVMzZszweso1OXbsmDIzM5WVlaW5c+fqxIkTXk/q1Pbt25Wbm6s5c+YoPT1dEydO1Jo1azzZktDBPXv2rFpaWjR48OB2zw8ePFhnzpzxaFXychxHJSUlmjp1qrKzs72e06lDhw7plltukd/vV1FRkbZs2aIxY8Z4PatTFRUVOnDggMLhsNdTrsndd9+t9evXa9euXVqzZo3OnDmjyZMn69y5c15P69CJEydUVlamkSNHateuXSoqKtLixYu1fv168y2p5p8YBz6fr91jx3Euew7dt3DhQr3zzjt66623vJ5yRXfccYfq6ur08ccf6/e//70KCwtVVVV1XUa3oaFBS5Ys0WuvvaY+ffp4PeeaFBQUtP37cePGKS8vTyNGjNC6detUUlLi4bKOtba2Kjc3VytXrpQkTZw4UYcPH1ZZWZm++tWvmm5J6DPc2267TSkpKZedzTY2Nl521ovuWbRokbZv36433nhDQ4YM8XrOFfXu3Vuf/exnlZubq3A4rAkTJuinP/2p17M6VFtbq8bGRuXk5Cg1NVWpqamqqqrSz372M6WmpqqlpcXriVfVr18/jRs3TseOHfN6SocyMjIu+x/b0aNHe/KD9YQObu/evZWTk6PKysp2z1dWVmry5MkerUoujuNo4cKF+sMf/qDXX39dWVlZXk9yzXEcxWIxr2d0aPr06Tp06JDq6urajtzcXM2bN091dXVKSUnxeuJVxWIxvffee8rIyPB6SoemTJly2a8yHj16VMOGDTPfkvCXFEpKSjR//nzl5uYqLy9P5eXlqq+vV1FRkdfTOnT+/HkdP3687fHJkydVV1enAQMGaOjQoR4u61hxcbE2btyobdu2KS0tre3/TQSDQfXt29fjdZd75plnVFBQoFAopKamJlVUVGjPnj3auXOn19M6lJaWdtn18H79+mngwIHX7XXyp556SrNmzdLQoUPV2NioH/3oR4pGoyosLPR6WoeefPJJTZ48WStXrtRXvvIV/eUvf1F5ebnKy8vtx3j7SxI94xe/+IUzbNgwp3fv3s7nPve56/pXlt544w1H0mVHYWGh19M61NFWSc7atWu9ntahBQsWtP2zMGjQIGf69OnOa6+95vUsV673Xwt75JFHnIyMDKdXr15OZmam86Uvfck5fPiw17Ou6I9//KOTnZ3t+P1+Z9SoUU55ebknO3yOw00kAcBCQl/DBYBEQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACNJE9xYLKbly5dft3+E81+xN/4SbTN74+t62Js0f/AhGo0qGAwqEokoEAh4Peeq2Bt/ibaZvfF1PexNmjNcALjeEVwAMGL+bWGtra06deqU0tLSevRLwqPRaLt/vd6xN/4SbTN74yueex3HUVNTkzIzM3XTTZ2fx5pfw/3ggw8UCoUsPxIATDQ0NFzxC/rNz3DT0tIkSVP1oFLVy/rjAaDHXdJFvaVX2/rWGfPgfnoZIVW9lOojuACSwP9cJ7jaZVJ+aAYARgguABghuABghOACgBGCCwBGCC4AGCG4AGCE4AKAEYILAEYILgAYIbgAYITgAoARggsARgguABjpUnBffvllZWVlqU+fPsrJydGbb77Z07sAIOm4Du7mzZu1dOlSPfvsszp48KDuueceFRQUqL6+Ph77ACBpuA7uiy++qK9//et64oknNHr0aL300ksKhUIqKyuLxz4ASBqugnvhwgXV1tYqPz+/3fP5+fnat29fjw4DgGTj6hY7Z8+eVUtLiwYPHtzu+cGDB+vMmTMdvicWiykWi7U9TpQ7fAJAT+vSD83+9b49juN0ei+fcDisYDDYdnDHXgA3KlfBve2225SSknLZ2WxjY+NlZ72fKi0tVSQSaTsaGhq6vhYAEpir4Pbu3Vs5OTmqrKxs93xlZaUmT57c4Xv8fr8CgUC7AwBuRK5vk15SUqL58+crNzdXeXl5Ki8vV319vYqKiuKxDwCShuvgPvLIIzp37pyef/55nT59WtnZ2Xr11Vc1bNiweOwDgKThcxzHsfzAaDSqYDCoaZqtVF8vy48GgLi45FzUHm1TJBK54mVTvksBAIwQXAAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIwQXAAwQnABwIjrLyDvKf//yF8VSEuM3j84JMfrCe7YfsUxgGuUGMUDgCRAcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIwQXAAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACMEFwCMEFwAMEJwAcCI6+Du3btXs2bNUmZmpnw+n7Zu3RqPXQCQdFwHt7m5WRMmTNDq1avjsQcAkpbrm0gWFBSooKAgHlsAIKnF/a69sVhMsVis7XE0Go33RwLAdSnuPzQLh8MKBoNtRygUivdHAsB1Ke7BLS0tVSQSaTsaGhri/ZEAcF2K+yUFv98vv98f748BgOsev4cLAEZcn+GeP39ex48fb3t88uRJ1dXVacCAARo6dGiPjgOAZOI6uDU1NbrvvvvaHpeUlEiSCgsL9Zvf/KbHhgFAsnEd3GnTpslxnHhsAYCkxjVcADBCcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIwQXAAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACNxv6dZZ758xwSl+np59fGu7Dp10OsJrnwx806vJwDoAGe4AGCE4AKAEYILAEYILgAYIbgAYITgAoARggsARgguABghuABghOACgBGCCwBGCC4AGCG4AGCE4AKAEYILAEYILgAYIbgAYMRVcMPhsCZNmqS0tDSlp6fr4Ycf1pEjR+K1DQCSiqvgVlVVqbi4WNXV1aqsrNSlS5eUn5+v5ubmeO0DgKTh6p5mO3fubPd47dq1Sk9PV21trb7whS/06DAASDbduolkJBKRJA0YMKDT18RiMcVisbbH0Wi0Ox8JAAmryz80cxxHJSUlmjp1qrKzszt9XTgcVjAYbDtCoVBXPxIAElqXg7tw4UK988472rRp0xVfV1paqkgk0nY0NDR09SMBIKF16ZLCokWLtH37du3du1dDhgy54mv9fr/8fn+XxgFAMnEVXMdxtGjRIm3ZskV79uxRVlZWvHYBQNJxFdzi4mJt3LhR27ZtU1pams6cOSNJCgaD6tu3b1wGAkCycHUNt6ysTJFIRNOmTVNGRkbbsXnz5njtA4Ck4fqSAgCga/guBQAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIwQXAAwQnABwAjBBQAj3bpN+o3i/w27y+sJrvzy/de9nuBaUda9Xk9wxdcrsf6r41y44PUE95Lw+7c5wwUAIwQXAIwQXAAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIwQXAAwQnABwAjBBQAjroJbVlam8ePHKxAIKBAIKC8vTzt27IjXNgBIKq6CO2TIEK1atUo1NTWqqanR/fffr9mzZ+vw4cPx2gcAScPVjZlmzZrV7vGKFStUVlam6upqjR07tkeHAUCy6fKd8FpaWvS73/1Ozc3NysvL68lNAJCUXAf30KFDysvL0yeffKJbbrlFW7Zs0ZgxYzp9fSwWUywWa3scjUa7thQAEpzr31K44447VFdXp+rqan3zm99UYWGh3n333U5fHw6HFQwG245QKNStwQCQqHyO072bv8+YMUMjRozQr371qw7/ekdnuKFQSNM0W6m+Xt35aDO+Xr29nuBK2fHXvZ7gWlHWvV5PcMXXq8tX4zzhXLjg9QT3upcmU5eci9qjbYpEIgoEAp2+rtv/1DiO0y6o/8rv98vv93f3YwAg4bkK7jPPPKOCggKFQiE1NTWpoqJCe/bs0c6dO+O1DwCShqvg/uMf/9D8+fN1+vRpBYNBjR8/Xjt37tQDDzwQr30AkDRcBfeVV16J1w4ASHp8lwIAGCG4AGCE4AKAEYILAEYILgAYIbgAYITgAoARggsARgguABghuABghOACgBGCCwBGCC4AGCG4AGCE4AKAkcS6MZNHnIuJdT+oouH3eD3Bte0fVHs9wZX/GHKX1xPcSaD7gyUzznABwAjBBQAjBBcAjBBcADBCcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIwQXAAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACPdCm44HJbP59PSpUt7ag8AJK0uB3f//v0qLy/X+PHje3IPACStLgX3/PnzmjdvntasWaP+/fv39CYASEpdCm5xcbFmzpypGTNmXPW1sVhM0Wi03QEANyLXt0mvqKjQgQMHtH///mt6fTgc1nPPPed6GAAkG1dnuA0NDVqyZIk2bNigPn36XNN7SktLFYlE2o6GhoYuDQWAROfqDLe2tlaNjY3Kyclpe66lpUV79+7V6tWrFYvFlJKS0u49fr9ffr+/Z9YCQAJzFdzp06fr0KFD7Z57/PHHNWrUKC1btuyy2AIA/per4KalpSk7O7vdc/369dPAgQMvex4A0B5/0gwAjLj+LYV/tWfPnh6YAQDJjzNcADBCcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIwQXAAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACPd/j5cXIccx+sFrv3Hv03yeoIru04d9HqCK1/MvNPrCRBnuABghuACgBGCCwBGCC4AGCG4AGCE4AKAEYILAEYILgAYIbgAYITgAoARggsARgguABghuABghOACgBGCCwBGCC4AGCG4AGDEVXCXL18un8/X7rj99tvjtQ0AkorrW+yMHTtWu3fvbnuckpLSo4MAIFm5Dm5qaipntQDQBa6v4R47dkyZmZnKysrS3LlzdeLEiSu+PhaLKRqNtjsA4EbkKrh333231q9fr127dmnNmjU6c+aMJk+erHPnznX6nnA4rGAw2HaEQqFujwaARORznK7fU7u5uVkjRozQd77zHZWUlHT4mlgsplgs1vY4Go0qFAppmmYr1derqx8NeGrXqTqvJ7jCbdLj65JzUXu0TZFIRIFAoNPXub6G+3/169dP48aN07Fjxzp9jd/vl9/v787HAEBS6Nbv4cZiMb333nvKyMjoqT0AkLRcBfepp55SVVWVTp48qT//+c/68pe/rGg0qsLCwnjtA4Ck4eqSwgcffKBHH31UZ8+e1aBBg/T5z39e1dXVGjZsWLz2AUDScBXcioqKeO0AgKTHdykAgBGCCwBGCC4AGCG4AGCE4AKAEYILAEYILgAYIbgAYITgAoARggsARgguABghuABghOACgBGCCwBGCC4AGOnWPc2AG1XByCleT3DlP3elez3BtYvrBns94Zq1XPxE+u22q76OM1wAMEJwAcAIwQUAIwQXAIwQXAAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIwQXAAw4jq4H374oR577DENHDhQN998s+68807V1tbGYxsAJBVXt9j56KOPNGXKFN13333asWOH0tPT9fe//1233nprvPYBQNJwFdwXXnhBoVBIa9eubXtu+PDhPb0JAJKSq0sK27dvV25urubMmaP09HRNnDhRa9asueJ7YrGYotFouwMAbkSugnvixAmVlZVp5MiR2rVrl4qKirR48WKtX7++0/eEw2EFg8G2IxQKdXs0ACQin+M4zrW+uHfv3srNzdW+ffvanlu8eLH279+vt99+u8P3xGIxxWKxtsfRaFShUEjTNFupvl7dmA5456Z+/bye4ErzH7hNejy1XPxEtb/9riKRiAKBQKevc3WGm5GRoTFjxrR7bvTo0aqvr+/0PX6/X4FAoN0BADciV8GdMmWKjhw50u65o0ePatiwYT06CgCSkavgPvnkk6qurtbKlSt1/Phxbdy4UeXl5SouLo7XPgBIGq6CO2nSJG3ZskWbNm1Sdna2fvjDH+qll17SvHnz4rUPAJKGq9/DlaSHHnpIDz30UDy2AEBS47sUAMAIwQUAIwQXAIwQXAAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIy4/j5cAFJrc7PXE1zp+8WTXk9wbe+pLV5PuGbRplb1/+3VX8cZLgAYIbgAYITgAoARggsARgguABghuABghOACgBGCCwBGCC4AGCG4AGCE4AKAEYILAEYILgAYIbgAYITgAoARggsARgguABhxFdzhw4fL5/NddhQXF8drHwAkDVe32Nm/f79aWlraHv/tb3/TAw88oDlz5vT4MABINq6CO2jQoHaPV61apREjRujee+/t0VEAkIy6fA33woUL2rBhgxYsWCCfz9eTmwAgKXX5rr1bt27Vxx9/rK997WtXfF0sFlMsFmt7HI1Gu/qRAJDQunyG+8orr6igoECZmZlXfF04HFYwGGw7QqFQVz8SABJal4L7/vvva/fu3XriiSeu+trS0lJFIpG2o6GhoSsfCQAJr0uXFNauXav09HTNnDnzqq/1+/3y+/1d+RgASCquz3BbW1u1du1aFRYWKjW1y5eAAeCG4zq4u3fvVn19vRYsWBCPPQCQtFyfoubn58txnHhsAYCkxncpAIARggsARgguABghuABghOACgBGCCwBGCC4AGCG4AGCE4AKAEYILAEYILgAYIbgAYITgAoARggsARgguABghuABghOACgBGCCwBGCC4AGCG4AGCE4AKAEYILAEYILgAYIbgAYITgAoARggsARgguABghuABghOACgBGCCwBGCC4AGCG4AGCE4AKAEYILAEZcBffSpUv67ne/q6ysLPXt21ef+cxn9Pzzz6u1tTVe+wAgaaS6efELL7ygX/7yl1q3bp3Gjh2rmpoaPf744woGg1qyZEm8NgJAUnAV3LfffluzZ8/WzJkzJUnDhw/Xpk2bVFNTE5dxAJBMXF1SmDp1qv70pz/p6NGjkqS//vWveuutt/Tggw92+p5YLKZoNNruAIAbkasz3GXLlikSiWjUqFFKSUlRS0uLVqxYoUcffbTT94TDYT333HPdHgoAic7VGe7mzZu1YcMGbdy4UQcOHNC6dev04x//WOvWrev0PaWlpYpEIm1HQ0NDt0cDQCJydYb77W9/W08//bTmzp0rSRo3bpzef/99hcNhFRYWdvgev98vv9/f/aUAkOBcneH+85//1E03tX9LSkoKvxYGANfA1RnurFmztGLFCg0dOlRjx47VwYMH9eKLL2rBggXx2gcAScNVcH/+85/re9/7nr71rW+psbFRmZmZ+sY3vqHvf//78doHAEnD5ziOY/mB0WhUwWBQ0zRbqb5elh8NIIHsOlXn9YRrFm1qVf9/P6FIJKJAINDp6/guBQAwQnABwAjBBQAjBBcAjBBcADBCcAHACMEFACMEFwCMEFwAMEJwAcAIwQUAIwQXAIwQXAAwQnABwIir78PtCZ9+G+QlXZRMvxgSQCKJNiXOnWSi5/9769W+7dY8uE1NTZKkt/Sq9UcDSCD9/93rBe41NTUpGAx2+tfNv4C8tbVVp06dUlpamnw+X4/9faPRqEKhkBoaGq74BcDXC/bGX6JtZm98xXOv4zhqampSZmbmZfd9/L/Mz3BvuukmDRkyJG5//0AgkBD/4X+KvfGXaJvZG1/x2nulM9tP8UMzADBCcAHASMry5cuXez2ip6SkpGjatGlKTTW/UtIl7I2/RNvM3vjyeq/5D80A4EbFJQUAMEJwAcAIwQUAIwQXAIwQXAAwQnABwAjBBQAjBBcAjPwXspHGcT8FqlQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x514.286 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
