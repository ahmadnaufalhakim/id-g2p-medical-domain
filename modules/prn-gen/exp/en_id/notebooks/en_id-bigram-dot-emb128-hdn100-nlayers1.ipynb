{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/notebooks/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"128\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [100, 125, 150, 175, 200]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369, 165, 398, 227, 577, 394, 107, 275, 416, 659, 1]\n",
      "tensor([[369],\n",
      "        [165],\n",
      "        [398],\n",
      "        [227],\n",
      "        [577],\n",
      "        [394],\n",
      "        [107],\n",
      "        [275],\n",
      "        [416],\n",
      "        [659],\n",
      "        [  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f24a85de4c0> ([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "valid grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "test grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {seconds}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 128\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 159880 parameters\n",
      "Decoder has a total number of 110844 parameters\n",
      "Total number of all parameters is 270724\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = .001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5, patience=1)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 50.23902487754822s (- 41m 1.712218999862671s) (1 2.0%). train avg loss: 1.3212, val avg loss: 1.1964\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 42.39224052429199s (- 40m 57.41377258300781s) (2 4.0%). train avg loss: 0.5757, val avg loss: 0.969\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 35.34883689880371s (- 40m 33.798444747924805s) (3 6.0%). train avg loss: 0.4936, val avg loss: 0.9908\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 3m 24.38279128074646s (- 39m 10.40209972858429s) (4 8.0%). train avg loss: 0.4247, val avg loss: 0.9358\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 4m 15.23719573020935s (- 38m 17.134761571884155s) (5 10.0%). train avg loss: 0.3874, val avg loss: 0.9005\n",
      "Training for epoch 6 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 6 finished in 5m 5.404600620269775s (- 37m 19.633737881978504s) (6 12.0%). train avg loss: 0.3697, val avg loss: 0.9019\n",
      "Training for epoch 7 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 7 finished in 6m 0.11667490005493164s (- 36m 52.145288671765684s) (7 14.0%). train avg loss: 0.361, val avg loss: 0.9107\n",
      "Training for epoch 8 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 8 finished in 6m 51.63697075843811s (- 36m 1.0940964818000793s) (8 16.0%). train avg loss: 0.3154, val avg loss: 0.7604\n",
      "Training for epoch 9 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 9 finished in 7m 41.78595018386841s (- 35m 3.691550837622799s) (9 18.0%). train avg loss: 0.289, val avg loss: 0.76\n",
      "Training for epoch 10 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 10 finished in 8m 28.556801319122314s (- 33m 54.22720527648926s) (10 20.0%). train avg loss: 0.2779, val avg loss: 0.7986\n",
      "Training for epoch 11 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 11 finished in 9m 17.78034996986389s (- 32m 57.58487716588115s) (11 22.0%). train avg loss: 0.2764, val avg loss: 0.7147\n",
      "Training for epoch 12 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 12 finished in 10m 4.433537721633911s (- 31m 54.039536118507385s) (12 24.0%). train avg loss: 0.2604, val avg loss: 0.8014\n",
      "Training for epoch 13 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 13 finished in 10m 53.06016969680786s (- 30m 58.7097137524529s) (13 26.0%). train avg loss: 0.2544, val avg loss: 0.7368\n",
      "Training for epoch 14 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 14 finished in 11m 49.7767608165741s (- 30m 25.140242099761508s) (14 28.0%). train avg loss: 0.2336, val avg loss: 0.6899\n",
      "Training for epoch 15 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 15 finished in 12m 39.6712703704834s (- 29m 32.56629753112793s) (15 30.0%). train avg loss: 0.2287, val avg loss: 0.7116\n",
      "Training for epoch 16 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 16 finished in 13m 30.10398244857788s (- 28m 41.470962703228s) (16 32.0%). train avg loss: 0.2203, val avg loss: 0.695\n",
      "Training for epoch 17 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 17 finished in 14m 18.593865394592285s (- 27m 46.68220929538484s) (17 34.0%). train avg loss: 0.2141, val avg loss: 0.6715\n",
      "Training for epoch 18 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 18 finished in 15m 6.436452388763428s (- 26m 51.442582024468265s) (18 36.0%). train avg loss: 0.2091, val avg loss: 0.6756\n",
      "Training for epoch 19 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 19 finished in 15m 56.5365674495697s (- 26m 0.6649258387715236s) (19 38.0%). train avg loss: 0.2066, val avg loss: 0.6678\n",
      "Training for epoch 20 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 20 finished in 16m 48.446349143981934s (- 25m 12.6695237159729s) (20 40.0%). train avg loss: 0.2025, val avg loss: 0.6618\n",
      "Training for epoch 21 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 21 finished in 17m 39.40652942657471s (- 24m 22.98996920812715s) (21 42.0%). train avg loss: 0.1998, val avg loss: 0.6826\n",
      "Training for epoch 22 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 22 finished in 18m 27.465354442596436s (- 23m 29.501360199668397s) (22 44.0%). train avg loss: 0.1955, val avg loss: 0.6651\n",
      "Training for epoch 23 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 23 finished in 19m 14.424177169799805s (- 22m 35.193599286286826s) (23 46.0%). train avg loss: 0.1906, val avg loss: 0.6473\n",
      "Training for epoch 24 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 24 finished in 20m 8.925610065460205s (- 21m 49.66941090424871s) (24 48.0%). train avg loss: 0.188, val avg loss: 0.663\n",
      "Training for epoch 25 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 25 finished in 21m 1.418473482131958s (- 21m 1.418473482131958s) (25 50.0%). train avg loss: 0.187, val avg loss: 0.6478\n",
      "Training for epoch 26 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 26 finished in 21m 50.54543089866638s (- 20m 9.734243906461415s) (26 52.0%). train avg loss: 0.1852, val avg loss: 0.6504\n",
      "Training for epoch 27 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 27 finished in 22m 43.71193599700928s (- 19m 21.68053807152637s) (27 54.0%). train avg loss: 0.1823, val avg loss: 0.6486\n",
      "Training for epoch 28 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 28 finished in 23m 34.33823108673096s (- 18m 31.265752996716856s) (28 56.0%). train avg loss: 0.1822, val avg loss: 0.646\n",
      "Training for epoch 29 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 29 finished in 24m 23.57406497001648s (- 17m 39.82949532311568s) (29 58.0%). train avg loss: 0.1847, val avg loss: 0.6453\n",
      "Training for epoch 30 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 30 finished in 25m 15.70606017112732s (- 16m 50.4707067807517s) (30 60.0%). train avg loss: 0.1833, val avg loss: 0.6488\n",
      "Training for epoch 31 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 31 finished in 26m 5.62422513961792s (- 15m 59.57613798879811s) (31 62.0%). train avg loss: 0.1824, val avg loss: 0.65\n",
      "Training for epoch 32 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 32 finished in 26m 56.62306237220764s (- 15m 9.350472584366798s) (32 64.0%). train avg loss: 0.1815, val avg loss: 0.6468\n",
      "Training for epoch 33 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 33 finished in 27m 45.25681924819946s (- 14m 17.859573552102574s) (33 66.0%). train avg loss: 0.181, val avg loss: 0.6498\n",
      "Training for epoch 34 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 34 finished in 28m 30.770435571670532s (- 13m 25.068440269021266s) (34 68.0%). train avg loss: 0.1798, val avg loss: 0.6502\n",
      "Training for epoch 35 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 35 finished in 29m 18.918213844299316s (- 12m 33.822091647557045s) (35 70.0%). train avg loss: 0.1796, val avg loss: 0.6477\n",
      "Training for epoch 36 has started (lr=1.953125e-06). Found 1916 batch(es).\n",
      "Epoch 36 finished in 30m 4.886187553405762s (- 11m 41.900184048546635s) (36 72.0%). train avg loss: 0.1798, val avg loss: 0.6487\n",
      "Training for epoch 37 has started (lr=1.953125e-06). Found 1916 batch(es).\n",
      "Epoch 37 finished in 30m 51.66287422180176s (- 10m 50.5842531049575s) (37 74.0%). train avg loss: 0.1797, val avg loss: 0.6486\n",
      "Training for epoch 38 has started (lr=9.765625e-07). Found 1916 batch(es).\n",
      "Epoch 38 finished in 31m 41.17031645774841s (- 10m 0.36957361823624524s) (38 76.0%). train avg loss: 0.1793, val avg loss: 0.6477\n",
      "Training for epoch 39 has started (lr=9.765625e-07). Found 1916 batch(es).\n",
      "Epoch 39 finished in 32m 27.990089178085327s (- 9m 9.43310207587001s) (39 78.0%). train avg loss: 0.178, val avg loss: 0.6484\n",
      "Early stopping after 39 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 50\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-2, min_value+.075, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 15.009584029652512%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    ' '.join(output_phonemes) if output_phonemes else [\"<EOS>\"],\n",
    "    arpabet_phoneme_sequence if output_phonemes else arpabet_phoneme_sequence+\" <EOS>\"\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> rohmer\n",
      "= ['R', 'AO', 'W', 'M', 'AX', 'R']\n",
      "< R AO W M AX R ['R', 'AO', 'W', 'M', 'AX', 'R']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2301686d60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x560 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
