{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/notebooks/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"128\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [100, 125, 150, 175, 200]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 30, 6, 1]\n",
      "tensor([[18],\n",
      "        [10],\n",
      "        [19],\n",
      "        [12],\n",
      "        [26],\n",
      "        [19],\n",
      "        [ 8],\n",
      "        [14],\n",
      "        [19],\n",
      "        [30],\n",
      "        [ 6],\n",
      "        [ 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f1b98774d00> ([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "valid grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "test grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {seconds}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 128\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 73096 parameters\n",
      "Decoder has a total number of 110844 parameters\n",
      "Total number of all parameters is 183940\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = .001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5, patience=1)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 45.7852623462677s (- 37m 23.47785496711731s) (1 2.0%). train avg loss: 1.0684, val avg loss: 1.1504\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 28.511475324630737s (- 35m 24.275407791137695s) (2 4.0%). train avg loss: 0.5762, val avg loss: 1.1151\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 12.245975732803345s (- 34m 31.853619813919067s) (3 6.0%). train avg loss: 0.5123, val avg loss: 1.0649\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 2m 58.38902473449707s (- 34m 11.473784446716309s) (4 8.0%). train avg loss: 0.4822, val avg loss: 1.0654\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 4m 4.667778253555298s (- 36m 42.01000428199768s) (5 10.0%). train avg loss: 0.452, val avg loss: 0.9667\n",
      "Training for epoch 6 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 6 finished in 5m 3.8831992149353027s (- 37m 8.476794242858887s) (6 12.0%). train avg loss: 0.4226, val avg loss: 0.9371\n",
      "Training for epoch 7 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 7 finished in 6m 2.60290265083313s (- 37m 7.417830569403122s) (7 14.0%). train avg loss: 0.4161, val avg loss: 0.9969\n",
      "Training for epoch 8 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 8 finished in 6m 57.37015986442566s (- 36m 31.19333928823471s) (8 16.0%). train avg loss: 0.4082, val avg loss: 1.0029\n",
      "Training for epoch 9 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 9 finished in 7m 50.64907670021057s (- 35m 44.068016078736946s) (9 18.0%). train avg loss: 0.3559, val avg loss: 0.9046\n",
      "Training for epoch 10 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 10 finished in 8m 44.348777055740356s (- 34m 57.395108222961426s) (10 20.0%). train avg loss: 0.3311, val avg loss: 0.8676\n",
      "Training for epoch 11 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 11 finished in 9m 37.84699034690857s (- 34m 8.730238502676002s) (11 22.0%). train avg loss: 0.3266, val avg loss: 0.8616\n",
      "Training for epoch 12 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 12 finished in 10m 33.88270902633667s (- 33m 27.295245250066273s) (12 24.0%). train avg loss: 0.3099, val avg loss: 0.8447\n",
      "Training for epoch 13 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 13 finished in 11m 56.73037028312683s (- 33m 59.92490003659168s) (13 26.0%). train avg loss: 0.2973, val avg loss: 0.8625\n",
      "Training for epoch 14 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 14 finished in 14m 11.722930908203125s (- 36m 30.144679478236412s) (14 28.0%). train avg loss: 0.3015, val avg loss: 0.8919\n",
      "Training for epoch 15 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 15 finished in 16m 49.188655853271484s (- 39m 14.773530324300282s) (15 30.0%). train avg loss: 0.28, val avg loss: 0.77\n",
      "Training for epoch 16 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 16 finished in 18m 50.61906957626343s (- 40m 2.565522849559784s) (16 32.0%). train avg loss: 0.2768, val avg loss: 0.8217\n",
      "Training for epoch 17 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 17 finished in 20m 48.178998708724976s (- 40m 22.935703375760113s) (17 34.0%). train avg loss: 0.2697, val avg loss: 0.778\n",
      "Training for epoch 18 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 18 finished in 22m 43.12238025665283s (- 40m 23.32867601182761s) (18 36.0%). train avg loss: 0.2596, val avg loss: 0.763\n",
      "Training for epoch 19 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 19 finished in 24m 41.46941113471985s (- 40m 17.134302377700806s) (19 38.0%). train avg loss: 0.252, val avg loss: 0.7868\n",
      "Training for epoch 20 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 20 finished in 26m 47.07435059547424s (- 40m 10.611525893211365s) (20 40.0%). train avg loss: 0.2477, val avg loss: 0.7688\n",
      "Training for epoch 21 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 21 finished in 28m 36.46068239212036s (- 39m 30.350466160547512s) (21 42.0%). train avg loss: 0.2443, val avg loss: 0.7832\n",
      "Training for epoch 22 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 22 finished in 29m 49.566669940948486s (- 37m 57.6303071975708s) (22 44.0%). train avg loss: 0.242, val avg loss: 0.7732\n",
      "Training for epoch 23 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 23 finished in 31m 1.7898516654968262s (- 36m 25.57939108558321s) (23 46.0%). train avg loss: 0.2378, val avg loss: 0.754\n",
      "Training for epoch 24 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 24 finished in 32m 17.708165168762207s (- 34m 59.18384559949254s) (24 48.0%). train avg loss: 0.2401, val avg loss: 0.7799\n",
      "Training for epoch 25 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 25 finished in 34m 12.348181962966919s (- 34m 12.348181962966919s) (25 50.0%). train avg loss: 0.2367, val avg loss: 0.7691\n",
      "Training for epoch 26 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 26 finished in 37m 51.91183948516846s (- 34m 57.14939029400193s) (26 52.0%). train avg loss: 0.238, val avg loss: 0.7555\n",
      "Training for epoch 27 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 27 finished in 39m 54.1653938293457s (- 33m 59.47422437314617s) (27 54.0%). train avg loss: 0.2382, val avg loss: 0.7637\n",
      "Training for epoch 28 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 28 finished in 41m 38.39865851402283s (- 32m 43.0275174038743s) (28 56.0%). train avg loss: 0.2372, val avg loss: 0.7715\n",
      "Training for epoch 29 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 29 finished in 42m 54.996028661727905s (- 31m 4.652296617113279s) (29 58.0%). train avg loss: 0.2346, val avg loss: 0.7686\n",
      "Training for epoch 30 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 30 finished in 43m 47.21273064613342s (- 29m 11.475153764089555s) (30 60.0%). train avg loss: 0.2368, val avg loss: 0.7633\n",
      "Training for epoch 31 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 31 finished in 44m 34.890434980392456s (- 27m 19.448976278305054s) (31 62.0%). train avg loss: 0.2347, val avg loss: 0.7654\n",
      "Training for epoch 32 has started (lr=1.953125e-06). Found 1916 batch(es).\n",
      "Epoch 32 finished in 45m 27.751514196395874s (- 25m 34.36022673547268s) (32 64.0%). train avg loss: 0.2359, val avg loss: 0.7643\n",
      "Training for epoch 33 has started (lr=1.953125e-06). Found 1916 batch(es).\n",
      "Epoch 33 finished in 46m 17.847726345062256s (- 23m 51.01246508685017s) (33 66.0%). train avg loss: 0.2336, val avg loss: 0.7604\n",
      "Early stopping after 33 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 50\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-1.5, min_value+.05, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 24.063789295186105%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    ' '.join(output_phonemes) if output_phonemes else [\"<EOS>\"],\n",
    "    arpabet_phoneme_sequence if output_phonemes else arpabet_phoneme_sequence+\" <EOS>\"\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> publishing's\n",
      "= ['P', 'AA', 'B', 'L', 'IY', 'SH', 'IY', 'NG', 'Z']\n",
      "< P AA B L IY SH IY NG Z ['P', 'AA', 'B', 'L', 'IY', 'SH', 'IY', 'NG', 'Z']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f19f44059a0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
