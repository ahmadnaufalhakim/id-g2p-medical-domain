{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/notebooks/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"32\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [100, 125, 150, 175, 200]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 30, 6, 1]\n",
      "tensor([[18],\n",
      "        [10],\n",
      "        [19],\n",
      "        [12],\n",
      "        [26],\n",
      "        [19],\n",
      "        [ 8],\n",
      "        [14],\n",
      "        [19],\n",
      "        [30],\n",
      "        [ 6],\n",
      "        [ 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f8a14fe5670> ([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "valid grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "test grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {seconds}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 32\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 41224 parameters\n",
      "Decoder has a total number of 78588 parameters\n",
      "Total number of all parameters is 119812\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = .001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5, patience=1)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 42.52773356437683s (- 34m 43.85894465446472s) (1 2.0%). train avg loss: 1.2191, val avg loss: 1.1498\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 24.28378391265869s (- 33m 42.810813903808594s) (2 4.0%). train avg loss: 0.5562, val avg loss: 1.0432\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 6.082469940185547s (- 32m 55.292029062907204s) (3 6.0%). train avg loss: 0.4778, val avg loss: 1.005\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 2m 52.0830135345459s (- 32m 58.95465564727783s) (4 8.0%). train avg loss: 0.418, val avg loss: 0.9124\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 3m 48.72579550743103s (- 34m 18.532159566879272s) (5 10.0%). train avg loss: 0.3901, val avg loss: 0.8989\n",
      "Training for epoch 6 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 6 finished in 4m 41.75452494621277s (- 34m 26.199849605560303s) (6 12.0%). train avg loss: 0.3586, val avg loss: 0.8994\n",
      "Training for epoch 7 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 7 finished in 5m 51.64147090911865s (- 36m 0.08332129887139672s) (7 14.0%). train avg loss: 0.3489, val avg loss: 0.9196\n",
      "Training for epoch 8 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 8 finished in 6m 51.21710729598999s (- 35m 58.88981330394745s) (8 16.0%). train avg loss: 0.3171, val avg loss: 0.8098\n",
      "Training for epoch 9 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 9 finished in 7m 55.328455686569214s (- 36m 5.385187016593136s) (9 18.0%). train avg loss: 0.2838, val avg loss: 0.815\n",
      "Training for epoch 10 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 10 finished in 8m 57.53222179412842s (- 35m 50.12888717651367s) (10 20.0%). train avg loss: 0.2763, val avg loss: 0.7588\n",
      "Training for epoch 11 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 11 finished in 10m 5.060318470001221s (- 35m 45.213856393640526s) (11 22.0%). train avg loss: 0.2759, val avg loss: 0.801\n",
      "Training for epoch 12 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 12 finished in 11m 12.722013235092163s (- 35m 30.286375244458668s) (12 24.0%). train avg loss: 0.2644, val avg loss: 0.7454\n",
      "Training for epoch 13 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 13 finished in 12m 18.24470543861389s (- 35m 1.1580077868238732s) (13 26.0%). train avg loss: 0.2591, val avg loss: 0.7665\n",
      "Training for epoch 14 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 14 finished in 13m 25.359360218048096s (- 34m 30.92406913212335s) (14 28.0%). train avg loss: 0.2492, val avg loss: 0.7331\n",
      "Training for epoch 15 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 15 finished in 14m 16.104552507400513s (- 33m 17.57728918393468s) (15 30.0%). train avg loss: 0.2462, val avg loss: 0.7262\n",
      "Training for epoch 16 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 16 finished in 15m 19.757791996002197s (- 32m 34.48530799150467s) (16 32.0%). train avg loss: 0.243, val avg loss: 0.7314\n",
      "Training for epoch 17 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 17 finished in 16m 21.983355045318604s (- 31m 46.20298332326547s) (17 34.0%). train avg loss: 0.2396, val avg loss: 0.7702\n",
      "Training for epoch 18 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 18 finished in 17m 26.667850494384766s (- 31m 0.7428453233505934s) (18 36.0%). train avg loss: 0.2237, val avg loss: 0.6805\n",
      "Training for epoch 19 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 19 finished in 18m 32.49532437324524s (- 30m 15.12395029318941s) (19 38.0%). train avg loss: 0.2161, val avg loss: 0.6976\n",
      "Training for epoch 20 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 20 finished in 19m 36.87688112258911s (- 29m 25.315321683883667s) (20 40.0%). train avg loss: 0.2152, val avg loss: 0.7083\n",
      "Training for epoch 21 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 21 finished in 20m 41.044004917144775s (- 28m 33.82267345700939s) (21 42.0%). train avg loss: 0.2048, val avg loss: 0.6732\n",
      "Training for epoch 22 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 22 finished in 21m 47.98219275474548s (- 27m 44.70460896058512s) (22 44.0%). train avg loss: 0.2049, val avg loss: 0.6765\n",
      "Training for epoch 23 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 23 finished in 22m 48.90101170539856s (- 26m 46.97075287155485s) (23 46.0%). train avg loss: 0.2014, val avg loss: 0.6615\n",
      "Training for epoch 24 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 24 finished in 23m 56.78887319564819s (- 25m 56.521279295285694s) (24 48.0%). train avg loss: 0.1984, val avg loss: 0.6723\n",
      "Training for epoch 25 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 25 finished in 25m 3.351863145828247s (- 25m 3.351863145828247s) (25 50.0%). train avg loss: 0.1974, val avg loss: 0.6708\n",
      "Training for epoch 26 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 26 finished in 26m 2.7033965587615967s (- 24m 2.495442977318362s) (26 52.0%). train avg loss: 0.1941, val avg loss: 0.6512\n",
      "Training for epoch 27 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 27 finished in 27m 4.0731117725372314s (- 23m 3.469687806235015s) (27 54.0%). train avg loss: 0.1926, val avg loss: 0.6696\n",
      "Training for epoch 28 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 28 finished in 28m 1.3120613098144531s (- 22m 1.0309053148539533s) (28 56.0%). train avg loss: 0.193, val avg loss: 0.6616\n",
      "Training for epoch 29 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 29 finished in 29m 4.946623086929321s (- 21m 3.5820374077766246s) (29 58.0%). train avg loss: 0.1893, val avg loss: 0.6626\n",
      "Training for epoch 30 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 30 finished in 30m 12.864425897598267s (- 20m 8.576283931732178s) (30 60.0%). train avg loss: 0.1916, val avg loss: 0.669\n",
      "Training for epoch 31 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 31 finished in 31m 11.78380560874939s (- 19m 7.222332469878893s) (31 62.0%). train avg loss: 0.1876, val avg loss: 0.6535\n",
      "Training for epoch 32 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 32 finished in 32m 13.735826253890991s (- 18m 7.726402267813683s) (32 64.0%). train avg loss: 0.1902, val avg loss: 0.6628\n",
      "Training for epoch 33 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 33 finished in 33m 2.1063125133514404s (- 17m 1.0850700826354114s) (33 66.0%). train avg loss: 0.1881, val avg loss: 0.6517\n",
      "Training for epoch 34 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 34 finished in 33m 49.94540548324585s (- 15m 55.26842610976246s) (34 68.0%). train avg loss: 0.1874, val avg loss: 0.6595\n",
      "Training for epoch 35 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 35 finished in 34m 33.544607639312744s (- 14m 48.66197470256293s) (35 70.0%). train avg loss: 0.1873, val avg loss: 0.6577\n",
      "Training for epoch 36 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 36 finished in 35m 18.05825161933899s (- 13m 43.689320074187435s) (36 72.0%). train avg loss: 0.1868, val avg loss: 0.6556\n",
      "Early stopping after 36 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 50\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGxCAYAAACwbLZkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdeXiU5dn+8e9kspMFEiSEPewg+75IBVEQMC6ooKgIlSrFDXm1Sq32BflJ39YioAW1iqhFxAXBhYpRVFAUBQmgIDsESAKEJSH7Nr8/7iyEJGQmmcmTkPNzHHPM5MnMM9dE2zm9nnuxORwOByIiIiIW8bK6ABEREanbFEZERETEUgojIiIiYimFEREREbGUwoiIiIhYSmFERERELKUwIiIiIpZSGBERERFLKYyIiIiIpRRGRKRKli5dis1mY/PmzVaXIiK1lMKIiIiIWEphRERERCylMCIiHhcXF8edd95Jo0aN8PPzo1OnTvzzn/8kPz+/xPMWL15M9+7dCQoKIjg4mI4dO/LnP/+56Pfp6ek8+uijREVF4e/vT1hYGH369GH58uXV/ZFExI28rS5ARC5tJ0+eZNCgQWRnZ/PMM8/QqlUrPvnkEx599FH279/PokWLAHjnnXeYNm0aDz74IM899xxeXl7s27ePnTt3Fp1rxowZvPXWW8yZM4eePXuSlpbGL7/8wqlTp6z6eCLiBgojIuJR8+bN49ixY2zatIl+/foBMHLkSPLy8njppZeYPn067du357vvvqN+/fosXLiw6LXDhw8vca7vvvuOESNG8MgjjxQdGzNmTPV8EBHxGF2mERGPWrduHZ07dy4KIoUmTZqEw+Fg3bp1APTr14+zZ89y++23s3r1apKSkkqdq1+/fvz3v//liSee4OuvvyYjI6NaPoOIeJbCiIh41KlTp4iMjCx1vEmTJkW/B7jrrrtYsmQJhw8f5uabb6ZRo0b079+fmJiYotcsXLiQxx9/nFWrVjFs2DDCwsK48cYb2bt3b/V8GBHxCIUREfGo8PBwEhISSh2Pj48HoGHDhkXHJk+ezMaNG0lOTubTTz/F4XBw3XXXcfjwYQDq1avHrFmz+O2330hMTGTx4sX88MMPREdHV8+HERGPUBgREY8aPnw4O3fu5Oeffy5x/M0338RmszFs2LBSr6lXrx6jRo3iySefJDs7m19//bXUcyIiIpg0aRK33347u3fvJj093WOfQUQ8SwNYRcQt1q1bx6FDh0odv++++3jzzTcZM2YMs2fPpmXLlnz66acsWrSIP/7xj7Rv3x6AP/zhDwQEBDB48GAiIyNJTExk7ty5hIaG0rdvXwD69+/PddddR7du3WjQoAG7du3irbfeYuDAgQQGBlbnxxURN7I5HA6H1UWISO21dOlSJk+eXO7vDx48iJeXFzNnzmTt2rWkpKTQunVrpkyZwowZM/DyMg3aN998k6VLl7Jz507OnDlDw4YNueKKK/jLX/5C165dAZg5cyZffPEF+/fvJz09naZNm3LDDTfw5JNPEh4eXi2fV0TcT2FERERELKUxIyIiImIphRERERGxlMKIiIiIWEphRERERCylMCIiIiKWUhgRERERS9WKRc/y8/OJj48nODgYm81mdTkiIiLiBIfDwblz52jSpEnRmkJlqRVhJD4+nubNm1tdhoiIiFTCkSNHaNasWbm/rxVhJDg4GDAfJiQkxOJqRERExBkpKSk0b9686Hu8PLUijBRemgkJCVEYERERqWUqGmKhAawiIiJiKYURERERsZTCiIiIiFiqVowZERER8QSHw0Fubi55eXlWl1Ir2e12vL29q7zshsKIiIjUSdnZ2SQkJJCenm51KbVaYGAgkZGR+Pr6VvocCiMiIlLn5Ofnc/DgQex2O02aNMHX11eLarrI4XCQnZ3NyZMnOXjwIO3atbvowmYXozAiIiJ1TnZ2Nvn5+TRv3pzAwECry6m1AgIC8PHx4fDhw2RnZ+Pv71+p82gAq4iI1FmV/S95KeaOv6H+KYiIiIilFEZERETEUgojIiIidVSrVq2YP3++1WVoAKuIiEhtMnToUHr06OGWEPHTTz9Rr149N1RVNXU6jHyw5Sjbj55ldNdI+rcOt7ocERGRKnM4HOTl5eHtXfFX/GWXXVYNFVWsTl+m+Wr3Cd74/jC/xKdYXYqIiFjM4XCQnp1ryc3hcDhV46RJk/jmm29YsGABNpsNm83G0qVLsdlsrF27lj59+uDn58eGDRvYv38/N9xwAxEREQQFBdG3b1+++OKLEue78DKNzWbj1Vdf5aabbiIwMJB27drx0UcfufXvXJY63RkJCfAB4FxmjsWViIiI1TJy8uj89FpL3nvn7JEE+lb8lbxgwQL27NlDly5dmD17NgC//vorAH/605947rnnaN26NfXr1+fo0aOMHj2aOXPm4O/vzxtvvEF0dDS7d++mRYsW5b7HrFmz+Pvf/84//vEPXnjhBe644w4OHz5MWFiYez5sGep0ZyTY3/yDT8nItbgSERGRioWGhuLr60tgYCCNGzemcePG2O12AGbPns0111xDmzZtCA8Pp3v37tx333107dqVdu3aMWfOHFq3bl1hp2PSpEncfvvttG3blmeffZa0tDR+/PFHj34ulzsj69ev5x//+AdbtmwhISGBDz/8kBtvvLHc569cuZLFixcTGxtLVlYWl19+Of/7v//LyJEjq1S4O4T4qzMiIiJGgI+dnbOt+W4K8LFX+Rx9+vQp8XNaWhqzZs3ik08+IT4+ntzcXDIyMoiLi7voebp161b0uF69egQHB3PixIkq13cxLoeRtLQ0unfvzuTJk7n55psrfP769eu55pprePbZZ6lfvz6vv/460dHRbNq0iZ49e1aqaHcJKeiMnMtUZ0REpK6z2WxOXSqpqS6cFfPYY4+xdu1annvuOdq2bUtAQAC33HIL2dnZFz2Pj49PiZ9tNhv5+flur/d8Lv/VR40axahRo5x+/oVTj5599llWr17Nxx9/bHkYCS7sjGSpMyIiIrWDr68veXl5FT5vw4YNTJo0iZtuugmA1NRUDh065OHqKqfaI2B+fj7nzp276ECYrKwssrKyin5OSfHMbBeNGRERkdqmVatWbNq0iUOHDhEUFFRu16Jt27asXLmS6OhobDYbTz31lMc7HJVV7QNY//nPf5KWlsa4cePKfc7cuXMJDQ0tujVv3twjtQRrzIiIiNQyjz76KHa7nc6dO3PZZZeVOwbk+eefp0GDBgwaNIjo6GhGjhxJr169qrla59gczk5uLuvFNluFA1jPt3z5cqZMmcLq1au5+uqry31eWZ2R5s2bk5ycTEhISGXLLeW3xBSunb+B8Hq+bHnqGredV0REarbMzEwOHjxIVFRUpbe9F+Nif8uUlBRCQ0Mr/P6utss0K1as4J577uG99967aBAB8PPzw8/Pz+M1FXZGUjJzcDgc2Gw2j7+niIiIlFQtl2mWL1/OpEmTePvttxkzZkx1vKVTCseM5OQ5yMqtmdfRRERELnUud0ZSU1PZt29f0c8HDx4kNjaWsLAwWrRowcyZMzl27BhvvvkmYILIxIkTWbBgAQMGDCAxMRGAgIAAQkND3fQxKifI1xubDRwO0x3xd8M8bxEREXGNy52RzZs307Nnz6JpuTNmzKBnz548/fTTACQkJJQYTPPyyy+Tm5vL/fffT2RkZNHt4YcfdtNHqDwvLxtBflprRERExEoud0aGDh160Q19li5dWuLnr7/+2tW3qFYh/j6cy8wlJUMzakRERKxQp/emgeJxI+qMiIiIWKPOh5Hi/WkURkRERKxQ58NI0SqsWvhMRETEEgojRZdpFEZERESsUOfDSEiALtOIiEjd0apVq1Kb2FqtzoeR4s3y1BkRERGxgsKIBrCKiIhYqs6HkUacwptcUhRGRETqNocDstOsuTm5Z+3LL79M06ZNyc8vuYXJ9ddfz913383+/fu54YYbiIiIICgoiL59+/LFF1944q/lVtW2UV6NtPJebtr+Lp94/Q/nMhtZXY2IiFgpJx2ebWLNe/85HnzrVfi0W2+9lYceeoivvvqK4cOHA3DmzBnWrl3Lxx9/TGpqKqNHj2bOnDn4+/vzxhtvEB0dze7du2nRooWnP0Wl1e3OSGA4NhzcbF+vzoiIiNR4YWFhXHvttbz99ttFx9577z3CwsIYPnw43bt357777qNr1660a9eOOXPm0Lp1az766CMLq65Y3e6MdL8dfljE1V4/82LGaaurERERK/kEmg6FVe/tpDvuuIN7772XRYsW4efnx7Jly7jtttuw2+2kpaUxa9YsPvnkE+Lj48nNzSUjI6PEnnE1Ud0OI5HdyArvjN+pnVyRuR64weqKRETEKjabU5dKrBYdHU1+fj6ffvopffv2ZcOGDcybNw+Axx57jLVr1/Lcc8/Rtm1bAgICuOWWW8jOzra46our22EEyO4yHr9v/sqY/K9wOBzYbDarSxIRESlXQEAAY8eOZdmyZezbt4/27dvTu3dvADZs2MCkSZO46aabAEhNTeXQoUMWVuucuj1mBLB3H0euw4seXvtJT9hldTkiIiIVuuOOO/j0009ZsmQJd955Z9Hxtm3bsnLlSmJjY9m2bRsTJkwoNfOmJqrzYSSgQSTfOHoA4Nj6dgXPFhERsd5VV11FWFgYu3fvZsKECUXHn3/+eRo0aMCgQYOIjo5m5MiR9OrVy8JKnVPnL9PYbDY+sw9luONn/Ha+B6NmgZfd6rJERETKZbfbiY8vPdi2VatWrFu3rsSx+++/v8TPNfGyTZ3vjAD87D+Qs456+KQlwsFvrC5HRESkTlEYAfwDAvg4b6D5IXa5tcWIiIjUMQojQIi/Dx/k/c78sOtjyEyxtiAREZE6RGEEs3NvrKMNyfWiIDcDdq62uiQREZE6Q2GEwp17bexsNMYc2KZLNSIidYHDyQ3qpHzu+BsqjGA6IwBb648AbHD4Ozh90NqiRETEY3x8fABIT0+3uJLar/BvWPg3rYw6P7UXICTA/AHjHWHQeigc+Aq2vQPDZlpal4iIeIbdbqd+/fqcOHECgMDAQK3A7SKHw0F6ejonTpygfv362O2VXxZDYQQIKeiMpGTkQo8JBWFkOVz5OHipeSQicilq3LgxQFEgkcqpX79+0d+yshRGKL5Mcy4zBzpeB77BcPYwxH0PrQZbXJ2IiHiCzWYjMjKSRo0akZOTY3U5tZKPj0+VOiKFFEYwU3sBzmXmgm8gXH4DbP0PbHtbYURE5BJnt9vd8oUqladrEBTOpoGUzIJk3L1gnf9fV0O2BjeJiIh4ksII51+myTUHWgyE+i0h+xz89omFlYmIiFz6FEYoI4x4eZmBrACx2slXRETEkxRGKJ7am5qVS15+weIt3W8z9we+huRj1hQmIiJSByiMUNwZAUgt7I40aAUtBwMO2L7CkrpERETqAoURwM/bjq+3+VMUDWIF6H67ud+2HLRksIiIiEcojBQoMb23UOcbwDsAkvbAsZ8tqkxEROTSpjBSoGgV1vM7I/4h0CnaPN6mgawiIiKeoDBSoNSMmkI9Ci7V7HgfcrOquSoREZFLn8JIgeCiyzQXLAkcdSUEN4HMs7D7vxZUJiIicmlTGCkQElBOZ8TLDt3Hm8fblldzVSIiIpc+hZECwX4FS8JnlLFZUuHy8HtjIFW7O4qIiLiTwkiBojEjWbmlf3lZe2jaBxx5sOO9aq5MRETk0qYwUqBwFdZSY0YKFQ5kjdWlGhEREXdSGClQ2BlJySijMwJw+Viw+8LxHZC4oxorExERubQpjBQonE2TUl5nJDAMOowyj9UdERERcRuFkQLlrjNyvsKBrDvehbxyQouIiIi4RGGkQEh564ycr+1wqHcZpJ2EL2drvxoRERE3UBgpUDRm5GKdEbsPXPWUebxxIXz2hAKJiIhIFSmMFHCqMwLQ+24YM8883vQSfDId8vM9XJ2IiMilS2GkQOEKrJk5+WTnVhAu+t4DNywCmxdsWQqr/gh5F+moiIiISLkURgoE+XkXPa6wOwLQ8w4Y+2+w2WH7O/DBPRrUKiIiUgkKIwW87V4E+tqBCmbUnK/rLTDuTfDygZ2r4N2J2tlXRETERQoj53Fqeu+FOl0Hty8Hb3/YvQaW3w7Z6R6qUERE5NKjMHIepwexXqjdNTDhXfAJhP1fwtvjICvVAxWKiIhcehRGzlM8vbcSYz9aXwl3fQi+wXBoA7x1E2Qmu7lCERGRS4/CyHmKl4Sv5MyYFgPg7tXgXx+O/ghvXA/pp91YoYiIyKVHYeQ8xTv3VmGabtPeMOkTCAyHhFhYeh2knnRThSIiIpcel8PI+vXriY6OpkmTJthsNlatWlXha7755ht69+6Nv78/rVu35qWXXqpUsZ5WvHNvFafoNu4Kk9ZAUASc+BWWjoaUBDdUKCIiculxOYykpaXRvXt3XnzxRaeef/DgQUaPHs2QIUPYunUrf/7zn3nooYf44IMPXC7W0yo1m6Y8jTrC5P9CSDNI2mMCSWZK1c8rIiJyifGu+CkljRo1ilGjRjn9/JdeeokWLVowf/58ADp16sTmzZt57rnnuPnmm119e4+q9Gya8oS3gclr4PXRcPoAbP0PDJzmnnOLiIhcIjw+ZuT7779nxIgRJY6NHDmSzZs3k5NT9pd+VlYWKSkpJW7VIcSdnZFCDVrC7/7HPP7xFe1jIyIicgGPh5HExEQiIiJKHIuIiCA3N5ekpKQyXzN37lxCQ0OLbs2bN/d0mcD5s2ncvKx7t/HgHwpnDsK+GPeeW0REpJarltk0NputxM8Oh6PM44VmzpxJcnJy0e3IkSMerxHcPGbkfL71oOdd5vGml917bhERkVrO42GkcePGJCYmljh24sQJvL29CQ8PL/M1fn5+hISElLhVh+KpvR7Y8K7vFMBmVmhN2uv+84uIiNRSHg8jAwcOJCam5KWJzz//nD59+uDj4+Ppt3dJ8Qqsbu6MAIRFQftrzeMfX3H/+UVERGopl8NIamoqsbGxxMbGAmbqbmxsLHFxcYC5xDJx4sSi50+dOpXDhw8zY8YMdu3axZIlS3jttdd49NFH3fQR3Cf4vNk0hZeS3Kr/feY+9m1N8xURESngchjZvHkzPXv2pGfPngDMmDGDnj178vTTTwOQkJBQFEwAoqKiWLNmDV9//TU9evTgmWeeYeHChTVuWi8Ud0Zy8hxk5Xpg1kvrodCwA2Snwrbl7j+/iIhILWRzeKQF4F4pKSmEhoaSnJzs0fEj+fkO2jy5BocDfnxyOI2C/d3/Jj+9Cp/+D4S1gQc2g5dW5BcRkUuTs9/f+iY8j5eXjSC/wiXhPTBuBKDbbeAXCqf3w/51nnkPERGRWkRh5AJuX4X1Qn5B0PMO8/hHTfMVERFRGLmAx9YaOV/hNN+9n8Op/Z57HxERkVpAYeQCIZ5ahfV84W2gXcES+T/+23PvIyIiUgsojFygWjojAP3vNfdb/wNZ5zz7XiIiIjWYwsgFisOIBzsjAK2vgvB2kH0Otr3j2fcSERGpwRRGLlC8JLyHOyNeXtCvoDui3XxFRKQOUxi5QNGS8Bke7owA9LgdfIMhaQ8c+Mrz7yciIlIDKYxcoHhJeA93RgD8gs+b5qv9akREpG5SGLlA8WyaaggjAH3/YO73rIXTB6rnPUVERGoQhZELFO/cWw2XaQAatoW21wAO+PHV6nlPERGRGkRh5ALVNrX3fIW7+W79D2SlVt/7ioiI1AAKIxcI9vRy8GVpM9xsnJeVDNtXVN/7ioiI1AAKIxcIDbCgM3LhNN+av5GyiIiI2yiMXOD8zoijOkNBjwngGwQnf4OD31Tf+4qIiFhMYeQChWNG8h2Qlp1XfW/sH2ICCcAm7eYrIiJ1h8LIBQJ87Hh72YBqHjcCxZdqdv8Xzhyq3vcWERGxiMLIBWw223mrsFbjuBGAhu2gzVWAA37SNF8REakbFEbKYMmMmkL9p5r7n9+E7LTqf/+LST4KH0+HORHw5TNWVyMiIpcIhZEyWLLWSKG210CDKMhMhu3vVv/7lyUlAdY8Bgt7wpbXITcTNr1U88KSiIjUSgojZSheEt6CzoiXF/QrWCL+uwVmmfjc7OqvAyD1BHz2Z1jYw0w5zsuGVkMgpBlkp5qxLSIiIlWkMFKG4iXhLeiMAPS4A/zrw5mD8PY4eK4drL4f9n0JedVQU9opiHkaFnSHH/5lOiHNB8DdH8OkT4o399v2judrERGRS5631QXURJaOGQEIqA9TvjCDWH9dBamJZqn4rf+BwHDodD10GQstB4OX3X3vm3EGNr5YcAmmYFn6pr1h2J/NKrE2M8uIruPgm/+D/etM9ySokftqEBGROkdhpAwhVqzCeqGG7WDU/8HIZyHue/hlJexcDelJZtzGltchKAI63wCXj4Xm/c0lnsrITIYfFsP3/4KsFHOscTcY9iS0H1kcQopqa2tCyrEtpq4BU6v2WUVEpE5TGClDYWckJcOizsj5vOzQ6gpzG/V3OLTeBIBdH0PqcTOW48dXIKQpdL4RIruDIw/y8867z7/g5/OOZyabjkvmWfN+jTqbTkjH60qHkPN1G2/CyPYVCiMiIlIlCiNlCLFyNs3F2L3NOiRtroIx8+DA1/DrSvjtU0g5ZsZ3VFbD9jD0Ceh8k3Mdli43w2czIf5nSNprOjkiIiKVoDBShuKpvTWgM1Ieb19oP8LccjJh/5ew8yPTLfGyg81ecO9lbiWO2U3gsNnByxtaDDRjUFwZf1KvIbS9GvauNd2Rq/7iuc8qIiKXNIWRMoQUDWCtYZ2R8vj4Q8cx5laduo0rDiPDnrz4ZR0REZFyaGpvGYKtXGekNukwGnyD4WwcHNlkdTUiIlJLKYyUwdIVWGsT30DofL15rDVHRESkkhRGyhASUMsu01ip2zhz/+uHkJtlbS0iIlIrKYyUobAzkpqVS16+w+JqarhWQyA40kwN3htjdTUiIlILKYyUoTCMAKSqO3JxXnboeot5vH2FtbWIiEitpDBSBj9vO77e5k+jQaxO6Habud/zmVlSXkRExAUKI+WoddN7rdS4CzS63Ozqu3O11dWIiEgtozBSjpCinXvVGXFK4UDW7e9aW4eIiNQ6CiPl0PReF3W9BbDB4e/MuiMiIiJOUhgpR/H0XnVGnBLazGzmB7DjPWtrERGRWkVhpByFnZEasXNvbdG9YCDrthXg0JRoERFxjsJIOYL9NIDVZZ2iwdsfknZDwjarqxERkVpCYaQcRWNGshRGnOYfCh1GmccayCoiIk5SGCmHxoxUUrfx5v6X9yFPQU5ERCqmMFKO4jEj+kJ1SdurISAMUo/Dwa+trkZERGoBhZFyBBcseqZ1Rlxk94EuN5vHulQjIiJOUBgpR4jWGam8wks1uz6GrFRraxERkRpPYaQc6oxUQbM+ENYactJh9xqrqxERkRpOYaQcWoG1Cmy24u7ItnesrUVERGo8hZFyFG+Up85IpXS91dwf+ArOHbe2FhERqdEURsoREmA6I5k5+WTn5ltcTS0U3gaa9QVHPvzygdXViIhIDaYwUo4gP++ix+qOVFLhpZrtK6ytQ0REajSFkXJ4270I9LUDGjdSaZePBS9vSIiFk7utrkZERGoohZGLKB43ojBSKfXCoe015nFluiP5ujwmIlIXeFf8lLor2N+bxBRN762SbuNgz39h+3sw7C/gdV7+zc+DlGNw5lDJ2+mD5j4rBW58CbrdaknpIiJSPRRGLqJ4eq/CSKV1GAW+wZAcB5//BXIzikPH2SOQX8Hf9pPp0Ky3WbdEREQuSZW6TLNo0SKioqLw9/end+/ebNiw4aLPX7ZsGd27dycwMJDIyEgmT57MqVOnKlVwdSpe+EyXaSrNJwA632Ae//Av2LwE9q+D0wdMEPHygfC2Zk+bvlNgxBwY/x+4bwO0GATZqbDyPm26JyJyCXO5M7JixQqmT5/OokWLGDx4MC+//DKjRo1i586dtGjRotTzv/32WyZOnMjzzz9PdHQ0x44dY+rUqUyZMoUPP/zQLR/CU4p37tUXYZX87lFIPwW+9aBBKwiLMvcNWkFwJHjZy37dTS/BS1fA0R/h23lw5Z+qsWgREakuLndG5s2bxz333MOUKVPo1KkT8+fPp3nz5ixevLjM5//www+0atWKhx56iKioKK644gruu+8+Nm/eXOXiPa14515dpqmSsCiY8A7c8hoMfwp63gmtroDQZuUHEYAGLWH0c+bx13+Do1vcX9up/fDbp+BwuP/cIiLiFJfCSHZ2Nlu2bGHEiBEljo8YMYKNGzeW+ZpBgwZx9OhR1qxZg8Ph4Pjx47z//vuMGTOm3PfJysoiJSWlxM0KWhK+Bug2zuwC7MiDlVPcu/FefCy8MhTemQDf/8t95xUREZe4FEaSkpLIy8sjIiKixPGIiAgSExPLfM2gQYNYtmwZ48ePx9fXl8aNG1O/fn1eeOGFct9n7ty5hIaGFt2aN2/uSpluoyXhawCbDcb8E0KamnEma//snvOe3A3/GWtm7AB8OQsStrvn3CIi4pJKDWC12WwlfnY4HKWOFdq5cycPPfQQTz/9NFu2bOGzzz7j4MGDTJ06tdzzz5w5k+Tk5KLbkSNHKlNmlYUUXqZRGLFWQAMzfgQb/PyGuaxSFWcOw5s3mnEskT2g3UjIy4YPpkB2ultKFhER57k0gLVhw4bY7fZSXZATJ06U6pYUmjt3LoMHD+axxx4DoFu3btSrV48hQ4YwZ84cIiMjS73Gz88PPz8/V0rziGAtelZzRP0OBj0IGxfCRw9C0z4QXPa/cxd1LhHevAHOxcNlHeHOleb44oGQtBtinjKdGBERqTYudUZ8fX3p3bs3MTExJY7HxMQwaNCgMl+Tnp6Ol1fJt7HbzaBFRw0fNKgxIzXMVX+BiK6mo7F6muuDTtNPw1s3wZmDUL8l3PWhWSW2XnhB5wX46VXY/V/31y4iIuVy+TLNjBkzePXVV1myZAm7du3ikUceIS4uruiyy8yZM5k4cWLR86Ojo1m5ciWLFy/mwIEDfPfddzz00EP069ePJk2auO+TeEDx1F5dpqkRvP3g5n+Dtz/s+8IEB2dlnYNlt8CJnRDUGCauhpDz/oMmL6cAACAASURBVP1rcxUMfMA8Xn0/nDvu3tpFRKRcLoeR8ePHM3/+fGbPnk2PHj1Yv349a9asoWXLlgAkJCQQFxdX9PxJkyYxb948XnzxRbp06cKtt95Khw4dWLlypfs+hYcUTe1VZ6TmaNQJrpltHn/+F+c24MvJhOW3w7EtZvzJxFVmuvGFhj9d3HlZ9UftjSMiUk1sjpp+rQRISUkhNDSU5ORkQkJCqu19j53NYPDf1uFjt7FnzqhyB+lKNcvPN12O/V9C424w5Uvw9i37uXk58O5E2L3GLEt/92po2rv8c5/4DV65EnIz4dq/wYA/euYziIjUAc5+f2vX3osonE2Tk+cgK1f/lVxjeHnBjYsgIAwSt8NX/6/s5+Xnw6ppJoh4+5uF1y4WRAAadTRL0gPEPA2Jv7i3dhERKUVh5CLq+XpT2AzRKqw1THBjuH6hefzdAjj0bcnfOxyw5lHY8S54ecO4N82qr87oOwXaX1s83Tcnw72113aZybB1mbkXEXEDhZGL8PKyEeSncSM1Vqdo6HkX4DCb6WWcLf7dl7Nh82uADW56GdqPdP68Nhtc/yLUawQnd0HMX6te67njkJZU9fNYLe0ULB1jZjN95qYF6ESkzlMYqYBWYa3hrv0bNIiClKOmEwLw7fNmYz2A656Hrre4ft6gy+DGgv2WfnwZ9nxeufrOHYdP/wee7wwLusOetZU7T02QehLeiIbEHebnHe9q1pGIuIXLu/bWNVprpIbzC4Kx/4YlI2HHe5CfB78WzNS6Zjb0mVz5c7e7GvpPhU0vmU7AHzdCUCPnXpuZDN8thB8WQU7Bqq7ZqbD8NjMmZcA0qE0Dos8dhzevh5O/QVCEGa9zcpfpPg1Th8QdkpOTSU+vOysABwYGEhoaanUZUkMojFSgsDOiJeFrsOZ94co/wddzi4PIkEdh8MNVP/fVs+DgerM+yer7YcK7Fw8RORnw479NZybjjDnWtI9ZsG3nKtiy1Oyvk7TH7Ehs96l6jZ6WkmA6Iqf2QnATuPtjM3D4/cnw02twxQzw8be6ylotOTmZF198kZycuvP/Mz4+PjzwwAMKJAIojFRInZFaYsijsO9LOPoj9LvXfPm7g48/3Pya2d137+cmaPS/t/Tz8nIhdhl8/Tez1DxAww5m7ZKOY0yAaT0UGraHtU+aUHL6gBlYG9DAPbV6QvJRE0ROH4DQ5nD3RxDWGhq0Mj8nHzGXa3pNrPBUUr709HRycnIYO3Ysl112mdXleNzJkydZuXIl6enpCiMCKIxUSKuw1hJ2b/Nf7Md/haa93HsJJKIzjHgG/vsns9Ba1BCz+BqYWTu7PoIvnzGdA4CQZubSRffbwMtefB6bDQbeD2Ft4IN7TMfl1atNtyW8jfvqdZezcbD0Ojh7GOq3gLs/gQZmcUPs3ib0xTwF3y8yA4lr02WnGuqyyy4rc78ukUudBrBWoGgV1gx1Rmo8H39o1tszX4r97oW2V0NeVsF030w48DX8e5hZVO3UXjOOYuRceHAL9LyjZBA5X4dr4fdrTWfh1D7491VwcIP7a66K0wfh9TEmiDSIgklrioNIoV4TwaeeGTty4Ctr6hSRS4LCSAWKL9OoM1Kn2Wxmdk1gQzj+C7zY1+z+G7/VfCFf+Tg8vA0GTnNu/ETjLvCHddCsL2SehbduhC1veP5zOOPUfjN9NzkOwtvC5DVQv3np5wXUN6ELTHdERKSSFEYqEFw0tVedkTovqJFZ+RXMF7WXj5lt8/A2c1nG38WtCoIamUsfXW6B/Fz4+CEzniQ/z/21OytpL7w+GlKOmTEvkz4tuaHghfpPBWywLwZO7qm2MkXk0qIwUoHi2TQKI4JZPO2656H/H+HBzTDq/8yaJJXl4w83vwpDC6bHfv8ivHMHZKW6p15XnPjNBJHURGjUGSZ9Yla6vZjwNtBhlHm8abHnaxSRS5LCSAWKd+7VZRop0Of3MOpvZkaJO9hsMPRxuGWJ2UNnz39hybVw9oh7zu+MxF/MpZm0E2bn4rs/dn5NlQHTzH3sckg/7bkahUWLFhEVFYW/vz+9e/dmw4aLjzXKysriySefpGXLlvj5+dGmTRuWLFlS9PulS5dis9lK3TIzM4ues379eqKjo2nSpAk2m41Vq1aVeI+cnBwef/xxunbtSr169WjSpAkTJ04kPj7evR9eLmkKIxXQ1F6pNl1uNpdF6jWC4zvMwNZfV5lQ4snNtRO2m+m76UkQ2d1M363X0PnXt7oCGneF3AzY8rrn6qzjVqxYwfTp03nyySfZunUrQ4YMYdSoUcTFxZX7mnHjxvHll1/y2muvsXv3bpYvX07Hjh1LPCckJISEhIQSN3//4nFPaWlpdO/enRdffLHM90hPT+fnn3/mqaee4ueff2blypXs2bOH66+/3j0fXOoETe2tgKb2SrVq1scMbF1+mxko+97d5rhfqJlOHNHZXEKJuNzcB9R3/tw5GWa67plDJW+HvoWsFGjSC+5a6fq6JzYbDLgfVk0167AMfBC8fV07h1Ro3rx53HPPPUyZMgWA+fPns3btWhYvXszcuXNLPf+zzz7jm2++4cCBA4SFhQHQqlWrUs+z2Ww0blz+5bhRo0YxatSocn8fGhpKTExMiWMvvPAC/fr1Iy4ujhYtWjjz8aSOUxipQEjR1F6FEakm9ZvD7z8za5cc+haSdkNWMhz5wdzOF9K0IJx0hkaXm8CSk3Fe2DhY/PhcQvnv2awf3Pk++FdyAaouN8MXfzXvsXMVdBtXufNImbKzs9myZQtPPPFEieMjRoxg48aNZb7mo48+ok+fPvz973/nrbfeol69elx//fU888wzBAQEFD0vNTWVli1bkpeXR48ePXjmmWfo2bNnlepNTk7GZrNRv74LYVnqNIWRChTOpknNysXhcGDTwk5SHfyCYfTfzePcbLOOyfGdcOLXgvudZvXTlGPmti/m4ucr5BsMYa3MeJfCW1hraDWkakvTe/tC3z/AV3Pg+39B11u1CJobJSUlkZeXR0RERInjERERJCYmlvmaAwcO8O233+Lv78+HH35IUlIS06ZN4/Tp00XjRjp27MjSpUvp2rUrKSkpLFiwgMGDB7Nt2zbatWtXqVozMzN54oknmDBhAiEhLs4wkzpLYaQChWNG8h2Qlp1HkJ/+ZFLNvH3NZZmIy4Fbi49nJsOJXWbV2RM7zf3J3eAbZBYoKxE4osziZQENPBcS+kyG9f+AhFiI+x5aDvLM+9RhF/7H0MX+Ayk/Px+bzcayZcuKllyfN28et9xyC//6178ICAhgwIABDBgwoOg1gwcPplevXrzwwgssXLjQ5fpycnK47bbbyM/PZ9EirT0jztM3awUCfOx4e9nIzXdwLjNHYURqDv9QaDHA3GqCeg2h+3j4+U2zW7HCiNs0bNgQu91eqgty4sSJUt2SQpGRkTRt2rTE3i+dOnXC4XBw9OjRMjsfXl5e9O3bl71797pcY05ODuPGjePgwYOsW7dOXRFxiWbTVMBms2lJeBFnFU7z/e1TM05F3MLX15fevXuXGigaExPDoEFlh77BgwcTHx9PamrxmjV79uzBy8uLZs2alfkah8NBbGysy/vjFAaRvXv38sUXXxAeHu7S60UURpxQvAqrBrGKXFSjTtDmKnDkw6aXra7mkjJjxgxeffVVlixZwq5du3jkkUeIi4tj6tSpAMycOZOJE4t3T54wYQLh4eFMnjyZnTt3sn79eh577DF+//vfFw1gnTVrFmvXruXAgQPExsZyzz33EBsbW3ROMANcY2NjiY2NBeDgwYPExsYWTSnOzc3llltuYfPmzSxbtoy8vDwSExNJTEwkOzu7uv48UsvpmoMTQgK01oiI0wbcD/vXwc9vwdCZri+TX1kOhxlDk5cFTao2G6QmGj9+PKdOnWL27NkkJCTQpUsX1qxZQ8uWZgPDhISEEmuOBAUFERMTw4MPPkifPn0IDw9n3LhxzJkzp+g5Z8+e5d577yUxMZHQ0FB69uzJ+vXr6devX9FzNm/ezLBhw4p+njFjBgB33303S5cu5ejRo3z00UcA9OjRo0TNX331FUOHDnX730IuPQojTgj2K1wSXp0RkQq1HW72tUnaDVvfgoH3e+69Uk+Y3ZP3r4P9X5ml7AGG/QV+9+glN6Nn2rRpTJs2rczfLV26tNSxjh07lrq0c77nn3+e559//qLvOXToUBwXWXSvVatWF/29iDMURpxQvCS8OiMiFbLZYMAf4ZPpsOkls5mel909587JNDN19q+DA19B4o6Sv7f7mc7IV3Mg5SiM/ifY9X9zIjWd/lfqBI0ZEXFR99vgy9lmxdffPoHON1TuPA6Hmba8f525Hd4IuZkln9O4mxmn0uYqaN7fdGPWPAZblsK5RLPnj2+9Kn8kEfEchREnFI4Z0WwaESf5BJgNBTc8B98vcj2MnNpvlpb/dSWkHi/5u+Am0GaYCR9RV5beNbnfH8xuwx9MgT2fwRvXw4QVru23IyLVSmHECeqMiFRC3ynw3QKzhP2xLdC098Wf73CY8R+bXoI9a4GCcQg+gWYzvtYFAeSyDhWPBekUDRM/guXj4dhmeO0auPMDs9qsiNQ4CiNOCNHOvSKuC4mELmNh+wr4YTHc/GrZz8tOh+3vmKnAJ38rPt5uhFlivvWV4O3n+vu36A/3xMB/xsLpA/DaCJjwLjTtVbnPIyIeo3VGnBCizohI5RQugvbrh5ASX/J3Z+Mg5mmY1wk+ecQEEd8g6HcfPLAF7ngP2o+oXBAp1LAd3POFGVeSdhKWXgd7ndzHR0SqjTojTtBsGpFKatIDWg6Gw9/Bj6/A8L+a2TA/LDYDWx355nkNWpkQ0vOOyu8cXJ7gCJi8Bt6daAbBvj0eohdAr7vc+z5ucPLkSatLqBZ15XOK8xRGnKAxIyJVMGCaCSObl8C+LyFxe/Hvoq40U3/bj3Tf9N+y+AXD7Svg44dg23L46AHTqbnyTzViLZLAwEB8fHxYuXKl1aVUGx8fHwIDA60uQ2oIhREnBGvMiEjldRhlOh9nDpkg4u0P3cabEBLRufrq8PaFGxdDSBPY8E/4+llIOQZj5lm+FkloaCgPPPAA6enpltZRnQIDA0ts4id1m8KIE0ICClZgzVBnRMRlXnaz+NiG58yg1N6TIDDMmlpsNhj+tAkkax6Dn98wU4dvWQLeAZBxBtKTzPiStKSCx6fMz+lJBcdOmVtIU+h+O3S9xS2fJzQ0VF/OUmcpjDihsDOSlp1HXr4Du5f1bV2RWqXd1eZWU/SdAkGN4YN7zFok/2gHuRnFY1ickXoc4n+Gz5803Z8ed0Cb4e7psmSmwKFvzeWt/DyzbotPYMF9gFnE7cJjhY/9QqwLeyKVpDDihMIwApCamUtooI+F1YiIW3S6rmAtktsg43Txcf/6ZoG0wIbm/vzHhfcBDeDIJtj6H3PpaedqcwuKMJeget5p1kNxVn4exG8t3mPn6I+QX4XLwi2vMONhon7n/jExORlm3E3cDxDezmxK2KSHtYvKORyQfNTsh5S01wyC7hRtxgpJrWBz1IIdjlJSUggNDSU5OZmQkGraAfQCHf7yX7Jy89nwp2E0D9OgK5FLRnaaGc8SGG5udhf/YyNxB8S+bdZTST9VfLxpb9Mt6XIzBNQv/bozh4v32DnwDWSeLfn7BlHQeqgJPjkZkJNWcJ8BOekX3F9wrFCLgSaUtB5W9VCScQZ+etWsB5NWxmyY0OYmlDTpCZEF9+7u0OTlwOmDJnSc3A1Jewru95q/z/l8g8w6N73uNv8sasBA5brI2e9vhREn9ZnzBUmpWXz60BVc3kTXdUXkArnZsPdziF1mVpB15Jnjdj/Thek+AfKyi/fZOb2/5Ov9QqH178wqs62HQVhU5epIPgbfzYctb5hNAwGa9YUrH4e2V5f8Us7Lgw0bICEBIiNhyBCwXzCrKfmomYq9ZSlkp5pjoc2h662QfMR0dE7tK7uW+i2LA0qTnhDe1nR8crNNbbnZ5m9S9DgLcrMKjmWbY6nHC8LHHrN4XX45Y/e8vCGsjVlb5sSukn/fRp2h10TTtapqQMrNNt2wYz+bhf2irgR/a76XagOFETe76rmvOZCUxjv3DmBA63BLahCRWiL1BGx/1wSTEzvLfo7NbkJC4SZ/TXq6d1ZPSgJsXGimVBduLtiklwkl7UfChx/Cww/D0aPFr2nWDBYsgLFjzRf6dwthx7vFl4waXQ5XTIfLbyrZQcpMhoTtJpjEb4WEWBMcPMGnngkcl3WAhu0L7juY8FZYk8NhNlX8+U3Yuar489t9zeWbXhOh1e/Ay4l1PzOT4chPZluDuB/g6GYzvqiQlzc06wdth5uw17ibc+etSNY58zdN2g1ePmackG9Qwf2Fj+t5dmp8FSiMuNkNL37LtqPJ/HtiH67pHGFJDSJSyzgckLDNhJKdq80XSOEmf62ucP8Cb2U5d7w4lBRewkloAv/+rWj7nyI2G+CAhwdB6C/Fx1sNgcHTzReus5c7Ms6az35+QDl7xKyoa/cxHSNvPxMQyrz3M9Ox/UNN2LisvbkPaeral33GWdjxnpk5lbij+Hj9lmbhux53mNlVhZKPmYX54grCx/FfKPWHCmgATfvAmYOlu0L1LjMDmdsON/+cnRlLk5li/lYJsQV/s9iC87rw9ewdUDKo+AWZMTO+Bfd+IWUcO+/mG2TGPPm6dxiCwoib3fXaJjbsTeKft3bn5t7NLKlBRKTSUk/C9y/CD6/Ac8ch5SL/1x9ig4eD4fLrTQhpVsEmh7VFfKzplux4D7JSzDGbl5ly7hcMcZsgOa706xpEQYsBBbeBZuBuYSA6fRD2fwn71sHBb4ovZZmTm8tUba82t6Z9zO8TthWHj/jY0pfsCoU0hYjLzXmy08xrs9POu51zbQZYRW56Gbrf5r7z4fz3t2bTOKl44TOtNSIitVDQZXDNLMjtBSk3Xvy5KQ7ovQhuuKN6aqsuTXqY24g5plP185sQt9FM7y5ks0PjriZ0FAaQ4MblnzMsCsKmmOniudlmltW+L8xqw8d3FHeG1v/DdC/Ov8RzvtDmENnd1BfZ0zwOuuzin8fhMGNsSgWVc5CVai71ZKea4FX4c9Gxc6V/tnD2kcKIk4L9CpeE1yqsIlKLnXVyldf0S3gfVd9A6HG7uSXtNZ0Sm5cJHk37mMsZleHtC1FDzO2aWWbczv51pnOyf52ZkQRQv0XBjKMeJnRE9oR6lRiLaLOBj7+5Veb1NYjCiJNCAgo6I1kKIyJSi0VGuvd5tV3DdjDsz545d0ik2fyx5x1mLZmkPWZchhalK+USjr7uVbhZnpaEF5FabcgQM2umvIGoNhs0b26eJ+7jZYdGnRREyqEw4iRtlicilwS73UzfhdKBpPDn+fNLrzci4kEKI04q6oxoAKuI1HZjx8L770PTpiWPN2tmjo8da01dUmdpzIiTQgo6IynqjIjIpWDsWLjhhopXYBWpBgojTirsjGhqr4hcMux2GDrU6ipEdJnGWRozIiIi4hkKI04KDVBnRERExBMURpxU2BnJzMknO9eNy++KiIjUcQojTgryKx5eo+6IiIiI+yiMOMnb7kWgrxllrnEjIiIi7qMw4oIQrTUiIiLidpUKI4sWLSIqKgp/f3969+7Nhg0bLvr8rKwsnnzySVq2bImfnx9t2rRhyZIllSrYSppRIyIi4n4urzOyYsUKpk+fzqJFixg8eDAvv/wyo0aNYufOnbRo0aLM14wbN47jx4/z2muv0bZtW06cOEFubu37Qi8OI+qMiIiIuIvLYWTevHncc889TJkyBYD58+ezdu1aFi9ezNy5c0s9/7PPPuObb77hwIEDhIWZDYJatWpVtaotEhJQeJmm9gUpERGRmsqlyzTZ2dls2bKFESNGlDg+YsQINm7cWOZrPvroI/r06cPf//53mjZtSvv27Xn00UfJyMgo932ysrJISUkpcasJtHOviIiI+7nUGUlKSiIvL4+IiIgSxyMiIkhMTCzzNQcOHODbb7/F39+fDz/8kKSkJKZNm8bp06fLHTcyd+5cZs2a5Upp1UJjRkRERNyvUgNYbRdsO+1wOEodK5Sfn4/NZmPZsmX069eP0aNHM2/ePJYuXVpud2TmzJkkJycX3Y4cOVKZMt1OYURERMT9XOqMNGzYELvdXqoLcuLEiVLdkkKRkZE0bdqU0NDQomOdOnXC4XBw9OhR2rVrV+o1fn5++Pn5uVJatdDUXhEREfdzqTPi6+tL7969iYmJKXE8JiaGQYMGlfmawYMHEx8fT2pqatGxPXv24OXlRbNmzSpRsnVCNJtGRETE7Vy+TDNjxgxeffVVlixZwq5du3jkkUeIi4tj6tSpgLnEMnHixKLnT5gwgfDwcCZPnszOnTtZv349jz32GL///e8JCAhw3yepBoUDWHWZRkRExH1cnto7fvx4Tp06xezZs0lISKBLly6sWbOGli1bApCQkEBcXFzR84OCgoiJieHBBx+kT58+hIeHM27cOObMmeO+T1FNQgI0ZkRERMTdbA6Hw2F1ERVJSUkhNDSU5ORkQkJCLKvjp0OnufWl72kZHsg3jw2zrA4REZHawNnvb+1N4wLNphEREXE/hREXFI8ZyaEWNJRERERqBYURFxTOpsnJc5CZk29xNSIiIpcGhREX1PP1pnBtN03vFRERcQ+FERd4edkI8jPdEW2WJyIi4h4KIy4KOW/ciIiIiFSdwoiLCmfUqDMiIiLiHgojLmrWwKwau27XcYsrERERuTQojLjo94OjAFj+4xESksvedVhEREScpzDiooFtwukXFUZ2Xj6LvtpvdTkiIiK1nsKIi2w2G49c3R6AFT8d4dhZdUdERESqQmGkEga2CWdAa9Md+ddX+6wuR0REpFZTGKmkwu7Ie5uPcPRMusXViIiI1F4KI5XUv3U4g9uGk5PnUHdERESkChRGqqC4O3KUI6fVHREREakMhZEq6NMqjCHtGpKb7+CFdXutLkdERKRWUhipoukF3ZEPfj7G4VNpFlcjIiJS+yiMVFHvlg24sv1l5OU7eGGdxo6IiIi4SmHEDR65xnRHPtx6jENJ6o6IiIi4QmHEDXo0r8+wDqY7slBjR0RERFyiMOImhd2RVVuPsf9kqsXViIiI1B4KI27SrVl9ru7UiHwHvPCluiMiIiLOUhhxo8KZNR9ti2ffCXVHREREnKEw4kZdmoZyTecI8h2wUN0RERERpyiMuNn0q9sB8PH2ePYcP2dxNSIiIjWfwoibXd4klGsvb4zDAQvUHREREamQwogHPFzQHVmzI4HdieqOiIiIXIzCiAd0igxhdNfC7sgeq8sRERGp0RRGPOTh4e2x2WDNjkR2xqdYXY6IiEiNpTDiIR0aBzOmaySg7oiIiMjFKIx40MPD22Gzwdpfj/NrfLLV5YiIiNRICiMe1C4imOhuTQCY/4Vm1oiIiJRFYcTDHhreDi8bxOw8zqYDp6wuR0REpMZRGPGwto2CGN+3BQB/+mA7Gdl5FlckIiJSsyiMVIOZozsSGerP4VPp/GPtbqvLERERqVEURqpBiL8Pz47tCsDrGw+y+dBpiysSERGpORRGqsmwDo24pXczHA740/vbyczR5RoRERFQGKlWT43pTKNgPw4kpTEvRmuPiIiIgMJItQoN9OHZm8zlmlc3HGBr3BmLKxIREbGewkg1u7pzBDf1bEq+Ax7T5RoRERGFESv8NbozDYP82HcilQVfajE0ERGp2xRGLFA/0Jf/d1MXAF5Zf4DtR89aXJGIiIh1FEYsMvLyxkR3b0JevoPH3ttOVq4u14iISN2kMGKhWddfTng9X3YfP8e/1u2zuhwRERFLKIxYKKyeL8/caC7X/Ovr/fxyTDv7iohI3aMwYrHRXSMZ3bWxuVzz/nayc/OtLklERKRaKYzUALNv6EKDQB92JaSw+Ov9VpcjIiJSrRRGaoCGQX7MusFcrnlh3V52JaRYXJGIiEj1URipIaK7RTKicwS5+Q4ee38bOXm6XCMiInWDwkgNYbPZmHNTF0IDfPjlWAqvrD9gdUkiIiLVQmGkBmkU7M//Xt8ZgAVf7GXP8XMWVyQiIuJ5CiM1zI09mjK8YyOy8/J5ZEUsJ85lWl2SiIiIRymM1DA2m41nx3YlNMCHX+NTuHb+Bj7/NdHqskRERDxGYaQGigjx5/2pA+kUGcLptGzufWsLT3ywnbSsXKtLExERcbtKhZFFixYRFRWFv78/vXv3ZsOGDU697rvvvsPb25sePXpU5m3rlHYRway6fxD3Xdkamw3e+ekIYxZuYGvcGatLExERcSuXw8iKFSuYPn06Tz75JFu3bmXIkCGMGjWKuLi4i74uOTmZiRMnMnz48EoXW9f4eduZOaoTb08ZQJNQfw6dSueWl75n/hd7yNXUXxERuUTYHA6Hw5UX9O/fn169erF48eKiY506deLGG29k7ty55b7utttuo127dtjtdlatWkVsbKzT75mSkkJoaCjJycmEhIS4Uu4lIzkjh6dX/8Lq2HgAejSvz/zxPWjVsJ7FlYmIiJTN2e9vlzoj2dnZbNmyhREjRpQ4PmLECDZu3Fju615//XX279/PX//6V6feJysri5SUlBK3ui40wIcFt/VkwW09CPb3JvbIWUYv3MCKn+JwMU+KiIjUKC6FkaSkJPLy8oiIiChxPCIigsTEsmd87N27lyeeeIJly5bh7e3t1PvMnTuX0NDQolvz5s1dKfOSdkOPpnw2/Xf0jwojPTuPxz/YwX1vbeF0WrbVpYmIiFRKpQaw2my2Ej87HI5SxwDy8vKYMGECs2bNon379k6ff+bMmSQnJxfdjhw5UpkyL1lN6wfw9h8GMHNUR3zsNj7feZyR89fz1e4TVpcmIiLiMudaFQUaNmyI3W4v1QU5ceJEqW4JwLlz59i8eTNbt27lgQceACA/Px+Hw4G3tzeff/45V111d6lJbwAAGSdJREFUVanX+fn54efn50ppdY7dy8Z9V7bhinYNmf5OLHtPpDL59Z+YNKgVfx7dCV9vzdoWEZHawaVvLF9fX3r37k1MTEyJ4zExMQwaNKjU80NCQtixYwexsbFFt6lTp9KhQwdiY2Pp379/1aoXLm8SyscPXsGkQa0AWLrxEBP+/YNWbhURkVrDpc4IwIwZM7jrrrvo06cPAwcO5JVXXiEuLo6pU6cC5hLLsWPHePPNN/Hy8qJLly4lXt+oUSP8/f1LHZfK8/ex87/XX84VbRvyyIpYNh8+Q/QL37L4zt70atHA6vJEREQuyuVe/vjx45k/fz6zZ8+mR48erF+/njVr1tCyZUsAEhISKlxzRDzj6s4RrH5gMG0bBXE8JYvxL3/P8h/1z0JERGo2l9cZsYLWGXFNalYuj767jc8K9rS5vV8L/vf6zvh52y2uTERE6hKPrDMitUOQnzeL7+zFYyM7YLPB8h/juO2VHzieonEkIiJS8yiMXKJsNhv3D2vL65P6EuLvzda4s1z3wrdsPnTa6tJERERKUBi5xA3t0IiPH7yCjo2DOXkui9te+YG3vj+kVVtFRKTGUBipA1qG12PltEGM6RZJbr6Dp1b/yp/e305mTp7VpYmIiCiM1BWBvt68eHtPZo7qiJcN3ttylPEvf0/82QyrSxMRkTpOYaQOsdnMqq1v/L4f9QN92HY0megXvuX9LUdJTNbgVhERsYam9tZRR06nc99bW9iZULwjcsvwQPpHhdE/Kpz+rcNo1iDQwgpFRKS2c/b7W2GkDsvIzmPR1/v4evdJfo1PJv+CfxOa1g+gf+swBhSEkxZhgWVuiCgiIlIWhRFxSUpmDlsOneGHg6fYdOA0O44lk3dBOmkc4k+/qDD6tw7juq5NCA30sahaERGpDRRGpErSsnLZcvgMmwrCybajZ8nJK/5XpUmoPy/f1YeuzUItrFJERGoyhRFxq4zsPLbGneGHg6dZtfUYcafT8fP24m83d+Wmns2sLk9ERGoghRHxmJTMHKa/E8u6304AcM8VUcwc1RFvuyZniYhIMe1NIx4T4u/DqxP78OBVbQF47duDTFzyI6fTsi2uTEREaiOFEakULy8b/zOiAy/d2YtAXzsb958i+oVv+TU+2erSRESkllEYkSq5tkskH04bTMvwQI6dzeDmxRv5aFu81WWJiEgtojAiVdahcTAf3X8FV7a/jMycfB5avpW5a3aVmhosIiJSFoURcYvQQB+WTOrLH4e2AeDl9QeY9PqPnE3XOBIREbk4hRFxG7uXjcev7ciLE3oS4GNnw94krn/xO35LTKn4xSIiUmcpjIjbXdetCSunDaJ5WABxp9MZu2gja3YkWF2WiMj/b+/uY6M4DzSAPzOzn/bu2l5/b4yNwzeGcAECGEJCaeKWRj0obUqvOuocTU+oUF1E+kdKL43bq+ocTXpNj0JA6EijpJRLS0ik0iqWQkgDhQMCDYEEQiGx4w+Mje39snd2Z977Y9drL9jYBK/H3n1+0mg+PX7fmVf2o5l3ZmiMYhihpJhR7MLrG+7F0il5CKoavvvyu3j2jfPQ2Y+EiIiuwzBCSZOTacHuR+7Bv953JwDgv9+8iI173kW3qhlcMiIiGksYRiipTIqMzV+agWcengOzIuHAmRas2flXXPH2GF00IiIaIxhGaFR8bV4JXn50EXIyzHjv0y6s3HoY7zfyBWlERMQwQqNoQbkbr224F5MLHGjx9uDh5/+KP7/Pjq1EROmOYYRGVWluBvZ9dzHum5qP7rCG9S+9i21vXcQ4+F4jERElCcMIjTqXzYz/qZ6PRxZPBABs+fN5PP7K3xCKsGMrEVE6YhghQ5gUGTX/WIH/WFkBRZaw791G/POuY2j3h4wuGhERjTKGETLU2sqJeOFf7oHTZsLxjzuwatthXLjiM7pYREQ0ihhGyHBLp+THv/zbcK0bX912BG+dbzW6WERENEoYRmhMmFzgwP7vLsGCcjd8oQjWvXAcuw9f5pd/iYjSgCTGwWMMXq8XWVlZ6OrqgsvlMro4lERqRMe/7z+D/z3xKQDAblYwvdiJmcUuVHiyMNPjwrRCJ+wWxeCSEhHRUIb7/9s0imUiGpLFJOM/v3oXphY68V91FxBQNZyq78Sp+s74NrIETMp3YKbHhQqPCzOLoyHFnWkxsORERPRZ8coIjVmaLnC5LYBzzV6ca/LGxl1o86sDbl+cZcPkAgcsigxZlqBIEhRZik0DsiTFl8uyBEUGFElClt2Me6fkY25pNkwK71wSEY2U4f7/ZhihcUUIgau+EM7Gw4kXZ5u68HF78Lb37bKZcN/UfCyfXoD7p+Yj12EdgRITEaUvhhFKK/5QBB82e/FxexCarkPTAU0I6LqApgvoIjpoOmLj2DJd4JNrQbx94So6guH4/iQJmFOSjeXTC7B8egEqPC5IkmRgDYmIxh+GEaJboOkCpxs6cfDDVrz5YSvONXsT1hc4rVg2LXrV5N4p+XBY2d2KiGgoDCNEt6GlqwdvnY8Gk3cutiGo9r2q3qxImH1HFibmZaLUnZEw5DutvIJCRBTDMEI0QkIRDccvd+DND1tx8HwrLrcFBt3WZpYxIScaTCb0Dyq5GSjLzYDVxEeSiSh9MIwQJcnltgDeb+xC/bUgGq4FUR8bmjq7cbN3tFlMMuaWZmPRnblYdGcu/mFCNmxmhhMiSl0MI0SjLKzpaOrsjoeT/mHlk/YgfD2RhO0ZTogo1TGMEI0hQghcagvg6KV2HL10DUcvteOqL/ELxQwnRJRqGEaIxjAhoi906w0mfx0knEzMzUBOhiU6ZJrj09kZZrgzLcjOsMCdaUFOhhkumxmyzM6zRDR2MIwQjSPXh5Ojl9rRel04GYosAdkZ0WDizrTEh5wMS8J8/8FuVvj0DxElDcMI0TgmhMDH7UE0dnSjI6iiM6jiWiCMjqAaG8KxZSo6g2H4Q5GhdzoAq0lGbqYFuQ4rch0W5GZakeewxKdzHRbkxda5My18GoiIbgk/lEc0jkmShPK8TJTnZQ5r+1BEQ1cwjGuxgNIRiE37o+GlPaCiI9A3vhZQoWo6QhEdTV09aOrqGdbvcdpMyHNYke+04sEZhfjqvBJ+oJCIbhuvjBClISEEAqoWDyjXAiG0+VW0+1W0+0NoD6ho84ei84HoODLAc8sWRcaK2UX45oJSLCh385YPESXgbRoiGjFCCHi7I2iLBZPzV3zYe7we7zf2vTZ/Un4m/mlBKb42rwTZGbxaQkQMI0Q0Ct77tBO/PVaP1//WFH9lvsUk46HZxfjmwlLML8vh1RKiNMYwQkSjxtcTxv7TTfjtsXp80O8jg1MKHPjmwlKsvrsEWRlmA0tIREZgGCGiUSdE9OvHe/4verWkJ6wDiD6186XZxVhQ7kaFx4WphU6+0I0oDTCMEJGhvD1h7D/ViN8eq8eHLb6EdSZZwuQCByo8WajwuFDhcWGmxwWnjVdPiFIJwwgRjQlCCLxb34k3zrbgbJMXZ5u60BEMD7htWW5GLJxkYabHhcn5DtgtCsyKDIsiw6RIMMkS+6EQjRMMI0Q0Jgkh0NzVg/cbu2LhxItzTV3DftcJEH2k2KxIMJvkhKBiVmS4My1YdGculkzKxd2lObCY5CTWhohuhmGEiMaVjoAav3LSO2641g1V0z/zPu1mBQvK3VgyORdLJudhRpGL3+8hGkVJDSPbtm3Dz3/+czQ3N6OiogK//OUvsXTp0gG33bdvH7Zv347Tp08jFAqhoqICNTU1+MIXvjDilSGi1COEQEQXCGs6whEBVdOj07FBjQhE9L7pT9oDOPz3dhy52Ib2gJqwL3emBZWTcrFkUh6WTM5FqTuDt3yIkihpYWTv3r1Yu3Yttm3bhiVLlmDHjh3YtWsXzp07h9LS0hu2f+yxx+DxePC5z30O2dnZ2L17N5555hkcO3YMd99994hWhoiol64LnL/iw+GLbTjy93Ycu9SOQOxdKL1KcuxYMikP04qckCRACKD3D+L1fxp7Z0VsC7vFhHxH9Ls+eY7oN30cVhPDDVE/SQsjCxcuxNy5c7F9+/b4shkzZmDVqlWora0d1j4qKiqwZs0a/OhHPxrW9gwjRHS7wpqOvzV04vDFdhy+2IZTDR0IayN7l9pqkuPBJC8WUno/NpjntCI304Ls2FeVczIsfLyZUl5SPpSnqipOnjyJJ554ImF5VVUVjhw5Mqx96LoOn88Ht9s96DahUAihUN/n071e76DbEhENh1mRMX+iG/MnuvFvD0xBUI3g/y5fw5G/t6OpsxuSJKH3mkbvxY2+eSlhHgD8oUj0+z0BFW2+EAKqhlBER2NnNxo7u4dVJrtZgbtfQMnOsMCdYY6OY8vtZgVmkxzrtBvruKvIsJj65uPrYtux0y6NN7cURtra2qBpGgoLCxOWFxYWoqWlZVj7ePbZZxEIBPD1r3990G1qa2vx4x//+FaKRkR0SzIsJiybVoBl0wpGZH/dqoY2fwhX/SG0+fpCSps/+hHCq/4QOgIqOoJhdARVaLpAd1i7pfAyXBmWaMjJzYzeRnJnWpDriM67M63x6VxH9GoNr9CQ0W4pjPS6/p6oEGJY90n37NmDmpoavPbaaygoGPwPwA9+8ANs2rQpPu/1ejFhwoTPUlQiolFhtyiY4M7ABHfGkNsKIeALRfrCSUBFR1DFtYCKzmAY14IqOmPzoYge77wb1nSomo6I1jcd7cgroPX7qnJQ1RBUu/Fpx/Cv0GRaFdjMCuxmBXZLv+nr5y0y7GYFFpMMXQCaLqDrIjototO9Y10IaDpiYwGTIiHPYUW+MzoUxMa5mVYot/mUkxACoYgOX08EiiwhJ8PM/jvjyC2Fkby8PCiKcsNVkNbW1huullxv7969+Pa3v41XXnkFDzzwwE23tVqtsFqtt1I0IqJxQ5IkuGxmuGxmlOWOzD612BNHobCOjqCK9kA0zLTHbiXdOB0dq5qO7rCG7rA29C9JElkC3Jl94aR/ULGbFfh6IvD1hOHticSnfT0R+EKxcWxZ/z5AFkWO7sdlRaHTFh27bMh3RscFsfH1oUXTRfR3dUfQ1R2GtyccHXeHE+Z9PRGYZBkOqwKHzQSH1ZwwnWlV4LSa4bCZ4tM2s8yANIhbCiMWiwXz5s1DXV0dvvKVr8SX19XVYeXKlYP+3J49e7Bu3Trs2bMHDz300GcvLRERDUiRJShy9ApGVoYZE/Myh/yZ3is0nYEwguEIutVoKOkJa+hW+0JKT2x5d1hDtxpdH4rokCRAkSQosgRZlvqmJQmKDMhS33JZlhDWdLT5Qmj1hXDVF72l1e4PQReI3c4KAc23dxx6n4pSteH13zErEgqcNgCAtzsMXyhyewW4CUWWYFYkmOS+twmbZBmKLCXMx6eV6DoJ/fsxSX3TUnQe/dfH+j7JEvqdi4HPT8J6ScLX5pVg1h1ZSav/zdzybZpNmzZh7dq1mD9/PiorK7Fz507U19dj/fr1AKK3WBobG/Hiiy8CiAaRb33rW3juueewaNGi+FUVu92OrCxjKk1ERIlXaIwS0XRcC6q42j+k9BtCEQ1OmxlOmyk2mOGwRqdd8eV96zMtJoR1Pb6/Vm8Irb4etHpDuOLtQasvOr4a69cT1sSAgSXDoiDLHj02WXYzXPbo73PZY4PNhIguEAhFr8z4QxEEQtFxwnxPBH41AhG7pRW9nfbZX+SXTHPLcsZPGFmzZg3a29vxk5/8BM3NzZg1axYOHDiAsrIyAEBzczPq6+vj2+/YsQORSAQbNmzAhg0b4surq6vxwgsv3H4NiIho3DIpMgqcNhQ4bagYoX1aZQUlORkoybl5/x01oqPNHw0nkiTFwocJLrsZZmXknkjSY52V/aEI1IgOTY++qC+iC0Q0ERv3n9f7lut6v3fcJL7/pv+7b4Tom9dFdF6L9dXp7bOjDdCPp//6KQWOEavzreLr4ImIiCgphvv/mw+jExERkaEYRoiIiMhQDCNERERkKIYRIiIiMhTDCBERERmKYYSIiIgMxTBCREREhmIYISIiIkMxjBAREZGhGEaIiIjIUAwjREREZCiGESIiIjIUwwgREREZymR0AYaj98PCXq/X4JIQERHRcPX+3+79Pz6YcRFGfD4fAGDChAkGl4SIiIhulc/nQ1ZW1qDrJTFUXBkDdF1HU1MTnE4nJEkasf16vV5MmDABDQ0NcLlcI7bf8STdj0G61x/gMWD907v+AI9BMusvhIDP54PH44EsD94zZFxcGZFlGSUlJUnbv8vlSssG2F+6H4N0rz/AY8D6p3f9AR6DZNX/ZldEerEDKxERERmKYYSIiIgMpdTU1NQYXQgjKYqCZcuWwWQaF3eskiLdj0G61x/gMWD907v+AI+B0fUfFx1YiYiIKHXxNg0REREZimGEiIiIDMUwQkRERIZiGCEiIiJDMYwQERGRodI6jGzbtg3l5eWw2WyYN28e/vKXvxhdpFFRU1MDSZIShqKiIqOLlVRvv/02vvzlL8Pj8UCSJOzfvz9hvRACNTU18Hg8sNvtWLZsGc6ePWtQaUfeUPV/5JFHbmgTixYtMqi0I6+2thb33HMPnE4nCgoKsGrVKpw/fz5hm1RvA8M5BqncDrZv34677ror/pbRyspK/OlPf4qvT/XzP1T9jT73aRtG9u7di8ceeww//OEPcerUKSxduhQrVqxAfX290UUbFRUVFWhubo4PZ86cMbpISRUIBDBnzhxs3bp1wPVbtmzBL37xC2zduhXHjx9HUVERHnzwwfhHGse7oeoPAF/84hcT2sSBAwdGsYTJdejQIWzYsAFHjx5FXV0dIpEIqqqqEAgE4tukehsYzjEAUrcdlJSU4Omnn8aJEydw4sQJLF++HCtXrowHjlQ//0PVHzD43Is0tWDBArF+/fqEZdOnTxdPPPGEQSUaPU899ZSYM2eO0cUwDADx6quvxud1XRdFRUXi6aefji/r6ekRWVlZ4vnnnzeiiEl1ff2FEKK6ulqsXLnSoBKNvtbWVgFAHDp0SAiRfm1AiBuPgRDp1w5ycnLErl270vL8C9FXfyGMP/dpeWVEVVWcPHkSVVVVCcurqqpw5MgRg0o1uj766CN4PB6Ul5fjG9/4Bi5dumR0kQxz+fJltLS0JLQHq9WK+++/P23aAwC89dZbKCgowNSpU/Gd73wHra2tRhcpabq6ugAAbrcbQHq2geuPQa90aAeapuF3v/sdAoEAKisr0+78X1//Xkae+7R8721bWxs0TUNhYWHC8sLCQrS0tBhUqtGzcOFCvPjii5g6dSquXLmCn/70p1i8eDHOnj2L3Nxco4s36nrP+UDt4ZNPPjGiSKNuxYoVePjhh1FWVobLly/jySefxPLly3Hy5ElYrVajizeihBDYtGkT7r33XsyaNQtA+rWBgY4BkPrt4MyZM6isrERPTw8cDgdeffVVzJw5Mx44Uv38D1Z/wPhzn5ZhpJckSQnzQogblqWiFStWxKdnz56NyspKTJo0Cb/5zW+wadMmA0tmrHRtDwCwZs2a+PSsWbMwf/58lJWV4Y9//CNWr15tYMlG3saNG/Hee+/hnXfeuWFdurSBwY5BqreDadOm4fTp0+js7MQf/vAHVFdX49ChQ/H1qX7+B6v/zJkzDT/3aXmbJi8vD4qi3HAVpLW19YZknA4yMzMxe/ZsfPTRR0YXxRC9TxKxPfQpLi5GWVlZyrWJ733ve3j99ddx8OBBlJSUxJenUxsY7BgMJNXagcViweTJkzF//nzU1tZizpw5eO6559Lm/A9W/4GM9rlPyzBisVgwb9481NXVJSyvq6vD4sWLDSqVcUKhED744AMUFxcbXRRDlJeXo6ioKKE9qKqKQ4cOpWV7AID29nY0NDSkTJsQQmDjxo3Yt28f3nzzTZSXlyesT4c2MNQxGEiqtYPrCSEQCoXS4vwPpLf+Axntc6/U1NTUjMpvGmNcLheefPJJ3HHHHbDZbPjZz36GgwcPYvfu3cjOzja6eEn1/e9/H1arFUIIXLhwARs3bsSFCxewY8eOlK273+/HuXPn0NLSgh07dmDhwoWw2+1QVRXZ2dnQNA21tbWYNm0aNE3D448/jsbGRuzcuTMl7pXfrP6KomDz5s1wOp3QNA2nT5/Go48+inA4jK1bt6ZE/Tds2ICXX34Zv//97+HxeOD3++H3+6EoCsxmMyRJSvk2MNQx8Pv9Kd0ONm/eDIvFAiEEGhoa8Ktf/QovvfQStmzZgkmTJqX8+b9Z/QsLC40/94Y8wzNG/PrXvxZlZWXCYrGIuXPnJjzilsrWrFkjiouLhdlsFh6PR6xevVqcPXvW6GIl1cGDBwWAG4bq6mohRPTRzqeeekoUFRUJq9Uq7rvvPnHmzBljCz2Cblb/YDAoqqqqRH5+vjCbzaK0tFRUV1eL+vp6o4s9YgaqOwCxe/fu+Dap3gaGOgap3g7WrVsX/3ufn58vPv/5z4s33ngjvj7Vz//N6j8Wzr0khBDJjzxEREREA0vLPiNEREQ0djCMEBERkaEYRoiIiMhQDCNERERkKIYRIiIiMhTDCBERERmKYYSIiIgMxTBCREREhmIYISIiIkMxjBAREZGhGEaIiIjIUP8P0ugPtectb44AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-2, min_value+.06, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 17.79436983152355%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    ' '.join(output_phonemes) if output_phonemes else [\"<EOS>\"],\n",
    "    arpabet_phoneme_sequence if output_phonemes else arpabet_phoneme_sequence+\" <EOS>\"\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> swooning\n",
      "= ['S', 'W', 'UW', 'N', 'IY', 'NG']\n",
      "< S W UW N IY NG ['S', 'W', 'UW', 'N', 'IY', 'NG']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8880ec1520>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 548.571x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
