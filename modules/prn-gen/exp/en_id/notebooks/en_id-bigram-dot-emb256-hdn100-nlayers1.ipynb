{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/notebooks/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"256\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [100, 125, 150, 175, 200]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369, 165, 398, 227, 577, 394, 107, 275, 416, 659, 1]\n",
      "tensor([[369],\n",
      "        [165],\n",
      "        [398],\n",
      "        [227],\n",
      "        [577],\n",
      "        [394],\n",
      "        [107],\n",
      "        [275],\n",
      "        [416],\n",
      "        [659],\n",
      "        [  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f127475b370> ([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "valid grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "test grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {seconds}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 256\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 289160 parameters\n",
      "Decoder has a total number of 153852 parameters\n",
      "Total number of all parameters is 443012\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = .001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5, patience=1)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 55.29543113708496s (- 45m 9.476125717163086s) (1 2.0%). train avg loss: 1.4642, val avg loss: 1.5335\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 50.08874535560608s (- 44m 2.1298885345458984s) (2 4.0%). train avg loss: 0.7524, val avg loss: 1.1841\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 46.60159921646118s (- 43m 30.091721057891846s) (3 6.0%). train avg loss: 0.5781, val avg loss: 1.1467\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 3m 45.10029339790344s (- 43m 8.653374075889587s) (4 8.0%). train avg loss: 0.5348, val avg loss: 1.088\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 4m 42.30509686470032s (- 42m 20.745871782302856s) (5 10.0%). train avg loss: 0.5066, val avg loss: 1.0566\n",
      "Training for epoch 6 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 6 finished in 5m 41.27066373825073s (- 41m 42.65153408050537s) (6 12.0%). train avg loss: 0.4528, val avg loss: 0.9838\n",
      "Training for epoch 7 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 7 finished in 6m 46.343459606170654s (- 41m 36.109823295048045s) (7 14.0%). train avg loss: 0.4426, val avg loss: 0.9961\n",
      "Training for epoch 8 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 8 finished in 7m 45.41952848434448s (- 40m 43.45252454280853s) (8 16.0%). train avg loss: 0.4596, val avg loss: 1.065\n",
      "Training for epoch 9 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 9 finished in 8m 43.84388446807861s (- 39m 46.39991813235838s) (9 18.0%). train avg loss: 0.3967, val avg loss: 0.8849\n",
      "Training for epoch 10 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 10 finished in 9m 42.974427461624146s (- 38m 51.89770984649658s) (10 20.0%). train avg loss: 0.3568, val avg loss: 0.8743\n",
      "Training for epoch 11 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 11 finished in 10m 41.401679277420044s (- 37m 54.060499256307594s) (11 22.0%). train avg loss: 0.3412, val avg loss: 0.8362\n",
      "Training for epoch 12 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 12 finished in 11m 39.77815365791321s (- 36m 55.964153250058644s) (12 24.0%). train avg loss: 0.3366, val avg loss: 0.8591\n",
      "Training for epoch 13 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 13 finished in 12m 36.85167145729065s (- 35m 54.11629568613489s) (13 26.0%). train avg loss: 0.3229, val avg loss: 0.815\n",
      "Training for epoch 14 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 14 finished in 13m 35.15345358848572s (- 34m 56.10888065610607s) (14 28.0%). train avg loss: 0.3113, val avg loss: 0.812\n",
      "Training for epoch 15 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 15 finished in 14m 34.34140396118164s (- 34m 0.12994257609079796s) (15 30.0%). train avg loss: 0.2957, val avg loss: 0.7751\n",
      "Training for epoch 16 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 16 finished in 15m 32.6752552986145s (- 33m 1.9349175095558167s) (16 32.0%). train avg loss: 0.2892, val avg loss: 0.8048\n",
      "Training for epoch 17 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 17 finished in 16m 30.82870316505432s (- 32m 3.373364967458201s) (17 34.0%). train avg loss: 0.2899, val avg loss: 0.809\n",
      "Training for epoch 18 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 18 finished in 17m 28.44682002067566s (- 31m 3.905457814534657s) (18 36.0%). train avg loss: 0.2638, val avg loss: 0.7793\n",
      "Training for epoch 19 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 19 finished in 18m 25.543567895889282s (- 30m 3.7816107775033743s) (19 38.0%). train avg loss: 0.2503, val avg loss: 0.7638\n",
      "Training for epoch 20 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 20 finished in 19m 22.356613159179688s (- 29m 3.5349197387695312s) (20 40.0%). train avg loss: 0.2423, val avg loss: 0.7628\n",
      "Training for epoch 21 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 21 finished in 20m 22.956515550613403s (- 28m 8.84471195084734s) (21 42.0%). train avg loss: 0.2377, val avg loss: 0.7895\n",
      "Training for epoch 22 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 22 finished in 21m 19.857635736465454s (- 27m 8.909718210046776s) (22 44.0%). train avg loss: 0.2302, val avg loss: 0.7315\n",
      "Training for epoch 23 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 23 finished in 22m 18.986558437347412s (- 26m 11.853785991668701s) (23 46.0%). train avg loss: 0.2319, val avg loss: 0.7292\n",
      "Training for epoch 24 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 24 finished in 23m 17.559807777404785s (- 25m 14.02312509218882s) (24 48.0%). train avg loss: 0.2357, val avg loss: 0.7225\n",
      "Training for epoch 25 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 25 finished in 24m 18.748712062835693s (- 24m 18.748712062835693s) (25 50.0%). train avg loss: 0.2271, val avg loss: 0.718\n",
      "Training for epoch 26 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 26 finished in 25m 20.450332403182983s (- 23m 23.49261452601513s) (26 52.0%). train avg loss: 0.2194, val avg loss: 0.7456\n",
      "Training for epoch 27 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 27 finished in 26m 21.138496160507202s (- 22m 26.895755988579822s) (27 54.0%). train avg loss: 0.2215, val avg loss: 0.7252\n",
      "Training for epoch 28 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 28 finished in 27m 12.778942346572876s (- 21m 22.897740415164208s) (28 56.0%). train avg loss: 0.207, val avg loss: 0.7335\n",
      "Training for epoch 29 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 29 finished in 28m 1.743661642074585s (- 20m 17.8143756718473s) (29 58.0%). train avg loss: 0.2, val avg loss: 0.7087\n",
      "Training for epoch 30 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 30 finished in 28m 52.38179969787598s (- 19m 14.921199798583984s) (30 60.0%). train avg loss: 0.1989, val avg loss: 0.7444\n",
      "Training for epoch 31 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 31 finished in 29m 43.62122392654419s (- 18m 13.187201761430515s) (31 62.0%). train avg loss: 0.199, val avg loss: 0.7146\n",
      "Training for epoch 32 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 32 finished in 30m 37.402425050735474s (- 17m 13.538864091038704s) (32 64.0%). train avg loss: 0.1914, val avg loss: 0.6943\n",
      "Training for epoch 33 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 33 finished in 31m 21.90656805038452s (- 16m 9.46701990474321s) (33 66.0%). train avg loss: 0.1886, val avg loss: 0.6949\n",
      "Training for epoch 34 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 34 finished in 32m 11.022639989852905s (- 15m 8.716536465813078s) (34 68.0%). train avg loss: 0.1913, val avg loss: 0.6875\n",
      "Training for epoch 35 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 35 finished in 33m 0.39804935455322266s (- 14m 8.742021151951576s) (35 70.0%). train avg loss: 0.1906, val avg loss: 0.6973\n",
      "Training for epoch 36 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 36 finished in 33m 51.62127614021301s (- 13m 10.074940721194253s) (36 72.0%). train avg loss: 0.187, val avg loss: 0.6907\n",
      "Training for epoch 37 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 37 finished in 34m 41.65082859992981s (- 12m 11.390831670245461s) (37 74.0%). train avg loss: 0.1848, val avg loss: 0.7026\n",
      "Training for epoch 38 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 38 finished in 35m 30.49092149734497s (- 11m 12.786606788635254s) (38 76.0%). train avg loss: 0.1829, val avg loss: 0.7018\n",
      "Training for epoch 39 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 39 finished in 36m 22.424134492874146s (- 10m 15.555525113374642s) (39 78.0%). train avg loss: 0.1809, val avg loss: 0.7043\n",
      "Training for epoch 40 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 40 finished in 37m 12.421326637268066s (- 9m 18.105331659317017s) (40 80.0%). train avg loss: 0.1832, val avg loss: 0.6988\n",
      "Training for epoch 41 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 41 finished in 38m 3.2807774543762207s (- 8m 21.207975538765822s) (41 82.0%). train avg loss: 0.1812, val avg loss: 0.6922\n",
      "Training for epoch 42 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 42 finished in 38m 53.82623338699341s (- 7m 24.538330168951234s) (42 84.0%). train avg loss: 0.1781, val avg loss: 0.6955\n",
      "Training for epoch 43 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 43 finished in 39m 50.25471067428589s (- 6m 29.11123197023244s) (43 86.0%). train avg loss: 0.1814, val avg loss: 0.6924\n",
      "Training for epoch 44 has started (lr=3.90625e-06). Found 1916 batch(es).\n",
      "Epoch 44 finished in 40m 37.24450492858887s (- 5m 32.35152339935303s) (44 88.0%). train avg loss: 0.1812, val avg loss: 0.6972\n",
      "Early stopping after 44 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 50\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-2, min_value+.075, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 15.925012064388627%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    ' '.join(output_phonemes) if output_phonemes else [\"<EOS>\"],\n",
    "    arpabet_phoneme_sequence if output_phonemes else arpabet_phoneme_sequence+\" <EOS>\"\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> membentengi\n",
      "= ['M', 'AX', 'M', 'B', 'EH', 'N', 'T', 'EH', 'NG', 'IY']\n",
      "< M AX M B AX N T EH NG IY ['M', 'AX', 'M', 'B', 'AX', 'N', 'T', 'EH', 'NG', 'IY']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f10ceb5d7c0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 436.364x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
