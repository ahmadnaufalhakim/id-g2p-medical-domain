{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/notebooks/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"16\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [100, 125, 150, 175, 200]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 30, 6, 1]\n",
      "tensor([[18],\n",
      "        [10],\n",
      "        [19],\n",
      "        [12],\n",
      "        [26],\n",
      "        [19],\n",
      "        [ 8],\n",
      "        [14],\n",
      "        [19],\n",
      "        [30],\n",
      "        [ 6],\n",
      "        [ 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7fa30982bdc0> ([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "valid grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "test grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {seconds}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 16\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 35912 parameters\n",
      "Decoder has a total number of 73212 parameters\n",
      "Total number of all parameters is 109124\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = .001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5, patience=1)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 45.852309226989746s (- 37m 26.76315212249756s) (1 2.0%). train avg loss: 1.6501, val avg loss: 1.5058\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 38.32513737678528s (- 39m 19.80329704284668s) (2 4.0%). train avg loss: 0.7356, val avg loss: 1.1042\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 25.47256326675415s (- 37m 59.070157845815174s) (3 6.0%). train avg loss: 0.5264, val avg loss: 0.9745\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 3m 12.451181888580322s (- 36m 53.188591718673706s) (4 8.0%). train avg loss: 0.4777, val avg loss: 1.16\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 3m 59.79876637458801s (- 35m 58.188897371292114s) (5 10.0%). train avg loss: 0.4184, val avg loss: 0.8987\n",
      "Training for epoch 6 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 6 finished in 4m 48.246517181396484s (- 35m 13.807792663574219s) (6 12.0%). train avg loss: 0.3741, val avg loss: 0.8449\n",
      "Training for epoch 7 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 7 finished in 5m 35.69671654701233s (- 34m 22.136973074504112s) (7 14.0%). train avg loss: 0.3661, val avg loss: 0.8563\n",
      "Training for epoch 8 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 8 finished in 6m 23.31110405921936s (- 33m 32.38329631090164s) (8 16.0%). train avg loss: 0.3461, val avg loss: 0.784\n",
      "Training for epoch 9 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 9 finished in 7m 9.876034021377563s (- 32m 38.324154986275516s) (9 18.0%). train avg loss: 0.324, val avg loss: 0.8755\n",
      "Training for epoch 10 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 10 finished in 7m 56.95424938201904s (- 31m 47.81699752807617s) (10 20.0%). train avg loss: 0.3271, val avg loss: 0.8137\n",
      "Training for epoch 11 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 11 finished in 8m 43.319884300231934s (- 30m 55.406862519004335s) (11 22.0%). train avg loss: 0.2739, val avg loss: 0.7493\n",
      "Training for epoch 12 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 12 finished in 9m 30.111482620239258s (- 30m 5.353028297424316s) (12 24.0%). train avg loss: 0.2724, val avg loss: 0.7269\n",
      "Training for epoch 13 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 13 finished in 10m 17.28040361404419s (- 29m 16.87499490151049s) (13 26.0%). train avg loss: 0.2552, val avg loss: 0.7456\n",
      "Training for epoch 14 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 14 finished in 11m 4.519499063491821s (- 28m 28.76442616326449s) (14 28.0%). train avg loss: 0.2539, val avg loss: 0.7233\n",
      "Training for epoch 15 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 15 finished in 11m 52.05881714820862s (- 27m 41.47057334582041s) (15 30.0%). train avg loss: 0.2453, val avg loss: 0.6972\n",
      "Training for epoch 16 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 16 finished in 12m 38.89165759086609s (- 26m 52.64477238059044s) (16 32.0%). train avg loss: 0.2369, val avg loss: 0.6925\n",
      "Training for epoch 17 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 17 finished in 13m 25.701509475708008s (- 26m 4.008812511668111s) (17 34.0%). train avg loss: 0.2379, val avg loss: 0.7137\n",
      "Training for epoch 18 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 18 finished in 14m 13.173941612243652s (- 25m 16.75367397732225s) (18 36.0%). train avg loss: 0.2244, val avg loss: 0.7147\n",
      "Training for epoch 19 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 19 finished in 15m 0.2193284034729004s (- 24m 28.778904237245115s) (19 38.0%). train avg loss: 0.2182, val avg loss: 0.7002\n",
      "Training for epoch 20 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 20 finished in 15m 46.679301738739014s (- 23m 40.01895260810852s) (20 40.0%). train avg loss: 0.2062, val avg loss: 0.7081\n",
      "Training for epoch 21 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 21 finished in 16m 33.95027422904968s (- 22m 52.597997744878285s) (21 42.0%). train avg loss: 0.2035, val avg loss: 0.705\n",
      "Training for epoch 22 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 22 finished in 17m 20.76360058784485s (- 22m 4.6082189299845595s) (22 44.0%). train avg loss: 0.2017, val avg loss: 0.7155\n",
      "Training for epoch 23 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 23 finished in 18m 8.028301000595093s (- 21m 17.250614218089595s) (23 46.0%). train avg loss: 0.1982, val avg loss: 0.6988\n",
      "Training for epoch 24 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 24 finished in 18m 56.2142698764801s (- 20m 30.89879236618708s) (24 48.0%). train avg loss: 0.1991, val avg loss: 0.6958\n",
      "Training for epoch 25 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 25 finished in 19m 47.06983804702759s (- 19m 47.06983804702759s) (25 50.0%). train avg loss: 0.1928, val avg loss: 0.6998\n",
      "Training for epoch 26 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 26 finished in 20m 35.32195734977722s (- 19m 0.29719139979442843s) (26 52.0%). train avg loss: 0.1931, val avg loss: 0.7016\n",
      "Early stopping after 26 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 50\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-1.5, min_value+.075, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 27.43665107627688%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    ' '.join(output_phonemes) if output_phonemes else [\"<EOS>\"],\n",
    "    arpabet_phoneme_sequence if output_phonemes else arpabet_phoneme_sequence+\" <EOS>\"\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> menyusun\n",
      "= ['M', 'AX', 'NY', 'UW', 'S', 'UW', 'N']\n",
      "< M AX NY UW S UW N ['M', 'AX', 'NY', 'UW', 'S', 'UW', 'N']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa18b470520>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
