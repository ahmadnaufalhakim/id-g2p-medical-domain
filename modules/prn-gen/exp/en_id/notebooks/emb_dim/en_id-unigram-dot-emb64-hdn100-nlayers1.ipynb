{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/notebooks/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [100, 125, 150, 175, 200]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 30, 6, 1]\n",
      "tensor([[18],\n",
      "        [10],\n",
      "        [19],\n",
      "        [12],\n",
      "        [26],\n",
      "        [19],\n",
      "        [ 8],\n",
      "        [14],\n",
      "        [19],\n",
      "        [30],\n",
      "        [ 6],\n",
      "        [ 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f851232f970> ([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "valid grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "test grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {seconds}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 51848 parameters\n",
      "Decoder has a total number of 89340 parameters\n",
      "Total number of all parameters is 141188\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = .001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5, patience=1)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 47.27032780647278s (- 38m 36.24606251716614s) (1 2.0%). train avg loss: 1.1177, val avg loss: 1.1932\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 36.18703627586365s (- 38m 28.48887062072754s) (2 4.0%). train avg loss: 0.5588, val avg loss: 1.0582\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 25.73556923866272s (- 38m 3.1905847390494273s) (3 6.0%). train avg loss: 0.4714, val avg loss: 1.1526\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 3m 14.862490177154541s (- 37m 20.91863703727722s) (4 8.0%). train avg loss: 0.4727, val avg loss: 0.9846\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 4m 6.135616302490234s (- 36m 55.22054672241211s) (5 10.0%). train avg loss: 0.4144, val avg loss: 0.9853\n",
      "Training for epoch 6 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 6 finished in 4m 58.53678607940674s (- 36m 29.269764582316384s) (6 12.0%). train avg loss: 0.3842, val avg loss: 0.8844\n",
      "Training for epoch 7 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 7 finished in 5m 51.61112713813782s (- 35m 59.89692384856062s) (7 14.0%). train avg loss: 0.3745, val avg loss: 0.9209\n",
      "Training for epoch 8 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 8 finished in 6m 44.64832305908203s (- 35m 24.403696060180664s) (8 16.0%). train avg loss: 0.3545, val avg loss: 0.9195\n",
      "Training for epoch 9 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 9 finished in 7m 36.04700183868408s (- 34m 37.54745282067188s) (9 18.0%). train avg loss: 0.3178, val avg loss: 0.7952\n",
      "Training for epoch 10 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 10 finished in 8m 22.92986273765564s (- 33m 31.71945095062256s) (10 20.0%). train avg loss: 0.2962, val avg loss: 0.8138\n",
      "Training for epoch 11 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 11 finished in 9m 12.314037322998047s (- 32m 38.204314145174976s) (11 22.0%). train avg loss: 0.2784, val avg loss: 0.8073\n",
      "Training for epoch 12 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 12 finished in 9m 57.77273917198181s (- 31m 32.947007377942555s) (12 24.0%). train avg loss: 0.2617, val avg loss: 0.782\n",
      "Training for epoch 13 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 13 finished in 10m 42.59713101387024s (- 30m 28.930295962553828s) (13 26.0%). train avg loss: 0.2551, val avg loss: 0.7636\n",
      "Training for epoch 14 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 14 finished in 11m 27.173667192459106s (- 29m 27.018001352037572s) (14 28.0%). train avg loss: 0.2492, val avg loss: 0.7505\n",
      "Training for epoch 15 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 15 finished in 12m 12.086374998092651s (- 28m 28.201541662216187s) (15 30.0%). train avg loss: 0.2492, val avg loss: 0.7478\n",
      "Training for epoch 16 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 16 finished in 12m 56.945314168930054s (- 27m 31.008792608976364s) (16 32.0%). train avg loss: 0.2415, val avg loss: 0.7559\n",
      "Training for epoch 17 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 17 finished in 13m 41.30364179611206s (- 26m 34.29530466304095s) (17 34.0%). train avg loss: 0.2419, val avg loss: 0.731\n",
      "Training for epoch 18 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 18 finished in 14m 26.63390278816223s (- 25m 40.68249384562205s) (18 36.0%). train avg loss: 0.2371, val avg loss: 0.735\n",
      "Training for epoch 19 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 19 finished in 15m 11.633429288864136s (- 24m 47.401910944988686s) (19 38.0%). train avg loss: 0.2402, val avg loss: 0.7105\n",
      "Training for epoch 20 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 20 finished in 15m 56.57774639129639s (- 23m 54.86661958694458s) (20 40.0%). train avg loss: 0.2304, val avg loss: 0.7122\n",
      "Training for epoch 21 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 21 finished in 16m 43.05582404136658s (- 23m 5.172328438077784s) (21 42.0%). train avg loss: 0.2249, val avg loss: 0.7029\n",
      "Training for epoch 22 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 22 finished in 17m 28.04870295524597s (- 22m 13.880167397585865s) (22 44.0%). train avg loss: 0.2238, val avg loss: 0.679\n",
      "Training for epoch 23 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 23 finished in 18m 12.201173543930054s (- 21m 22.14920372548295s) (23 46.0%). train avg loss: 0.2305, val avg loss: 0.6648\n",
      "Training for epoch 24 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 24 finished in 18m 56.964213371276855s (- 20m 31.711231152216897s) (24 48.0%). train avg loss: 0.2163, val avg loss: 0.6964\n",
      "Training for epoch 25 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 25 finished in 19m 40.643304109573364s (- 19m 40.643304109573364s) (25 50.0%). train avg loss: 0.2182, val avg loss: 0.6883\n",
      "Training for epoch 26 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 26 finished in 20m 24.37441873550415s (- 18m 50.191771140465335s) (26 52.0%). train avg loss: 0.2069, val avg loss: 0.6724\n",
      "Training for epoch 27 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 27 finished in 21m 9.12524962425232s (- 18m 1.1066941243630026s) (27 54.0%). train avg loss: 0.2048, val avg loss: 0.6801\n",
      "Training for epoch 28 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 28 finished in 21m 54.12950658798218s (- 17m 12.53032660484314s) (28 56.0%). train avg loss: 0.2011, val avg loss: 0.6687\n",
      "Training for epoch 29 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 29 finished in 22m 39.048717737197876s (- 16m 24.138726637281252s) (29 58.0%). train avg loss: 0.2016, val avg loss: 0.665\n",
      "Training for epoch 30 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 30 finished in 23m 23.13277816772461s (- 15m 35.421852111816406s) (30 60.0%). train avg loss: 0.199, val avg loss: 0.6675\n",
      "Training for epoch 31 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 31 finished in 24m 7.324750185012817s (- 14m 47.07000817791095s) (31 62.0%). train avg loss: 0.1979, val avg loss: 0.6762\n",
      "Training for epoch 32 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 32 finished in 24m 51.64156937599182s (- 13m 59.0483827739954s) (32 64.0%). train avg loss: 0.1963, val avg loss: 0.6737\n",
      "Training for epoch 33 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 33 finished in 25m 36.090726375579834s (- 13m 11.319465102571485s) (33 66.0%). train avg loss: 0.1987, val avg loss: 0.6732\n",
      "Early stopping after 33 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 50\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-1.5, min_value+.05, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 21.539204244196412%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    ' '.join(output_phonemes) if output_phonemes else [\"<EOS>\"],\n",
    "    arpabet_phoneme_sequence if output_phonemes else arpabet_phoneme_sequence+\" <EOS>\"\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> limb\n",
      "= ['L', 'IY', 'M']\n",
      "< L AY M ['L', 'AY', 'M']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8468522fa0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
