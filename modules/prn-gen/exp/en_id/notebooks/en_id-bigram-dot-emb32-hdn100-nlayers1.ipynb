{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/notebooks/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"32\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [100, 125, 150, 175, 200]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369, 165, 398, 227, 577, 394, 107, 275, 416, 659, 1]\n",
      "tensor([[369],\n",
      "        [165],\n",
      "        [398],\n",
      "        [227],\n",
      "        [577],\n",
      "        [394],\n",
      "        [107],\n",
      "        [275],\n",
      "        [416],\n",
      "        [659],\n",
      "        [  1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f0a4c3b7220> ([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "valid grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "test grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {seconds}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 32\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 62920 parameters\n",
      "Decoder has a total number of 78588 parameters\n",
      "Total number of all parameters is 141508\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = .001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5, patience=1)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5, patience=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 45.164594411849976s (- 36m 53.065126180648804s) (1 2.0%). train avg loss: 1.6163, val avg loss: 1.3028\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 29.869364261627197s (- 35m 56.864742279052734s) (2 4.0%). train avg loss: 0.6526, val avg loss: 1.0351\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 15.60537600517273s (- 35m 24.48422408103943s) (3 6.0%). train avg loss: 0.5069, val avg loss: 0.9991\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 3m 2.554966449737549s (- 34m 59.38211417198181s) (4 8.0%). train avg loss: 0.4374, val avg loss: 0.9008\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 3m 49.48058772087097s (- 34m 25.325289487838745s) (5 10.0%). train avg loss: 0.3753, val avg loss: 0.9311\n",
      "Training for epoch 6 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 6 finished in 4m 38.73164701461792s (- 34m 4.032078107198231s) (6 12.0%). train avg loss: 0.4154, val avg loss: 0.8559\n",
      "Training for epoch 7 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 7 finished in 5m 24.737584829330444s (- 33m 14.816592523029612s) (7 14.0%). train avg loss: 0.3451, val avg loss: 0.9088\n",
      "Training for epoch 8 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 8 finished in 6m 10.74650239944458s (- 32m 26.419137597084045s) (8 16.0%). train avg loss: 0.3182, val avg loss: 0.8082\n",
      "Training for epoch 9 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 9 finished in 6m 56.452144622802734s (- 31m 37.17088105943458s) (9 18.0%). train avg loss: 0.339, val avg loss: 0.7581\n",
      "Training for epoch 10 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 10 finished in 7m 42.33954691886902s (- 30m 49.358187675476074s) (10 20.0%). train avg loss: 0.3235, val avg loss: 0.8421\n",
      "Training for epoch 11 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 11 finished in 8m 28.578107118606567s (- 30m 3.140561602332582s) (11 22.0%). train avg loss: 0.3211, val avg loss: 0.7668\n",
      "Training for epoch 12 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 12 finished in 9m 14.74069356918335s (- 29m 16.67886296908091s) (12 24.0%). train avg loss: 0.2704, val avg loss: 0.7337\n",
      "Training for epoch 13 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 13 finished in 10m 0.8779842853546143s (- 28m 30.19118604293226s) (13 26.0%). train avg loss: 0.2525, val avg loss: 0.7269\n",
      "Training for epoch 14 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 14 finished in 10m 47.67235851287842s (- 27m 45.443207604544114s) (14 28.0%). train avg loss: 0.2461, val avg loss: 0.6898\n",
      "Training for epoch 15 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 15 finished in 11m 35.10940933227539s (- 27m 1.9219551086425781s) (15 30.0%). train avg loss: 0.2382, val avg loss: 0.7081\n",
      "Training for epoch 16 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 16 finished in 12m 20.830615997314453s (- 26m 14.265058994293213s) (16 32.0%). train avg loss: 0.2319, val avg loss: 0.6944\n",
      "Training for epoch 17 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 17 finished in 13m 8.495657682418823s (- 25m 30.6092178541071s) (17 34.0%). train avg loss: 0.2174, val avg loss: 0.6467\n",
      "Training for epoch 18 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 18 finished in 13m 54.02377390861511s (- 24m 42.708931393093735s) (18 36.0%). train avg loss: 0.209, val avg loss: 0.6865\n",
      "Training for epoch 19 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 19 finished in 14m 39.288737535476685s (- 23m 54.62899282104081s) (19 38.0%). train avg loss: 0.2069, val avg loss: 0.6579\n",
      "Training for epoch 20 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 20 finished in 15m 26.919177770614624s (- 23m 10.378766655921936s) (20 40.0%). train avg loss: 0.2062, val avg loss: 0.6461\n",
      "Training for epoch 21 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 21 finished in 16m 13.323695182800293s (- 22m 24.113674300057482s) (21 42.0%). train avg loss: 0.1984, val avg loss: 0.6354\n",
      "Training for epoch 22 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 22 finished in 16m 58.928804874420166s (- 21m 36.818478931080335s) (22 44.0%). train avg loss: 0.1943, val avg loss: 0.6468\n",
      "Training for epoch 23 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 23 finished in 17m 43.99476909637451s (- 20m 49.03733763487435s) (23 46.0%). train avg loss: 0.1956, val avg loss: 0.6552\n",
      "Training for epoch 24 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 24 finished in 18m 30.999042987823486s (- 20m 3.5822965701422618s) (24 48.0%). train avg loss: 0.1903, val avg loss: 0.6539\n",
      "Training for epoch 25 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 25 finished in 19m 19.861904621124268s (- 19m 19.861904621124268s) (25 50.0%). train avg loss: 0.1879, val avg loss: 0.6385\n",
      "Training for epoch 26 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 26 finished in 20m 5.935422897338867s (- 18m 33.171159597543465s) (26 52.0%). train avg loss: 0.1881, val avg loss: 0.6412\n",
      "Training for epoch 27 has started (lr=3.125e-05). Found 1916 batch(es).\n",
      "Epoch 27 finished in 20m 54.80307078361511s (- 17m 48.906319556412654s) (27 54.0%). train avg loss: 0.1863, val avg loss: 0.6448\n",
      "Training for epoch 28 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 28 finished in 21m 42.32967805862427s (- 17m 3.259032760347509s) (28 56.0%). train avg loss: 0.1845, val avg loss: 0.6449\n",
      "Training for epoch 29 has started (lr=1.5625e-05). Found 1916 batch(es).\n",
      "Epoch 29 finished in 22m 37.3754620552063s (- 16m 22.927058729632336s) (29 58.0%). train avg loss: 0.1805, val avg loss: 0.6407\n",
      "Training for epoch 30 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 30 finished in 23m 23.295290231704712s (- 15m 35.530193487803444s) (30 60.0%). train avg loss: 0.1828, val avg loss: 0.6379\n",
      "Training for epoch 31 has started (lr=7.8125e-06). Found 1916 batch(es).\n",
      "Epoch 31 finished in 24m 17.9574134349823s (- 14m 53.58680178273107s) (31 62.0%). train avg loss: 0.1816, val avg loss: 0.6397\n",
      "Early stopping after 31 epochs\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 50\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"encoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"decoder-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-1.5, min_value+.075, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 17.43181983298895%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    ' '.join(output_phonemes) if output_phonemes else [\"<EOS>\"],\n",
    "    arpabet_phoneme_sequence if output_phonemes else arpabet_phoneme_sequence+\" <EOS>\"\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> maharajalela\n",
      "= ['M', 'AA', 'HH', 'AA', 'R', 'AA', 'JH', 'AA', 'L', 'EH', 'L', 'AA']\n",
      "< M AA HH AA R AA JH AA L L AA ['M', 'AA', 'HH', 'AA', 'R', 'AA', 'JH', 'AA', 'L', 'L', 'AA']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f099f3e82b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 440x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
