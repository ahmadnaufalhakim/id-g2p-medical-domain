{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from Multi30k, still not succeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.datasets import multi30k, Multi30k\n",
    "# # URL and MD5 replacement for the data (because the server that hosts the data is currently down (as of December 2023))\n",
    "# URL = {\n",
    "#   \"train\": \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\",\n",
    "#   \"valid\": \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\",\n",
    "#   \"test\": \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n",
    "# }\n",
    "# MD5 = {\n",
    "#   \"train\": \"20140d013d05dd9a72dfde46478663ba05737ce983f478f960c1123c6671be5e\",\n",
    "#   \"valid\": \"a7aa20e9ebd5ba5adce7909498b94410996040857154dab029851af3a866da8c\",\n",
    "#   \"test\": \"6d1ca1dba99e2c5dd54cae1226ff11c2551e6ce63527ebb072a1f70f72a5cd36\",\n",
    "# }\n",
    "# multi30k.URL[\"train\"] = URL[\"train\"]\n",
    "# multi30k.URL[\"valid\"] = URL[\"valid\"]\n",
    "# multi30k.URL[\"test\"] = URL[\"test\"]\n",
    "# multi30k.MD5[\"train\"] = MD5[\"train\"]\n",
    "# multi30k.MD5[\"valid\"] = MD5[\"valid\"]\n",
    "# multi30k.MD5[\"test\"] = MD5[\"test\"]\n",
    "\n",
    "# print(Multi30k)\n",
    "# import spacy\n",
    "\n",
    "# train_data, valid_data, test_data = Multi30k(\n",
    "#   root=\".data\",\n",
    "#   split=(\"train\", \"valid\", \"test\"),\n",
    "#   language_pair=(\"de\", \"en\")\n",
    "# )\n",
    "# print(test_data)\n",
    "# print(list(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_sentences = [\n",
    "  \"Selamat pagi!\",\n",
    "  \"Apa kabar?\",\n",
    "  \"Saya lapar.\",\n",
    "  \"Tolong bantu saya.\",\n",
    "  \"Terima kasih banyak.\",\n",
    "  \"Saya suka musik.\",\n",
    "  \"Ini buku saya.\",\n",
    "  \"Berapa umurmu?\",\n",
    "  \"Hujan turun.\",\n",
    "  \"Tolong berikan saya air.\",\n",
    "  \"Kamu cantik.\",\n",
    "  \"Saya mencintai keluarga saya.\",\n",
    "  \"Apa pekerjaanmu?\",\n",
    "  \"Saya ingin belajar bahasa Inggris.\",\n",
    "  \"Di mana kamar mandi?\",\n",
    "  \"Apa warna favoritmu?\",\n",
    "  \"Saya pergi ke sekolah setiap hari.\",\n",
    "  \"Bunga ini indah.\",\n",
    "  \"Tolong tunggu sebentar.\",\n",
    "  \"Saya suka makan nasi goreng.\",\n",
    "  \"Ayah saya seorang dokter.\",\n",
    "  \"Apa nama hewan ini?\",\n",
    "  \"Saya senang bertemu denganmu.\",\n",
    "  \"Ini mobil baru saya.\",\n",
    "  \"Anjing itu lucu.\",\n",
    "  \"Saya suka berenang di pantai.\",\n",
    "  \"Apa cita-citamu?\",\n",
    "  \"Saya belajar bahasa Indonesia.\",\n",
    "  \"Hari ini cuaca panas.\",\n",
    "  \"Saya suka menonton film.\",\n",
    "  \"Saya punya dua saudara.\",\n",
    "  \"Di mana toko buku terdekat?\",\n",
    "  \"Apa yang kamu lakukan kemarin?\",\n",
    "  \"Saya ingin memesan makanan.\",\n",
    "  \"Kamu pandai.\",\n",
    "  \"Apa rencanamu untuk liburan?\",\n",
    "  \"Mobil itu sangat cepat.\",\n",
    "  \"Saya suka minum teh hangat.\",\n",
    "  \"Anak-anak bermain di taman.\",\n",
    "  \"Apa nama kota ini?\",\n",
    "  \"Saya suka sekali dengan film ini.\",\n",
    "  \"Apa yang kamu pelajari di sekolah?\",\n",
    "  \"Hari ini adalah ulang tahun saya.\",\n",
    "  \"Saya ingin menjadi penulis.\",\n",
    "  \"Saya suka pergi ke pantai pada malam hari.\",\n",
    "  \"Tolong jangan lupa membayar tagihan listrik.\",\n",
    "  \"Anak itu pintar sekali.\",\n",
    "  \"Saya senang bisa membantu.\",\n",
    "  \"Buku ini sangat tebal.\",\n",
    "  \"Kamu memiliki hobi apa?\",\n",
    "  \"Apa yang kamu pikirkan tentang film itu?\",\n",
    "  \"Pohon-pohon itu tinggi sekali.\",\n",
    "  \"Saya ingin mencoba makanan khas Indonesia.\",\n",
    "  \"Di sini sangat ramai.\",\n",
    "  \"Tolong tutup pintu.\",\n",
    "  \"Apa makanan favoritmu?\",\n",
    "  \"Rumah ini sangat bersih.\",\n",
    "  \"Saya suka bermain game komputer.\",\n",
    "  \"Tiket pesawat sudah dibeli.\",\n",
    "  \"Mobil itu rusak.\",\n",
    "  \"Saya senang bisa belajar dari kesalahan.\",\n",
    "  \"Pemandangan di gunung ini sangat indah.\",\n",
    "  \"Hewan-hewan itu liar.\",\n",
    "  \"Tolong beri tahu saya arah yang benar.\",\n",
    "  \"Saya suka menulis puisi.\",\n",
    "  \"Apakah kamu suka olahraga?\",\n",
    "  \"Tolong kirim email ini segera.\",\n",
    "  \"Saya ingin berlibur ke pantai.\",\n",
    "  \"Kamu punya hewan peliharaan?\",\n",
    "  \"Pakaian ini terlalu kecil.\",\n",
    "  \"Apa acara favoritmu di televisi?\",\n",
    "  \"Saya suka berjalan-jalan di taman.\",\n",
    "  \"Anak-anak itu sedang bermain sepak bola.\",\n",
    "  \"Saya akan pergi ke pasar besok.\",\n",
    "  \"Apa pendapatmu tentang musik klasik?\",\n",
    "  \"Saya ingin memesan kamar untuk dua malam.\",\n",
    "  \"Tolong beri tahu saya cara ke stasiun kereta.\",\n",
    "  \"Saya belajar memasak dari ibu.\",\n",
    "  \"Kami akan pergi ke pantai bersama-sama.\",\n",
    "  \"Apakah kamu suka pergi ke konser musik?\",\n",
    "  \"Saya merasa bangga dengan prestasi anak-anak saya.\"\n",
    "]\n",
    "train_trg_sentences = [\n",
    "  \"Good morning!\",\n",
    "  \"How are you?\",\n",
    "  \"I am hungry.\",\n",
    "  \"Please help me.\",\n",
    "  \"Thank you very much.\",\n",
    "  \"I like music.\",\n",
    "  \"This is my book.\",\n",
    "  \"How old are you?\",\n",
    "  \"It's raining.\",\n",
    "  \"Please give me water.\",\n",
    "  \"You are beautiful.\",\n",
    "  \"I love my family.\",\n",
    "  \"What is your job?\",\n",
    "  \"I want to learn English.\",\n",
    "  \"Where is the bathroom?\",\n",
    "  \"What is your favorite color?\",\n",
    "  \"I go to school every day.\",\n",
    "  \"These flowers are beautiful.\",\n",
    "  \"Please wait a moment.\",\n",
    "  \"I like to eat fried rice.\",\n",
    "  \"My father is a doctor.\",\n",
    "  \"What is the name of this animal?\",\n",
    "  \"I am happy to meet you.\",\n",
    "  \"This is my new car.\",\n",
    "  \"That dog is cute.\",\n",
    "  \"I like to swim at the beach.\",\n",
    "  \"What is your ambition?\",\n",
    "  \"I am learning Indonesian.\",\n",
    "  \"Today, the weather is hot.\",\n",
    "  \"I like to watch movies.\",\n",
    "  \"I have two siblings.\",\n",
    "  \"Where is the nearest bookstore?\",\n",
    "  \"What did you do yesterday?\",\n",
    "  \"I would like to order food.\",\n",
    "  \"You are clever.\",\n",
    "  \"What are your plans for the holiday?\",\n",
    "  \"That car is very fast.\",\n",
    "  \"I like to drink hot tea.\",\n",
    "  \"The children are playing in the park.\",\n",
    "  \"What is the name of this city?\",\n",
    "  \"I really like this movie.\",\n",
    "  \"What do you study in school?\",\n",
    "  \"Today is my birthday.\",\n",
    "  \"I want to be a writer.\",\n",
    "  \"I like to go to the beach at night.\",\n",
    "  \"Please don't forget to pay the electricity bill.\",\n",
    "  \"That child is very smart.\",\n",
    "  \"I am happy to help.\",\n",
    "  \"This book is very thick.\",\n",
    "  \"What is your hobby?\",\n",
    "  \"What do you think about that movie?\",\n",
    "  \"Those trees are very tall.\",\n",
    "  \"I want to try Indonesian cuisine.\",\n",
    "  \"It is very crowded here.\",\n",
    "  \"Please close the door.\",\n",
    "  \"What is your favorite food?\",\n",
    "  \"This house is very clean.\",\n",
    "  \"I like to play computer games.\",\n",
    "  \"The plane tickets have been purchased.\",\n",
    "  \"That car is broken.\",\n",
    "  \"I am happy to learn from mistakes.\",\n",
    "  \"The scenery on this mountain is very beautiful.\",\n",
    "  \"Those animals are wild.\",\n",
    "  \"Please tell me the right direction.\",\n",
    "  \"I like to write poetry.\",\n",
    "  \"Do you like sports?\",\n",
    "  \"Please send this email immediately.\",\n",
    "  \"I want to vacation at the beach.\",\n",
    "  \"Do you have any pets?\",\n",
    "  \"This clothes are too small.\",\n",
    "  \"What is your favorite TV show?\",\n",
    "  \"I like to take a stroll in the park.\",\n",
    "  \"The children are playing soccer.\",\n",
    "  \"I will go to the market tomorrow.\",\n",
    "  \"What is your opinion about classical music?\",\n",
    "  \"I would like to book a room for two nights.\",\n",
    "  \"Please tell me the way to the train station.\",\n",
    "  \"I learn to cook from my mother.\",\n",
    "  \"We will go to the beach together.\",\n",
    "  \"Do you like to go to music concerts?\",\n",
    "  \"I feel proud of my children's achievements.\"\n",
    "]\n",
    "valid_src_sentences = [\n",
    "  \"Saya suka menonton film animasi.\",\n",
    "  \"Apa rencanamu untuk akhir pekan ini?\",\n",
    "  \"Rumah itu terlihat sangat nyaman.\",\n",
    "  \"Saya ingin mencoba makanan Korea.\",\n",
    "  \"Buku ini sangat menarik.\",\n",
    "  \"Tolong beritahu saya nomor teleponnya.\",\n",
    "  \"Kami akan mengadakan pesta ulang tahun besok.\",\n",
    "  \"Apakah kamu punya rencana perjalanan musim panas ini?\",\n",
    "  \"Saya suka bermain game papan dengan teman-teman.\",\n",
    "]\n",
    "valid_trg_sentences = [\n",
    "  \"I like to watch animated movies.\",\n",
    "  \"What are your plans for this weekend?\",\n",
    "  \"That house looks very comfortable.\",\n",
    "  \"I want to try Korean food.\",\n",
    "  \"This book is very interesting.\",\n",
    "  \"Please tell me his/her phone number.\",\n",
    "  \"We will have a birthday party tomorrow.\",\n",
    "  \"Do you have any plans for this summer vacation?\",\n",
    "  \"I like to play board games with friends.\"\n",
    "]\n",
    "test_src_sentences = [\n",
    "  \"Tolong ambilkan saya secangkir kopi.\",\n",
    "  \"Saya suka menghabiskan waktu di perpustakaan.\",\n",
    "  \"Hewan-hewan di kebun binatang terlihat bahagia.\",\n",
    "  \"Saya akan mengunjungi kakek nenek saya akhir pekan ini.\",\n",
    "  \"Tolong beri tahu saya cara menggunakan mesin cuci.\",\n",
    "  \"Pesta itu akan dimulai pukul delapan malam.\",\n",
    "  \"Anak-anak sedang bermain di taman bermain.\",\n",
    "  \"Apakah kamu suka memasak masakan Thailand?\",\n",
    "  \"Saya ingin belajar bermain gitar.\",\n",
    "  \"Hari ini adalah hari libur nasional.\"\n",
    "]\n",
    "test_trg_sentences = [\n",
    "  \"Please get me a cup of coffee.\",\n",
    "  \"I like to spend time in the library.\",\n",
    "  \"The animals at the zoo look happy.\",\n",
    "  \"I will visit my grandparents this weekend.\",\n",
    "  \"Please tell me how to use the washing machine.\",\n",
    "  \"The party will start at eight in the evening.\",\n",
    "  \"The children are playing at the playground.\",\n",
    "  \"Do you like to cook Thai dishes?\",\n",
    "  \"I want to learn to play the guitar.\",\n",
    "  \"Today is a national holiday.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def build_vocab(text_corpus, min_freq=1) :\n",
    "  # Tokenize the text into words\n",
    "  tokenized_text = [[\"<sos>\"] + re.findall(r\"\\b\\w+\\b\", sentence.lower()) + [\"<eos>\"] for sentence in text_corpus]\n",
    "  # print(\"tokenized_text\", tokenized_text)\n",
    "\n",
    "  # Flatten the list of tokenized words\n",
    "  flat_tokens = [token for sentence_tokens in tokenized_text for token in sentence_tokens]\n",
    "  # print(\"flat_tokens\", flat_tokens)\n",
    "\n",
    "  # Count the frequency of each token\n",
    "  token_freq = Counter(flat_tokens)\n",
    "  # print(\"token_freq\", token_freq)\n",
    "\n",
    "  # Filter tokens based on minimum frequency\n",
    "  tokens = [token for token, freq in token_freq.items() if freq >= min_freq]\n",
    "  # print(\"tokens\", tokens)\n",
    "\n",
    "  # Create a vocabulary mapping from token to index\n",
    "  vocab = {token: idx for idx, token in enumerate(tokens)}\n",
    "  # print(\"vocab\", vocab)\n",
    "\n",
    "  # Create a reverse mapping from index to token\n",
    "  idx_to_token = {idx: token for token, idx in vocab.items()}\n",
    "  # print(\"idx_to_token\", idx_to_token)\n",
    "\n",
    "  return vocab, idx_to_token, tokenized_text\n",
    "\n",
    "train_src_vocab, train_src_itos, train_src_tokenized_sentences = build_vocab(train_src_sentences)\n",
    "train_trg_vocab, train_trg_itos, train_trg_tokenized_sentences = build_vocab(train_trg_sentences)\n",
    "valid_src_vocab, valid_src_itos, valid_src_tokenized_sentences = build_vocab(valid_src_sentences)\n",
    "valid_trg_vocab, valid_trg_itos, valid_trg_tokenized_sentences = build_vocab(valid_trg_sentences)\n",
    "test_src_vocab, test_src_itos, test_src_tokenized_sentences = build_vocab(test_src_sentences)\n",
    "test_trg_vocab, test_trg_itos, test_trg_tokenized_sentences = build_vocab(test_trg_sentences)\n",
    "\n",
    "train_src_data = [torch.tensor([train_src_vocab[word] for word in sentence]) for sentence in train_src_tokenized_sentences]\n",
    "train_trg_data = [torch.tensor([train_trg_vocab[word] for word in sentence]) for sentence in train_trg_tokenized_sentences]\n",
    "valid_src_data = [torch.tensor([valid_src_vocab[word] for word in sentence]) for sentence in valid_src_tokenized_sentences]\n",
    "valid_trg_data = [torch.tensor([valid_trg_vocab[word] for word in sentence]) for sentence in valid_trg_tokenized_sentences]\n",
    "test_src_data = [torch.tensor([test_src_vocab[word] for word in sentence]) for sentence in test_src_tokenized_sentences]\n",
    "test_trg_data = [torch.tensor([test_trg_vocab[word] for word in sentence]) for sentence in test_trg_tokenized_sentences]\n",
    "train_src_data = pad_sequence(train_src_data, batch_first=True, padding_value=-1)\n",
    "train_trg_data = pad_sequence(train_trg_data, batch_first=True, padding_value=-1)\n",
    "valid_src_data = pad_sequence(valid_src_data, batch_first=True, padding_value=-1)\n",
    "valid_trg_data = pad_sequence(valid_trg_data, batch_first=True, padding_value=-1)\n",
    "test_src_data = pad_sequence(test_src_data, batch_first=True, padding_value=-1)\n",
    "test_trg_data = pad_sequence(test_trg_data, batch_first=True, padding_value=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Tensor Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 10]) torch.Size([32, 12])\n",
      "1 torch.Size([32, 10]) torch.Size([32, 12])\n",
      "2 torch.Size([17, 10]) torch.Size([17, 12])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create TensorDataset(s)\n",
    "train_dataset = TensorDataset(train_src_data, train_trg_data)\n",
    "valid_dataset = TensorDataset(valid_src_data, valid_trg_data)\n",
    "test_dataset = TensorDataset(test_src_data, test_trg_data)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader(s)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i, batch in enumerate(train_loader) :\n",
    "  print(i, batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, vocab_len, emb_dim, enc_hidden_dim, num_layers=1, dropout=.5) -> None :\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_len, emb_dim)\n",
    "    self.rnn = nn.LSTM(\n",
    "      input_size=emb_dim,\n",
    "      hidden_size=enc_hidden_dim,\n",
    "      num_layers=num_layers,\n",
    "      dropout=dropout\n",
    "    )\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, input_batch) :\n",
    "    embedded = self.dropout(self.embedding(input_batch))\n",
    "    outputs, hidden = self.rnn(embedded)\n",
    "    return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention (Additive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module) :\n",
    "  def __init__(self, enc_hidden_dim, dec_hidden_dim) -> None :\n",
    "    super().__init__()\n",
    "    # The input dim will be the concatenation of\n",
    "    # enc_hidden_dim (hidden) and dec_hidden_dim (encoder_outputs)\n",
    "    self.attn_hidden_vector = nn.Linear(\n",
    "      in_features=enc_hidden_dim+dec_hidden_dim,\n",
    "      out_features=dec_hidden_dim,\n",
    "      # bias=False\n",
    "    )\n",
    "\n",
    "    # We need source len number of values for n batch as the dimension\n",
    "    # of the alignment or attention weights. The attn_hidden_vector will have the\n",
    "    # dimension of [source len, batch size, decoder hidden dim]\n",
    "\n",
    "    # If we set the output dim of this Linear layer to 1, then the effective\n",
    "    # output dimension will be [source len, batch size]\n",
    "    self.attn_scoring_fn = nn.Linear(dec_hidden_dim, 1, bias=False)\n",
    "  \n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden dimension = [1, batch size, decoder hidden dim]\n",
    "    src_len = encoder_outputs.shape[0]\n",
    "\n",
    "    # We need to calculate the attn_hidden for each source token.\n",
    "    # Instead of repeating this using a loop, we can duplicate\n",
    "    # hidden by src_len number of times and perform the operations.\n",
    "    hidden = hidden.repeat(src_len, 1, 1)\n",
    "\n",
    "    # Calculate attention hidden values (tanh(W_c[E_o + D_h^(t-1)]))\n",
    "    attn_hidden = torch.tanh(self.attn_hidden_vector(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "\n",
    "    # Calculate the attention scoring vector. Remove 3rd dimension.\n",
    "    attn_scoring_vector = self.attn_scoring_fn(attn_hidden).squeeze(2)\n",
    "\n",
    "    # The attn_scoring_vector now has dimension of [source len, batch size]\n",
    "    # Since we need to calculate the softmax per record in the batch, we will\n",
    "    # have to switch the dimension to [batch size, source len]\n",
    "    attn_scoring_vector = attn_scoring_vector.permute(1, 0)\n",
    "\n",
    "    # Softmax function for normalizing the weights to probability distribution\n",
    "    # Further dot product with encoder outputs will be done in OneStepDecoder\n",
    "    return F.softmax(attn_scoring_vector, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneStepDecoder\n",
    "OneStepDecoder will handle most of the data flow in the Decoder class, such as applying the embedding function to the data output, calculating the alignment or attention weights, predicting the token outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepDecoder(nn.Module) :\n",
    "  def __init__(self, input_output_dim, emb_dim, enc_hidden_dim, dec_hidden_dim, attn, dropout=.5) -> None :\n",
    "    super().__init__()\n",
    "    self.output_dim = input_output_dim\n",
    "    self.attn = attn\n",
    "    self.embedding = nn.Embedding(input_output_dim, embedding_dim=emb_dim)\n",
    "\n",
    "    # Add the enc_hidden_dim and emb_dim\n",
    "    self.rnn = nn.LSTM(\n",
    "      input_size=enc_hidden_dim+emb_dim,\n",
    "      hidden_size=dec_hidden_dim\n",
    "    )\n",
    "    # Combine all the features for better prediction\n",
    "    self.fc = nn.Linear(\n",
    "      in_features=enc_hidden_dim+dec_hidden_dim+emb_dim,\n",
    "      out_features=input_output_dim\n",
    "    )\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "  \n",
    "  def forward(self, input, hidden, encoder_outputs) :\n",
    "    # Add the source len dimension\n",
    "    input = input.unsqueeze(0)\n",
    "    embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "    # Calculate the alignment or attention weights\n",
    "    s_t = self.attn(hidden, encoder_outputs).unsqueeze(1)\n",
    "\n",
    "    # We need to perform the batch-wise dot product.\n",
    "    # Hence we need to shift the batch size dimension to the front.\n",
    "    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "\n",
    "    # Use PyTorch's bmm function to calculate the attention matrix c_t\n",
    "    c_t = torch.bmm(s_t, encoder_outputs)\n",
    "    # Revert the batch dimension\n",
    "    c_t = c_t.permute(1, 0, 2)\n",
    "\n",
    "    # Concatenate the previous output with c_t\n",
    "    rnn_input = torch.cat((embedded, c_t), dim=2)\n",
    "\n",
    "    output, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "    # Remove the token length dimension and pass them to the Linear layer\n",
    "    predicted_token = self.fc(torch.cat((output.squeeze(0), c_t.squeeze(0), embedded.squeeze(0)), dim=1))\n",
    "\n",
    "    return predicted_token, hidden, s_t.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Here is the **Decoder** module. We need to pass the `encoder_outputs` to the `one_step_decoder`'s forward method.\n",
    "\n",
    "Also notice that for the first loop of the hidden state will be the encoder's hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, one_step_dec, device) -> None :\n",
    "    super().__init__()\n",
    "    self.one_step_dec = one_step_dec\n",
    "    self.device = device\n",
    "\n",
    "  def forward(self, target, encoder_outputs, hidden, teacher_forcing_ratio=.5) :\n",
    "    batch_size = target.shape[1]\n",
    "    trg_len = target.shape[0]\n",
    "    trg_vocab_size = self.one_step_dec.output_dim\n",
    "\n",
    "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "    input = target[0, :]\n",
    "\n",
    "    for t in range(1, trg_len) :\n",
    "      # Pass the encoder_outputs. For the first timestep, the hidden state\n",
    "      # comes from the encoder model\n",
    "      output, hidden, s_t = self.one_step_dec(input, hidden, encoder_outputs)\n",
    "      outputs[t] = output\n",
    "\n",
    "      teacher_force = random.random() < teacher_forcing_ratio\n",
    "      top1 = output.argmax(1)\n",
    "\n",
    "      input = target[t] if teacher_force else top1\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderDecoder\n",
    "\n",
    "A wrapper class for whole encoder and decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module) :\n",
    "  def __init__(self, enc, dec) -> None :\n",
    "    super().__init__()\n",
    "    self.enc = enc\n",
    "    self.dec = dec\n",
    "  \n",
    "  def forward(self, source, target, teacher_forcing_ratio=.5) :\n",
    "    encoder_outputs, hidden = self.enc(source)\n",
    "    return self.dec(target, encoder_outputs, hidden, teacher_forcing_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderDecoder(\n",
      "  (enc): Encoder(\n",
      "    (embedding): Embedding(180, 256)\n",
      "    (rnn): LSTM(256, 1024, dropout=0.5)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (dec): Decoder(\n",
      "    (one_step_dec): OneStepDecoder(\n",
      "      (attn): Attention(\n",
      "        (attn_hidden_vector): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "        (attn_scoring_fn): Linear(in_features=1024, out_features=1, bias=False)\n",
      "      )\n",
      "      (embedding): Embedding(184, 256)\n",
      "      (rnn): LSTM(1280, 1024)\n",
      "      (fc): Linear(in_features=2304, out_features=184, bias=True)\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      ") Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ") CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "def create_model(source_vocab, target_vocab) :\n",
    "  # Define the required dimensions and hyperparameters\n",
    "  embedding_dim = 256\n",
    "  hidden_dim = 1024\n",
    "  dropout = .5\n",
    "\n",
    "  # Instantiate the models\n",
    "  attention = Attention(hidden_dim, hidden_dim)\n",
    "  encoder = Encoder(len(source_vocab), embedding_dim, hidden_dim, dropout=dropout)\n",
    "  one_step_decoder = OneStepDecoder(len(target_vocab), embedding_dim, hidden_dim, hidden_dim, attention, dropout=dropout)\n",
    "  decoder = Decoder(one_step_decoder, device)\n",
    "\n",
    "  model = EncoderDecoder(encoder, decoder)\n",
    "  # model = model.to(device)\n",
    "\n",
    "  # Define the optimizer\n",
    "  optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "  # Make sure the CrossEntropyLoss ignores the padding tokens.\n",
    "  TARGET_PAD_IDX = -1\n",
    "  criterion = nn.CrossEntropyLoss(ignore_index=TARGET_PAD_IDX)\n",
    "\n",
    "  return model, optimizer, criterion\n",
    "model, optimizer, criterion = create_model(train_src_vocab, train_trg_vocab)\n",
    "print(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, source_vocab, target_vocab, epochs=10) :\n",
    "  model, optimizer, criterion = create_model(source_vocab, target_vocab)\n",
    "\n",
    "  clip = 1\n",
    "\n",
    "  for epoch in range(epochs) :\n",
    "    progress_bar = tqdm(total=len(train_loader.dataset), bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\", unit=\" data\", ncols=200)\n",
    "\n",
    "    train_losses = []\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # Loop through the training batch\n",
    "    for i, batch in enumerate(train_loader) :\n",
    "      # Get the source and target tokens\n",
    "      src = batch[0].permute(1,0)\n",
    "      trg = batch[1].permute(1,0)\n",
    "      print(src.shape, trg.shape)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward propagation\n",
    "      output = model(src, trg)\n",
    "      # Reshape the output\n",
    "      output_dim = output.shape[-1]\n",
    "      # Discard the first token as this will always be 0\n",
    "      output = output[1:].view(-1, output_dim)\n",
    "      # Discard the <sos> token from target\n",
    "      trg = trg[1:].view(-1)\n",
    "      # Calculate the loss\n",
    "      loss = criterion(output, trg)\n",
    "      # Back propagation\n",
    "      loss.backward()\n",
    "      #TODO: Gradient clipping for stability\n",
    "      # Apply gradient descent to model\n",
    "      optimizer.step()\n",
    "\n",
    "      train_losses.append(loss.item())\n",
    "      progress_bar.set_postfix(\n",
    "        epoch=f\" {epoch}, train loss= {round(sum(train_losses)/len(train_losses), 4)}\", refresh=True\n",
    "      )\n",
    "      progress_bar.update()\n",
    "    \n",
    "    # Evaluate model\n",
    "    with torch.no_grad() :\n",
    "      # Set the model to evaluation mode\n",
    "      model.eval()\n",
    "\n",
    "      valid_losses = []\n",
    "\n",
    "      # Loop through the validation batch\n",
    "      for i, batch in enumerate(valid_loader) :\n",
    "        src = batch[0]\n",
    "        trg = batch[1]\n",
    "\n",
    "        # Forward propagation\n",
    "        output = model(src, trg, teacher_forcing_ratio=0)\n",
    "        # Reshape the output\n",
    "        output_dim = output.shape[-1]\n",
    "        # Discard the first token as this will always be 0\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # Discard the <sos> token from target\n",
    "        trg = trg[1:].view(-1)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        valid_losses.append(loss.item())\n",
    "      \n",
    "    progress_bar.set_postfix(\n",
    "      epoch=f\" {epoch}, train loss= {round(sum(train_losses)/len(train_losses), 4)}, val loss= {round(sum(valid_losses)/len(valid_losses), 4)}\", refresh=True\n",
    "    )\n",
    "    progress_bar.close()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bismillah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32]) torch.Size([12, 32])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m CUDA_LAUNCH_BLOCKING\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_src_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trg_vocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, valid_loader, source_vocab, target_vocab, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Reshape the output\u001b[39;00m\n\u001b[1;32m     25\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[0;34m(self, source, target, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, source, target, teacher_forcing_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.5\u001b[39m) :\n\u001b[0;32m----> 8\u001b[0m   encoder_outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec(target, encoder_outputs, hidden, teacher_forcing_ratio)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, input_batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_batch) :\n\u001b[0;32m---> 14\u001b[0m   embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m   outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(embedded)\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m outputs, hidden\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "CUDA_LAUNCH_BLOCKING=\"1\"\n",
    "model = train(train_loader, valid_loader, train_src_vocab, train_trg_vocab, epochs=10)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
