{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn_gen/exp/en\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL =\"dot\"\n",
    "EMB_DIM = \"128\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"50\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"validation_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models_fallback\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  if split_name == \"train+val\" :\n",
    "    print(f\"Merging train and val entries ..\")\n",
    "    with open(os.path.join(DATA_DIR, f\"train.csv\"), encoding=\"utf-8\") as f_train_csv, \\\n",
    "         open(os.path.join(DATA_DIR, f\"val.csv\"), encoding=\"utf-8\") as f_val_csv :\n",
    "      next(f_train_csv, None)\n",
    "      next(f_val_csv, None)\n",
    "      train_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_train_csv]\n",
    "      val_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_val_csv]\n",
    "      pairs = train_pairs + val_pairs\n",
    "      graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "      phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "      g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "    return g2p_dataset, pairs\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging train and val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train+val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[370, 167, 399, 230, 580, 395, 108, 278, 417, 668, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f3ffe42ee50> ([6, 100, 68, 585, 515, 1], [18, 6, 34, 1])\n",
      "([6, 100, 68, 585, 515, 1], [18, 6, 34, 1])\n",
      "([6, 100, 68, 585, 515, 1], [18, 6, 34, 1])\n",
      "train grp 719 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: '-a', 23: '-b', 24: '-c', 25: '-d', 26: '-e', 27: '-f', 28: '-g', 29: '-h', 30: '-i', 31: '-j', 32: '-k', 33: '-l', 34: '-m', 35: '-n', 36: '-o', 37: '-p', 38: '-q', 39: '-r', 40: '-s', 41: '-t', 42: '-u', 43: '-v', 44: '-w', 45: '-y', 46: \"a'\", 47: 'a-', 48: 'aa', 49: 'ab', 50: 'ac', 51: 'ad', 52: 'ae', 53: 'af', 54: 'ag', 55: 'ah', 56: 'ai', 57: 'aj', 58: 'ak', 59: 'al', 60: 'am', 61: 'an', 62: 'ao', 63: 'ap', 64: 'aq', 65: 'ar', 66: 'as', 67: 'at', 68: 'au', 69: 'av', 70: 'aw', 71: 'ax', 72: 'ay', 73: 'az', 74: \"b'\", 75: 'ba', 76: 'bb', 77: 'bc', 78: 'bd', 79: 'be', 80: 'bf', 81: 'bg', 82: 'bh', 83: 'bi', 84: 'bj', 85: 'bk', 86: 'bl', 87: 'bm', 88: 'bn', 89: 'bo', 90: 'bp', 91: 'br', 92: 'bs', 93: 'bt', 94: 'bu', 95: 'bv', 96: 'bw', 97: 'by', 98: 'bz', 99: \"c'\", 100: 'ca', 101: 'cb', 102: 'cc', 103: 'cd', 104: 'ce', 105: 'cf', 106: 'cg', 107: 'ch', 108: 'ci', 109: 'cj', 110: 'ck', 111: 'cl', 112: 'cm', 113: 'cn', 114: 'co', 115: 'cp', 116: 'cq', 117: 'cr', 118: 'cs', 119: 'ct', 120: 'cu', 121: 'cv', 122: 'cw', 123: 'cy', 124: 'cz', 125: \"d'\", 126: 'd-', 127: 'da', 128: 'db', 129: 'dc', 130: 'dd', 131: 'de', 132: 'df', 133: 'dg', 134: 'dh', 135: 'di', 136: 'dj', 137: 'dk', 138: 'dl', 139: 'dm', 140: 'dn', 141: 'do', 142: 'dp', 143: 'dq', 144: 'dr', 145: 'ds', 146: 'dt', 147: 'du', 148: 'dv', 149: 'dw', 150: 'dy', 151: 'dz', 152: \"e'\", 153: 'e-', 154: 'ea', 155: 'eb', 156: 'ec', 157: 'ed', 158: 'ee', 159: 'ef', 160: 'eg', 161: 'eh', 162: 'ei', 163: 'ej', 164: 'ek', 165: 'el', 166: 'em', 167: 'en', 168: 'eo', 169: 'ep', 170: 'eq', 171: 'er', 172: 'es', 173: 'et', 174: 'eu', 175: 'ev', 176: 'ew', 177: 'ex', 178: 'ey', 179: 'ez', 180: \"f'\", 181: 'f-', 182: 'fa', 183: 'fb', 184: 'fc', 185: 'fd', 186: 'fe', 187: 'ff', 188: 'fg', 189: 'fh', 190: 'fi', 191: 'fj', 192: 'fk', 193: 'fl', 194: 'fm', 195: 'fn', 196: 'fo', 197: 'fp', 198: 'fq', 199: 'fr', 200: 'fs', 201: 'ft', 202: 'fu', 203: 'fv', 204: 'fw', 205: 'fx', 206: 'fy', 207: 'fz', 208: \"g'\", 209: 'g-', 210: 'ga', 211: 'gb', 212: 'gc', 213: 'gd', 214: 'ge', 215: 'gf', 216: 'gg', 217: 'gh', 218: 'gi', 219: 'gj', 220: 'gk', 221: 'gl', 222: 'gm', 223: 'gn', 224: 'go', 225: 'gp', 226: 'gq', 227: 'gr', 228: 'gs', 229: 'gt', 230: 'gu', 231: 'gv', 232: 'gw', 233: 'gx', 234: 'gy', 235: 'gz', 236: \"h'\", 237: 'h-', 238: 'ha', 239: 'hb', 240: 'hc', 241: 'hd', 242: 'he', 243: 'hf', 244: 'hg', 245: 'hh', 246: 'hi', 247: 'hj', 248: 'hk', 249: 'hl', 250: 'hm', 251: 'hn', 252: 'ho', 253: 'hp', 254: 'hq', 255: 'hr', 256: 'hs', 257: 'ht', 258: 'hu', 259: 'hv', 260: 'hw', 261: 'hy', 262: 'i', 263: \"i'\", 264: 'i-', 265: 'ia', 266: 'ib', 267: 'ic', 268: 'id', 269: 'ie', 270: 'if', 271: 'ig', 272: 'ih', 273: 'ii', 274: 'ij', 275: 'ik', 276: 'il', 277: 'im', 278: 'in', 279: 'io', 280: 'ip', 281: 'iq', 282: 'ir', 283: 'is', 284: 'it', 285: 'iu', 286: 'iv', 287: 'iw', 288: 'ix', 289: 'iy', 290: 'iz', 291: \"j'\", 292: 'ja', 293: 'jc', 294: 'jd', 295: 'je', 296: 'jf', 297: 'jh', 298: 'ji', 299: 'jj', 300: 'jk', 301: 'jm', 302: 'jn', 303: 'jo', 304: 'jr', 305: 'js', 306: 'jt', 307: 'ju', 308: 'jv', 309: 'jy', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'm-', 366: 'ma', 367: 'mb', 368: 'mc', 369: 'md', 370: 'me', 371: 'mf', 372: 'mg', 373: 'mh', 374: 'mi', 375: 'mj', 376: 'mk', 377: 'ml', 378: 'mm', 379: 'mn', 380: 'mo', 381: 'mp', 382: 'mq', 383: 'mr', 384: 'ms', 385: 'mt', 386: 'mu', 387: 'mv', 388: 'mw', 389: 'my', 390: 'mz', 391: \"n'\", 392: 'n-', 393: 'na', 394: 'nb', 395: 'nc', 396: 'nd', 397: 'ne', 398: 'nf', 399: 'ng', 400: 'nh', 401: 'ni', 402: 'nj', 403: 'nk', 404: 'nl', 405: 'nm', 406: 'nn', 407: 'no', 408: 'np', 409: 'nq', 410: 'nr', 411: 'ns', 412: 'nt', 413: 'nu', 414: 'nv', 415: 'nw', 416: 'nx', 417: 'ny', 418: 'nz', 419: 'o', 420: \"o'\", 421: 'o-', 422: 'oa', 423: 'ob', 424: 'oc', 425: 'od', 426: 'oe', 427: 'of', 428: 'og', 429: 'oh', 430: 'oi', 431: 'oj', 432: 'ok', 433: 'ol', 434: 'om', 435: 'on', 436: 'oo', 437: 'op', 438: 'oq', 439: 'or', 440: 'os', 441: 'ot', 442: 'ou', 443: 'ov', 444: 'ow', 445: 'ox', 446: 'oy', 447: 'oz', 448: \"p'\", 449: 'p-', 450: 'pa', 451: 'pb', 452: 'pc', 453: 'pd', 454: 'pe', 455: 'pf', 456: 'pg', 457: 'ph', 458: 'pi', 459: 'pj', 460: 'pk', 461: 'pl', 462: 'pm', 463: 'pn', 464: 'po', 465: 'pp', 466: 'pr', 467: 'ps', 468: 'pt', 469: 'pu', 470: 'pw', 471: 'py', 472: 'pz', 473: \"q'\", 474: 'qa', 475: 'qb', 476: 'qg', 477: 'qi', 478: 'qo', 479: 'qu', 480: 'qv', 481: \"r'\", 482: 'r-', 483: 'ra', 484: 'rb', 485: 'rc', 486: 'rd', 487: 're', 488: 'rf', 489: 'rg', 490: 'rh', 491: 'ri', 492: 'rj', 493: 'rk', 494: 'rl', 495: 'rm', 496: 'rn', 497: 'ro', 498: 'rp', 499: 'rq', 500: 'rr', 501: 'rs', 502: 'rt', 503: 'ru', 504: 'rv', 505: 'rw', 506: 'rx', 507: 'ry', 508: 'rz', 509: \"s'\", 510: 's-', 511: 'sa', 512: 'sb', 513: 'sc', 514: 'sd', 515: 'se', 516: 'sf', 517: 'sg', 518: 'sh', 519: 'si', 520: 'sj', 521: 'sk', 522: 'sl', 523: 'sm', 524: 'sn', 525: 'so', 526: 'sp', 527: 'sq', 528: 'sr', 529: 'ss', 530: 'st', 531: 'su', 532: 'sv', 533: 'sw', 534: 'sx', 535: 'sy', 536: 'sz', 537: \"t'\", 538: 't-', 539: 'ta', 540: 'tb', 541: 'tc', 542: 'td', 543: 'te', 544: 'tf', 545: 'tg', 546: 'th', 547: 'ti', 548: 'tj', 549: 'tk', 550: 'tl', 551: 'tm', 552: 'tn', 553: 'to', 554: 'tp', 555: 'tq', 556: 'tr', 557: 'ts', 558: 'tt', 559: 'tu', 560: 'tv', 561: 'tw', 562: 'tx', 563: 'ty', 564: 'tz', 565: \"u'\", 566: 'u-', 567: 'ua', 568: 'ub', 569: 'uc', 570: 'ud', 571: 'ue', 572: 'uf', 573: 'ug', 574: 'uh', 575: 'ui', 576: 'uj', 577: 'uk', 578: 'ul', 579: 'um', 580: 'un', 581: 'uo', 582: 'up', 583: 'uq', 584: 'ur', 585: 'us', 586: 'ut', 587: 'uu', 588: 'uv', 589: 'uw', 590: 'ux', 591: 'uy', 592: 'uz', 593: \"v'\", 594: 'va', 595: 'vc', 596: 'vd', 597: 've', 598: 'vg', 599: 'vh', 600: 'vi', 601: 'vj', 602: 'vk', 603: 'vl', 604: 'vm', 605: 'vn', 606: 'vo', 607: 'vr', 608: 'vs', 609: 'vt', 610: 'vu', 611: 'vv', 612: 'vy', 613: 'vz', 614: \"w'\", 615: 'w-', 616: 'wa', 617: 'wb', 618: 'wc', 619: 'wd', 620: 'we', 621: 'wf', 622: 'wg', 623: 'wh', 624: 'wi', 625: 'wk', 626: 'wl', 627: 'wm', 628: 'wn', 629: 'wo', 630: 'wp', 631: 'wq', 632: 'wr', 633: 'ws', 634: 'wt', 635: 'wu', 636: 'wv', 637: 'ww', 638: 'wy', 639: 'wz', 640: \"x'\", 641: 'x-', 642: 'xa', 643: 'xb', 644: 'xc', 645: 'xd', 646: 'xe', 647: 'xf', 648: 'xg', 649: 'xh', 650: 'xi', 651: 'xl', 652: 'xm', 653: 'xn', 654: 'xo', 655: 'xp', 656: 'xq', 657: 'xr', 658: 'xs', 659: 'xt', 660: 'xu', 661: 'xv', 662: 'xw', 663: 'xx', 664: 'xy', 665: 'xz', 666: \"y'\", 667: 'y-', 668: 'ya', 669: 'yb', 670: 'yc', 671: 'yd', 672: 'ye', 673: 'yf', 674: 'yg', 675: 'yh', 676: 'yi', 677: 'yj', 678: 'yk', 679: 'yl', 680: 'ym', 681: 'yn', 682: 'yo', 683: 'yp', 684: 'yq', 685: 'yr', 686: 'ys', 687: 'yt', 688: 'yu', 689: 'yv', 690: 'yw', 691: 'yx', 692: 'yy', 693: 'yz', 694: \"z'\", 695: 'za', 696: 'zb', 697: 'zc', 698: 'zd', 699: 'ze', 700: 'zf', 701: 'zg', 702: 'zh', 703: 'zi', 704: 'zk', 705: 'zl', 706: 'zm', 707: 'zn', 708: 'zo', 709: 'zp', 710: 'zq', 711: 'zr', 712: 'zs', 713: 'zt', 714: 'zu', 715: 'zv', 716: 'zw', 717: 'zy', 718: 'zz'}\n",
      "test grp 719 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: '-a', 23: '-b', 24: '-c', 25: '-d', 26: '-e', 27: '-f', 28: '-g', 29: '-h', 30: '-i', 31: '-j', 32: '-k', 33: '-l', 34: '-m', 35: '-n', 36: '-o', 37: '-p', 38: '-q', 39: '-r', 40: '-s', 41: '-t', 42: '-u', 43: '-v', 44: '-w', 45: '-y', 46: \"a'\", 47: 'a-', 48: 'aa', 49: 'ab', 50: 'ac', 51: 'ad', 52: 'ae', 53: 'af', 54: 'ag', 55: 'ah', 56: 'ai', 57: 'aj', 58: 'ak', 59: 'al', 60: 'am', 61: 'an', 62: 'ao', 63: 'ap', 64: 'aq', 65: 'ar', 66: 'as', 67: 'at', 68: 'au', 69: 'av', 70: 'aw', 71: 'ax', 72: 'ay', 73: 'az', 74: \"b'\", 75: 'ba', 76: 'bb', 77: 'bc', 78: 'bd', 79: 'be', 80: 'bf', 81: 'bg', 82: 'bh', 83: 'bi', 84: 'bj', 85: 'bk', 86: 'bl', 87: 'bm', 88: 'bn', 89: 'bo', 90: 'bp', 91: 'br', 92: 'bs', 93: 'bt', 94: 'bu', 95: 'bv', 96: 'bw', 97: 'by', 98: 'bz', 99: \"c'\", 100: 'ca', 101: 'cb', 102: 'cc', 103: 'cd', 104: 'ce', 105: 'cf', 106: 'cg', 107: 'ch', 108: 'ci', 109: 'cj', 110: 'ck', 111: 'cl', 112: 'cm', 113: 'cn', 114: 'co', 115: 'cp', 116: 'cq', 117: 'cr', 118: 'cs', 119: 'ct', 120: 'cu', 121: 'cv', 122: 'cw', 123: 'cy', 124: 'cz', 125: \"d'\", 126: 'd-', 127: 'da', 128: 'db', 129: 'dc', 130: 'dd', 131: 'de', 132: 'df', 133: 'dg', 134: 'dh', 135: 'di', 136: 'dj', 137: 'dk', 138: 'dl', 139: 'dm', 140: 'dn', 141: 'do', 142: 'dp', 143: 'dq', 144: 'dr', 145: 'ds', 146: 'dt', 147: 'du', 148: 'dv', 149: 'dw', 150: 'dy', 151: 'dz', 152: \"e'\", 153: 'e-', 154: 'ea', 155: 'eb', 156: 'ec', 157: 'ed', 158: 'ee', 159: 'ef', 160: 'eg', 161: 'eh', 162: 'ei', 163: 'ej', 164: 'ek', 165: 'el', 166: 'em', 167: 'en', 168: 'eo', 169: 'ep', 170: 'eq', 171: 'er', 172: 'es', 173: 'et', 174: 'eu', 175: 'ev', 176: 'ew', 177: 'ex', 178: 'ey', 179: 'ez', 180: \"f'\", 181: 'f-', 182: 'fa', 183: 'fb', 184: 'fc', 185: 'fd', 186: 'fe', 187: 'ff', 188: 'fg', 189: 'fh', 190: 'fi', 191: 'fj', 192: 'fk', 193: 'fl', 194: 'fm', 195: 'fn', 196: 'fo', 197: 'fp', 198: 'fq', 199: 'fr', 200: 'fs', 201: 'ft', 202: 'fu', 203: 'fv', 204: 'fw', 205: 'fx', 206: 'fy', 207: 'fz', 208: \"g'\", 209: 'g-', 210: 'ga', 211: 'gb', 212: 'gc', 213: 'gd', 214: 'ge', 215: 'gf', 216: 'gg', 217: 'gh', 218: 'gi', 219: 'gj', 220: 'gk', 221: 'gl', 222: 'gm', 223: 'gn', 224: 'go', 225: 'gp', 226: 'gq', 227: 'gr', 228: 'gs', 229: 'gt', 230: 'gu', 231: 'gv', 232: 'gw', 233: 'gx', 234: 'gy', 235: 'gz', 236: \"h'\", 237: 'h-', 238: 'ha', 239: 'hb', 240: 'hc', 241: 'hd', 242: 'he', 243: 'hf', 244: 'hg', 245: 'hh', 246: 'hi', 247: 'hj', 248: 'hk', 249: 'hl', 250: 'hm', 251: 'hn', 252: 'ho', 253: 'hp', 254: 'hq', 255: 'hr', 256: 'hs', 257: 'ht', 258: 'hu', 259: 'hv', 260: 'hw', 261: 'hy', 262: 'i', 263: \"i'\", 264: 'i-', 265: 'ia', 266: 'ib', 267: 'ic', 268: 'id', 269: 'ie', 270: 'if', 271: 'ig', 272: 'ih', 273: 'ii', 274: 'ij', 275: 'ik', 276: 'il', 277: 'im', 278: 'in', 279: 'io', 280: 'ip', 281: 'iq', 282: 'ir', 283: 'is', 284: 'it', 285: 'iu', 286: 'iv', 287: 'iw', 288: 'ix', 289: 'iy', 290: 'iz', 291: \"j'\", 292: 'ja', 293: 'jc', 294: 'jd', 295: 'je', 296: 'jf', 297: 'jh', 298: 'ji', 299: 'jj', 300: 'jk', 301: 'jm', 302: 'jn', 303: 'jo', 304: 'jr', 305: 'js', 306: 'jt', 307: 'ju', 308: 'jv', 309: 'jy', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'm-', 366: 'ma', 367: 'mb', 368: 'mc', 369: 'md', 370: 'me', 371: 'mf', 372: 'mg', 373: 'mh', 374: 'mi', 375: 'mj', 376: 'mk', 377: 'ml', 378: 'mm', 379: 'mn', 380: 'mo', 381: 'mp', 382: 'mq', 383: 'mr', 384: 'ms', 385: 'mt', 386: 'mu', 387: 'mv', 388: 'mw', 389: 'my', 390: 'mz', 391: \"n'\", 392: 'n-', 393: 'na', 394: 'nb', 395: 'nc', 396: 'nd', 397: 'ne', 398: 'nf', 399: 'ng', 400: 'nh', 401: 'ni', 402: 'nj', 403: 'nk', 404: 'nl', 405: 'nm', 406: 'nn', 407: 'no', 408: 'np', 409: 'nq', 410: 'nr', 411: 'ns', 412: 'nt', 413: 'nu', 414: 'nv', 415: 'nw', 416: 'nx', 417: 'ny', 418: 'nz', 419: 'o', 420: \"o'\", 421: 'o-', 422: 'oa', 423: 'ob', 424: 'oc', 425: 'od', 426: 'oe', 427: 'of', 428: 'og', 429: 'oh', 430: 'oi', 431: 'oj', 432: 'ok', 433: 'ol', 434: 'om', 435: 'on', 436: 'oo', 437: 'op', 438: 'oq', 439: 'or', 440: 'os', 441: 'ot', 442: 'ou', 443: 'ov', 444: 'ow', 445: 'ox', 446: 'oy', 447: 'oz', 448: \"p'\", 449: 'p-', 450: 'pa', 451: 'pb', 452: 'pc', 453: 'pd', 454: 'pe', 455: 'pf', 456: 'pg', 457: 'ph', 458: 'pi', 459: 'pj', 460: 'pk', 461: 'pl', 462: 'pm', 463: 'pn', 464: 'po', 465: 'pp', 466: 'pr', 467: 'ps', 468: 'pt', 469: 'pu', 470: 'pw', 471: 'py', 472: 'pz', 473: \"q'\", 474: 'qa', 475: 'qb', 476: 'qg', 477: 'qi', 478: 'qo', 479: 'qu', 480: 'qv', 481: \"r'\", 482: 'r-', 483: 'ra', 484: 'rb', 485: 'rc', 486: 'rd', 487: 're', 488: 'rf', 489: 'rg', 490: 'rh', 491: 'ri', 492: 'rj', 493: 'rk', 494: 'rl', 495: 'rm', 496: 'rn', 497: 'ro', 498: 'rp', 499: 'rq', 500: 'rr', 501: 'rs', 502: 'rt', 503: 'ru', 504: 'rv', 505: 'rw', 506: 'rx', 507: 'ry', 508: 'rz', 509: \"s'\", 510: 's-', 511: 'sa', 512: 'sb', 513: 'sc', 514: 'sd', 515: 'se', 516: 'sf', 517: 'sg', 518: 'sh', 519: 'si', 520: 'sj', 521: 'sk', 522: 'sl', 523: 'sm', 524: 'sn', 525: 'so', 526: 'sp', 527: 'sq', 528: 'sr', 529: 'ss', 530: 'st', 531: 'su', 532: 'sv', 533: 'sw', 534: 'sx', 535: 'sy', 536: 'sz', 537: \"t'\", 538: 't-', 539: 'ta', 540: 'tb', 541: 'tc', 542: 'td', 543: 'te', 544: 'tf', 545: 'tg', 546: 'th', 547: 'ti', 548: 'tj', 549: 'tk', 550: 'tl', 551: 'tm', 552: 'tn', 553: 'to', 554: 'tp', 555: 'tq', 556: 'tr', 557: 'ts', 558: 'tt', 559: 'tu', 560: 'tv', 561: 'tw', 562: 'tx', 563: 'ty', 564: 'tz', 565: \"u'\", 566: 'u-', 567: 'ua', 568: 'ub', 569: 'uc', 570: 'ud', 571: 'ue', 572: 'uf', 573: 'ug', 574: 'uh', 575: 'ui', 576: 'uj', 577: 'uk', 578: 'ul', 579: 'um', 580: 'un', 581: 'uo', 582: 'up', 583: 'uq', 584: 'ur', 585: 'us', 586: 'ut', 587: 'uu', 588: 'uv', 589: 'uw', 590: 'ux', 591: 'uy', 592: 'uz', 593: \"v'\", 594: 'va', 595: 'vc', 596: 'vd', 597: 've', 598: 'vg', 599: 'vh', 600: 'vi', 601: 'vj', 602: 'vk', 603: 'vl', 604: 'vm', 605: 'vn', 606: 'vo', 607: 'vr', 608: 'vs', 609: 'vt', 610: 'vu', 611: 'vv', 612: 'vy', 613: 'vz', 614: \"w'\", 615: 'w-', 616: 'wa', 617: 'wb', 618: 'wc', 619: 'wd', 620: 'we', 621: 'wf', 622: 'wg', 623: 'wh', 624: 'wi', 625: 'wk', 626: 'wl', 627: 'wm', 628: 'wn', 629: 'wo', 630: 'wp', 631: 'wq', 632: 'wr', 633: 'ws', 634: 'wt', 635: 'wu', 636: 'wv', 637: 'ww', 638: 'wy', 639: 'wz', 640: \"x'\", 641: 'x-', 642: 'xa', 643: 'xb', 644: 'xc', 645: 'xd', 646: 'xe', 647: 'xf', 648: 'xg', 649: 'xh', 650: 'xi', 651: 'xl', 652: 'xm', 653: 'xn', 654: 'xo', 655: 'xp', 656: 'xq', 657: 'xr', 658: 'xs', 659: 'xt', 660: 'xu', 661: 'xv', 662: 'xw', 663: 'xx', 664: 'xy', 665: 'xz', 666: \"y'\", 667: 'y-', 668: 'ya', 669: 'yb', 670: 'yc', 671: 'yd', 672: 'ye', 673: 'yf', 674: 'yg', 675: 'yh', 676: 'yi', 677: 'yj', 678: 'yk', 679: 'yl', 680: 'ym', 681: 'yn', 682: 'yo', 683: 'yp', 684: 'yq', 685: 'yr', 686: 'ys', 687: 'yt', 688: 'yu', 689: 'yv', 690: 'yw', 691: 'yx', 692: 'yy', 693: 'yz', 694: \"z'\", 695: 'za', 696: 'zb', 697: 'zc', 698: 'zd', 699: 'ze', 700: 'zf', 701: 'zg', 702: 'zh', 703: 'zi', 704: 'zk', 705: 'zl', 706: 'zm', 707: 'zn', 708: 'zo', 709: 'zp', 710: 'zq', 711: 'zr', 712: 'zs', 713: 'zt', 714: 'zu', 715: 'zv', 716: 'zw', 717: 'zy', 718: 'zz'}\n",
      "train phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "test phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "715 {\"'c\": 6, 'ca': 100, 'au': 68, 'us': 585, 'se': 515, 'co': 114, 'ou': 442, 'ur': 584, 'rs': 501, \"'e\": 8, 'em': 166, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 479, 'uo': 581, 'ot': 441, 'te': 543, \"'t\": 20, 'ti': 547, 'is': 283, 'aa': 48, 'ab': 49, 'be': 79, 'er': 171, 'rg': 489, 'ac': 50, 'ch': 107, 'he': 242, 'en': 167, 'ak': 58, 'ke': 316, 'al': 59, 'ls': 356, 'et': 173, 'th': 546, 'am': 60, 'mo': 380, 'od': 425, 'dt': 146, 'an': 61, 'nc': 395, 'or': 439, 'ar': 65, 'rd': 486, 'de': 131, 'ma': 366, 'dv': 148, 'va': 594, 'rk': 493, 'ro': 497, 'on': 435, \"n'\": 391, \"'s\": 19, 'ns': 411, 'so': 525, 'rt': 502, 'as': 66, 'ba': 75, 'ha': 238, 'ck': 110, 'cu': 120, 'ad': 51, 'da': 127, 'ka': 312, 'di': 135, 'ai': 56, 'ir': 282, 'lk': 348, 'ki': 320, 'in': 278, 'lo': 352, 'ne': 397, 'os': 440, 'nd': 396, 'do': 141, 'ni': 401, 'ng': 399, 'nm': 405, 'me': 370, 'nt': 412, 'ts': 557, 'to': 553, 'rc': 485, 're': 487, 'sc': 513, 'sh': 518, 'ed': 157, 'at': 67, 'es': 172, 'bb': 76, 'si': 519, 'ie': 269, 'el': 165, 'll': 349, 'nh': 400, 'tt': 558, 'ev': 175, 'vi': 600, 'il': 276, 'le': 342, 'ey': 178, \"y'\": 666, 'bi': 83, 'it': 284, 'bo': 89, \"t'\": 537, 'ud': 570, 'br': 91, 'ia': 265, 'io': 279, 'bs': 92, 'by': 97, 'bc': 77, 'ek': 164, 'bd': 78, 'la': 338, 'ah': 55, 'ic': 267, 'dn': 140, 'no': 407, 'ol': 433, 'om': 434, 'mi': 374, 'na': 393, 'du': 147, 'uc': 569, 'ct': 119, 'ee': 158, 'ul': 578, 'az': 73, 'zi': 703, 'iz': 290, 'ln': 351, 'ow': 444, 'dr': 144, 'cr': 117, 'mb': 367, 'rl': 494, 'rm': 495, 'rr': 500, 'ra': 483, 'ya': 668, 'ce': 104, 'yt': 687, 'ta': 539, 'bh': 82, 'ho': 252, 'id': 268, 'ig': 271, 'ga': 210, 'li': 346, 'ty': 563, 'gt': 229, \"a'\": 46, 'ib': 266, 'tz': 564, 'bj': 84, 'je': 295, 'ec': 156, 'bk': 85, 'kh': 319, 'bl': 86, 'ze': 699, 'e-': 153, '-b': 23, 'st': 530, 'oo': 436, 'ly': 362, 'bn': 88, \"o'\": 420, 'oa': 422, 'hi': 246, 'sm': 523, 'ri': 491, 'gi': 218, 'rn': 496, 'if': 270, 'fa': 182, 'ci': 108, 'iv': 286, 've': 597, 'uh': 574, 'im': 277, 'un': 580, 'ds': 145, 'ut': 586, 'ov': 443, \"e'\": 152, 'bp': 90, 'pl': 461, 'lp': 353, 'ms': 384, 'mc': 368, 'cz': 124, 'zy': 717, 'yk': 678, 'wi': 624, 'ea': 154, 'eg': 160, 'go': 224, 'dg': 133, 'ge': 214, 'og': 428, 'ru': 503, 'up': 582, 'pt': 468, 'tl': 550, 'tn': 552, 'ss': 529, 'yn': 681, 'uz': 592, 'zz': 718, 'zo': 708, 'sa': 511, 'ei': 162, 'lu': 358, 'lv': 359, 'rb': 484, 'rp': 498, 'tr': 556, 'su': 531, 'dl': 138, 'bt': 93, 'bu': 94, 'hm': 250, 'dz': 151, 'ys': 686, 'cs': 118, 'my': 389, 'mp': 381, 'po': 464, 'ap': 63, 'pu': 469, 'lc': 340, 'cc': 102, 'tu': 559, 'ua': 567, 'ep': 169, 'ry': 507, 't-': 538, '-p': 37, 'pr': 466, 'cl': 111, 'mm': 378, 'pa': 450, 'ny': 417, 'yi': 676, \"r'\": 481, 'gl': 221, 'cy': 123, \"s'\": 509, 'ps': 467, 'ue': 571, 'ui': 575, 'um': 579, 'mu': 386, 'ay': 72, 'op': 437, 'ph': 457, 'yl': 679, 'eb': 155, 'nb': 394, 'ht': 257, 'hy': 261, 'fi': 190, 'fy': 206, 'kl': 323, 'km': 324, 'kn': 325, 'wl': 626, 'gm': 222, 'kr': 328, 'oy': 446, 'yd': 671, \"d'\": 125, 'cm': 112, 'cn': 113, 'oc': 424, 'of': 427, 'ff': 187, 'cq': 116, 'av': 69, 'ag': 54, 'ob': 423, 'ym': 680, 'tm': 551, 'uf': 572, 'sy': 535, 'yc': 670, \"m'\": 364, 'ik': 275, 'za': 695, 'sk': 521, 'wn': 628, 'ex': 177, 'dc': 129, 'dd': 130, 'eo': 168, 'ks': 329, 'dw': 149, 'we': 620, 'dy': 150, 'lb': 339, \"l'\": 336, 'lm': 350, 'nn': 406, 'sb': 512, 'oi': 430, 'eq': 170, 'rh': 490, 'ld': 341, 'lt': 357, 'dh': 134, 'eu': 174, 'lf': 343, 'ip': 280, 'dj': 136, 'ja': 292, 'jo': 303, 'ju': 307, 'dk': 137, 'dm': 139, \"f'\": 180, 'fo': 196, 'hs': 256, '-f': 27, '-m': 34, '-s': 40, 'gn': 223, 'ae': 52, 'sd': 514, \"g'\": 208, 'vo': 606, 'ft': 201, 'oq': 438, \"h'\": 236, 'sp': 526, 'hl': 249, \"p'\": 448, 'af': 53, 'ye': 672, 'fe': 186, 'ix': 288, 'xe': 646, 'fl': 193, 'fr': 199, 'ax': 71, 'fg': 188, 'gh': 217, \"i'\": 263, 'fh': 189, 'ox': 445, 'xi': 650, 'fm': 194, 'fs': 200, 'ef': 159, 'ug': 573, 'rw': 505, 'wa': 616, 'mn': 379, 'gy': 234, 'pe': 454, 'gc': 212, 'gf': 215, 'gg': 216, 'gr': 227, 'iu': 285, 'ew': 176, 'aw': 70, 'xc': 644, 'fu': 202, 'pp': 465, 'ok': 432, 'ko': 326, 'gu': 230, 'hh': 245, 'lg': 344, 'lq': 354, 'hn': 251, 'hr': 255, 'hu': 258, 'ml': 377, 'sl': 522, 'sw': 533, 'wo': 629, \"c'\": 99, 'gs': 228, 'rf': 488, 'nk': 403, 'tc': 541, 'iw': 287, 'aj': 57, \"x'\": 640, 'ji': 298, 'kb': 313, 'kc': 314, 'iy': 289, 'kk': 322, 'ky': 334, 'kz': 335, 'tv': 560, 'uq': 583, 'rq': 499, 'oh': 429, 'eh': 161, 'ej': 163, 'xa': 642, 'xo': 654, 'xy': 664, 'nq': 409, 'lh': 345, 'nz': 418, 'ij': 274, 'iq': 281, 'l-': 337, '-i': 30, 'n-': 392, '-o': 36, 'nw': 415, 'yw': 690, 'nu': 413, 'lr': 355, 'pi': 458, 'uv': 588, 'lw': 360, 'ez': 179, 'mq': 382, 'fn': 195, \"k'\": 310, 'tf': 544, 'zh': 702, 'wy': 638, 'lz': 363, 'ku': 331, 'np': 408, 'xt': 659, 'md': 369, 'zc': 697, 'zq': 710, 'mf': 371, 'mg': 372, 'mh': 373, 'yv': 689, 'oe': 426, 'pc': 452, 'mr': 383, 'mt': 385, 'mv': 387, 'mw': 388, 'yo': 682, 'yx': 691, 'yz': 693, 'sq': 527, 'nv': 414, 'oz': 447, 'ws': 633, 'uj': 576, 'cd': 103, 'nf': 398, 'nj': 402, 'ih': 272, 'nr': 410, 'sg': 517, 'i-': 264, 'ub': 568, 'bm': 87, 'tk': 549, \"u'\": 565, 'tw': 561, 'nx': 416, 'yb': 669, 'yh': 675, 'yp': 683, 'wh': 623, 'ao': 62, 'pf': 455, 'pg': 456, 'pk': 460, 'aq': 64, 'qa': 474, 'rv': 504, 'ux': 590, 'hb': 239, 'hd': 241, 'uk': 577, 'zm': 706, 'rj': 492, 'kw': 333, 'wr': 632, 'd-': 126, '-t': 41, '-c': 24, \"w'\": 614, 'zt': 713, 'zu': 714, 'rx': 506, 'rz': 508, 'ii': 273, 'hc': 240, 'hf': 243, 'hw': 260, \"v'\": 593, 'pn': 463, 'yr': 685, 'fd': 185, \"'h\": 10, '-l': 33, 'uy': 591, 'vd': 596, 'vn': 605, 'vr': 607, 'vt': 609, 'wb': 617, 'wf': 621, 'wk': 625, 'wt': 634, 'xf': 647, 'xl': 651, 'xs': 658, 'yg': 674, 'yu': 688, 'yy': 692, 'zb': 696, \"b'\": 74, \"'r\": 18, 'hk': 248, 'k-': 311, '-a': 22, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'kt': 330, 'y-': 667, 'tj': 548, 'gb': 211, 'gd': 213, 'gp': 225, 'gw': 232, \"'i\": 11, 'sz': 536, 'gk': 220, 'kv': 332, 'xq': 656, 'fk': 192, 'sv': 532, 'vs': 608, 'wm': 627, 'sn': 524, 'sf': 516, 'tb': 540, 'sr': 528, 'td': 542, 'hg': 244, 'uw': 589, 'wd': 619, 'zl': 705, 'cv': 121, 'db': 128, 'df': 132, 'dp': 142, 'vu': 610, 'zr': 711, 'nl': 404, 'jy': 309, \"z'\": 694, 'wu': 635, 'gq': 226, '-k': 32, 'hp': 253, 'vy': 612, 'zd': 698, 'zn': 707, 'lj': 347, 'fb': 183, 'xu': 660, 'xb': 643, 'kj': 321, 'mk': 376, '-r': 39, '-g': 28, '-v': 43, 'bz': 98, 'tg': 545, 'sj': 520, 'oj': 431, 'gj': 219, 'qi': 477, 'wc': 618, 'xw': 662, 'xx': 663, 'yf': 673, 'jd': 294, '-n': 35, 'zk': 704, 'tp': 554, 'fc': 184, '-d': 25, 'r-': 482, 'uu': 587, '-u': 42, 'py': 471, 'zw': 716, 'pb': 451, 'pw': 470, \"q'\": 473, \"'v\": 21, 'jk': 300, 'pd': 453, 'pm': 462, 'gx': 233, 'jn': 302, 'zp': 709, 'o-': 421, 'bw': 96, '-e': 26, 'wg': 622, 'zf': 700, 's-': 510, 'vl': 603, 'g-': 209, 'cw': 122, \"'a\": 4, \"'o\": 16, 'hv': 259, 'vc': 595, 'zs': 712, 'mj': 375, 'xh': 649, 'vv': 611, 'xv': 661, 'mz': 390, 'bf': 80, 'hq': 254, 'dq': 143, 'lx': 361, '-h': 29, \"'d\": 7, 'vj': 601, 'x-': 641, '-w': 44, 'xn': 653, 'xp': 655, 'jv': 308, 'zg': 701, '-y': 45, 'fj': 191, 'jt': 306, 'w-': 615, 'xg': 648, 'xm': 652, 'tx': 562, 'gz': 235, 'gv': 231, 'jj': 299, 'f-': 181, 'fw': 204, 'wv': 636, \"'l\": 13, 'h-': 237, 'hj': 247, 'fp': 197, 'js': 305, 'vh': 599, 'wz': 639, 'i': 262, 'qb': 475, 'qg': 476, 'zv': 715, 'jf': 296, 'jh': 297, 'jc': 293, 'wp': 630, 'bv': 95, 'pz': 472, 'fq': 198, 'vg': 598, 'cb': 101, 'cf': 105, 'cg': 106, 'cp': 115, \"j'\": 291, 'jr': 304, 'xz': 665, 'cj': 109, 'fx': 205, 'fz': 207, 'qv': 480, 'wq': 631, 'ww': 637, 'xr': 657, 'xd': 645, 'o': 419, \"'b\": 5, \"'g\": 9, \"'k\": 12, 'a-': 47, '-q': 38, 'vk': 602, 'qo': 478, 'vz': 613, 'jm': 301, 'yj': 677, 'pj': 459, 'p-': 449, 'fv': 203, 'bg': 81, 'u-': 566, 'sx': 534, 'm-': 365, 'yq': 684, 'vm': 604, 'tq': 555, '-j': 31}\n",
      "715 {\"'c\": 6, 'ca': 100, 'au': 68, 'us': 585, 'se': 515, 'co': 114, 'ou': 442, 'ur': 584, 'rs': 501, \"'e\": 8, 'em': 166, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 479, 'uo': 581, 'ot': 441, 'te': 543, \"'t\": 20, 'ti': 547, 'is': 283, 'aa': 48, 'ab': 49, 'be': 79, 'er': 171, 'rg': 489, 'ac': 50, 'ch': 107, 'he': 242, 'en': 167, 'ak': 58, 'ke': 316, 'al': 59, 'ls': 356, 'et': 173, 'th': 546, 'am': 60, 'mo': 380, 'od': 425, 'dt': 146, 'an': 61, 'nc': 395, 'or': 439, 'ar': 65, 'rd': 486, 'de': 131, 'ma': 366, 'dv': 148, 'va': 594, 'rk': 493, 'ro': 497, 'on': 435, \"n'\": 391, \"'s\": 19, 'ns': 411, 'so': 525, 'rt': 502, 'as': 66, 'ba': 75, 'ha': 238, 'ck': 110, 'cu': 120, 'ad': 51, 'da': 127, 'ka': 312, 'di': 135, 'ai': 56, 'ir': 282, 'lk': 348, 'ki': 320, 'in': 278, 'lo': 352, 'ne': 397, 'os': 440, 'nd': 396, 'do': 141, 'ni': 401, 'ng': 399, 'nm': 405, 'me': 370, 'nt': 412, 'ts': 557, 'to': 553, 'rc': 485, 're': 487, 'sc': 513, 'sh': 518, 'ed': 157, 'at': 67, 'es': 172, 'bb': 76, 'si': 519, 'ie': 269, 'el': 165, 'll': 349, 'nh': 400, 'tt': 558, 'ev': 175, 'vi': 600, 'il': 276, 'le': 342, 'ey': 178, \"y'\": 666, 'bi': 83, 'it': 284, 'bo': 89, \"t'\": 537, 'ud': 570, 'br': 91, 'ia': 265, 'io': 279, 'bs': 92, 'by': 97, 'bc': 77, 'ek': 164, 'bd': 78, 'la': 338, 'ah': 55, 'ic': 267, 'dn': 140, 'no': 407, 'ol': 433, 'om': 434, 'mi': 374, 'na': 393, 'du': 147, 'uc': 569, 'ct': 119, 'ee': 158, 'ul': 578, 'az': 73, 'zi': 703, 'iz': 290, 'ln': 351, 'ow': 444, 'dr': 144, 'cr': 117, 'mb': 367, 'rl': 494, 'rm': 495, 'rr': 500, 'ra': 483, 'ya': 668, 'ce': 104, 'yt': 687, 'ta': 539, 'bh': 82, 'ho': 252, 'id': 268, 'ig': 271, 'ga': 210, 'li': 346, 'ty': 563, 'gt': 229, \"a'\": 46, 'ib': 266, 'tz': 564, 'bj': 84, 'je': 295, 'ec': 156, 'bk': 85, 'kh': 319, 'bl': 86, 'ze': 699, 'e-': 153, '-b': 23, 'st': 530, 'oo': 436, 'ly': 362, 'bn': 88, \"o'\": 420, 'oa': 422, 'hi': 246, 'sm': 523, 'ri': 491, 'gi': 218, 'rn': 496, 'if': 270, 'fa': 182, 'ci': 108, 'iv': 286, 've': 597, 'uh': 574, 'im': 277, 'un': 580, 'ds': 145, 'ut': 586, 'ov': 443, \"e'\": 152, 'bp': 90, 'pl': 461, 'lp': 353, 'ms': 384, 'mc': 368, 'cz': 124, 'zy': 717, 'yk': 678, 'wi': 624, 'ea': 154, 'eg': 160, 'go': 224, 'dg': 133, 'ge': 214, 'og': 428, 'ru': 503, 'up': 582, 'pt': 468, 'tl': 550, 'tn': 552, 'ss': 529, 'yn': 681, 'uz': 592, 'zz': 718, 'zo': 708, 'sa': 511, 'ei': 162, 'lu': 358, 'lv': 359, 'rb': 484, 'rp': 498, 'tr': 556, 'su': 531, 'dl': 138, 'bt': 93, 'bu': 94, 'hm': 250, 'dz': 151, 'ys': 686, 'cs': 118, 'my': 389, 'mp': 381, 'po': 464, 'ap': 63, 'pu': 469, 'lc': 340, 'cc': 102, 'tu': 559, 'ua': 567, 'ep': 169, 'ry': 507, 't-': 538, '-p': 37, 'pr': 466, 'cl': 111, 'mm': 378, 'pa': 450, 'ny': 417, 'yi': 676, \"r'\": 481, 'gl': 221, 'cy': 123, \"s'\": 509, 'ps': 467, 'ue': 571, 'ui': 575, 'um': 579, 'mu': 386, 'ay': 72, 'op': 437, 'ph': 457, 'yl': 679, 'eb': 155, 'nb': 394, 'ht': 257, 'hy': 261, 'fi': 190, 'fy': 206, 'kl': 323, 'km': 324, 'kn': 325, 'wl': 626, 'gm': 222, 'kr': 328, 'oy': 446, 'yd': 671, \"d'\": 125, 'cm': 112, 'cn': 113, 'oc': 424, 'of': 427, 'ff': 187, 'cq': 116, 'av': 69, 'ag': 54, 'ob': 423, 'ym': 680, 'tm': 551, 'uf': 572, 'sy': 535, 'yc': 670, \"m'\": 364, 'ik': 275, 'za': 695, 'sk': 521, 'wn': 628, 'ex': 177, 'dc': 129, 'dd': 130, 'eo': 168, 'ks': 329, 'dw': 149, 'we': 620, 'dy': 150, 'lb': 339, \"l'\": 336, 'lm': 350, 'nn': 406, 'sb': 512, 'oi': 430, 'eq': 170, 'rh': 490, 'ld': 341, 'lt': 357, 'dh': 134, 'eu': 174, 'lf': 343, 'ip': 280, 'dj': 136, 'ja': 292, 'jo': 303, 'ju': 307, 'dk': 137, 'dm': 139, \"f'\": 180, 'fo': 196, 'hs': 256, '-f': 27, '-m': 34, '-s': 40, 'gn': 223, 'ae': 52, 'sd': 514, \"g'\": 208, 'vo': 606, 'ft': 201, 'oq': 438, \"h'\": 236, 'sp': 526, 'hl': 249, \"p'\": 448, 'af': 53, 'ye': 672, 'fe': 186, 'ix': 288, 'xe': 646, 'fl': 193, 'fr': 199, 'ax': 71, 'fg': 188, 'gh': 217, \"i'\": 263, 'fh': 189, 'ox': 445, 'xi': 650, 'fm': 194, 'fs': 200, 'ef': 159, 'ug': 573, 'rw': 505, 'wa': 616, 'mn': 379, 'gy': 234, 'pe': 454, 'gc': 212, 'gf': 215, 'gg': 216, 'gr': 227, 'iu': 285, 'ew': 176, 'aw': 70, 'xc': 644, 'fu': 202, 'pp': 465, 'ok': 432, 'ko': 326, 'gu': 230, 'hh': 245, 'lg': 344, 'lq': 354, 'hn': 251, 'hr': 255, 'hu': 258, 'ml': 377, 'sl': 522, 'sw': 533, 'wo': 629, \"c'\": 99, 'gs': 228, 'rf': 488, 'nk': 403, 'tc': 541, 'iw': 287, 'aj': 57, \"x'\": 640, 'ji': 298, 'kb': 313, 'kc': 314, 'iy': 289, 'kk': 322, 'ky': 334, 'kz': 335, 'tv': 560, 'uq': 583, 'rq': 499, 'oh': 429, 'eh': 161, 'ej': 163, 'xa': 642, 'xo': 654, 'xy': 664, 'nq': 409, 'lh': 345, 'nz': 418, 'ij': 274, 'iq': 281, 'l-': 337, '-i': 30, 'n-': 392, '-o': 36, 'nw': 415, 'yw': 690, 'nu': 413, 'lr': 355, 'pi': 458, 'uv': 588, 'lw': 360, 'ez': 179, 'mq': 382, 'fn': 195, \"k'\": 310, 'tf': 544, 'zh': 702, 'wy': 638, 'lz': 363, 'ku': 331, 'np': 408, 'xt': 659, 'md': 369, 'zc': 697, 'zq': 710, 'mf': 371, 'mg': 372, 'mh': 373, 'yv': 689, 'oe': 426, 'pc': 452, 'mr': 383, 'mt': 385, 'mv': 387, 'mw': 388, 'yo': 682, 'yx': 691, 'yz': 693, 'sq': 527, 'nv': 414, 'oz': 447, 'ws': 633, 'uj': 576, 'cd': 103, 'nf': 398, 'nj': 402, 'ih': 272, 'nr': 410, 'sg': 517, 'i-': 264, 'ub': 568, 'bm': 87, 'tk': 549, \"u'\": 565, 'tw': 561, 'nx': 416, 'yb': 669, 'yh': 675, 'yp': 683, 'wh': 623, 'ao': 62, 'pf': 455, 'pg': 456, 'pk': 460, 'aq': 64, 'qa': 474, 'rv': 504, 'ux': 590, 'hb': 239, 'hd': 241, 'uk': 577, 'zm': 706, 'rj': 492, 'kw': 333, 'wr': 632, 'd-': 126, '-t': 41, '-c': 24, \"w'\": 614, 'zt': 713, 'zu': 714, 'rx': 506, 'rz': 508, 'ii': 273, 'hc': 240, 'hf': 243, 'hw': 260, \"v'\": 593, 'pn': 463, 'yr': 685, 'fd': 185, \"'h\": 10, '-l': 33, 'uy': 591, 'vd': 596, 'vn': 605, 'vr': 607, 'vt': 609, 'wb': 617, 'wf': 621, 'wk': 625, 'wt': 634, 'xf': 647, 'xl': 651, 'xs': 658, 'yg': 674, 'yu': 688, 'yy': 692, 'zb': 696, \"b'\": 74, \"'r\": 18, 'hk': 248, 'k-': 311, '-a': 22, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'kt': 330, 'y-': 667, 'tj': 548, 'gb': 211, 'gd': 213, 'gp': 225, 'gw': 232, \"'i\": 11, 'sz': 536, 'gk': 220, 'kv': 332, 'xq': 656, 'fk': 192, 'sv': 532, 'vs': 608, 'wm': 627, 'sn': 524, 'sf': 516, 'tb': 540, 'sr': 528, 'td': 542, 'hg': 244, 'uw': 589, 'wd': 619, 'zl': 705, 'cv': 121, 'db': 128, 'df': 132, 'dp': 142, 'vu': 610, 'zr': 711, 'nl': 404, 'jy': 309, \"z'\": 694, 'wu': 635, 'gq': 226, '-k': 32, 'hp': 253, 'vy': 612, 'zd': 698, 'zn': 707, 'lj': 347, 'fb': 183, 'xu': 660, 'xb': 643, 'kj': 321, 'mk': 376, '-r': 39, '-g': 28, '-v': 43, 'bz': 98, 'tg': 545, 'sj': 520, 'oj': 431, 'gj': 219, 'qi': 477, 'wc': 618, 'xw': 662, 'xx': 663, 'yf': 673, 'jd': 294, '-n': 35, 'zk': 704, 'tp': 554, 'fc': 184, '-d': 25, 'r-': 482, 'uu': 587, '-u': 42, 'py': 471, 'zw': 716, 'pb': 451, 'pw': 470, \"q'\": 473, \"'v\": 21, 'jk': 300, 'pd': 453, 'pm': 462, 'gx': 233, 'jn': 302, 'zp': 709, 'o-': 421, 'bw': 96, '-e': 26, 'wg': 622, 'zf': 700, 's-': 510, 'vl': 603, 'g-': 209, 'cw': 122, \"'a\": 4, \"'o\": 16, 'hv': 259, 'vc': 595, 'zs': 712, 'mj': 375, 'xh': 649, 'vv': 611, 'xv': 661, 'mz': 390, 'bf': 80, 'hq': 254, 'dq': 143, 'lx': 361, '-h': 29, \"'d\": 7, 'vj': 601, 'x-': 641, '-w': 44, 'xn': 653, 'xp': 655, 'jv': 308, 'zg': 701, '-y': 45, 'fj': 191, 'jt': 306, 'w-': 615, 'xg': 648, 'xm': 652, 'tx': 562, 'gz': 235, 'gv': 231, 'jj': 299, 'f-': 181, 'fw': 204, 'wv': 636, \"'l\": 13, 'h-': 237, 'hj': 247, 'fp': 197, 'js': 305, 'vh': 599, 'wz': 639, 'i': 262, 'qb': 475, 'qg': 476, 'zv': 715, 'jf': 296, 'jh': 297, 'jc': 293, 'wp': 630, 'bv': 95, 'pz': 472, 'fq': 198, 'vg': 598, 'cb': 101, 'cf': 105, 'cg': 106, 'cp': 115, \"j'\": 291, 'jr': 304, 'xz': 665, 'cj': 109, 'fx': 205, 'fz': 207, 'qv': 480, 'wq': 631, 'ww': 637, 'xr': 657, 'xd': 645, 'o': 419, \"'b\": 5, \"'g\": 9, \"'k\": 12, 'a-': 47, '-q': 38, 'vk': 602, 'qo': 478, 'vz': 613, 'jm': 301, 'yj': 677, 'pj': 459, 'p-': 449, 'fv': 203, 'bg': 81, 'u-': 566, 'sx': 534, 'm-': 365, 'yq': 684, 'vm': 604, 'tq': 555, '-j': 31}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 128\n",
      "hidden_size: 50\n",
      "n_layers: 1\n",
      "Encoder has a total number of 119032 parameters\n",
      "Decoder has a total number of 42515 parameters\n",
      "Total number of all parameters is 161547\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 1 finished in 0m 34.49s (- 56m 54.07s) (1 1.0%). train avg loss: 1.6724\n",
      "Training for epoch 2 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 2 finished in 1m 10.67s (- 57m 42.59s) (2 2.0%). train avg loss: 0.8205\n",
      "Training for epoch 3 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 3 finished in 1m 47.4s (- 57m 52.55s) (3 3.0%). train avg loss: 0.5952\n",
      "Training for epoch 4 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 4 finished in 2m 23.75s (- 57m 30.01s) (4 4.0%). train avg loss: 0.4968\n",
      "Training for epoch 5 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 5 finished in 3m 5.9s (- 58m 52.14s) (5 5.0%). train avg loss: 0.441\n",
      "Training for epoch 6 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 6 finished in 3m 40.58s (- 57m 35.78s) (6 6.0%). train avg loss: 0.4171\n",
      "Training for epoch 7 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 7 finished in 4m 15.07s (- 56m 28.8s) (7 7.0%). train avg loss: 0.3911\n",
      "Training for epoch 8 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 8 finished in 4m 50.33s (- 55m 38.78s) (8 8.0%). train avg loss: 0.363\n",
      "Training for epoch 9 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 9 finished in 5m 25.12s (- 54m 47.3s) (9 9.0%). train avg loss: 0.3669\n",
      "Training for epoch 10 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 10 finished in 6m 0.42s (- 54m 3.81s) (10 10.0%). train avg loss: 0.3525\n",
      "Training for epoch 11 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 11 finished in 6m 36.94s (- 53m 31.61s) (11 11.0%). train avg loss: 0.348\n",
      "Training for epoch 12 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 12 finished in 7m 11.4s (- 52m 43.59s) (12 12.0%). train avg loss: 0.3259\n",
      "Training for epoch 13 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 13 finished in 7m 47.83s (- 52m 10.89s) (13 13.0%). train avg loss: 0.314\n",
      "Training for epoch 14 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 14 finished in 8m 23.72s (- 51m 34.27s) (14 14.0%). train avg loss: 0.3064\n",
      "Training for epoch 15 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 15 finished in 8m 56.18s (- 50m 38.33s) (15 15.0%). train avg loss: 0.2953\n",
      "Training for epoch 16 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 16 finished in 9m 28.02s (- 49m 42.11s) (16 16.0%). train avg loss: 0.299\n",
      "Training for epoch 17 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 17 finished in 9m 59.59s (- 48m 47.41s) (17 17.0%). train avg loss: 0.2856\n",
      "Training for epoch 18 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 18 finished in 10m 31.16s (- 47m 55.27s) (18 18.0%). train avg loss: 0.2948\n",
      "Training for epoch 19 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 19 finished in 11m 2.35s (- 47m 3.72s) (19 19.0%). train avg loss: 0.2741\n",
      "Training for epoch 20 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 20 finished in 11m 34.25s (- 46m 16.99s) (20 20.0%). train avg loss: 0.278\n",
      "Training for epoch 21 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 21 finished in 12m 5.91s (- 45m 30.8s) (21 21.0%). train avg loss: 0.2694\n",
      "Training for epoch 22 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 22 finished in 12m 38.1s (- 44m 47.82s) (22 22.0%). train avg loss: 0.2737\n",
      "Training for epoch 23 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 23 finished in 13m 9.86s (- 44m 4.31s) (23 23.0%). train avg loss: 0.2688\n",
      "Training for epoch 24 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 24 finished in 13m 42.07s (- 43m 23.23s) (24 24.0%). train avg loss: 0.2738\n",
      "Training for epoch 25 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 25 finished in 14m 14.15s (- 42m 42.45s) (25 25.0%). train avg loss: 0.251\n",
      "Training for epoch 26 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 26 finished in 14m 45.78s (- 42m 1.06s) (26 26.0%). train avg loss: 0.2478\n",
      "Training for epoch 27 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 27 finished in 15m 17.45s (- 41m 20.51s) (27 27.0%). train avg loss: 0.254\n",
      "Training for epoch 28 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 28 finished in 15m 49.25s (- 40m 40.93s) (28 28.0%). train avg loss: 0.2592\n",
      "Training for epoch 29 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 29 finished in 16m 21.3s (- 40m 2.49s) (29 29.0%). train avg loss: 0.2662\n",
      "Training for epoch 30 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 30 finished in 16m 53.56s (- 39m 24.97s) (30 30.0%). train avg loss: 0.2357\n",
      "Training for epoch 31 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 31 finished in 17m 25.44s (- 38m 46.94s) (31 31.0%). train avg loss: 0.2402\n",
      "Training for epoch 32 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 32 finished in 17m 57.64s (- 38m 9.98s) (32 32.0%). train avg loss: 0.233\n",
      "Training for epoch 33 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 33 finished in 18m 29.4s (- 37m 32.42s) (33 33.0%). train avg loss: 0.2403\n",
      "Training for epoch 34 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 34 finished in 19m 1.17s (- 36m 55.22s) (34 34.0%). train avg loss: 0.2358\n",
      "Training for epoch 35 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 35 finished in 19m 33.1s (- 36m 18.62s) (35 35.0%). train avg loss: 0.2466\n",
      "Training for epoch 36 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 36 finished in 20m 4.95s (- 35m 42.14s) (36 36.0%). train avg loss: 0.2337\n",
      "Training for epoch 37 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 37 finished in 20m 36.84s (- 35m 5.96s) (37 37.0%). train avg loss: 0.2282\n",
      "Training for epoch 38 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 38 finished in 21m 8.74s (- 34m 30.04s) (38 38.0%). train avg loss: 0.226\n",
      "Training for epoch 39 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 39 finished in 21m 40.36s (- 33m 53.89s) (39 39.0%). train avg loss: 0.2297\n",
      "Training for epoch 40 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 40 finished in 22m 12.27s (- 33m 18.41s) (40 40.0%). train avg loss: 0.2234\n",
      "Training for epoch 41 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 41 finished in 22m 43.96s (- 32m 42.77s) (41 41.0%). train avg loss: 0.2127\n",
      "Training for epoch 42 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 42 finished in 23m 15.56s (- 32m 7.21s) (42 42.0%). train avg loss: 0.2208\n",
      "Training for epoch 43 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 43 finished in 23m 46.82s (- 31m 31.36s) (43 43.0%). train avg loss: 0.2237\n",
      "Training for epoch 44 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 44 finished in 24m 20.9s (- 30m 59.33s) (44 44.0%). train avg loss: 0.2122\n",
      "Training for epoch 45 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 45 finished in 24m 52.74s (- 30m 24.47s) (45 45.0%). train avg loss: 0.2222\n",
      "Training for epoch 46 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 46 finished in 25m 23.91s (- 29m 48.94s) (46 46.0%). train avg loss: 0.21\n",
      "Training for epoch 47 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 47 finished in 25m 54.84s (- 29m 13.33s) (47 47.0%). train avg loss: 0.2115\n",
      "Training for epoch 48 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 48 finished in 26m 25.7s (- 28m 37.84s) (48 48.0%). train avg loss: 0.2157\n",
      "Training for epoch 49 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 49 finished in 26m 57.02s (- 28m 3.02s) (49 49.0%). train avg loss: 0.2032\n",
      "Training for epoch 50 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 50 finished in 27m 28.54s (- 27m 28.54s) (50 50.0%). train avg loss: 0.213\n",
      "Training for epoch 51 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 51 finished in 27m 59.72s (- 26m 53.85s) (51 51.0%). train avg loss: 0.2083\n",
      "Training for epoch 52 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 52 finished in 28m 30.87s (- 26m 19.26s) (52 52.0%). train avg loss: 0.2004\n",
      "Training for epoch 53 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 53 finished in 29m 1.77s (- 25m 44.59s) (53 53.0%). train avg loss: 0.2107\n",
      "Training for epoch 54 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 54 finished in 29m 32.39s (- 25m 9.81s) (54 54.0%). train avg loss: 0.1939\n",
      "Training for epoch 55 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 55 finished in 30m 3.22s (- 24m 35.36s) (55 55.0%). train avg loss: 0.2044\n",
      "Training for epoch 56 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 56 finished in 30m 34.69s (- 24m 1.54s) (56 56.0%). train avg loss: 0.206\n",
      "Training for epoch 57 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 57 finished in 31m 6.12s (- 23m 27.77s) (57 57.0%). train avg loss: 0.1978\n",
      "Training for epoch 58 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 58 finished in 31m 37.48s (- 22m 54.04s) (58 58.0%). train avg loss: 0.2059\n",
      "Training for epoch 59 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 59 finished in 32m 8.71s (- 22m 20.29s) (59 59.0%). train avg loss: 0.2096\n",
      "Training for epoch 60 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 60 finished in 32m 40.15s (- 21m 46.77s) (60 60.0%). train avg loss: 0.2066\n",
      "Training for epoch 61 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 61 finished in 33m 11.28s (- 21m 13.11s) (61 61.0%). train avg loss: 0.2018\n",
      "Training for epoch 62 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 62 finished in 33m 42.75s (- 20m 39.75s) (62 62.0%). train avg loss: 0.1954\n",
      "Training for epoch 63 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 63 finished in 34m 13.98s (- 20m 6.3s) (63 63.0%). train avg loss: 0.1968\n",
      "Training for epoch 64 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 64 finished in 34m 45.46s (- 19m 33.07s) (64 64.0%). train avg loss: 0.1992\n",
      "Training for epoch 65 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 65 finished in 35m 16.81s (- 18m 59.82s) (65 65.0%). train avg loss: 0.1896\n",
      "Training for epoch 66 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 66 finished in 35m 48.21s (- 18m 26.65s) (66 66.0%). train avg loss: 0.1955\n",
      "Training for epoch 67 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 67 finished in 36m 19.79s (- 17m 53.63s) (67 67.0%). train avg loss: 0.2027\n",
      "Training for epoch 68 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 68 finished in 36m 51.31s (- 17m 20.62s) (68 68.0%). train avg loss: 0.2051\n",
      "Training for epoch 69 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 69 finished in 37m 22.53s (- 16m 47.51s) (69 69.0%). train avg loss: 0.1934\n",
      "Training for epoch 70 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 70 finished in 37m 53.86s (- 16m 14.51s) (70 70.0%). train avg loss: 0.1894\n",
      "Training for epoch 71 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 71 finished in 38m 25.43s (- 15m 41.65s) (71 71.0%). train avg loss: 0.1977\n",
      "Training for epoch 72 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 72 finished in 38m 57.45s (- 15m 9.01s) (72 72.0%). train avg loss: 0.1961\n",
      "Training for epoch 73 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 73 finished in 39m 28.92s (- 14m 36.17s) (73 73.0%). train avg loss: 0.1882\n",
      "Training for epoch 74 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 74 finished in 40m 0.74s (- 14m 3.5s) (74 74.0%). train avg loss: 0.1977\n",
      "Training for epoch 75 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 75 finished in 40m 32.17s (- 13m 30.72s) (75 75.0%). train avg loss: 0.1894\n",
      "Training for epoch 76 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 76 finished in 41m 3.27s (- 12m 57.87s) (76 76.0%). train avg loss: 0.1871\n",
      "Training for epoch 77 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 77 finished in 41m 35.2s (- 12m 25.32s) (77 77.0%). train avg loss: 0.1931\n",
      "Training for epoch 78 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 78 finished in 42m 6.74s (- 11m 52.67s) (78 78.0%). train avg loss: 0.1982\n",
      "Training for epoch 79 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 79 finished in 42m 38.41s (- 11m 20.08s) (79 79.0%). train avg loss: 0.1935\n",
      "Training for epoch 80 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 80 finished in 43m 10.18s (- 10m 47.55s) (80 80.0%). train avg loss: 0.1998\n",
      "Training for epoch 81 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 81 finished in 43m 41.35s (- 10m 14.89s) (81 81.0%). train avg loss: 0.1862\n",
      "Training for epoch 82 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 82 finished in 44m 12.8s (- 9m 42.32s) (82 82.0%). train avg loss: 0.1862\n",
      "Training for epoch 83 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 83 finished in 44m 44.28s (- 9m 9.79s) (83 83.0%). train avg loss: 0.1945\n",
      "Training for epoch 84 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 84 finished in 45m 15.65s (- 8m 37.27s) (84 84.0%). train avg loss: 0.2059\n",
      "Training for epoch 85 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 85 finished in 45m 47.44s (- 8m 4.84s) (85 85.0%). train avg loss: 0.2028\n",
      "Training for epoch 86 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 86 finished in 46m 18.74s (- 7m 32.35s) (86 86.0%). train avg loss: 0.1871\n",
      "Training for epoch 87 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 87 finished in 46m 50.15s (- 6m 59.91s) (87 87.0%). train avg loss: 0.1887\n",
      "Training for epoch 88 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 88 finished in 47m 21.17s (- 6m 27.43s) (88 88.0%). train avg loss: 0.1848\n",
      "Training for epoch 89 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 89 finished in 47m 52.39s (- 5m 55.01s) (89 89.0%). train avg loss: 0.177\n",
      "Training for epoch 90 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 90 finished in 48m 23.72s (- 5m 22.64s) (90 90.0%). train avg loss: 0.1794\n",
      "Training for epoch 91 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 91 finished in 48m 55.2s (- 4m 50.29s) (91 91.0%). train avg loss: 0.1828\n",
      "Training for epoch 92 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 92 finished in 49m 26.94s (- 4m 18.0s) (92 92.0%). train avg loss: 0.1847\n",
      "Training for epoch 93 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 93 finished in 49m 58.62s (- 3m 45.7s) (93 93.0%). train avg loss: 0.1939\n",
      "Training for epoch 94 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 94 finished in 50m 29.99s (- 3m 13.4s) (94 94.0%). train avg loss: 0.1798\n",
      "Training for epoch 95 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 95 finished in 51m 1.42s (- 2m 41.13s) (95 95.0%). train avg loss: 0.1826\n",
      "Training for epoch 96 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 96 finished in 51m 32.89s (- 2m 8.87s) (96 96.0%). train avg loss: 0.1814\n",
      "Training for epoch 97 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 97 finished in 52m 4.47s (- 1m 36.63s) (97 97.0%). train avg loss: 0.1818\n",
      "Training for epoch 98 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 98 finished in 52m 36.42s (- 1m 4.42s) (98 98.0%). train avg loss: 0.1961\n",
      "Training for epoch 99 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 99 finished in 53m 8.08s (- 0m 32.2s) (99 99.0%). train avg loss: 0.1775\n",
      "Training for epoch 100 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 100 finished in 53m 40.08s (- 0m 0.0s) (100 100.0%). train avg loss: 0.182\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "best_train_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on train loss\n",
    "  encoder_scheduler.step(avg_train_loss)\n",
    "  decoder_scheduler.step(avg_train_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  # Save the model if the train loss is better than the previous iterations' train loss\n",
    "  if avg_train_loss < best_train_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_train_loss = avg_train_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"train\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5, min_value+.1, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word))\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 14.187518254513643%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in test_pairs :\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word))\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(test_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> RECHRISTENED\n",
      "= ['R', 'IY', 'K', 'R', 'IY', 'S', 'AX', 'N', 'D']\n",
      "< R IY K R IY S T AX N D ['R', 'IY', 'K', 'R', 'IY', 'S', 'T', 'AX', 'N', 'D']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4331477f40>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 523.636x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
