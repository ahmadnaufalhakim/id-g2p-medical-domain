{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn_gen/exp/en\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL =\"dot\"\n",
    "EMB_DIM = \"128\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"50\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"validation_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369, 166, 398, 229, 578, 394, 107, 277, 416, 665, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f8f047a83d0> ([6, 99, 67, 583, 514, 1], [18, 6, 34, 1])\n",
      "([6, 99, 67, 583, 514, 1], [18, 6, 34, 1])\n",
      "([6, 99, 67, 583, 514, 1], [18, 6, 34, 1])\n",
      "train grp 716 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: '-a', 23: '-b', 24: '-c', 25: '-d', 26: '-e', 27: '-f', 28: '-g', 29: '-h', 30: '-i', 31: '-k', 32: '-l', 33: '-m', 34: '-n', 35: '-o', 36: '-p', 37: '-q', 38: '-r', 39: '-s', 40: '-t', 41: '-u', 42: '-v', 43: '-w', 44: '-y', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'd-', 126: 'da', 127: 'db', 128: 'dc', 129: 'dd', 130: 'de', 131: 'df', 132: 'dg', 133: 'dh', 134: 'di', 135: 'dj', 136: 'dk', 137: 'dl', 138: 'dm', 139: 'dn', 140: 'do', 141: 'dp', 142: 'dq', 143: 'dr', 144: 'ds', 145: 'dt', 146: 'du', 147: 'dv', 148: 'dw', 149: 'dy', 150: 'dz', 151: \"e'\", 152: 'e-', 153: 'ea', 154: 'eb', 155: 'ec', 156: 'ed', 157: 'ee', 158: 'ef', 159: 'eg', 160: 'eh', 161: 'ei', 162: 'ej', 163: 'ek', 164: 'el', 165: 'em', 166: 'en', 167: 'eo', 168: 'ep', 169: 'eq', 170: 'er', 171: 'es', 172: 'et', 173: 'eu', 174: 'ev', 175: 'ew', 176: 'ex', 177: 'ey', 178: 'ez', 179: \"f'\", 180: 'f-', 181: 'fa', 182: 'fb', 183: 'fc', 184: 'fd', 185: 'fe', 186: 'ff', 187: 'fg', 188: 'fh', 189: 'fi', 190: 'fj', 191: 'fk', 192: 'fl', 193: 'fm', 194: 'fn', 195: 'fo', 196: 'fp', 197: 'fq', 198: 'fr', 199: 'fs', 200: 'ft', 201: 'fu', 202: 'fv', 203: 'fw', 204: 'fx', 205: 'fy', 206: 'fz', 207: \"g'\", 208: 'g-', 209: 'ga', 210: 'gb', 211: 'gc', 212: 'gd', 213: 'ge', 214: 'gf', 215: 'gg', 216: 'gh', 217: 'gi', 218: 'gj', 219: 'gk', 220: 'gl', 221: 'gm', 222: 'gn', 223: 'go', 224: 'gp', 225: 'gq', 226: 'gr', 227: 'gs', 228: 'gt', 229: 'gu', 230: 'gv', 231: 'gw', 232: 'gx', 233: 'gy', 234: 'gz', 235: \"h'\", 236: 'h-', 237: 'ha', 238: 'hb', 239: 'hc', 240: 'hd', 241: 'he', 242: 'hf', 243: 'hg', 244: 'hh', 245: 'hi', 246: 'hj', 247: 'hk', 248: 'hl', 249: 'hm', 250: 'hn', 251: 'ho', 252: 'hp', 253: 'hq', 254: 'hr', 255: 'hs', 256: 'ht', 257: 'hu', 258: 'hv', 259: 'hw', 260: 'hy', 261: 'i', 262: \"i'\", 263: 'i-', 264: 'ia', 265: 'ib', 266: 'ic', 267: 'id', 268: 'ie', 269: 'if', 270: 'ig', 271: 'ih', 272: 'ii', 273: 'ij', 274: 'ik', 275: 'il', 276: 'im', 277: 'in', 278: 'io', 279: 'ip', 280: 'iq', 281: 'ir', 282: 'is', 283: 'it', 284: 'iu', 285: 'iv', 286: 'iw', 287: 'ix', 288: 'iy', 289: 'iz', 290: \"j'\", 291: 'ja', 292: 'jc', 293: 'jd', 294: 'je', 295: 'jf', 296: 'jh', 297: 'ji', 298: 'jj', 299: 'jk', 300: 'jm', 301: 'jn', 302: 'jo', 303: 'jr', 304: 'js', 305: 'jt', 306: 'ju', 307: 'jv', 308: 'jy', 309: \"k'\", 310: 'k-', 311: 'ka', 312: 'kb', 313: 'kc', 314: 'kd', 315: 'ke', 316: 'kf', 317: 'kg', 318: 'kh', 319: 'ki', 320: 'kj', 321: 'kk', 322: 'kl', 323: 'km', 324: 'kn', 325: 'ko', 326: 'kp', 327: 'kr', 328: 'ks', 329: 'kt', 330: 'ku', 331: 'kv', 332: 'kw', 333: 'ky', 334: 'kz', 335: \"l'\", 336: 'l-', 337: 'la', 338: 'lb', 339: 'lc', 340: 'ld', 341: 'le', 342: 'lf', 343: 'lg', 344: 'lh', 345: 'li', 346: 'lj', 347: 'lk', 348: 'll', 349: 'lm', 350: 'ln', 351: 'lo', 352: 'lp', 353: 'lq', 354: 'lr', 355: 'ls', 356: 'lt', 357: 'lu', 358: 'lv', 359: 'lw', 360: 'lx', 361: 'ly', 362: 'lz', 363: \"m'\", 364: 'm-', 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'p-', 449: 'pa', 450: 'pb', 451: 'pc', 452: 'pd', 453: 'pe', 454: 'pf', 455: 'pg', 456: 'ph', 457: 'pi', 458: 'pj', 459: 'pk', 460: 'pl', 461: 'pm', 462: 'pn', 463: 'po', 464: 'pp', 465: 'pr', 466: 'ps', 467: 'pt', 468: 'pu', 469: 'pw', 470: 'py', 471: 'pz', 472: \"q'\", 473: 'qa', 474: 'qb', 475: 'qg', 476: 'qi', 477: 'qo', 478: 'qu', 479: 'qv', 480: \"r'\", 481: 'r-', 482: 'ra', 483: 'rb', 484: 'rc', 485: 'rd', 486: 're', 487: 'rf', 488: 'rg', 489: 'rh', 490: 'ri', 491: 'rj', 492: 'rk', 493: 'rl', 494: 'rm', 495: 'rn', 496: 'ro', 497: 'rp', 498: 'rq', 499: 'rr', 500: 'rs', 501: 'rt', 502: 'ru', 503: 'rv', 504: 'rw', 505: 'rx', 506: 'ry', 507: 'rz', 508: \"s'\", 509: 's-', 510: 'sa', 511: 'sb', 512: 'sc', 513: 'sd', 514: 'se', 515: 'sf', 516: 'sg', 517: 'sh', 518: 'si', 519: 'sj', 520: 'sk', 521: 'sl', 522: 'sm', 523: 'sn', 524: 'so', 525: 'sp', 526: 'sq', 527: 'sr', 528: 'ss', 529: 'st', 530: 'su', 531: 'sv', 532: 'sw', 533: 'sx', 534: 'sy', 535: 'sz', 536: \"t'\", 537: 't-', 538: 'ta', 539: 'tb', 540: 'tc', 541: 'td', 542: 'te', 543: 'tf', 544: 'tg', 545: 'th', 546: 'ti', 547: 'tj', 548: 'tk', 549: 'tl', 550: 'tm', 551: 'tn', 552: 'to', 553: 'tp', 554: 'tr', 555: 'ts', 556: 'tt', 557: 'tu', 558: 'tv', 559: 'tw', 560: 'tx', 561: 'ty', 562: 'tz', 563: \"u'\", 564: 'u-', 565: 'ua', 566: 'ub', 567: 'uc', 568: 'ud', 569: 'ue', 570: 'uf', 571: 'ug', 572: 'uh', 573: 'ui', 574: 'uj', 575: 'uk', 576: 'ul', 577: 'um', 578: 'un', 579: 'uo', 580: 'up', 581: 'uq', 582: 'ur', 583: 'us', 584: 'ut', 585: 'uu', 586: 'uv', 587: 'uw', 588: 'ux', 589: 'uy', 590: 'uz', 591: \"v'\", 592: 'va', 593: 'vc', 594: 'vd', 595: 've', 596: 'vg', 597: 'vh', 598: 'vi', 599: 'vj', 600: 'vk', 601: 'vl', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: 'vz', 611: \"w'\", 612: 'w-', 613: 'wa', 614: 'wb', 615: 'wc', 616: 'wd', 617: 'we', 618: 'wf', 619: 'wg', 620: 'wh', 621: 'wi', 622: 'wk', 623: 'wl', 624: 'wm', 625: 'wn', 626: 'wo', 627: 'wp', 628: 'wq', 629: 'wr', 630: 'ws', 631: 'wt', 632: 'wu', 633: 'wv', 634: 'ww', 635: 'wy', 636: 'wz', 637: \"x'\", 638: 'x-', 639: 'xa', 640: 'xb', 641: 'xc', 642: 'xd', 643: 'xe', 644: 'xf', 645: 'xg', 646: 'xh', 647: 'xi', 648: 'xl', 649: 'xm', 650: 'xn', 651: 'xo', 652: 'xp', 653: 'xq', 654: 'xr', 655: 'xs', 656: 'xt', 657: 'xu', 658: 'xv', 659: 'xw', 660: 'xx', 661: 'xy', 662: 'xz', 663: \"y'\", 664: 'y-', 665: 'ya', 666: 'yb', 667: 'yc', 668: 'yd', 669: 'ye', 670: 'yf', 671: 'yg', 672: 'yh', 673: 'yi', 674: 'yj', 675: 'yk', 676: 'yl', 677: 'ym', 678: 'yn', 679: 'yo', 680: 'yp', 681: 'yq', 682: 'yr', 683: 'ys', 684: 'yt', 685: 'yu', 686: 'yv', 687: 'yw', 688: 'yx', 689: 'yy', 690: 'yz', 691: \"z'\", 692: 'za', 693: 'zb', 694: 'zc', 695: 'zd', 696: 'ze', 697: 'zf', 698: 'zg', 699: 'zh', 700: 'zi', 701: 'zk', 702: 'zl', 703: 'zm', 704: 'zn', 705: 'zo', 706: 'zp', 707: 'zq', 708: 'zr', 709: 'zs', 710: 'zt', 711: 'zu', 712: 'zv', 713: 'zw', 714: 'zy', 715: 'zz'}\n",
      "valid grp 716 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: '-a', 23: '-b', 24: '-c', 25: '-d', 26: '-e', 27: '-f', 28: '-g', 29: '-h', 30: '-i', 31: '-k', 32: '-l', 33: '-m', 34: '-n', 35: '-o', 36: '-p', 37: '-q', 38: '-r', 39: '-s', 40: '-t', 41: '-u', 42: '-v', 43: '-w', 44: '-y', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'd-', 126: 'da', 127: 'db', 128: 'dc', 129: 'dd', 130: 'de', 131: 'df', 132: 'dg', 133: 'dh', 134: 'di', 135: 'dj', 136: 'dk', 137: 'dl', 138: 'dm', 139: 'dn', 140: 'do', 141: 'dp', 142: 'dq', 143: 'dr', 144: 'ds', 145: 'dt', 146: 'du', 147: 'dv', 148: 'dw', 149: 'dy', 150: 'dz', 151: \"e'\", 152: 'e-', 153: 'ea', 154: 'eb', 155: 'ec', 156: 'ed', 157: 'ee', 158: 'ef', 159: 'eg', 160: 'eh', 161: 'ei', 162: 'ej', 163: 'ek', 164: 'el', 165: 'em', 166: 'en', 167: 'eo', 168: 'ep', 169: 'eq', 170: 'er', 171: 'es', 172: 'et', 173: 'eu', 174: 'ev', 175: 'ew', 176: 'ex', 177: 'ey', 178: 'ez', 179: \"f'\", 180: 'f-', 181: 'fa', 182: 'fb', 183: 'fc', 184: 'fd', 185: 'fe', 186: 'ff', 187: 'fg', 188: 'fh', 189: 'fi', 190: 'fj', 191: 'fk', 192: 'fl', 193: 'fm', 194: 'fn', 195: 'fo', 196: 'fp', 197: 'fq', 198: 'fr', 199: 'fs', 200: 'ft', 201: 'fu', 202: 'fv', 203: 'fw', 204: 'fx', 205: 'fy', 206: 'fz', 207: \"g'\", 208: 'g-', 209: 'ga', 210: 'gb', 211: 'gc', 212: 'gd', 213: 'ge', 214: 'gf', 215: 'gg', 216: 'gh', 217: 'gi', 218: 'gj', 219: 'gk', 220: 'gl', 221: 'gm', 222: 'gn', 223: 'go', 224: 'gp', 225: 'gq', 226: 'gr', 227: 'gs', 228: 'gt', 229: 'gu', 230: 'gv', 231: 'gw', 232: 'gx', 233: 'gy', 234: 'gz', 235: \"h'\", 236: 'h-', 237: 'ha', 238: 'hb', 239: 'hc', 240: 'hd', 241: 'he', 242: 'hf', 243: 'hg', 244: 'hh', 245: 'hi', 246: 'hj', 247: 'hk', 248: 'hl', 249: 'hm', 250: 'hn', 251: 'ho', 252: 'hp', 253: 'hq', 254: 'hr', 255: 'hs', 256: 'ht', 257: 'hu', 258: 'hv', 259: 'hw', 260: 'hy', 261: 'i', 262: \"i'\", 263: 'i-', 264: 'ia', 265: 'ib', 266: 'ic', 267: 'id', 268: 'ie', 269: 'if', 270: 'ig', 271: 'ih', 272: 'ii', 273: 'ij', 274: 'ik', 275: 'il', 276: 'im', 277: 'in', 278: 'io', 279: 'ip', 280: 'iq', 281: 'ir', 282: 'is', 283: 'it', 284: 'iu', 285: 'iv', 286: 'iw', 287: 'ix', 288: 'iy', 289: 'iz', 290: \"j'\", 291: 'ja', 292: 'jc', 293: 'jd', 294: 'je', 295: 'jf', 296: 'jh', 297: 'ji', 298: 'jj', 299: 'jk', 300: 'jm', 301: 'jn', 302: 'jo', 303: 'jr', 304: 'js', 305: 'jt', 306: 'ju', 307: 'jv', 308: 'jy', 309: \"k'\", 310: 'k-', 311: 'ka', 312: 'kb', 313: 'kc', 314: 'kd', 315: 'ke', 316: 'kf', 317: 'kg', 318: 'kh', 319: 'ki', 320: 'kj', 321: 'kk', 322: 'kl', 323: 'km', 324: 'kn', 325: 'ko', 326: 'kp', 327: 'kr', 328: 'ks', 329: 'kt', 330: 'ku', 331: 'kv', 332: 'kw', 333: 'ky', 334: 'kz', 335: \"l'\", 336: 'l-', 337: 'la', 338: 'lb', 339: 'lc', 340: 'ld', 341: 'le', 342: 'lf', 343: 'lg', 344: 'lh', 345: 'li', 346: 'lj', 347: 'lk', 348: 'll', 349: 'lm', 350: 'ln', 351: 'lo', 352: 'lp', 353: 'lq', 354: 'lr', 355: 'ls', 356: 'lt', 357: 'lu', 358: 'lv', 359: 'lw', 360: 'lx', 361: 'ly', 362: 'lz', 363: \"m'\", 364: 'm-', 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'p-', 449: 'pa', 450: 'pb', 451: 'pc', 452: 'pd', 453: 'pe', 454: 'pf', 455: 'pg', 456: 'ph', 457: 'pi', 458: 'pj', 459: 'pk', 460: 'pl', 461: 'pm', 462: 'pn', 463: 'po', 464: 'pp', 465: 'pr', 466: 'ps', 467: 'pt', 468: 'pu', 469: 'pw', 470: 'py', 471: 'pz', 472: \"q'\", 473: 'qa', 474: 'qb', 475: 'qg', 476: 'qi', 477: 'qo', 478: 'qu', 479: 'qv', 480: \"r'\", 481: 'r-', 482: 'ra', 483: 'rb', 484: 'rc', 485: 'rd', 486: 're', 487: 'rf', 488: 'rg', 489: 'rh', 490: 'ri', 491: 'rj', 492: 'rk', 493: 'rl', 494: 'rm', 495: 'rn', 496: 'ro', 497: 'rp', 498: 'rq', 499: 'rr', 500: 'rs', 501: 'rt', 502: 'ru', 503: 'rv', 504: 'rw', 505: 'rx', 506: 'ry', 507: 'rz', 508: \"s'\", 509: 's-', 510: 'sa', 511: 'sb', 512: 'sc', 513: 'sd', 514: 'se', 515: 'sf', 516: 'sg', 517: 'sh', 518: 'si', 519: 'sj', 520: 'sk', 521: 'sl', 522: 'sm', 523: 'sn', 524: 'so', 525: 'sp', 526: 'sq', 527: 'sr', 528: 'ss', 529: 'st', 530: 'su', 531: 'sv', 532: 'sw', 533: 'sx', 534: 'sy', 535: 'sz', 536: \"t'\", 537: 't-', 538: 'ta', 539: 'tb', 540: 'tc', 541: 'td', 542: 'te', 543: 'tf', 544: 'tg', 545: 'th', 546: 'ti', 547: 'tj', 548: 'tk', 549: 'tl', 550: 'tm', 551: 'tn', 552: 'to', 553: 'tp', 554: 'tr', 555: 'ts', 556: 'tt', 557: 'tu', 558: 'tv', 559: 'tw', 560: 'tx', 561: 'ty', 562: 'tz', 563: \"u'\", 564: 'u-', 565: 'ua', 566: 'ub', 567: 'uc', 568: 'ud', 569: 'ue', 570: 'uf', 571: 'ug', 572: 'uh', 573: 'ui', 574: 'uj', 575: 'uk', 576: 'ul', 577: 'um', 578: 'un', 579: 'uo', 580: 'up', 581: 'uq', 582: 'ur', 583: 'us', 584: 'ut', 585: 'uu', 586: 'uv', 587: 'uw', 588: 'ux', 589: 'uy', 590: 'uz', 591: \"v'\", 592: 'va', 593: 'vc', 594: 'vd', 595: 've', 596: 'vg', 597: 'vh', 598: 'vi', 599: 'vj', 600: 'vk', 601: 'vl', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: 'vz', 611: \"w'\", 612: 'w-', 613: 'wa', 614: 'wb', 615: 'wc', 616: 'wd', 617: 'we', 618: 'wf', 619: 'wg', 620: 'wh', 621: 'wi', 622: 'wk', 623: 'wl', 624: 'wm', 625: 'wn', 626: 'wo', 627: 'wp', 628: 'wq', 629: 'wr', 630: 'ws', 631: 'wt', 632: 'wu', 633: 'wv', 634: 'ww', 635: 'wy', 636: 'wz', 637: \"x'\", 638: 'x-', 639: 'xa', 640: 'xb', 641: 'xc', 642: 'xd', 643: 'xe', 644: 'xf', 645: 'xg', 646: 'xh', 647: 'xi', 648: 'xl', 649: 'xm', 650: 'xn', 651: 'xo', 652: 'xp', 653: 'xq', 654: 'xr', 655: 'xs', 656: 'xt', 657: 'xu', 658: 'xv', 659: 'xw', 660: 'xx', 661: 'xy', 662: 'xz', 663: \"y'\", 664: 'y-', 665: 'ya', 666: 'yb', 667: 'yc', 668: 'yd', 669: 'ye', 670: 'yf', 671: 'yg', 672: 'yh', 673: 'yi', 674: 'yj', 675: 'yk', 676: 'yl', 677: 'ym', 678: 'yn', 679: 'yo', 680: 'yp', 681: 'yq', 682: 'yr', 683: 'ys', 684: 'yt', 685: 'yu', 686: 'yv', 687: 'yw', 688: 'yx', 689: 'yy', 690: 'yz', 691: \"z'\", 692: 'za', 693: 'zb', 694: 'zc', 695: 'zd', 696: 'ze', 697: 'zf', 698: 'zg', 699: 'zh', 700: 'zi', 701: 'zk', 702: 'zl', 703: 'zm', 704: 'zn', 705: 'zo', 706: 'zp', 707: 'zq', 708: 'zr', 709: 'zs', 710: 'zt', 711: 'zu', 712: 'zv', 713: 'zw', 714: 'zy', 715: 'zz'}\n",
      "test grp 716 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: '-a', 23: '-b', 24: '-c', 25: '-d', 26: '-e', 27: '-f', 28: '-g', 29: '-h', 30: '-i', 31: '-k', 32: '-l', 33: '-m', 34: '-n', 35: '-o', 36: '-p', 37: '-q', 38: '-r', 39: '-s', 40: '-t', 41: '-u', 42: '-v', 43: '-w', 44: '-y', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'd-', 126: 'da', 127: 'db', 128: 'dc', 129: 'dd', 130: 'de', 131: 'df', 132: 'dg', 133: 'dh', 134: 'di', 135: 'dj', 136: 'dk', 137: 'dl', 138: 'dm', 139: 'dn', 140: 'do', 141: 'dp', 142: 'dq', 143: 'dr', 144: 'ds', 145: 'dt', 146: 'du', 147: 'dv', 148: 'dw', 149: 'dy', 150: 'dz', 151: \"e'\", 152: 'e-', 153: 'ea', 154: 'eb', 155: 'ec', 156: 'ed', 157: 'ee', 158: 'ef', 159: 'eg', 160: 'eh', 161: 'ei', 162: 'ej', 163: 'ek', 164: 'el', 165: 'em', 166: 'en', 167: 'eo', 168: 'ep', 169: 'eq', 170: 'er', 171: 'es', 172: 'et', 173: 'eu', 174: 'ev', 175: 'ew', 176: 'ex', 177: 'ey', 178: 'ez', 179: \"f'\", 180: 'f-', 181: 'fa', 182: 'fb', 183: 'fc', 184: 'fd', 185: 'fe', 186: 'ff', 187: 'fg', 188: 'fh', 189: 'fi', 190: 'fj', 191: 'fk', 192: 'fl', 193: 'fm', 194: 'fn', 195: 'fo', 196: 'fp', 197: 'fq', 198: 'fr', 199: 'fs', 200: 'ft', 201: 'fu', 202: 'fv', 203: 'fw', 204: 'fx', 205: 'fy', 206: 'fz', 207: \"g'\", 208: 'g-', 209: 'ga', 210: 'gb', 211: 'gc', 212: 'gd', 213: 'ge', 214: 'gf', 215: 'gg', 216: 'gh', 217: 'gi', 218: 'gj', 219: 'gk', 220: 'gl', 221: 'gm', 222: 'gn', 223: 'go', 224: 'gp', 225: 'gq', 226: 'gr', 227: 'gs', 228: 'gt', 229: 'gu', 230: 'gv', 231: 'gw', 232: 'gx', 233: 'gy', 234: 'gz', 235: \"h'\", 236: 'h-', 237: 'ha', 238: 'hb', 239: 'hc', 240: 'hd', 241: 'he', 242: 'hf', 243: 'hg', 244: 'hh', 245: 'hi', 246: 'hj', 247: 'hk', 248: 'hl', 249: 'hm', 250: 'hn', 251: 'ho', 252: 'hp', 253: 'hq', 254: 'hr', 255: 'hs', 256: 'ht', 257: 'hu', 258: 'hv', 259: 'hw', 260: 'hy', 261: 'i', 262: \"i'\", 263: 'i-', 264: 'ia', 265: 'ib', 266: 'ic', 267: 'id', 268: 'ie', 269: 'if', 270: 'ig', 271: 'ih', 272: 'ii', 273: 'ij', 274: 'ik', 275: 'il', 276: 'im', 277: 'in', 278: 'io', 279: 'ip', 280: 'iq', 281: 'ir', 282: 'is', 283: 'it', 284: 'iu', 285: 'iv', 286: 'iw', 287: 'ix', 288: 'iy', 289: 'iz', 290: \"j'\", 291: 'ja', 292: 'jc', 293: 'jd', 294: 'je', 295: 'jf', 296: 'jh', 297: 'ji', 298: 'jj', 299: 'jk', 300: 'jm', 301: 'jn', 302: 'jo', 303: 'jr', 304: 'js', 305: 'jt', 306: 'ju', 307: 'jv', 308: 'jy', 309: \"k'\", 310: 'k-', 311: 'ka', 312: 'kb', 313: 'kc', 314: 'kd', 315: 'ke', 316: 'kf', 317: 'kg', 318: 'kh', 319: 'ki', 320: 'kj', 321: 'kk', 322: 'kl', 323: 'km', 324: 'kn', 325: 'ko', 326: 'kp', 327: 'kr', 328: 'ks', 329: 'kt', 330: 'ku', 331: 'kv', 332: 'kw', 333: 'ky', 334: 'kz', 335: \"l'\", 336: 'l-', 337: 'la', 338: 'lb', 339: 'lc', 340: 'ld', 341: 'le', 342: 'lf', 343: 'lg', 344: 'lh', 345: 'li', 346: 'lj', 347: 'lk', 348: 'll', 349: 'lm', 350: 'ln', 351: 'lo', 352: 'lp', 353: 'lq', 354: 'lr', 355: 'ls', 356: 'lt', 357: 'lu', 358: 'lv', 359: 'lw', 360: 'lx', 361: 'ly', 362: 'lz', 363: \"m'\", 364: 'm-', 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'p-', 449: 'pa', 450: 'pb', 451: 'pc', 452: 'pd', 453: 'pe', 454: 'pf', 455: 'pg', 456: 'ph', 457: 'pi', 458: 'pj', 459: 'pk', 460: 'pl', 461: 'pm', 462: 'pn', 463: 'po', 464: 'pp', 465: 'pr', 466: 'ps', 467: 'pt', 468: 'pu', 469: 'pw', 470: 'py', 471: 'pz', 472: \"q'\", 473: 'qa', 474: 'qb', 475: 'qg', 476: 'qi', 477: 'qo', 478: 'qu', 479: 'qv', 480: \"r'\", 481: 'r-', 482: 'ra', 483: 'rb', 484: 'rc', 485: 'rd', 486: 're', 487: 'rf', 488: 'rg', 489: 'rh', 490: 'ri', 491: 'rj', 492: 'rk', 493: 'rl', 494: 'rm', 495: 'rn', 496: 'ro', 497: 'rp', 498: 'rq', 499: 'rr', 500: 'rs', 501: 'rt', 502: 'ru', 503: 'rv', 504: 'rw', 505: 'rx', 506: 'ry', 507: 'rz', 508: \"s'\", 509: 's-', 510: 'sa', 511: 'sb', 512: 'sc', 513: 'sd', 514: 'se', 515: 'sf', 516: 'sg', 517: 'sh', 518: 'si', 519: 'sj', 520: 'sk', 521: 'sl', 522: 'sm', 523: 'sn', 524: 'so', 525: 'sp', 526: 'sq', 527: 'sr', 528: 'ss', 529: 'st', 530: 'su', 531: 'sv', 532: 'sw', 533: 'sx', 534: 'sy', 535: 'sz', 536: \"t'\", 537: 't-', 538: 'ta', 539: 'tb', 540: 'tc', 541: 'td', 542: 'te', 543: 'tf', 544: 'tg', 545: 'th', 546: 'ti', 547: 'tj', 548: 'tk', 549: 'tl', 550: 'tm', 551: 'tn', 552: 'to', 553: 'tp', 554: 'tr', 555: 'ts', 556: 'tt', 557: 'tu', 558: 'tv', 559: 'tw', 560: 'tx', 561: 'ty', 562: 'tz', 563: \"u'\", 564: 'u-', 565: 'ua', 566: 'ub', 567: 'uc', 568: 'ud', 569: 'ue', 570: 'uf', 571: 'ug', 572: 'uh', 573: 'ui', 574: 'uj', 575: 'uk', 576: 'ul', 577: 'um', 578: 'un', 579: 'uo', 580: 'up', 581: 'uq', 582: 'ur', 583: 'us', 584: 'ut', 585: 'uu', 586: 'uv', 587: 'uw', 588: 'ux', 589: 'uy', 590: 'uz', 591: \"v'\", 592: 'va', 593: 'vc', 594: 'vd', 595: 've', 596: 'vg', 597: 'vh', 598: 'vi', 599: 'vj', 600: 'vk', 601: 'vl', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: 'vz', 611: \"w'\", 612: 'w-', 613: 'wa', 614: 'wb', 615: 'wc', 616: 'wd', 617: 'we', 618: 'wf', 619: 'wg', 620: 'wh', 621: 'wi', 622: 'wk', 623: 'wl', 624: 'wm', 625: 'wn', 626: 'wo', 627: 'wp', 628: 'wq', 629: 'wr', 630: 'ws', 631: 'wt', 632: 'wu', 633: 'wv', 634: 'ww', 635: 'wy', 636: 'wz', 637: \"x'\", 638: 'x-', 639: 'xa', 640: 'xb', 641: 'xc', 642: 'xd', 643: 'xe', 644: 'xf', 645: 'xg', 646: 'xh', 647: 'xi', 648: 'xl', 649: 'xm', 650: 'xn', 651: 'xo', 652: 'xp', 653: 'xq', 654: 'xr', 655: 'xs', 656: 'xt', 657: 'xu', 658: 'xv', 659: 'xw', 660: 'xx', 661: 'xy', 662: 'xz', 663: \"y'\", 664: 'y-', 665: 'ya', 666: 'yb', 667: 'yc', 668: 'yd', 669: 'ye', 670: 'yf', 671: 'yg', 672: 'yh', 673: 'yi', 674: 'yj', 675: 'yk', 676: 'yl', 677: 'ym', 678: 'yn', 679: 'yo', 680: 'yp', 681: 'yq', 682: 'yr', 683: 'ys', 684: 'yt', 685: 'yu', 686: 'yv', 687: 'yw', 688: 'yx', 689: 'yy', 690: 'yz', 691: \"z'\", 692: 'za', 693: 'zb', 694: 'zc', 695: 'zd', 696: 'ze', 697: 'zf', 698: 'zg', 699: 'zh', 700: 'zi', 701: 'zk', 702: 'zl', 703: 'zm', 704: 'zn', 705: 'zo', 706: 'zp', 707: 'zq', 708: 'zr', 709: 'zs', 710: 'zt', 711: 'zu', 712: 'zv', 713: 'zw', 714: 'zy', 715: 'zz'}\n",
      "train phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "valid phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "test phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "712 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 583, 'se': 514, 'co': 113, 'ou': 441, 'ur': 582, 'rs': 500, \"'e\": 8, 'em': 165, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 478, 'uo': 579, 'ot': 440, 'te': 542, \"'t\": 20, 'ti': 546, 'is': 282, 'aa': 47, 'ab': 48, 'be': 78, 'er': 170, 'rg': 488, 'ac': 49, 'ch': 106, 'he': 241, 'en': 166, 'ak': 57, 'ke': 315, 'al': 58, 'ls': 355, 'et': 172, 'th': 545, 'am': 59, 'mo': 379, 'od': 424, 'dt': 145, 'an': 60, 'nc': 394, 'or': 438, 'ar': 64, 'rd': 485, 'de': 130, 'ma': 365, 'dv': 147, 'va': 592, 'rk': 492, 'ro': 496, 'on': 434, \"n'\": 390, \"'s\": 19, 'ns': 410, 'so': 524, 'rt': 501, 'as': 65, 'ba': 74, 'ha': 237, 'ck': 109, 'cu': 119, 'ad': 50, 'da': 126, 'ka': 311, 'di': 134, 'ai': 55, 'ir': 281, 'lk': 347, 'ki': 319, 'in': 277, 'lo': 351, 'ne': 396, 'os': 439, 'nd': 395, 'do': 140, 'ni': 400, 'ng': 398, 'nm': 404, 'me': 369, 'nt': 411, 'ts': 555, 'to': 552, 'rc': 484, 're': 486, 'sc': 512, 'sh': 517, 'ed': 156, 'at': 66, 'es': 171, 'bb': 75, 'si': 518, 'ie': 268, 'el': 164, 'll': 348, 'nh': 399, 'tt': 556, 'ev': 174, 'vi': 598, 'il': 275, 'le': 341, 'ey': 177, \"y'\": 663, 'bi': 82, 'it': 283, 'bo': 88, \"t'\": 536, 'ud': 568, 'br': 90, 'ia': 264, 'io': 278, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 163, 'bd': 77, 'la': 337, 'ah': 54, 'ic': 266, 'dn': 139, 'no': 406, 'ol': 432, 'om': 433, 'mi': 373, 'na': 392, 'du': 146, 'uc': 567, 'ct': 118, 'ee': 157, 'ul': 576, 'az': 72, 'zi': 700, 'iz': 289, 'ln': 350, 'ow': 443, 'dr': 143, 'cr': 116, 'mb': 366, 'rl': 493, 'rm': 494, 'rr': 499, 'ra': 482, 'ya': 665, 'ce': 103, 'yt': 684, 'ta': 538, 'bh': 81, 'ho': 251, 'id': 267, 'ig': 270, 'ga': 209, 'li': 345, 'ty': 561, 'gt': 228, \"a'\": 45, 'ib': 265, 'tz': 562, 'bj': 83, 'je': 294, 'ec': 155, 'bk': 84, 'kh': 318, 'bl': 85, 'ze': 696, 'e-': 152, '-b': 23, 'st': 529, 'oo': 435, 'ly': 361, 'bn': 87, \"o'\": 419, 'oa': 421, 'hi': 245, 'sm': 522, 'ri': 490, 'gi': 217, 'rn': 495, 'if': 269, 'fa': 181, 'ci': 107, 'iv': 285, 've': 595, 'uh': 572, 'im': 276, 'un': 578, 'ds': 144, 'ut': 584, 'ov': 442, \"e'\": 151, 'bp': 89, 'pl': 460, 'lp': 352, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 714, 'yk': 675, 'wi': 621, 'ea': 153, 'eg': 159, 'go': 223, 'dg': 132, 'ge': 213, 'og': 427, 'ru': 502, 'up': 580, 'pt': 467, 'tl': 549, 'tn': 551, 'ss': 528, 'yn': 678, 'uz': 590, 'zz': 715, 'zo': 705, 'sa': 510, 'ei': 161, 'lu': 357, 'lv': 358, 'rb': 483, 'rp': 497, 'tr': 554, 'su': 530, 'dl': 137, 'bt': 92, 'bu': 93, 'hm': 249, 'dz': 150, 'ys': 683, 'cs': 117, 'my': 388, 'mp': 380, 'po': 463, 'ap': 62, 'pu': 468, 'lc': 339, 'cc': 101, 'tu': 557, 'ua': 565, 'ep': 168, 'ry': 506, 't-': 537, '-p': 36, 'pr': 465, 'cl': 110, 'mm': 377, 'pa': 449, 'ny': 416, 'yi': 673, \"r'\": 480, 'gl': 220, 'cy': 122, \"s'\": 508, 'ps': 466, 'ue': 569, 'ui': 573, 'um': 577, 'mu': 385, 'ay': 71, 'op': 436, 'ph': 456, 'yl': 676, 'eb': 154, 'nb': 393, 'ht': 256, 'hy': 260, 'fi': 189, 'fy': 205, 'kl': 322, 'km': 323, 'kn': 324, 'wl': 623, 'gm': 221, 'kr': 327, 'oy': 445, 'yd': 668, \"d'\": 124, 'cm': 111, 'cn': 112, 'oc': 423, 'of': 426, 'ff': 186, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 677, 'tm': 550, 'uf': 570, 'sy': 534, 'yc': 667, \"m'\": 363, 'ik': 274, 'za': 692, 'sk': 520, 'wn': 625, 'ex': 176, 'dc': 128, 'dd': 129, 'eo': 167, 'ks': 328, 'dw': 148, 'we': 617, 'dy': 149, 'lb': 338, \"l'\": 335, 'lm': 349, 'nn': 405, 'sb': 511, 'oi': 429, 'eq': 169, 'rh': 489, 'ld': 340, 'lt': 356, 'dh': 133, 'eu': 173, 'lf': 342, 'ip': 279, 'dj': 135, 'ja': 291, 'jo': 302, 'ju': 306, 'dk': 136, 'dm': 138, \"f'\": 179, 'fo': 195, 'hs': 255, '-f': 27, '-m': 33, '-s': 39, 'gn': 222, 'ae': 51, 'sd': 513, \"g'\": 207, 'vo': 603, 'ft': 200, 'oq': 437, \"h'\": 235, 'sp': 525, 'hl': 248, \"p'\": 447, 'af': 52, 'ye': 669, 'fe': 185, 'ix': 287, 'xe': 643, 'fl': 192, 'fr': 198, 'ax': 70, 'fg': 187, 'gh': 216, \"i'\": 262, 'fh': 188, 'ox': 444, 'xi': 647, 'fm': 193, 'fs': 199, 'ef': 158, 'ug': 571, 'rw': 504, 'wa': 613, 'mn': 378, 'gy': 233, 'pe': 453, 'gc': 211, 'gf': 214, 'gg': 215, 'gr': 226, 'iu': 284, 'ew': 175, 'aw': 69, 'xc': 641, 'fu': 201, 'pp': 464, 'ok': 431, 'ko': 325, 'gu': 229, 'hh': 244, 'lg': 343, 'lq': 353, 'hn': 250, 'hr': 254, 'hu': 257, 'ml': 376, 'sl': 521, 'sw': 532, 'wo': 626, \"c'\": 98, 'gs': 227, 'rf': 487, 'nk': 402, 'tc': 540, 'iw': 286, 'aj': 56, \"x'\": 637, 'ji': 297, 'kb': 312, 'kc': 313, 'iy': 288, 'kk': 321, 'ky': 333, 'kz': 334, 'tv': 558, 'uq': 581, 'rq': 498, 'oh': 428, 'eh': 160, 'ej': 162, 'xa': 639, 'xo': 651, 'xy': 661, 'nq': 408, 'lh': 344, 'nz': 417, 'ij': 273, 'iq': 280, 'l-': 336, '-i': 30, 'n-': 391, '-o': 35, 'nw': 414, 'yw': 687, 'nu': 412, 'lr': 354, 'pi': 457, 'uv': 586, 'lw': 359, 'ez': 178, 'mq': 381, 'fn': 194, \"k'\": 309, 'tf': 543, 'zh': 699, 'wy': 635, 'lz': 362, 'ku': 330, 'np': 407, 'xt': 656, 'md': 368, 'zc': 694, 'zq': 707, 'mf': 370, 'mg': 371, 'mh': 372, 'yv': 686, 'oe': 425, 'pc': 451, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yo': 679, 'yx': 688, 'yz': 690, 'sq': 526, 'nv': 413, 'oz': 446, 'ws': 630, 'uj': 574, 'cd': 102, 'nf': 397, 'nj': 401, 'ih': 271, 'nr': 409, 'sg': 516, 'i-': 263, 'ub': 566, 'bm': 86, 'tk': 548, \"u'\": 563, 'tw': 559, 'nx': 415, 'yb': 666, 'yh': 672, 'yp': 680, 'wh': 620, 'ao': 61, 'pf': 454, 'pg': 455, 'pk': 459, 'aq': 63, 'qa': 473, 'rv': 503, 'ux': 588, 'hb': 238, 'hd': 240, 'uk': 575, 'zm': 703, 'rj': 491, 'kw': 332, 'wr': 629, 'd-': 125, '-t': 40, '-c': 24, \"w'\": 611, 'zt': 710, 'zu': 711, 'rx': 505, 'rz': 507, 'ii': 272, 'hc': 239, 'hf': 242, 'hw': 259, \"v'\": 591, 'pn': 462, 'yr': 682, 'fd': 184, \"'h\": 10, '-l': 32, 'uy': 589, 'vd': 594, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 614, 'wf': 618, 'wk': 622, 'wt': 631, 'xf': 644, 'xl': 648, 'xs': 655, 'yg': 671, 'yu': 685, 'yy': 689, 'zb': 693, \"b'\": 73, \"'r\": 18, 'hk': 247, 'k-': 310, '-a': 22, 'kd': 314, 'kf': 316, 'kg': 317, 'kp': 326, 'kt': 329, 'y-': 664, 'tj': 547, 'gb': 210, 'gd': 212, 'gp': 224, 'gw': 231, \"'i\": 11, 'sz': 535, 'gk': 219, 'kv': 331, 'xq': 653, 'fk': 191, 'sv': 531, 'vs': 605, 'wm': 624, 'sn': 523, 'sf': 515, 'tb': 539, 'sr': 527, 'td': 541, 'hg': 243, 'uw': 587, 'wd': 616, 'zl': 702, 'cv': 120, 'db': 127, 'df': 131, 'dp': 141, 'vu': 607, 'zr': 708, 'nl': 403, 'jy': 308, \"z'\": 691, 'wu': 632, 'gq': 225, '-k': 31, 'hp': 252, 'vy': 609, 'zd': 695, 'zn': 704, 'lj': 346, 'fb': 182, 'xu': 657, 'xb': 640, 'kj': 320, 'mk': 375, '-r': 38, '-g': 28, '-v': 42, 'bz': 97, 'tg': 544, 'sj': 519, 'oj': 430, 'gj': 218, 'qi': 476, 'wc': 615, 'xw': 659, 'xx': 660, 'yf': 670, 'jd': 293, '-n': 34, 'zk': 701, 'tp': 553, 'fc': 183, '-d': 25, 'r-': 481, 'uu': 585, '-u': 41, 'py': 470, 'zw': 713, 'pb': 450, 'pw': 469, \"q'\": 472, \"'v\": 21, 'jk': 299, 'pd': 452, 'pm': 461, 'gx': 232, 'jn': 301, 'zp': 706, 'o-': 420, 'bw': 95, '-e': 26, 'wg': 619, 'zf': 697, 's-': 509, 'vl': 601, 'g-': 208, 'cw': 121, \"'a\": 4, \"'o\": 16, 'hv': 258, 'vc': 593, 'zs': 709, 'mj': 374, 'xh': 646, 'vv': 608, 'xv': 658, 'mz': 389, 'bf': 79, 'hq': 253, 'dq': 142, 'lx': 360, '-h': 29, \"'d\": 7, 'vj': 599, 'x-': 638, '-w': 43, 'xn': 650, 'xp': 652, 'jv': 307, 'zg': 698, '-y': 44, 'fj': 190, 'jt': 305, 'w-': 612, 'xg': 645, 'xm': 649, 'tx': 560, 'gz': 234, 'gv': 230, 'jj': 298, 'f-': 180, 'fw': 203, 'wv': 633, \"'l\": 13, 'h-': 236, 'hj': 246, 'fp': 196, 'js': 304, 'vh': 597, 'wz': 636, 'i': 261, 'qb': 474, 'qg': 475, 'zv': 712, 'jf': 295, 'jh': 296, 'jc': 292, 'wp': 627, 'bv': 94, 'pz': 471, 'fq': 197, 'vg': 596, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, \"j'\": 290, 'jr': 303, 'xz': 662, 'cj': 108, 'fx': 204, 'fz': 206, 'qv': 479, 'wq': 628, 'ww': 634, 'xr': 654, 'xd': 642, 'o': 418, \"'b\": 5, \"'g\": 9, \"'k\": 12, 'a-': 46, '-q': 37, 'vk': 600, 'qo': 477, 'vz': 610, 'jm': 300, 'yj': 674, 'pj': 458, 'p-': 448, 'fv': 202, 'bg': 80, 'u-': 564, 'sx': 533, 'm-': 364, 'yq': 681}\n",
      "712 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 583, 'se': 514, 'co': 113, 'ou': 441, 'ur': 582, 'rs': 500, \"'e\": 8, 'em': 165, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 478, 'uo': 579, 'ot': 440, 'te': 542, \"'t\": 20, 'ti': 546, 'is': 282, 'aa': 47, 'ab': 48, 'be': 78, 'er': 170, 'rg': 488, 'ac': 49, 'ch': 106, 'he': 241, 'en': 166, 'ak': 57, 'ke': 315, 'al': 58, 'ls': 355, 'et': 172, 'th': 545, 'am': 59, 'mo': 379, 'od': 424, 'dt': 145, 'an': 60, 'nc': 394, 'or': 438, 'ar': 64, 'rd': 485, 'de': 130, 'ma': 365, 'dv': 147, 'va': 592, 'rk': 492, 'ro': 496, 'on': 434, \"n'\": 390, \"'s\": 19, 'ns': 410, 'so': 524, 'rt': 501, 'as': 65, 'ba': 74, 'ha': 237, 'ck': 109, 'cu': 119, 'ad': 50, 'da': 126, 'ka': 311, 'di': 134, 'ai': 55, 'ir': 281, 'lk': 347, 'ki': 319, 'in': 277, 'lo': 351, 'ne': 396, 'os': 439, 'nd': 395, 'do': 140, 'ni': 400, 'ng': 398, 'nm': 404, 'me': 369, 'nt': 411, 'ts': 555, 'to': 552, 'rc': 484, 're': 486, 'sc': 512, 'sh': 517, 'ed': 156, 'at': 66, 'es': 171, 'bb': 75, 'si': 518, 'ie': 268, 'el': 164, 'll': 348, 'nh': 399, 'tt': 556, 'ev': 174, 'vi': 598, 'il': 275, 'le': 341, 'ey': 177, \"y'\": 663, 'bi': 82, 'it': 283, 'bo': 88, \"t'\": 536, 'ud': 568, 'br': 90, 'ia': 264, 'io': 278, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 163, 'bd': 77, 'la': 337, 'ah': 54, 'ic': 266, 'dn': 139, 'no': 406, 'ol': 432, 'om': 433, 'mi': 373, 'na': 392, 'du': 146, 'uc': 567, 'ct': 118, 'ee': 157, 'ul': 576, 'az': 72, 'zi': 700, 'iz': 289, 'ln': 350, 'ow': 443, 'dr': 143, 'cr': 116, 'mb': 366, 'rl': 493, 'rm': 494, 'rr': 499, 'ra': 482, 'ya': 665, 'ce': 103, 'yt': 684, 'ta': 538, 'bh': 81, 'ho': 251, 'id': 267, 'ig': 270, 'ga': 209, 'li': 345, 'ty': 561, 'gt': 228, \"a'\": 45, 'ib': 265, 'tz': 562, 'bj': 83, 'je': 294, 'ec': 155, 'bk': 84, 'kh': 318, 'bl': 85, 'ze': 696, 'e-': 152, '-b': 23, 'st': 529, 'oo': 435, 'ly': 361, 'bn': 87, \"o'\": 419, 'oa': 421, 'hi': 245, 'sm': 522, 'ri': 490, 'gi': 217, 'rn': 495, 'if': 269, 'fa': 181, 'ci': 107, 'iv': 285, 've': 595, 'uh': 572, 'im': 276, 'un': 578, 'ds': 144, 'ut': 584, 'ov': 442, \"e'\": 151, 'bp': 89, 'pl': 460, 'lp': 352, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 714, 'yk': 675, 'wi': 621, 'ea': 153, 'eg': 159, 'go': 223, 'dg': 132, 'ge': 213, 'og': 427, 'ru': 502, 'up': 580, 'pt': 467, 'tl': 549, 'tn': 551, 'ss': 528, 'yn': 678, 'uz': 590, 'zz': 715, 'zo': 705, 'sa': 510, 'ei': 161, 'lu': 357, 'lv': 358, 'rb': 483, 'rp': 497, 'tr': 554, 'su': 530, 'dl': 137, 'bt': 92, 'bu': 93, 'hm': 249, 'dz': 150, 'ys': 683, 'cs': 117, 'my': 388, 'mp': 380, 'po': 463, 'ap': 62, 'pu': 468, 'lc': 339, 'cc': 101, 'tu': 557, 'ua': 565, 'ep': 168, 'ry': 506, 't-': 537, '-p': 36, 'pr': 465, 'cl': 110, 'mm': 377, 'pa': 449, 'ny': 416, 'yi': 673, \"r'\": 480, 'gl': 220, 'cy': 122, \"s'\": 508, 'ps': 466, 'ue': 569, 'ui': 573, 'um': 577, 'mu': 385, 'ay': 71, 'op': 436, 'ph': 456, 'yl': 676, 'eb': 154, 'nb': 393, 'ht': 256, 'hy': 260, 'fi': 189, 'fy': 205, 'kl': 322, 'km': 323, 'kn': 324, 'wl': 623, 'gm': 221, 'kr': 327, 'oy': 445, 'yd': 668, \"d'\": 124, 'cm': 111, 'cn': 112, 'oc': 423, 'of': 426, 'ff': 186, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 677, 'tm': 550, 'uf': 570, 'sy': 534, 'yc': 667, \"m'\": 363, 'ik': 274, 'za': 692, 'sk': 520, 'wn': 625, 'ex': 176, 'dc': 128, 'dd': 129, 'eo': 167, 'ks': 328, 'dw': 148, 'we': 617, 'dy': 149, 'lb': 338, \"l'\": 335, 'lm': 349, 'nn': 405, 'sb': 511, 'oi': 429, 'eq': 169, 'rh': 489, 'ld': 340, 'lt': 356, 'dh': 133, 'eu': 173, 'lf': 342, 'ip': 279, 'dj': 135, 'ja': 291, 'jo': 302, 'ju': 306, 'dk': 136, 'dm': 138, \"f'\": 179, 'fo': 195, 'hs': 255, '-f': 27, '-m': 33, '-s': 39, 'gn': 222, 'ae': 51, 'sd': 513, \"g'\": 207, 'vo': 603, 'ft': 200, 'oq': 437, \"h'\": 235, 'sp': 525, 'hl': 248, \"p'\": 447, 'af': 52, 'ye': 669, 'fe': 185, 'ix': 287, 'xe': 643, 'fl': 192, 'fr': 198, 'ax': 70, 'fg': 187, 'gh': 216, \"i'\": 262, 'fh': 188, 'ox': 444, 'xi': 647, 'fm': 193, 'fs': 199, 'ef': 158, 'ug': 571, 'rw': 504, 'wa': 613, 'mn': 378, 'gy': 233, 'pe': 453, 'gc': 211, 'gf': 214, 'gg': 215, 'gr': 226, 'iu': 284, 'ew': 175, 'aw': 69, 'xc': 641, 'fu': 201, 'pp': 464, 'ok': 431, 'ko': 325, 'gu': 229, 'hh': 244, 'lg': 343, 'lq': 353, 'hn': 250, 'hr': 254, 'hu': 257, 'ml': 376, 'sl': 521, 'sw': 532, 'wo': 626, \"c'\": 98, 'gs': 227, 'rf': 487, 'nk': 402, 'tc': 540, 'iw': 286, 'aj': 56, \"x'\": 637, 'ji': 297, 'kb': 312, 'kc': 313, 'iy': 288, 'kk': 321, 'ky': 333, 'kz': 334, 'tv': 558, 'uq': 581, 'rq': 498, 'oh': 428, 'eh': 160, 'ej': 162, 'xa': 639, 'xo': 651, 'xy': 661, 'nq': 408, 'lh': 344, 'nz': 417, 'ij': 273, 'iq': 280, 'l-': 336, '-i': 30, 'n-': 391, '-o': 35, 'nw': 414, 'yw': 687, 'nu': 412, 'lr': 354, 'pi': 457, 'uv': 586, 'lw': 359, 'ez': 178, 'mq': 381, 'fn': 194, \"k'\": 309, 'tf': 543, 'zh': 699, 'wy': 635, 'lz': 362, 'ku': 330, 'np': 407, 'xt': 656, 'md': 368, 'zc': 694, 'zq': 707, 'mf': 370, 'mg': 371, 'mh': 372, 'yv': 686, 'oe': 425, 'pc': 451, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yo': 679, 'yx': 688, 'yz': 690, 'sq': 526, 'nv': 413, 'oz': 446, 'ws': 630, 'uj': 574, 'cd': 102, 'nf': 397, 'nj': 401, 'ih': 271, 'nr': 409, 'sg': 516, 'i-': 263, 'ub': 566, 'bm': 86, 'tk': 548, \"u'\": 563, 'tw': 559, 'nx': 415, 'yb': 666, 'yh': 672, 'yp': 680, 'wh': 620, 'ao': 61, 'pf': 454, 'pg': 455, 'pk': 459, 'aq': 63, 'qa': 473, 'rv': 503, 'ux': 588, 'hb': 238, 'hd': 240, 'uk': 575, 'zm': 703, 'rj': 491, 'kw': 332, 'wr': 629, 'd-': 125, '-t': 40, '-c': 24, \"w'\": 611, 'zt': 710, 'zu': 711, 'rx': 505, 'rz': 507, 'ii': 272, 'hc': 239, 'hf': 242, 'hw': 259, \"v'\": 591, 'pn': 462, 'yr': 682, 'fd': 184, \"'h\": 10, '-l': 32, 'uy': 589, 'vd': 594, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 614, 'wf': 618, 'wk': 622, 'wt': 631, 'xf': 644, 'xl': 648, 'xs': 655, 'yg': 671, 'yu': 685, 'yy': 689, 'zb': 693, \"b'\": 73, \"'r\": 18, 'hk': 247, 'k-': 310, '-a': 22, 'kd': 314, 'kf': 316, 'kg': 317, 'kp': 326, 'kt': 329, 'y-': 664, 'tj': 547, 'gb': 210, 'gd': 212, 'gp': 224, 'gw': 231, \"'i\": 11, 'sz': 535, 'gk': 219, 'kv': 331, 'xq': 653, 'fk': 191, 'sv': 531, 'vs': 605, 'wm': 624, 'sn': 523, 'sf': 515, 'tb': 539, 'sr': 527, 'td': 541, 'hg': 243, 'uw': 587, 'wd': 616, 'zl': 702, 'cv': 120, 'db': 127, 'df': 131, 'dp': 141, 'vu': 607, 'zr': 708, 'nl': 403, 'jy': 308, \"z'\": 691, 'wu': 632, 'gq': 225, '-k': 31, 'hp': 252, 'vy': 609, 'zd': 695, 'zn': 704, 'lj': 346, 'fb': 182, 'xu': 657, 'xb': 640, 'kj': 320, 'mk': 375, '-r': 38, '-g': 28, '-v': 42, 'bz': 97, 'tg': 544, 'sj': 519, 'oj': 430, 'gj': 218, 'qi': 476, 'wc': 615, 'xw': 659, 'xx': 660, 'yf': 670, 'jd': 293, '-n': 34, 'zk': 701, 'tp': 553, 'fc': 183, '-d': 25, 'r-': 481, 'uu': 585, '-u': 41, 'py': 470, 'zw': 713, 'pb': 450, 'pw': 469, \"q'\": 472, \"'v\": 21, 'jk': 299, 'pd': 452, 'pm': 461, 'gx': 232, 'jn': 301, 'zp': 706, 'o-': 420, 'bw': 95, '-e': 26, 'wg': 619, 'zf': 697, 's-': 509, 'vl': 601, 'g-': 208, 'cw': 121, \"'a\": 4, \"'o\": 16, 'hv': 258, 'vc': 593, 'zs': 709, 'mj': 374, 'xh': 646, 'vv': 608, 'xv': 658, 'mz': 389, 'bf': 79, 'hq': 253, 'dq': 142, 'lx': 360, '-h': 29, \"'d\": 7, 'vj': 599, 'x-': 638, '-w': 43, 'xn': 650, 'xp': 652, 'jv': 307, 'zg': 698, '-y': 44, 'fj': 190, 'jt': 305, 'w-': 612, 'xg': 645, 'xm': 649, 'tx': 560, 'gz': 234, 'gv': 230, 'jj': 298, 'f-': 180, 'fw': 203, 'wv': 633, \"'l\": 13, 'h-': 236, 'hj': 246, 'fp': 196, 'js': 304, 'vh': 597, 'wz': 636, 'i': 261, 'qb': 474, 'qg': 475, 'zv': 712, 'jf': 295, 'jh': 296, 'jc': 292, 'wp': 627, 'bv': 94, 'pz': 471, 'fq': 197, 'vg': 596, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, \"j'\": 290, 'jr': 303, 'xz': 662, 'cj': 108, 'fx': 204, 'fz': 206, 'qv': 479, 'wq': 628, 'ww': 634, 'xr': 654, 'xd': 642, 'o': 418, \"'b\": 5, \"'g\": 9, \"'k\": 12, 'a-': 46, '-q': 37, 'vk': 600, 'qo': 477, 'vz': 610, 'jm': 300, 'yj': 674, 'pj': 458, 'p-': 448, 'fv': 202, 'bg': 80, 'u-': 564, 'sx': 533, 'm-': 364, 'yq': 681}\n",
      "712 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 583, 'se': 514, 'co': 113, 'ou': 441, 'ur': 582, 'rs': 500, \"'e\": 8, 'em': 165, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 478, 'uo': 579, 'ot': 440, 'te': 542, \"'t\": 20, 'ti': 546, 'is': 282, 'aa': 47, 'ab': 48, 'be': 78, 'er': 170, 'rg': 488, 'ac': 49, 'ch': 106, 'he': 241, 'en': 166, 'ak': 57, 'ke': 315, 'al': 58, 'ls': 355, 'et': 172, 'th': 545, 'am': 59, 'mo': 379, 'od': 424, 'dt': 145, 'an': 60, 'nc': 394, 'or': 438, 'ar': 64, 'rd': 485, 'de': 130, 'ma': 365, 'dv': 147, 'va': 592, 'rk': 492, 'ro': 496, 'on': 434, \"n'\": 390, \"'s\": 19, 'ns': 410, 'so': 524, 'rt': 501, 'as': 65, 'ba': 74, 'ha': 237, 'ck': 109, 'cu': 119, 'ad': 50, 'da': 126, 'ka': 311, 'di': 134, 'ai': 55, 'ir': 281, 'lk': 347, 'ki': 319, 'in': 277, 'lo': 351, 'ne': 396, 'os': 439, 'nd': 395, 'do': 140, 'ni': 400, 'ng': 398, 'nm': 404, 'me': 369, 'nt': 411, 'ts': 555, 'to': 552, 'rc': 484, 're': 486, 'sc': 512, 'sh': 517, 'ed': 156, 'at': 66, 'es': 171, 'bb': 75, 'si': 518, 'ie': 268, 'el': 164, 'll': 348, 'nh': 399, 'tt': 556, 'ev': 174, 'vi': 598, 'il': 275, 'le': 341, 'ey': 177, \"y'\": 663, 'bi': 82, 'it': 283, 'bo': 88, \"t'\": 536, 'ud': 568, 'br': 90, 'ia': 264, 'io': 278, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 163, 'bd': 77, 'la': 337, 'ah': 54, 'ic': 266, 'dn': 139, 'no': 406, 'ol': 432, 'om': 433, 'mi': 373, 'na': 392, 'du': 146, 'uc': 567, 'ct': 118, 'ee': 157, 'ul': 576, 'az': 72, 'zi': 700, 'iz': 289, 'ln': 350, 'ow': 443, 'dr': 143, 'cr': 116, 'mb': 366, 'rl': 493, 'rm': 494, 'rr': 499, 'ra': 482, 'ya': 665, 'ce': 103, 'yt': 684, 'ta': 538, 'bh': 81, 'ho': 251, 'id': 267, 'ig': 270, 'ga': 209, 'li': 345, 'ty': 561, 'gt': 228, \"a'\": 45, 'ib': 265, 'tz': 562, 'bj': 83, 'je': 294, 'ec': 155, 'bk': 84, 'kh': 318, 'bl': 85, 'ze': 696, 'e-': 152, '-b': 23, 'st': 529, 'oo': 435, 'ly': 361, 'bn': 87, \"o'\": 419, 'oa': 421, 'hi': 245, 'sm': 522, 'ri': 490, 'gi': 217, 'rn': 495, 'if': 269, 'fa': 181, 'ci': 107, 'iv': 285, 've': 595, 'uh': 572, 'im': 276, 'un': 578, 'ds': 144, 'ut': 584, 'ov': 442, \"e'\": 151, 'bp': 89, 'pl': 460, 'lp': 352, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 714, 'yk': 675, 'wi': 621, 'ea': 153, 'eg': 159, 'go': 223, 'dg': 132, 'ge': 213, 'og': 427, 'ru': 502, 'up': 580, 'pt': 467, 'tl': 549, 'tn': 551, 'ss': 528, 'yn': 678, 'uz': 590, 'zz': 715, 'zo': 705, 'sa': 510, 'ei': 161, 'lu': 357, 'lv': 358, 'rb': 483, 'rp': 497, 'tr': 554, 'su': 530, 'dl': 137, 'bt': 92, 'bu': 93, 'hm': 249, 'dz': 150, 'ys': 683, 'cs': 117, 'my': 388, 'mp': 380, 'po': 463, 'ap': 62, 'pu': 468, 'lc': 339, 'cc': 101, 'tu': 557, 'ua': 565, 'ep': 168, 'ry': 506, 't-': 537, '-p': 36, 'pr': 465, 'cl': 110, 'mm': 377, 'pa': 449, 'ny': 416, 'yi': 673, \"r'\": 480, 'gl': 220, 'cy': 122, \"s'\": 508, 'ps': 466, 'ue': 569, 'ui': 573, 'um': 577, 'mu': 385, 'ay': 71, 'op': 436, 'ph': 456, 'yl': 676, 'eb': 154, 'nb': 393, 'ht': 256, 'hy': 260, 'fi': 189, 'fy': 205, 'kl': 322, 'km': 323, 'kn': 324, 'wl': 623, 'gm': 221, 'kr': 327, 'oy': 445, 'yd': 668, \"d'\": 124, 'cm': 111, 'cn': 112, 'oc': 423, 'of': 426, 'ff': 186, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 677, 'tm': 550, 'uf': 570, 'sy': 534, 'yc': 667, \"m'\": 363, 'ik': 274, 'za': 692, 'sk': 520, 'wn': 625, 'ex': 176, 'dc': 128, 'dd': 129, 'eo': 167, 'ks': 328, 'dw': 148, 'we': 617, 'dy': 149, 'lb': 338, \"l'\": 335, 'lm': 349, 'nn': 405, 'sb': 511, 'oi': 429, 'eq': 169, 'rh': 489, 'ld': 340, 'lt': 356, 'dh': 133, 'eu': 173, 'lf': 342, 'ip': 279, 'dj': 135, 'ja': 291, 'jo': 302, 'ju': 306, 'dk': 136, 'dm': 138, \"f'\": 179, 'fo': 195, 'hs': 255, '-f': 27, '-m': 33, '-s': 39, 'gn': 222, 'ae': 51, 'sd': 513, \"g'\": 207, 'vo': 603, 'ft': 200, 'oq': 437, \"h'\": 235, 'sp': 525, 'hl': 248, \"p'\": 447, 'af': 52, 'ye': 669, 'fe': 185, 'ix': 287, 'xe': 643, 'fl': 192, 'fr': 198, 'ax': 70, 'fg': 187, 'gh': 216, \"i'\": 262, 'fh': 188, 'ox': 444, 'xi': 647, 'fm': 193, 'fs': 199, 'ef': 158, 'ug': 571, 'rw': 504, 'wa': 613, 'mn': 378, 'gy': 233, 'pe': 453, 'gc': 211, 'gf': 214, 'gg': 215, 'gr': 226, 'iu': 284, 'ew': 175, 'aw': 69, 'xc': 641, 'fu': 201, 'pp': 464, 'ok': 431, 'ko': 325, 'gu': 229, 'hh': 244, 'lg': 343, 'lq': 353, 'hn': 250, 'hr': 254, 'hu': 257, 'ml': 376, 'sl': 521, 'sw': 532, 'wo': 626, \"c'\": 98, 'gs': 227, 'rf': 487, 'nk': 402, 'tc': 540, 'iw': 286, 'aj': 56, \"x'\": 637, 'ji': 297, 'kb': 312, 'kc': 313, 'iy': 288, 'kk': 321, 'ky': 333, 'kz': 334, 'tv': 558, 'uq': 581, 'rq': 498, 'oh': 428, 'eh': 160, 'ej': 162, 'xa': 639, 'xo': 651, 'xy': 661, 'nq': 408, 'lh': 344, 'nz': 417, 'ij': 273, 'iq': 280, 'l-': 336, '-i': 30, 'n-': 391, '-o': 35, 'nw': 414, 'yw': 687, 'nu': 412, 'lr': 354, 'pi': 457, 'uv': 586, 'lw': 359, 'ez': 178, 'mq': 381, 'fn': 194, \"k'\": 309, 'tf': 543, 'zh': 699, 'wy': 635, 'lz': 362, 'ku': 330, 'np': 407, 'xt': 656, 'md': 368, 'zc': 694, 'zq': 707, 'mf': 370, 'mg': 371, 'mh': 372, 'yv': 686, 'oe': 425, 'pc': 451, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yo': 679, 'yx': 688, 'yz': 690, 'sq': 526, 'nv': 413, 'oz': 446, 'ws': 630, 'uj': 574, 'cd': 102, 'nf': 397, 'nj': 401, 'ih': 271, 'nr': 409, 'sg': 516, 'i-': 263, 'ub': 566, 'bm': 86, 'tk': 548, \"u'\": 563, 'tw': 559, 'nx': 415, 'yb': 666, 'yh': 672, 'yp': 680, 'wh': 620, 'ao': 61, 'pf': 454, 'pg': 455, 'pk': 459, 'aq': 63, 'qa': 473, 'rv': 503, 'ux': 588, 'hb': 238, 'hd': 240, 'uk': 575, 'zm': 703, 'rj': 491, 'kw': 332, 'wr': 629, 'd-': 125, '-t': 40, '-c': 24, \"w'\": 611, 'zt': 710, 'zu': 711, 'rx': 505, 'rz': 507, 'ii': 272, 'hc': 239, 'hf': 242, 'hw': 259, \"v'\": 591, 'pn': 462, 'yr': 682, 'fd': 184, \"'h\": 10, '-l': 32, 'uy': 589, 'vd': 594, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 614, 'wf': 618, 'wk': 622, 'wt': 631, 'xf': 644, 'xl': 648, 'xs': 655, 'yg': 671, 'yu': 685, 'yy': 689, 'zb': 693, \"b'\": 73, \"'r\": 18, 'hk': 247, 'k-': 310, '-a': 22, 'kd': 314, 'kf': 316, 'kg': 317, 'kp': 326, 'kt': 329, 'y-': 664, 'tj': 547, 'gb': 210, 'gd': 212, 'gp': 224, 'gw': 231, \"'i\": 11, 'sz': 535, 'gk': 219, 'kv': 331, 'xq': 653, 'fk': 191, 'sv': 531, 'vs': 605, 'wm': 624, 'sn': 523, 'sf': 515, 'tb': 539, 'sr': 527, 'td': 541, 'hg': 243, 'uw': 587, 'wd': 616, 'zl': 702, 'cv': 120, 'db': 127, 'df': 131, 'dp': 141, 'vu': 607, 'zr': 708, 'nl': 403, 'jy': 308, \"z'\": 691, 'wu': 632, 'gq': 225, '-k': 31, 'hp': 252, 'vy': 609, 'zd': 695, 'zn': 704, 'lj': 346, 'fb': 182, 'xu': 657, 'xb': 640, 'kj': 320, 'mk': 375, '-r': 38, '-g': 28, '-v': 42, 'bz': 97, 'tg': 544, 'sj': 519, 'oj': 430, 'gj': 218, 'qi': 476, 'wc': 615, 'xw': 659, 'xx': 660, 'yf': 670, 'jd': 293, '-n': 34, 'zk': 701, 'tp': 553, 'fc': 183, '-d': 25, 'r-': 481, 'uu': 585, '-u': 41, 'py': 470, 'zw': 713, 'pb': 450, 'pw': 469, \"q'\": 472, \"'v\": 21, 'jk': 299, 'pd': 452, 'pm': 461, 'gx': 232, 'jn': 301, 'zp': 706, 'o-': 420, 'bw': 95, '-e': 26, 'wg': 619, 'zf': 697, 's-': 509, 'vl': 601, 'g-': 208, 'cw': 121, \"'a\": 4, \"'o\": 16, 'hv': 258, 'vc': 593, 'zs': 709, 'mj': 374, 'xh': 646, 'vv': 608, 'xv': 658, 'mz': 389, 'bf': 79, 'hq': 253, 'dq': 142, 'lx': 360, '-h': 29, \"'d\": 7, 'vj': 599, 'x-': 638, '-w': 43, 'xn': 650, 'xp': 652, 'jv': 307, 'zg': 698, '-y': 44, 'fj': 190, 'jt': 305, 'w-': 612, 'xg': 645, 'xm': 649, 'tx': 560, 'gz': 234, 'gv': 230, 'jj': 298, 'f-': 180, 'fw': 203, 'wv': 633, \"'l\": 13, 'h-': 236, 'hj': 246, 'fp': 196, 'js': 304, 'vh': 597, 'wz': 636, 'i': 261, 'qb': 474, 'qg': 475, 'zv': 712, 'jf': 295, 'jh': 296, 'jc': 292, 'wp': 627, 'bv': 94, 'pz': 471, 'fq': 197, 'vg': 596, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, \"j'\": 290, 'jr': 303, 'xz': 662, 'cj': 108, 'fx': 204, 'fz': 206, 'qv': 479, 'wq': 628, 'ww': 634, 'xr': 654, 'xd': 642, 'o': 418, \"'b\": 5, \"'g\": 9, \"'k\": 12, 'a-': 46, '-q': 37, 'vk': 600, 'qo': 477, 'vz': 610, 'jm': 300, 'yj': 674, 'pj': 458, 'p-': 448, 'fv': 202, 'bg': 80, 'u-': 564, 'sx': 533, 'm-': 364, 'yq': 681}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "      )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 128\n",
      "hidden_size: 50\n",
      "n_layers: 1\n",
      "Encoder has a total number of 118648 parameters\n",
      "Decoder has a total number of 42515 parameters\n",
      "Total number of all parameters is 161163\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 1 finished in 0m 37.49s (- 61m 51.82s) (1 1.0%). train avg loss: 1.7188, val avg loss: 1.6163\n",
      "Training for epoch 2 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 2 finished in 1m 13.57s (- 60m 5.04s) (2 2.0%). train avg loss: 0.8499, val avg loss: 1.258\n",
      "Training for epoch 3 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 3 finished in 1m 46.18s (- 57m 13.31s) (3 3.0%). train avg loss: 0.6056, val avg loss: 1.0523\n",
      "Training for epoch 4 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 4 finished in 2m 20.3s (- 56m 7.23s) (4 4.0%). train avg loss: 0.5235, val avg loss: 0.9676\n",
      "Training for epoch 5 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 5 finished in 2m 53.31s (- 54m 52.89s) (5 5.0%). train avg loss: 0.4607, val avg loss: 0.947\n",
      "Training for epoch 6 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 6 finished in 3m 32.76s (- 55m 33.31s) (6 6.0%). train avg loss: 0.4272, val avg loss: 1.001\n",
      "Training for epoch 7 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 7 finished in 4m 5.61s (- 54m 23.12s) (7 7.0%). train avg loss: 0.4088, val avg loss: 0.9484\n",
      "Training for epoch 8 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 8 finished in 4m 37.26s (- 53m 8.45s) (8 8.0%). train avg loss: 0.4019, val avg loss: 0.8723\n",
      "Training for epoch 9 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 9 finished in 5m 9.12s (- 52m 5.55s) (9 9.0%). train avg loss: 0.374, val avg loss: 0.8287\n",
      "Training for epoch 10 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 10 finished in 5m 40.94s (- 51m 8.49s) (10 10.0%). train avg loss: 0.351, val avg loss: 0.8202\n",
      "Training for epoch 11 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 11 finished in 6m 12.78s (- 50m 16.12s) (11 11.0%). train avg loss: 0.3396, val avg loss: 0.7745\n",
      "Training for epoch 12 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 12 finished in 6m 44.39s (- 49m 25.55s) (12 12.0%). train avg loss: 0.3334, val avg loss: 0.7975\n",
      "Training for epoch 13 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 13 finished in 7m 16.16s (- 48m 38.93s) (13 13.0%). train avg loss: 0.3366, val avg loss: 0.7775\n",
      "Training for epoch 14 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 14 finished in 7m 48.37s (- 47m 57.13s) (14 14.0%). train avg loss: 0.3088, val avg loss: 0.828\n",
      "Training for epoch 15 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 15 finished in 8m 20.93s (- 47m 18.59s) (15 15.0%). train avg loss: 0.3106, val avg loss: 0.723\n",
      "Training for epoch 16 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 16 finished in 8m 53.3s (- 46m 39.8s) (16 16.0%). train avg loss: 0.301, val avg loss: 0.7623\n",
      "Training for epoch 17 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 17 finished in 9m 25.43s (- 46m 0.61s) (17 17.0%). train avg loss: 0.2987, val avg loss: 0.7681\n",
      "Training for epoch 18 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 18 finished in 9m 57.84s (- 45m 23.52s) (18 18.0%). train avg loss: 0.2856, val avg loss: 0.7217\n",
      "Training for epoch 19 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 19 finished in 10m 30.78s (- 44m 49.13s) (19 19.0%). train avg loss: 0.296, val avg loss: 0.7533\n",
      "Training for epoch 20 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 20 finished in 11m 2.76s (- 44m 11.03s) (20 20.0%). train avg loss: 0.2738, val avg loss: 0.7466\n",
      "Training for epoch 21 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 21 finished in 11m 34.85s (- 43m 33.97s) (21 21.0%). train avg loss: 0.2797, val avg loss: 0.7133\n",
      "Training for epoch 22 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 22 finished in 12m 6.95s (- 42m 57.37s) (22 22.0%). train avg loss: 0.2784, val avg loss: 0.6963\n",
      "Training for epoch 23 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 23 finished in 12m 40.27s (- 42m 25.26s) (23 23.0%). train avg loss: 0.2698, val avg loss: 0.7143\n",
      "Training for epoch 24 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 24 finished in 13m 13.97s (- 41m 54.23s) (24 24.0%). train avg loss: 0.264, val avg loss: 0.7265\n",
      "Training for epoch 25 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 25 finished in 13m 46.14s (- 41m 18.41s) (25 25.0%). train avg loss: 0.2535, val avg loss: 0.808\n",
      "Training for epoch 26 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 26 finished in 14m 17.84s (- 40m 41.55s) (26 26.0%). train avg loss: 0.2598, val avg loss: 0.7898\n",
      "Training for epoch 27 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 27 finished in 14m 50.55s (- 40m 7.78s) (27 27.0%). train avg loss: 0.2591, val avg loss: 0.7059\n",
      "Training for epoch 28 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 28 finished in 15m 22.99s (- 39m 33.4s) (28 28.0%). train avg loss: 0.2505, val avg loss: 0.6688\n",
      "Training for epoch 29 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 29 finished in 15m 55.8s (- 39m 0.07s) (29 29.0%). train avg loss: 0.2394, val avg loss: 0.6776\n",
      "Training for epoch 30 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 30 finished in 16m 29.28s (- 38m 28.32s) (30 30.0%). train avg loss: 0.2628, val avg loss: 0.71\n",
      "Training for epoch 31 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 31 finished in 17m 1.34s (- 37m 53.29s) (31 31.0%). train avg loss: 0.2475, val avg loss: 0.6398\n",
      "Training for epoch 32 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 32 finished in 17m 33.99s (- 37m 19.72s) (32 32.0%). train avg loss: 0.2363, val avg loss: 0.8397\n",
      "Training for epoch 33 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 33 finished in 18m 7.65s (- 36m 48.25s) (33 33.0%). train avg loss: 0.2498, val avg loss: 0.6989\n",
      "Training for epoch 34 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 34 finished in 18m 39.67s (- 36m 13.47s) (34 34.0%). train avg loss: 0.2351, val avg loss: 0.6896\n",
      "Training for epoch 35 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 35 finished in 19m 11.57s (- 35m 38.63s) (35 35.0%). train avg loss: 0.2474, val avg loss: 0.7143\n",
      "Training for epoch 36 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 36 finished in 19m 43.37s (- 35m 3.77s) (36 36.0%). train avg loss: 0.2278, val avg loss: 0.6548\n",
      "Training for epoch 37 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 37 finished in 20m 15.72s (- 34m 30.01s) (37 37.0%). train avg loss: 0.2336, val avg loss: 0.6598\n",
      "Training for epoch 38 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 38 finished in 20m 48.38s (- 33m 56.83s) (38 38.0%). train avg loss: 0.2277, val avg loss: 0.6097\n",
      "Training for epoch 39 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 39 finished in 21m 23.31s (- 33m 27.22s) (39 39.0%). train avg loss: 0.2395, val avg loss: 0.6516\n",
      "Training for epoch 40 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 40 finished in 21m 57.97s (- 32m 56.96s) (40 40.0%). train avg loss: 0.2286, val avg loss: 0.6898\n",
      "Training for epoch 41 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 41 finished in 22m 31.54s (- 32m 24.91s) (41 41.0%). train avg loss: 0.2318, val avg loss: 0.694\n",
      "Training for epoch 42 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 42 finished in 23m 6.8s (- 31m 55.1s) (42 42.0%). train avg loss: 0.2459, val avg loss: 0.6971\n",
      "Training for epoch 43 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 43 finished in 23m 43.55s (- 31m 27.03s) (43 43.0%). train avg loss: 0.2237, val avg loss: 0.6372\n",
      "Training for epoch 44 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 44 finished in 24m 18.27s (- 30m 55.98s) (44 44.0%). train avg loss: 0.2195, val avg loss: 0.6108\n",
      "Training for epoch 45 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 45 finished in 24m 53.5s (- 30m 25.39s) (45 45.0%). train avg loss: 0.2304, val avg loss: 0.6343\n",
      "Training for epoch 46 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 46 finished in 25m 29.12s (- 29m 55.06s) (46 46.0%). train avg loss: 0.2097, val avg loss: 0.6195\n",
      "Training for epoch 47 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 47 finished in 26m 4.59s (- 29m 24.32s) (47 47.0%). train avg loss: 0.221, val avg loss: 0.73\n",
      "Training for epoch 48 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 48 finished in 26m 44.53s (- 28m 58.24s) (48 48.0%). train avg loss: 0.2181, val avg loss: 0.6297\n",
      "Training for epoch 49 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 49 finished in 27m 18.53s (- 28m 25.4s) (49 49.0%). train avg loss: 0.2196, val avg loss: 0.7462\n",
      "Training for epoch 50 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 50 finished in 27m 53.28s (- 27m 53.28s) (50 50.0%). train avg loss: 0.2057, val avg loss: 0.576\n",
      "Training for epoch 51 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 51 finished in 28m 27.31s (- 27m 20.36s) (51 51.0%). train avg loss: 0.1871, val avg loss: 0.5915\n",
      "Training for epoch 52 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 52 finished in 29m 0.68s (- 26m 46.79s) (52 52.0%). train avg loss: 0.1914, val avg loss: 0.6147\n",
      "Training for epoch 53 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 53 finished in 29m 36.04s (- 26m 14.98s) (53 53.0%). train avg loss: 0.1816, val avg loss: 0.6244\n",
      "Training for epoch 54 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 54 finished in 30m 9.94s (- 25m 41.8s) (54 54.0%). train avg loss: 0.1842, val avg loss: 0.6099\n",
      "Training for epoch 55 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 55 finished in 30m 45.1s (- 25m 9.62s) (55 55.0%). train avg loss: 0.1795, val avg loss: 0.6038\n",
      "Training for epoch 56 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 56 finished in 31m 19.49s (- 24m 36.74s) (56 56.0%). train avg loss: 0.1836, val avg loss: 0.5763\n",
      "Training for epoch 57 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 57 finished in 31m 54.06s (- 24m 3.94s) (57 57.0%). train avg loss: 0.185, val avg loss: 0.5606\n",
      "Training for epoch 58 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 58 finished in 32m 28.07s (- 23m 30.67s) (58 58.0%). train avg loss: 0.1782, val avg loss: 0.6057\n",
      "Training for epoch 59 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 59 finished in 33m 5.93s (- 23m 0.05s) (59 59.0%). train avg loss: 0.1812, val avg loss: 0.5609\n",
      "Training for epoch 60 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 60 finished in 33m 40.96s (- 22m 27.31s) (60 60.0%). train avg loss: 0.1817, val avg loss: 0.577\n",
      "Training for epoch 61 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 61 finished in 34m 14.05s (- 21m 53.25s) (61 61.0%). train avg loss: 0.1708, val avg loss: 0.5997\n",
      "Training for epoch 62 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 62 finished in 34m 53.51s (- 21m 23.12s) (62 62.0%). train avg loss: 0.1781, val avg loss: 0.5878\n",
      "Training for epoch 63 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 63 finished in 35m 28.48s (- 20m 50.06s) (63 63.0%). train avg loss: 0.1745, val avg loss: 0.6191\n",
      "Training for epoch 64 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 64 finished in 36m 6.07s (- 20m 18.42s) (64 64.0%). train avg loss: 0.1746, val avg loss: 0.6044\n",
      "Training for epoch 65 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 65 finished in 36m 40.26s (- 19m 44.75s) (65 65.0%). train avg loss: 0.1759, val avg loss: 0.5895\n",
      "Training for epoch 66 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 66 finished in 37m 13.88s (- 19m 10.79s) (66 66.0%). train avg loss: 0.1785, val avg loss: 0.5681\n",
      "Training for epoch 67 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 67 finished in 37m 48.21s (- 18m 37.18s) (67 67.0%). train avg loss: 0.1747, val avg loss: 0.5851\n",
      "Training for epoch 68 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 68 finished in 38m 22.09s (- 18m 3.33s) (68 68.0%). train avg loss: 0.1732, val avg loss: 0.5772\n",
      "Training for epoch 69 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 69 finished in 38m 54.98s (- 17m 29.05s) (69 69.0%). train avg loss: 0.1608, val avg loss: 0.5823\n",
      "Training for epoch 70 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 70 finished in 39m 30.38s (- 16m 55.88s) (70 70.0%). train avg loss: 0.1604, val avg loss: 0.5567\n",
      "Training for epoch 71 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 71 finished in 40m 5.54s (- 16m 22.54s) (71 71.0%). train avg loss: 0.1608, val avg loss: 0.586\n",
      "Training for epoch 72 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 72 finished in 40m 43.02s (- 15m 50.06s) (72 72.0%). train avg loss: 0.1612, val avg loss: 0.5544\n",
      "Training for epoch 73 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 73 finished in 41m 17.69s (- 15m 16.41s) (73 73.0%). train avg loss: 0.1582, val avg loss: 0.5635\n",
      "Training for epoch 74 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 74 finished in 41m 50.49s (- 14m 42.07s) (74 74.0%). train avg loss: 0.1566, val avg loss: 0.5589\n",
      "Training for epoch 75 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 75 finished in 42m 24.25s (- 14m 8.08s) (75 75.0%). train avg loss: 0.1573, val avg loss: 0.5681\n",
      "Training for epoch 76 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 76 finished in 43m 1.37s (- 13m 35.17s) (76 76.0%). train avg loss: 0.1597, val avg loss: 0.5482\n",
      "Training for epoch 77 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 77 finished in 43m 36.64s (- 13m 1.59s) (77 77.0%). train avg loss: 0.1599, val avg loss: 0.5473\n",
      "Training for epoch 78 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 78 finished in 44m 11.36s (- 12m 27.82s) (78 78.0%). train avg loss: 0.1563, val avg loss: 0.6062\n",
      "Training for epoch 79 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 79 finished in 44m 46.73s (- 11m 54.2s) (79 79.0%). train avg loss: 0.1564, val avg loss: 0.5828\n",
      "Training for epoch 80 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 80 finished in 45m 20.46s (- 11m 20.12s) (80 80.0%). train avg loss: 0.1561, val avg loss: 0.5747\n",
      "Training for epoch 81 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 81 finished in 45m 54.0s (- 10m 46.0s) (81 81.0%). train avg loss: 0.1555, val avg loss: 0.5824\n",
      "Training for epoch 82 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 82 finished in 46m 28.72s (- 10m 12.16s) (82 82.0%). train avg loss: 0.1559, val avg loss: 0.569\n",
      "Training for epoch 83 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 83 finished in 47m 3.4s (- 9m 38.29s) (83 83.0%). train avg loss: 0.1559, val avg loss: 0.5773\n",
      "Training for epoch 84 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 84 finished in 47m 38.55s (- 9m 4.49s) (84 84.0%). train avg loss: 0.1544, val avg loss: 0.5667\n",
      "Training for epoch 85 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 85 finished in 48m 14.19s (- 8m 30.74s) (85 85.0%). train avg loss: 0.1544, val avg loss: 0.5724\n",
      "Training for epoch 86 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 86 finished in 48m 55.07s (- 7m 57.8s) (86 86.0%). train avg loss: 0.1519, val avg loss: 0.5737\n",
      "Training for epoch 87 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 87 finished in 49m 30.95s (- 7m 23.93s) (87 87.0%). train avg loss: 0.1523, val avg loss: 0.5931\n",
      "Training for epoch 88 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 88 finished in 50m 6.21s (- 6m 49.94s) (88 88.0%). train avg loss: 0.1527, val avg loss: 0.5565\n",
      "Training for epoch 89 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 89 finished in 50m 44.27s (- 6m 16.26s) (89 89.0%). train avg loss: 0.1459, val avg loss: 0.582\n",
      "Training for epoch 90 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 90 finished in 51m 21.06s (- 5m 42.34s) (90 90.0%). train avg loss: 0.1456, val avg loss: 0.5567\n",
      "Training for epoch 91 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 91 finished in 51m 57.79s (- 5m 8.35s) (91 91.0%). train avg loss: 0.1443, val avg loss: 0.5819\n",
      "Training for epoch 92 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 92 finished in 52m 33.12s (- 4m 34.18s) (92 92.0%). train avg loss: 0.1455, val avg loss: 0.5919\n",
      "Training for epoch 93 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 93 finished in 53m 5.68s (- 3m 59.78s) (93 93.0%). train avg loss: 0.1473, val avg loss: 0.5518\n",
      "Training for epoch 94 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 94 finished in 53m 38.44s (- 3m 25.43s) (94 94.0%). train avg loss: 0.1455, val avg loss: 0.5813\n",
      "Training for epoch 95 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 95 finished in 54m 11.13s (- 2m 51.11s) (95 95.0%). train avg loss: 0.143, val avg loss: 0.5649\n",
      "Training for epoch 96 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 96 finished in 54m 43.61s (- 2m 16.82s) (96 96.0%). train avg loss: 0.1487, val avg loss: 0.5827\n",
      "Training for epoch 97 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 97 finished in 55m 17.11s (- 1m 42.59s) (97 97.0%). train avg loss: 0.1449, val avg loss: 0.5897\n",
      "Training for epoch 98 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 98 finished in 55m 53.67s (- 1m 8.44s) (98 98.0%). train avg loss: 0.1483, val avg loss: 0.567\n",
      "Training for epoch 99 has started (lr=0.000125). Found 1586 batch(es).\n",
      "Epoch 99 finished in 56m 26.49s (- 0m 34.21s) (99 99.0%). train avg loss: 0.1446, val avg loss: 0.5706\n",
      "Training for epoch 100 has started (lr=6.25e-05). Found 1586 batch(es).\n",
      "Epoch 100 finished in 57m 0.42s (- 0m 0.0s) (100 100.0%). train avg loss: 0.1425, val avg loss: 0.5715\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"train-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"train-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"train-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"train-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5.5, min_value-.125, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 12.517865772679663%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> LOCI\n",
      "= ['L', 'AO', 'W', 'K', 'IY']\n",
      "< L AO S IY ['L', 'AO', 'S', 'IY']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8e59b84490>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVwAAAI1CAYAAACaBUIjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAR6klEQVR4nO3cX4iV953H8e9xJh5DOzMbE5SKkyBbdpvWNUs0FEMSTNsMSJF414siobQXEpWKN8XmJi2U6c2WlrWR2pb0oqSR0pqEJRUGWseE4FZNQkIpWQKhTkmsTaEzo0uPq569WDLbiX+SMzqf4xxfL3gI5/E5/L7wyJtfnnM8jXa73S4A5t2ibg8AcKMQXIAQwQUIEVyAEMEFCBFcgBDBBQgRXIAQwQUIEVyAEMG9Ck888UStWrWqlixZUmvXrq0XXnih2yNRVYcPH65NmzbVihUrqtFo1DPPPNPtkaiq0dHRuueee2pgYKCWLVtWmzdvrjfeeKPbY0UJ7hzt37+/du7cWY899li98sordf/999fGjRvrxIkT3R7thnfmzJm66667as+ePd0ehb8zPj5e27ZtqyNHjtTY2FidO3euRkZG6syZM90eLabhx2vm5tOf/nTdfffdtXfv3plzd955Z23evLlGR0e7OBl/r9Fo1IEDB2rz5s3dHoX3+fOf/1zLli2r8fHxeuCBB7o9ToQd7hycPXu2jh8/XiMjI7POj4yM1EsvvdSlqWBhmZycrKqqpUuXdnmSHMGdg3fffbfOnz9fy5cvn3V++fLldfLkyS5NBQtHu92uXbt21X333VerV6/u9jgx/d0eYCFrNBqzXrfb7YvOARfbvn17vfbaa/Xiiy92e5QowZ2D2267rfr6+i7azZ46deqiXS8w244dO+q5556rw4cP18qVK7s9TpRHCnOwePHiWrt2bY2Njc06PzY2Vvfee2+XpoLrW7vdru3bt9cvf/nL+vWvf12rVq3q9khxdrhztGvXrtqyZUutW7eu1q9fX/v27asTJ07U1q1buz3aDe/06dP15ptvzrx+66236tVXX62lS5fW7bff3sXJbmzbtm2rp556qp599tkaGBiY+T/EoaGhuvnmm7s8XUibOfv+97/fvuOOO9qLFy9u33333e3x8fFuj0S73f7Nb37TrqqLjkceeaTbo93QLnVPqqr95JNPdnu0GN/DBQjxDBcgRHABQgQXIERwAUIEFyBEcAFCBPcqtFqtevzxx6vVanV7FC7B/bl+3aj3xvdwr8LU1FQNDQ3V5ORkDQ4Odnsc3sf9uX7dqPfGDhcgRHABQuI/XnPhwoV6++23a2BgYMH/duzU1NSs/3J9cX+uX712b9rtdk1PT9eKFStq0aLL72Pjz3D/+Mc/1vDwcHJJgIiJiYkr/sZvfIc7MDBQVVX392+u/sZN6eX5AItuGer2CFzJrf/Q7Qm4hHPnWzX+X/8+07fLiQf3vccI/Y2bBPc6tGjR4m6PwJX0Nbs9AVfwQY9JfWgGECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAyp+A+8cQTtWrVqlqyZEmtXbu2XnjhhWs9F0DP6Ti4+/fvr507d9Zjjz1Wr7zySt1///21cePGOnHixHzMB9AzOg7ud77znfryl79cX/nKV+rOO++s7373uzU8PFx79+6dj/kAekZHwT179mwdP368RkZGZp0fGRmpl1566ZLvabVaNTU1NesAuBF1FNx33323zp8/X8uXL591fvny5XXy5MlLvmd0dLSGhoZmjuHh4blPC7CAzelDs0ajMet1u92+6Nx7du/eXZOTkzPHxMTEXJYEWPD6O7n4tttuq76+vot2s6dOnbpo1/ueZrNZzWZz7hMC9IiOdriLFy+utWvX1tjY2KzzY2Njde+9917TwQB6TUc73KqqXbt21ZYtW2rdunW1fv362rdvX504caK2bt06H/MB9IyOg/uFL3yh/vKXv9Q3v/nNeuedd2r16tX1/PPP1x133DEf8wH0jI6DW1X16KOP1qOPPnqtZwHoaX5LASBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFC+ru28r/8Y1Xfkq4tz6X94aHBbo/AFVxodnsCLuX83/5W9fsPvs4OFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIKTj4B4+fLg2bdpUK1asqEajUc8888x8zAXQczoO7pkzZ+quu+6qPXv2zMc8AD2rv9M3bNy4sTZu3DgfswD0tI6D26lWq1WtVmvm9dTU1HwvCXBdmvcPzUZHR2toaGjmGB4enu8lAa5L8x7c3bt31+Tk5MwxMTEx30sCXJfm/ZFCs9msZrM538sAXPd8DxcgpOMd7unTp+vNN9+cef3WW2/Vq6++WkuXLq3bb7/9mg4H0Es6Du6xY8fqwQcfnHm9a9euqqp65JFH6ic/+ck1Gwyg13Qc3A0bNlS73Z6PWQB6mme4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAh/d1auP3y76vduKlby3MZix68t9sjcAXn/vV0t0fgEtr//bcPdZ0dLkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQEhHwR0dHa177rmnBgYGatmyZbV58+Z644035ms2gJ7SUXDHx8dr27ZtdeTIkRobG6tz587VyMhInTlzZr7mA+gZ/Z1cfPDgwVmvn3zyyVq2bFkdP368HnjggWs6GECv6Si47zc5OVlVVUuXLr3sNa1Wq1qt1szrqampq1kSYMGa84dm7Xa7du3aVffdd1+tXr36steNjo7W0NDQzDE8PDzXJQEWtDkHd/v27fXaa6/Vz372sytet3v37pqcnJw5JiYm5rokwII2p0cKO3bsqOeee64OHz5cK1euvOK1zWazms3mnIYD6CUdBbfdbteOHTvqwIEDdejQoVq1atV8zQXQczoK7rZt2+qpp56qZ599tgYGBurkyZNVVTU0NFQ333zzvAwI0Cs6eoa7d+/empycrA0bNtTHPvaxmWP//v3zNR9Az+j4kQIAc+O3FABCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQgQXIERwAUIEFyBEcAFCBBcgpL/bA3B9WfFv/9ntEbiC/5j4bbdH4BKmpi/Usg9xnR0uQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5ASEfB3bt3b61Zs6YGBwdrcHCw1q9fX7/61a/mazaAntJRcFeuXFnf/va369ixY3Xs2LH6zGc+Uw8//HD97ne/m6/5AHpGfycXb9q0adbrb33rW7V37946cuRIfepTn7qmgwH0mo6C+/fOnz9fP//5z+vMmTO1fv36y17XarWq1WrNvJ6amprrkgALWscfmr3++uv10Y9+tJrNZm3durUOHDhQn/zkJy97/ejoaA0NDc0cw8PDVzUwwELVaLfb7U7ecPbs2Tpx4kT99a9/rV/84hf1ox/9qMbHxy8b3UvtcIeHh2tDPVz9jZuubnquvUV93Z6AK/iPid92ewQuYWr6Qi375z/U5ORkDQ4OXva6jh8pLF68uD7+8Y9XVdW6devq6NGj9b3vfa9+8IMfXPL6ZrNZzWaz02UAes5Vfw+33W7P2sECcGkd7XC//vWv18aNG2t4eLimp6fr6aefrkOHDtXBgwfnaz6AntFRcP/0pz/Vli1b6p133qmhoaFas2ZNHTx4sB566KH5mg+gZ3QU3B//+MfzNQdAz/NbCgAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQIrgAIYILECK4ACGCCxAiuAAhggsQ0t/tAbjOXDjf7Qm4gpsafd0egUu4qdH4UNfZ4QKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoQILkCI4AKECC5AiOAChAguQIjgAoRcVXBHR0er0WjUzp07r9U8AD1rzsE9evRo7du3r9asWXMt5wHoWXMK7unTp+uLX/xi/fCHP6xbbrnlWs8E0JPmFNxt27bV5z//+frc5z73gde2Wq2ampqadQDciPo7fcPTTz9dL7/8ch09evRDXT86Olrf+MY3Oh4MoNd0tMOdmJior371q/XTn/60lixZ8qHes3v37pqcnJw5JiYm5jQowELX0Q73+PHjderUqVq7du3MufPnz9fhw4drz5491Wq1qq+vb9Z7ms1mNZvNazMtwALWUXA/+9nP1uuvvz7r3Je+9KX6xCc+UV/72tcuii0A/6+j4A4MDNTq1atnnfvIRz5St95660XnAZjNvzQDCOn4Wwrvd+jQoWswBkDvs8MFCBFcgBDBBQgRXIAQwQUIEVyAEMEFCBFcgBDBBQgRXIAQwQUIEVyAEMEFCBFcgBDBBQgRXIAQwQUIEVyAEMEFCBFcgBDBBQgRXIAQwQUIEVyAEMEFCBFcgBDBBQgRXIAQwQUIEVyAEMEFCBFcgBDBBQgRXIAQwQUIEVyAEMEFCBFcgBDBBQgRXIAQwQUIEVyAEMEFCBFcgJD+9ILtdruqqs7V/1S106vDwjY1faHbI3AJU6f/776817fLiQd3enq6qqperOfTS8OCd8s/dXsCrmR6erqGhoYu++eN9gcl+Rq7cOFCvf322zUwMFCNRiO59DU3NTVVw8PDNTExUYODg90eh/dxf65fvXZv2u12TU9P14oVK2rRoss/qY3vcBctWlQrV65MLzuvBgcHe+IvTa9yf65fvXRvrrSzfY8PzQBCBBcgpO/xxx9/vNtDLGR9fX21YcOG6u+PP53hQ3B/rl834r2Jf2gGcKPySAEgRHABQgQXIERwAUIEFyBEcAFCBBcgRHABQv4XYR56j0AlO88AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 400x666.667 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
