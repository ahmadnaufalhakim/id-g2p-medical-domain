{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn_gen/exp/en\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL =\"dot\"\n",
    "EMB_DIM = \"32\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"validation_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models_fallback\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  if split_name == \"train+val\" :\n",
    "    print(f\"Merging train and val entries ..\")\n",
    "    with open(os.path.join(DATA_DIR, f\"train.csv\"), encoding=\"utf-8\") as f_train_csv, \\\n",
    "         open(os.path.join(DATA_DIR, f\"val.csv\"), encoding=\"utf-8\") as f_val_csv :\n",
    "      next(f_train_csv, None)\n",
    "      next(f_val_csv, None)\n",
    "      train_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_train_csv]\n",
    "      val_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_val_csv]\n",
    "      pairs = train_pairs + val_pairs\n",
    "      graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "      phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "      g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "    return g2p_dataset, pairs\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging train and val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train+val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 30, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f3f2baaf0d0> ([4, 8, 6, 26, 24, 10, 1], [18, 6, 34, 1])\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 34, 1])\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 34, 1])\n",
      "train grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "test grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "train phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "test phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'b': 7, 'g': 12, 'h': 13, 'k': 16, 'l': 17, 'd': 9, 'v': 27, 'y': 30, 'z': 31, 'w': 28, 'j': 15, '-': 5, 'f': 11, 'p': 21, 'x': 29}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'b': 7, 'g': 12, 'h': 13, 'k': 16, 'l': 17, 'd': 9, 'v': 27, 'y': 30, 'z': 31, 'w': 28, 'j': 15, '-': 5, 'f': 11, 'p': 21, 'x': 29}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 32\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 41224 parameters\n",
      "Decoder has a total number of 78355 parameters\n",
      "Total number of all parameters is 119579\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 1 finished in 0m 36.54s (- 60m 17.68s) (1 1.0%). train avg loss: 1.2613\n",
      "Training for epoch 2 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 2 finished in 1m 11.91s (- 58m 43.36s) (2 2.0%). train avg loss: 0.594\n",
      "Training for epoch 3 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 3 finished in 1m 46.86s (- 57m 35.15s) (3 3.0%). train avg loss: 0.4787\n",
      "Training for epoch 4 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 4 finished in 2m 22.51s (- 57m 0.16s) (4 4.0%). train avg loss: 0.4395\n",
      "Training for epoch 5 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 5 finished in 2m 57.49s (- 56m 12.4s) (5 5.0%). train avg loss: 0.3859\n",
      "Training for epoch 6 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 6 finished in 3m 33.7s (- 55m 48.04s) (6 6.0%). train avg loss: 0.3767\n",
      "Training for epoch 7 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 7 finished in 4m 9.74s (- 55m 17.94s) (7 7.0%). train avg loss: 0.3581\n",
      "Training for epoch 8 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 8 finished in 4m 45.93s (- 54m 48.22s) (8 8.0%). train avg loss: 0.3284\n",
      "Training for epoch 9 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 9 finished in 5m 21.87s (- 54m 14.5s) (9 9.0%). train avg loss: 0.3376\n",
      "Training for epoch 10 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 10 finished in 5m 57.31s (- 53m 35.8s) (10 10.0%). train avg loss: 0.3334\n",
      "Training for epoch 11 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 11 finished in 6m 32.76s (- 52m 57.79s) (11 11.0%). train avg loss: 0.3043\n",
      "Training for epoch 12 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 12 finished in 7m 8.69s (- 52m 23.71s) (12 12.0%). train avg loss: 0.3038\n",
      "Training for epoch 13 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 13 finished in 7m 44.62s (- 51m 49.38s) (13 13.0%). train avg loss: 0.3056\n",
      "Training for epoch 14 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 14 finished in 8m 19.97s (- 51m 11.26s) (14 14.0%). train avg loss: 0.2932\n",
      "Training for epoch 15 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 15 finished in 8m 55.66s (- 50m 35.42s) (15 15.0%). train avg loss: 0.2948\n",
      "Training for epoch 16 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 16 finished in 9m 31.35s (- 49m 59.6s) (16 16.0%). train avg loss: 0.2906\n",
      "Training for epoch 17 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 17 finished in 10m 7.28s (- 49m 24.94s) (17 17.0%). train avg loss: 0.2668\n",
      "Training for epoch 18 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 18 finished in 10m 43.1s (- 48m 49.68s) (18 18.0%). train avg loss: 0.2716\n",
      "Training for epoch 19 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 19 finished in 11m 18.87s (- 48m 14.15s) (19 19.0%). train avg loss: 0.262\n",
      "Training for epoch 20 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 20 finished in 11m 55.11s (- 47m 40.46s) (20 20.0%). train avg loss: 0.247\n",
      "Training for epoch 21 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 21 finished in 12m 31.17s (- 47m 5.84s) (21 21.0%). train avg loss: 0.2681\n",
      "Training for epoch 22 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 22 finished in 13m 6.57s (- 46m 28.75s) (22 22.0%). train avg loss: 0.2375\n",
      "Training for epoch 23 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 23 finished in 13m 42.63s (- 45m 54.01s) (23 23.0%). train avg loss: 0.2442\n",
      "Training for epoch 24 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 24 finished in 14m 18.26s (- 45m 17.81s) (24 24.0%). train avg loss: 0.2511\n",
      "Training for epoch 25 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 25 finished in 14m 54.31s (- 44m 42.94s) (25 25.0%). train avg loss: 0.2351\n",
      "Training for epoch 26 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 26 finished in 15m 30.34s (- 44m 7.89s) (26 26.0%). train avg loss: 0.2558\n",
      "Training for epoch 27 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 27 finished in 16m 6.24s (- 43m 32.42s) (27 27.0%). train avg loss: 0.2552\n",
      "Training for epoch 28 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 28 finished in 16m 41.83s (- 42m 56.12s) (28 28.0%). train avg loss: 0.2225\n",
      "Training for epoch 29 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 29 finished in 17m 17.84s (- 42m 20.91s) (29 29.0%). train avg loss: 0.2302\n",
      "Training for epoch 30 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 30 finished in 17m 53.18s (- 41m 44.08s) (30 30.0%). train avg loss: 0.226\n",
      "Training for epoch 31 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 31 finished in 18m 28.96s (- 41m 8.34s) (31 31.0%). train avg loss: 0.2427\n",
      "Training for epoch 32 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 32 finished in 19m 4.4s (- 40m 31.85s) (32 32.0%). train avg loss: 0.223\n",
      "Training for epoch 33 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 33 finished in 19m 40.43s (- 39m 56.63s) (33 33.0%). train avg loss: 0.2209\n",
      "Training for epoch 34 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 34 finished in 20m 16.43s (- 39m 21.3s) (34 34.0%). train avg loss: 0.2209\n",
      "Training for epoch 35 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 35 finished in 20m 52.39s (- 38m 45.87s) (35 35.0%). train avg loss: 0.2194\n",
      "Training for epoch 36 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 36 finished in 21m 27.8s (- 38m 9.42s) (36 36.0%). train avg loss: 0.2166\n",
      "Training for epoch 37 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 37 finished in 22m 3.32s (- 37m 33.23s) (37 37.0%). train avg loss: 0.2272\n",
      "Training for epoch 38 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 38 finished in 22m 39.71s (- 36m 58.48s) (38 38.0%). train avg loss: 0.2088\n",
      "Training for epoch 39 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 39 finished in 23m 15.74s (- 36m 23.08s) (39 39.0%). train avg loss: 0.2123\n",
      "Training for epoch 40 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 40 finished in 23m 51.3s (- 35m 46.96s) (40 40.0%). train avg loss: 0.2032\n",
      "Training for epoch 41 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 41 finished in 24m 27.11s (- 35m 11.2s) (41 41.0%). train avg loss: 0.1992\n",
      "Training for epoch 42 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 42 finished in 25m 2.1s (- 34m 34.33s) (42 42.0%). train avg loss: 0.2219\n",
      "Training for epoch 43 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 43 finished in 25m 37.33s (- 33m 57.85s) (43 43.0%). train avg loss: 0.1986\n",
      "Training for epoch 44 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 44 finished in 26m 12.9s (- 33m 21.87s) (44 44.0%). train avg loss: 0.2021\n",
      "Training for epoch 45 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 45 finished in 26m 48.12s (- 32m 45.48s) (45 45.0%). train avg loss: 0.2148\n",
      "Training for epoch 46 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 46 finished in 27m 23.99s (- 32m 9.9s) (46 46.0%). train avg loss: 0.1946\n",
      "Training for epoch 47 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 47 finished in 28m 0.12s (- 31m 34.61s) (47 47.0%). train avg loss: 0.2182\n",
      "Training for epoch 48 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 48 finished in 28m 35.58s (- 30m 58.54s) (48 48.0%). train avg loss: 0.1991\n",
      "Training for epoch 49 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 49 finished in 29m 11.33s (- 30m 22.81s) (49 49.0%). train avg loss: 0.1998\n",
      "Training for epoch 50 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 50 finished in 29m 46.35s (- 29m 46.35s) (50 50.0%). train avg loss: 0.1941\n",
      "Training for epoch 51 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 51 finished in 30m 22.19s (- 29m 10.73s) (51 51.0%). train avg loss: 0.2176\n",
      "Training for epoch 52 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 52 finished in 30m 57.54s (- 28m 34.65s) (52 52.0%). train avg loss: 0.2013\n",
      "Training for epoch 53 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 53 finished in 31m 32.77s (- 27m 58.5s) (53 53.0%). train avg loss: 0.1921\n",
      "Training for epoch 54 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 54 finished in 32m 8.51s (- 27m 22.81s) (54 54.0%). train avg loss: 0.1992\n",
      "Training for epoch 55 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 55 finished in 32m 44.24s (- 26m 47.1s) (55 55.0%). train avg loss: 0.1968\n",
      "Training for epoch 56 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 56 finished in 33m 19.76s (- 26m 11.24s) (56 56.0%). train avg loss: 0.19\n",
      "Training for epoch 57 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 57 finished in 33m 55.14s (- 25m 35.28s) (57 57.0%). train avg loss: 0.2063\n",
      "Training for epoch 58 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 58 finished in 34m 30.67s (- 24m 59.45s) (58 58.0%). train avg loss: 0.1982\n",
      "Training for epoch 59 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 59 finished in 35m 5.39s (- 24m 23.07s) (59 59.0%). train avg loss: 0.1908\n",
      "Training for epoch 60 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 60 finished in 35m 40.8s (- 23m 47.2s) (60 60.0%). train avg loss: 0.1961\n",
      "Training for epoch 61 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 61 finished in 36m 14.98s (- 23m 10.56s) (61 61.0%). train avg loss: 0.1838\n",
      "Training for epoch 62 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 62 finished in 36m 49.02s (- 22m 33.92s) (62 62.0%). train avg loss: 0.1838\n",
      "Training for epoch 63 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 63 finished in 37m 24.1s (- 21m 57.97s) (63 63.0%). train avg loss: 0.1972\n",
      "Training for epoch 64 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 64 finished in 37m 57.59s (- 21m 21.14s) (64 64.0%). train avg loss: 0.1754\n",
      "Training for epoch 65 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 65 finished in 38m 31.49s (- 20m 44.65s) (65 65.0%). train avg loss: 0.1977\n",
      "Training for epoch 66 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 66 finished in 39m 5.39s (- 20m 8.23s) (66 66.0%). train avg loss: 0.2102\n",
      "Training for epoch 67 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 67 finished in 39m 39.28s (- 19m 31.89s) (67 67.0%). train avg loss: 0.1824\n",
      "Training for epoch 68 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 68 finished in 40m 13.23s (- 18m 55.64s) (68 68.0%). train avg loss: 0.1829\n",
      "Training for epoch 69 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 69 finished in 40m 47.26s (- 18m 19.49s) (69 69.0%). train avg loss: 0.1902\n",
      "Training for epoch 70 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 70 finished in 41m 21.3s (- 17m 43.42s) (70 70.0%). train avg loss: 0.1823\n",
      "Training for epoch 71 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 71 finished in 41m 55.3s (- 17m 7.38s) (71 71.0%). train avg loss: 0.1832\n",
      "Training for epoch 72 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 72 finished in 42m 29.76s (- 16m 31.57s) (72 72.0%). train avg loss: 0.1703\n",
      "Training for epoch 73 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 73 finished in 43m 3.81s (- 15m 55.65s) (73 73.0%). train avg loss: 0.1889\n",
      "Training for epoch 74 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 74 finished in 43m 41.62s (- 15m 21.11s) (74 74.0%). train avg loss: 0.1751\n",
      "Training for epoch 75 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 75 finished in 44m 19.69s (- 14m 46.56s) (75 75.0%). train avg loss: 0.1806\n",
      "Training for epoch 76 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 76 finished in 44m 58.51s (- 14m 12.16s) (76 76.0%). train avg loss: 0.1756\n",
      "Training for epoch 77 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 77 finished in 45m 37.94s (- 13m 37.83s) (77 77.0%). train avg loss: 0.1673\n",
      "Training for epoch 78 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 78 finished in 46m 16.61s (- 13m 3.15s) (78 78.0%). train avg loss: 0.1755\n",
      "Training for epoch 79 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 79 finished in 46m 54.57s (- 12m 28.18s) (79 79.0%). train avg loss: 0.1734\n",
      "Training for epoch 80 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 80 finished in 47m 34.58s (- 11m 53.64s) (80 80.0%). train avg loss: 0.1832\n",
      "Training for epoch 81 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 81 finished in 48m 12.75s (- 11m 18.55s) (81 81.0%). train avg loss: 0.1777\n",
      "Training for epoch 82 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 82 finished in 48m 46.85s (- 10m 42.48s) (82 82.0%). train avg loss: 0.1881\n",
      "Training for epoch 83 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 83 finished in 49m 20.13s (- 10m 6.29s) (83 83.0%). train avg loss: 0.1748\n",
      "Training for epoch 84 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 84 finished in 49m 54.67s (- 9m 30.41s) (84 84.0%). train avg loss: 0.1776\n",
      "Training for epoch 85 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 85 finished in 50m 32.29s (- 8m 55.11s) (85 85.0%). train avg loss: 0.1854\n",
      "Training for epoch 86 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 86 finished in 51m 10.21s (- 8m 19.8s) (86 86.0%). train avg loss: 0.1836\n",
      "Training for epoch 87 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 87 finished in 51m 49.2s (- 7m 44.59s) (87 87.0%). train avg loss: 0.1742\n",
      "Training for epoch 88 has started (lr=0.001). Found 1762 batch(es).\n",
      "Epoch 88 finished in 52m 29.39s (- 7m 9.46s) (88 88.0%). train avg loss: 0.1701\n",
      "Training for epoch 89 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 89 finished in 53m 8.81s (- 6m 34.12s) (89 89.0%). train avg loss: 0.1524\n",
      "Training for epoch 90 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 90 finished in 53m 47.94s (- 5m 58.66s) (90 90.0%). train avg loss: 0.1502\n",
      "Training for epoch 91 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 91 finished in 54m 25.92s (- 5m 23.0s) (91 91.0%). train avg loss: 0.1422\n",
      "Training for epoch 92 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 92 finished in 55m 4.47s (- 4m 47.35s) (92 92.0%). train avg loss: 0.1461\n",
      "Training for epoch 93 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 93 finished in 55m 41.53s (- 4m 11.51s) (93 93.0%). train avg loss: 0.1407\n",
      "Training for epoch 94 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 94 finished in 56m 19.92s (- 3m 35.74s) (94 94.0%). train avg loss: 0.1467\n",
      "Training for epoch 95 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 95 finished in 56m 55.13s (- 2m 59.74s) (95 95.0%). train avg loss: 0.1425\n",
      "Training for epoch 96 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 96 finished in 57m 33.79s (- 2m 23.91s) (96 96.0%). train avg loss: 0.1405\n",
      "Training for epoch 97 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 97 finished in 58m 12.98s (- 1m 48.03s) (97 97.0%). train avg loss: 0.1391\n",
      "Training for epoch 98 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 98 finished in 58m 51.53s (- 1m 12.07s) (98 98.0%). train avg loss: 0.139\n",
      "Training for epoch 99 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 99 finished in 59m 34.58s (- 0m 36.11s) (99 99.0%). train avg loss: 0.1409\n",
      "Training for epoch 100 has started (lr=0.0005). Found 1762 batch(es).\n",
      "Epoch 100 finished in 60m 14.64s (- 0m 0.0s) (100 100.0%). train avg loss: 0.1376\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "best_train_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on train loss\n",
    "  encoder_scheduler.step(avg_train_loss)\n",
    "  decoder_scheduler.step(avg_train_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  # Save the model if the train loss is better than the previous iterations' train loss\n",
    "  if avg_train_loss < best_train_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_train_loss = avg_train_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"train\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-7.5, min_value+.065, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 12.578145361871924%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in test_pairs :\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(test_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> FLUCTUATION\n",
      "= ['F', 'L', 'AA', 'Q', 'CH', 'UW', 'EY', 'SH', 'AX', 'N']\n",
      "< F L AA Q CH UW EY SH AX N ['F', 'L', 'AA', 'Q', 'CH', 'UW', 'EY', 'SH', 'AX', 'N']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3f9ee27c40>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
