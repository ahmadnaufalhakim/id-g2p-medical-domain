{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/exp/en\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL =\"dot\"\n",
    "EMB_DIM = \"32\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"100\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"validation_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 30, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f433079f6a0> ([4, 8, 6, 26, 24, 10, 1], [18, 6, 34, 1])\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 34, 1])\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 34, 1])\n",
      "train grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "valid grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "test grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "train phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "valid phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "test phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'b': 7, 'g': 12, 'h': 13, 'k': 16, 'l': 17, 'd': 9, 'v': 27, 'y': 30, 'z': 31, 'w': 28, 'j': 15, '-': 5, 'f': 11, 'p': 21, 'x': 29}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'b': 7, 'g': 12, 'h': 13, 'k': 16, 'l': 17, 'd': 9, 'v': 27, 'y': 30, 'z': 31, 'w': 28, 'j': 15, '-': 5, 'f': 11, 'p': 21, 'x': 29}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'b': 7, 'g': 12, 'h': 13, 'k': 16, 'l': 17, 'd': 9, 'v': 27, 'y': 30, 'z': 31, 'w': 28, 'j': 15, '-': 5, 'f': 11, 'p': 21, 'x': 29}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "      )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 32\n",
      "hidden_size: 100\n",
      "n_layers: 1\n",
      "Encoder has a total number of 41224 parameters\n",
      "Decoder has a total number of 78355 parameters\n",
      "Total number of all parameters is 119579\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 1 finished in 0m 36.13s (- 59m 37.28s) (1 1.0%). train avg loss: 1.3224, val avg loss: 1.2215\n",
      "Training for epoch 2 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 2 finished in 1m 10.73s (- 57m 45.55s) (2 2.0%). train avg loss: 0.6152, val avg loss: 1.0183\n",
      "Training for epoch 3 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 3 finished in 1m 45.13s (- 56m 39.34s) (3 3.0%). train avg loss: 0.5043, val avg loss: 0.9745\n",
      "Training for epoch 4 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 4 finished in 2m 19.33s (- 55m 43.9s) (4 4.0%). train avg loss: 0.4538, val avg loss: 0.9431\n",
      "Training for epoch 5 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 5 finished in 2m 52.13s (- 54m 30.48s) (5 5.0%). train avg loss: 0.444, val avg loss: 0.9175\n",
      "Training for epoch 6 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 6 finished in 3m 24.97s (- 53m 31.17s) (6 6.0%). train avg loss: 0.4025, val avg loss: 1.0822\n",
      "Training for epoch 7 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 7 finished in 3m 57.93s (- 52m 41.1s) (7 7.0%). train avg loss: 0.3736, val avg loss: 0.9382\n",
      "Training for epoch 8 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 8 finished in 4m 30.39s (- 51m 49.52s) (8 8.0%). train avg loss: 0.3379, val avg loss: 0.8554\n",
      "Training for epoch 9 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 9 finished in 5m 3.26s (- 51m 6.27s) (9 9.0%). train avg loss: 0.3387, val avg loss: 0.8451\n",
      "Training for epoch 10 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 10 finished in 5m 36.78s (- 50m 31.01s) (10 10.0%). train avg loss: 0.3504, val avg loss: 0.7972\n",
      "Training for epoch 11 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 11 finished in 6m 10.11s (- 49m 54.53s) (11 11.0%). train avg loss: 0.3065, val avg loss: 0.794\n",
      "Training for epoch 12 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 12 finished in 6m 43.07s (- 49m 15.83s) (12 12.0%). train avg loss: 0.3209, val avg loss: 0.7941\n",
      "Training for epoch 13 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 13 finished in 7m 16.16s (- 48m 38.93s) (13 13.0%). train avg loss: 0.3077, val avg loss: 0.8442\n",
      "Training for epoch 14 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 14 finished in 7m 48.97s (- 48m 0.81s) (14 14.0%). train avg loss: 0.3059, val avg loss: 0.7714\n",
      "Training for epoch 15 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 15 finished in 8m 21.41s (- 47m 21.35s) (15 15.0%). train avg loss: 0.2755, val avg loss: 0.7301\n",
      "Training for epoch 16 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 16 finished in 8m 54.36s (- 46m 45.37s) (16 16.0%). train avg loss: 0.2847, val avg loss: 0.7843\n",
      "Training for epoch 17 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 17 finished in 9m 27.37s (- 46m 10.08s) (17 17.0%). train avg loss: 0.2763, val avg loss: 0.7078\n",
      "Training for epoch 18 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 18 finished in 9m 59.52s (- 45m 31.16s) (18 18.0%). train avg loss: 0.294, val avg loss: 0.7254\n",
      "Training for epoch 19 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 19 finished in 10m 33.15s (- 44m 59.2s) (19 19.0%). train avg loss: 0.2644, val avg loss: 0.8038\n",
      "Training for epoch 20 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 20 finished in 11m 15.06s (- 45m 0.26s) (20 20.0%). train avg loss: 0.2809, val avg loss: 0.7534\n",
      "Training for epoch 21 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 21 finished in 11m 56.04s (- 44m 53.67s) (21 21.0%). train avg loss: 0.2718, val avg loss: 0.7021\n",
      "Training for epoch 22 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 22 finished in 12m 38.72s (- 44m 50.01s) (22 22.0%). train avg loss: 0.2479, val avg loss: 0.7131\n",
      "Training for epoch 23 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 23 finished in 13m 22.58s (- 44m 46.88s) (23 23.0%). train avg loss: 0.2485, val avg loss: 0.7234\n",
      "Training for epoch 24 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 24 finished in 14m 1.73s (- 44m 25.49s) (24 24.0%). train avg loss: 0.2419, val avg loss: 0.6649\n",
      "Training for epoch 25 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 25 finished in 14m 44.95s (- 44m 14.85s) (25 25.0%). train avg loss: 0.2504, val avg loss: 0.7051\n",
      "Training for epoch 26 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 26 finished in 15m 25.51s (- 43m 54.13s) (26 26.0%). train avg loss: 0.2568, val avg loss: 0.7593\n",
      "Training for epoch 27 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 27 finished in 16m 7.12s (- 43m 34.82s) (27 27.0%). train avg loss: 0.241, val avg loss: 0.6969\n",
      "Training for epoch 28 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 28 finished in 16m 48.77s (- 43m 13.99s) (28 28.0%). train avg loss: 0.2386, val avg loss: 0.7286\n",
      "Training for epoch 29 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 29 finished in 17m 30.51s (- 42m 51.95s) (29 29.0%). train avg loss: 0.2344, val avg loss: 0.6676\n",
      "Training for epoch 30 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 30 finished in 18m 10.54s (- 42m 24.59s) (30 30.0%). train avg loss: 0.2235, val avg loss: 0.6578\n",
      "Training for epoch 31 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 31 finished in 18m 52.91s (- 42m 1.65s) (31 31.0%). train avg loss: 0.2367, val avg loss: 0.6914\n",
      "Training for epoch 32 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 32 finished in 19m 35.9s (- 41m 38.79s) (32 32.0%). train avg loss: 0.2229, val avg loss: 0.6666\n",
      "Training for epoch 33 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 33 finished in 20m 18.22s (- 41m 13.36s) (33 33.0%). train avg loss: 0.2279, val avg loss: 0.6702\n",
      "Training for epoch 34 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 34 finished in 20m 56.65s (- 40m 39.37s) (34 34.0%). train avg loss: 0.2186, val avg loss: 0.7458\n",
      "Training for epoch 35 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 35 finished in 21m 37.3s (- 40m 9.26s) (35 35.0%). train avg loss: 0.2213, val avg loss: 0.6397\n",
      "Training for epoch 36 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 36 finished in 22m 18.75s (- 39m 40.0s) (36 36.0%). train avg loss: 0.2149, val avg loss: 0.6322\n",
      "Training for epoch 37 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 37 finished in 22m 58.71s (- 39m 7.53s) (37 37.0%). train avg loss: 0.2153, val avg loss: 0.6656\n",
      "Training for epoch 38 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 38 finished in 23m 39.29s (- 38m 35.69s) (38 38.0%). train avg loss: 0.2134, val avg loss: 0.661\n",
      "Training for epoch 39 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 39 finished in 24m 21.22s (- 38m 5.51s) (39 39.0%). train avg loss: 0.2213, val avg loss: 0.6642\n",
      "Training for epoch 40 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 40 finished in 25m 3.47s (- 37m 35.21s) (40 40.0%). train avg loss: 0.2038, val avg loss: 0.616\n",
      "Training for epoch 41 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 41 finished in 25m 45.07s (- 37m 3.39s) (41 41.0%). train avg loss: 0.1994, val avg loss: 0.6998\n",
      "Training for epoch 42 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 42 finished in 26m 25.76s (- 36m 29.85s) (42 42.0%). train avg loss: 0.2376, val avg loss: 0.6242\n",
      "Training for epoch 43 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 43 finished in 27m 7.72s (- 35m 57.68s) (43 43.0%). train avg loss: 0.2128, val avg loss: 0.6488\n",
      "Training for epoch 44 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 44 finished in 27m 50.14s (- 35m 25.64s) (44 44.0%). train avg loss: 0.2113, val avg loss: 0.6606\n",
      "Training for epoch 45 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 45 finished in 28m 32.8s (- 34m 53.42s) (45 45.0%). train avg loss: 0.2062, val avg loss: 0.6941\n",
      "Training for epoch 46 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 46 finished in 29m 16.66s (- 34m 22.17s) (46 46.0%). train avg loss: 0.2327, val avg loss: 0.683\n",
      "Training for epoch 47 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 47 finished in 29m 57.93s (- 33m 47.45s) (47 47.0%). train avg loss: 0.2155, val avg loss: 0.6399\n",
      "Training for epoch 48 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 48 finished in 30m 40.46s (- 33m 13.83s) (48 48.0%). train avg loss: 0.2027, val avg loss: 0.6726\n",
      "Training for epoch 49 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 49 finished in 31m 23.21s (- 32m 40.08s) (49 49.0%). train avg loss: 0.2113, val avg loss: 0.6513\n",
      "Training for epoch 50 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 50 finished in 32m 5.17s (- 32m 5.17s) (50 50.0%). train avg loss: 0.2235, val avg loss: 0.6572\n",
      "Training for epoch 51 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 51 finished in 32m 38.88s (- 31m 22.06s) (51 51.0%). train avg loss: 0.2069, val avg loss: 0.6158\n",
      "Training for epoch 52 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 52 finished in 33m 11.83s (- 30m 38.61s) (52 52.0%). train avg loss: 0.2018, val avg loss: 0.6505\n",
      "Training for epoch 53 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 53 finished in 33m 44.42s (- 29m 55.24s) (53 53.0%). train avg loss: 0.1914, val avg loss: 0.8004\n",
      "Training for epoch 54 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 54 finished in 34m 24.32s (- 29m 18.5s) (54 54.0%). train avg loss: 0.2141, val avg loss: 0.6764\n",
      "Training for epoch 55 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 55 finished in 35m 4.13s (- 28m 41.56s) (55 55.0%). train avg loss: 0.2063, val avg loss: 0.6426\n",
      "Training for epoch 56 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 56 finished in 35m 49.65s (- 28m 9.01s) (56 56.0%). train avg loss: 0.1976, val avg loss: 0.6037\n",
      "Training for epoch 57 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 57 finished in 36m 38.81s (- 27m 38.75s) (57 57.0%). train avg loss: 0.1883, val avg loss: 0.6242\n",
      "Training for epoch 58 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 58 finished in 37m 27.56s (- 27m 7.54s) (58 58.0%). train avg loss: 0.1917, val avg loss: 0.6307\n",
      "Training for epoch 59 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 59 finished in 38m 16.62s (- 26m 35.96s) (59 59.0%). train avg loss: 0.1931, val avg loss: 0.6282\n",
      "Training for epoch 60 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 60 finished in 39m 6.96s (- 26m 4.64s) (60 60.0%). train avg loss: 0.1828, val avg loss: 0.6583\n",
      "Training for epoch 61 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 61 finished in 39m 55.06s (- 25m 31.27s) (61 61.0%). train avg loss: 0.2183, val avg loss: 0.6318\n",
      "Training for epoch 62 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 62 finished in 40m 40.83s (- 24m 55.99s) (62 62.0%). train avg loss: 0.1854, val avg loss: 0.6003\n",
      "Training for epoch 63 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 63 finished in 41m 25.58s (- 24m 19.79s) (63 63.0%). train avg loss: 0.1845, val avg loss: 0.6598\n",
      "Training for epoch 64 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 64 finished in 42m 13.3s (- 23m 44.98s) (64 64.0%). train avg loss: 0.196, val avg loss: 0.6454\n",
      "Training for epoch 65 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 65 finished in 42m 57.79s (- 23m 8.04s) (65 65.0%). train avg loss: 0.1966, val avg loss: 0.6344\n",
      "Training for epoch 66 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 66 finished in 43m 41.38s (- 22m 30.41s) (66 66.0%). train avg loss: 0.1867, val avg loss: 0.6515\n",
      "Training for epoch 67 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 67 finished in 44m 24.14s (- 21m 52.19s) (67 67.0%). train avg loss: 0.1865, val avg loss: 0.6594\n",
      "Training for epoch 68 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 68 finished in 45m 8.01s (- 21m 14.36s) (68 68.0%). train avg loss: 0.2158, val avg loss: 0.6542\n",
      "Training for epoch 69 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 69 finished in 45m 54.37s (- 20m 37.47s) (69 69.0%). train avg loss: 0.1913, val avg loss: 0.6583\n",
      "Training for epoch 70 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 70 finished in 46m 39.45s (- 19m 59.77s) (70 70.0%). train avg loss: 0.1794, val avg loss: 0.5995\n",
      "Training for epoch 71 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 71 finished in 47m 24.2s (- 19m 21.72s) (71 71.0%). train avg loss: 0.184, val avg loss: 0.6433\n",
      "Training for epoch 72 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 72 finished in 48m 9.55s (- 18m 43.71s) (72 72.0%). train avg loss: 0.1859, val avg loss: 0.6185\n",
      "Training for epoch 73 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 73 finished in 48m 53.21s (- 18m 4.88s) (73 73.0%). train avg loss: 0.1907, val avg loss: 0.616\n",
      "Training for epoch 74 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 74 finished in 49m 38.45s (- 17m 26.48s) (74 74.0%). train avg loss: 0.1849, val avg loss: 0.6933\n",
      "Training for epoch 75 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 75 finished in 50m 22.74s (- 16m 47.58s) (75 75.0%). train avg loss: 0.1855, val avg loss: 0.7641\n",
      "Training for epoch 76 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 76 finished in 51m 8.34s (- 16m 8.95s) (76 76.0%). train avg loss: 0.1894, val avg loss: 0.5919\n",
      "Training for epoch 77 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 77 finished in 51m 50.17s (- 15m 29.01s) (77 77.0%). train avg loss: 0.1815, val avg loss: 0.6212\n",
      "Training for epoch 78 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 78 finished in 52m 34.36s (- 14m 49.69s) (78 78.0%). train avg loss: 0.1803, val avg loss: 0.6037\n",
      "Training for epoch 79 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 79 finished in 53m 18.74s (- 14m 10.3s) (79 79.0%). train avg loss: 0.1733, val avg loss: 0.6173\n",
      "Training for epoch 80 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 80 finished in 54m 3.03s (- 13m 30.76s) (80 80.0%). train avg loss: 0.1728, val avg loss: 0.5984\n",
      "Training for epoch 81 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 81 finished in 54m 47.04s (- 12m 51.03s) (81 81.0%). train avg loss: 0.1843, val avg loss: 0.6729\n",
      "Training for epoch 82 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 82 finished in 55m 33.78s (- 12m 11.8s) (82 82.0%). train avg loss: 0.1792, val avg loss: 0.6103\n",
      "Training for epoch 83 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 83 finished in 56m 18.71s (- 11m 32.03s) (83 83.0%). train avg loss: 0.1811, val avg loss: 0.6314\n",
      "Training for epoch 84 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 84 finished in 57m 2.43s (- 10m 51.89s) (84 84.0%). train avg loss: 0.2072, val avg loss: 0.6924\n",
      "Training for epoch 85 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 85 finished in 57m 48.23s (- 10m 12.04s) (85 85.0%). train avg loss: 0.1815, val avg loss: 0.6213\n",
      "Training for epoch 86 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 86 finished in 58m 31.45s (- 9m 31.63s) (86 86.0%). train avg loss: 0.1697, val avg loss: 0.6558\n",
      "Training for epoch 87 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 87 finished in 59m 15.86s (- 8m 51.33s) (87 87.0%). train avg loss: 0.1792, val avg loss: 0.6234\n",
      "Training for epoch 88 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 88 finished in 60m 1.98s (- 8m 11.18s) (88 88.0%). train avg loss: 0.1533, val avg loss: 0.5747\n",
      "Training for epoch 89 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 89 finished in 60m 46.74s (- 7m 30.72s) (89 89.0%). train avg loss: 0.1459, val avg loss: 0.5987\n",
      "Training for epoch 90 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 90 finished in 61m 28.43s (- 6m 49.83s) (90 90.0%). train avg loss: 0.1482, val avg loss: 0.6009\n",
      "Training for epoch 91 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 91 finished in 62m 10.23s (- 6m 8.92s) (91 91.0%). train avg loss: 0.1488, val avg loss: 0.6013\n",
      "Training for epoch 92 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 92 finished in 62m 54.68s (- 5m 28.23s) (92 92.0%). train avg loss: 0.1412, val avg loss: 0.5897\n",
      "Training for epoch 93 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 93 finished in 63m 34.8s (- 4m 47.14s) (93 93.0%). train avg loss: 0.1421, val avg loss: 0.5866\n",
      "Training for epoch 94 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 94 finished in 64m 16.29s (- 4m 6.15s) (94 94.0%). train avg loss: 0.1412, val avg loss: 0.5865\n",
      "Training for epoch 95 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 95 finished in 65m 2.72s (- 3m 25.41s) (95 95.0%). train avg loss: 0.1432, val avg loss: 0.6069\n",
      "Training for epoch 96 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 96 finished in 65m 47.66s (- 2m 44.49s) (96 96.0%). train avg loss: 0.1387, val avg loss: 0.6069\n",
      "Training for epoch 97 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 97 finished in 66m 34.05s (- 2m 3.53s) (97 97.0%). train avg loss: 0.1351, val avg loss: 0.5982\n",
      "Training for epoch 98 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 98 finished in 67m 19.37s (- 1m 22.44s) (98 98.0%). train avg loss: 0.1374, val avg loss: 0.6208\n",
      "Training for epoch 99 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 99 finished in 68m 8.48s (- 0m 41.3s) (99 99.0%). train avg loss: 0.1385, val avg loss: 0.5864\n",
      "Training for epoch 100 has started (lr=0.00025). Found 1586 batch(es).\n",
      "Epoch 100 finished in 68m 54.78s (- 0m 0.0s) (100 100.0%). train avg loss: 0.1271, val avg loss: 0.5698\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-7.5, min_value-.085, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 18.01947040125701%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> RECHRISTENED\n",
      "= ['R', 'IY', 'K', 'R', 'IY', 'S', 'AX', 'N', 'D']\n",
      "< R IY K R IY S T AX N D ['R', 'IY', 'K', 'R', 'IY', 'S', 'T', 'AX', 'N', 'D']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4331477f40>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 523.636x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
