{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/exp/en\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"128\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"50\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"validation_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[337, 140, 365, 201, 539, 361, 83, 247, 383, 621, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f7453ccaac0> ([6, 75, 43, 544, 477, 1], [18, 6, 34, 1])\n",
      "([6, 75, 43, 544, 477, 1], [18, 6, 34, 1])\n",
      "([6, 75, 43, 544, 477, 1], [18, 6, 34, 1])\n",
      "train grp 672 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: \"a'\", 23: 'aa', 24: 'ab', 25: 'ac', 26: 'ad', 27: 'ae', 28: 'af', 29: 'ag', 30: 'ah', 31: 'ai', 32: 'aj', 33: 'ak', 34: 'al', 35: 'am', 36: 'an', 37: 'ao', 38: 'ap', 39: 'aq', 40: 'ar', 41: 'as', 42: 'at', 43: 'au', 44: 'av', 45: 'aw', 46: 'ax', 47: 'ay', 48: 'az', 49: \"b'\", 50: 'ba', 51: 'bb', 52: 'bc', 53: 'bd', 54: 'be', 55: 'bf', 56: 'bg', 57: 'bh', 58: 'bi', 59: 'bj', 60: 'bk', 61: 'bl', 62: 'bm', 63: 'bn', 64: 'bo', 65: 'bp', 66: 'br', 67: 'bs', 68: 'bt', 69: 'bu', 70: 'bv', 71: 'bw', 72: 'by', 73: 'bz', 74: \"c'\", 75: 'ca', 76: 'cb', 77: 'cc', 78: 'cd', 79: 'ce', 80: 'cf', 81: 'cg', 82: 'ch', 83: 'ci', 84: 'cj', 85: 'ck', 86: 'cl', 87: 'cm', 88: 'cn', 89: 'co', 90: 'cp', 91: 'cq', 92: 'cr', 93: 'cs', 94: 'ct', 95: 'cu', 96: 'cv', 97: 'cw', 98: 'cy', 99: 'cz', 100: \"d'\", 101: 'da', 102: 'db', 103: 'dc', 104: 'dd', 105: 'de', 106: 'df', 107: 'dg', 108: 'dh', 109: 'di', 110: 'dj', 111: 'dk', 112: 'dl', 113: 'dm', 114: 'dn', 115: 'do', 116: 'dp', 117: 'dq', 118: 'dr', 119: 'ds', 120: 'dt', 121: 'du', 122: 'dv', 123: 'dw', 124: 'dy', 125: 'dz', 126: \"e'\", 127: 'ea', 128: 'eb', 129: 'ec', 130: 'ed', 131: 'ee', 132: 'ef', 133: 'eg', 134: 'eh', 135: 'ei', 136: 'ej', 137: 'ek', 138: 'el', 139: 'em', 140: 'en', 141: 'eo', 142: 'ep', 143: 'eq', 144: 'er', 145: 'es', 146: 'et', 147: 'eu', 148: 'ev', 149: 'ew', 150: 'ex', 151: 'ey', 152: 'ez', 153: \"f'\", 154: 'fa', 155: 'fb', 156: 'fc', 157: 'fd', 158: 'fe', 159: 'ff', 160: 'fg', 161: 'fh', 162: 'fi', 163: 'fj', 164: 'fk', 165: 'fl', 166: 'fm', 167: 'fn', 168: 'fo', 169: 'fp', 170: 'fq', 171: 'fr', 172: 'fs', 173: 'ft', 174: 'fu', 175: 'fv', 176: 'fw', 177: 'fx', 178: 'fy', 179: 'fz', 180: \"g'\", 181: 'ga', 182: 'gb', 183: 'gc', 184: 'gd', 185: 'ge', 186: 'gf', 187: 'gg', 188: 'gh', 189: 'gi', 190: 'gj', 191: 'gk', 192: 'gl', 193: 'gm', 194: 'gn', 195: 'go', 196: 'gp', 197: 'gq', 198: 'gr', 199: 'gs', 200: 'gt', 201: 'gu', 202: 'gv', 203: 'gw', 204: 'gx', 205: 'gy', 206: 'gz', 207: \"h'\", 208: 'ha', 209: 'hb', 210: 'hc', 211: 'hd', 212: 'he', 213: 'hf', 214: 'hg', 215: 'hh', 216: 'hi', 217: 'hj', 218: 'hk', 219: 'hl', 220: 'hm', 221: 'hn', 222: 'ho', 223: 'hp', 224: 'hq', 225: 'hr', 226: 'hs', 227: 'ht', 228: 'hu', 229: 'hv', 230: 'hw', 231: 'hy', 232: 'i', 233: \"i'\", 234: 'ia', 235: 'ib', 236: 'ic', 237: 'id', 238: 'ie', 239: 'if', 240: 'ig', 241: 'ih', 242: 'ii', 243: 'ij', 244: 'ik', 245: 'il', 246: 'im', 247: 'in', 248: 'io', 249: 'ip', 250: 'iq', 251: 'ir', 252: 'is', 253: 'it', 254: 'iu', 255: 'iv', 256: 'iw', 257: 'ix', 258: 'iy', 259: 'iz', 260: \"j'\", 261: 'ja', 262: 'jc', 263: 'jd', 264: 'je', 265: 'jf', 266: 'jh', 267: 'ji', 268: 'jj', 269: 'jk', 270: 'jl', 271: 'jm', 272: 'jn', 273: 'jo', 274: 'js', 275: 'jt', 276: 'ju', 277: 'jv', 278: 'jy', 279: 'jz', 280: \"k'\", 281: 'ka', 282: 'kb', 283: 'kc', 284: 'kd', 285: 'ke', 286: 'kf', 287: 'kg', 288: 'kh', 289: 'ki', 290: 'kj', 291: 'kk', 292: 'kl', 293: 'km', 294: 'kn', 295: 'ko', 296: 'kp', 297: 'kr', 298: 'ks', 299: 'kt', 300: 'ku', 301: 'kv', 302: 'kw', 303: 'ky', 304: 'kz', 305: \"l'\", 306: 'la', 307: 'lb', 308: 'lc', 309: 'ld', 310: 'le', 311: 'lf', 312: 'lg', 313: 'lh', 314: 'li', 315: 'lj', 316: 'lk', 317: 'll', 318: 'lm', 319: 'ln', 320: 'lo', 321: 'lp', 322: 'lq', 323: 'lr', 324: 'ls', 325: 'lt', 326: 'lu', 327: 'lv', 328: 'lw', 329: 'lx', 330: 'ly', 331: 'lz', 332: \"m'\", 333: 'ma', 334: 'mb', 335: 'mc', 336: 'md', 337: 'me', 338: 'mf', 339: 'mg', 340: 'mh', 341: 'mi', 342: 'mj', 343: 'mk', 344: 'ml', 345: 'mm', 346: 'mn', 347: 'mo', 348: 'mp', 349: 'mq', 350: 'mr', 351: 'ms', 352: 'mt', 353: 'mu', 354: 'mv', 355: 'mw', 356: 'my', 357: 'mz', 358: \"n'\", 359: 'na', 360: 'nb', 361: 'nc', 362: 'nd', 363: 'ne', 364: 'nf', 365: 'ng', 366: 'nh', 367: 'ni', 368: 'nj', 369: 'nk', 370: 'nl', 371: 'nm', 372: 'nn', 373: 'no', 374: 'np', 375: 'nq', 376: 'nr', 377: 'ns', 378: 'nt', 379: 'nu', 380: 'nv', 381: 'nw', 382: 'nx', 383: 'ny', 384: 'nz', 385: 'o', 386: \"o'\", 387: 'oa', 388: 'ob', 389: 'oc', 390: 'od', 391: 'oe', 392: 'of', 393: 'og', 394: 'oh', 395: 'oi', 396: 'oj', 397: 'ok', 398: 'ol', 399: 'om', 400: 'on', 401: 'oo', 402: 'op', 403: 'oq', 404: 'or', 405: 'os', 406: 'ot', 407: 'ou', 408: 'ov', 409: 'ow', 410: 'ox', 411: 'oy', 412: 'oz', 413: \"p'\", 414: 'pa', 415: 'pb', 416: 'pc', 417: 'pd', 418: 'pe', 419: 'pf', 420: 'pg', 421: 'ph', 422: 'pi', 423: 'pj', 424: 'pk', 425: 'pl', 426: 'pm', 427: 'pn', 428: 'po', 429: 'pp', 430: 'pr', 431: 'ps', 432: 'pt', 433: 'pu', 434: 'pw', 435: 'py', 436: 'pz', 437: \"q'\", 438: 'qa', 439: 'qb', 440: 'qg', 441: 'qi', 442: 'qo', 443: 'qu', 444: 'qv', 445: \"r'\", 446: 'ra', 447: 'rb', 448: 'rc', 449: 'rd', 450: 're', 451: 'rf', 452: 'rg', 453: 'rh', 454: 'ri', 455: 'rj', 456: 'rk', 457: 'rl', 458: 'rm', 459: 'rn', 460: 'ro', 461: 'rp', 462: 'rq', 463: 'rr', 464: 'rs', 465: 'rt', 466: 'ru', 467: 'rv', 468: 'rw', 469: 'rx', 470: 'ry', 471: 'rz', 472: \"s'\", 473: 'sa', 474: 'sb', 475: 'sc', 476: 'sd', 477: 'se', 478: 'sf', 479: 'sg', 480: 'sh', 481: 'si', 482: 'sj', 483: 'sk', 484: 'sl', 485: 'sm', 486: 'sn', 487: 'so', 488: 'sp', 489: 'sq', 490: 'sr', 491: 'ss', 492: 'st', 493: 'su', 494: 'sv', 495: 'sw', 496: 'sx', 497: 'sy', 498: 'sz', 499: \"t'\", 500: 'ta', 501: 'tb', 502: 'tc', 503: 'td', 504: 'te', 505: 'tf', 506: 'tg', 507: 'th', 508: 'ti', 509: 'tj', 510: 'tk', 511: 'tl', 512: 'tm', 513: 'tn', 514: 'to', 515: 'tp', 516: 'tr', 517: 'ts', 518: 'tt', 519: 'tu', 520: 'tv', 521: 'tw', 522: 'tx', 523: 'ty', 524: 'tz', 525: \"u'\", 526: 'ua', 527: 'ub', 528: 'uc', 529: 'ud', 530: 'ue', 531: 'uf', 532: 'ug', 533: 'uh', 534: 'ui', 535: 'uj', 536: 'uk', 537: 'ul', 538: 'um', 539: 'un', 540: 'uo', 541: 'up', 542: 'uq', 543: 'ur', 544: 'us', 545: 'ut', 546: 'uu', 547: 'uv', 548: 'uw', 549: 'ux', 550: 'uy', 551: 'uz', 552: \"v'\", 553: 'va', 554: 'vc', 555: 'vd', 556: 've', 557: 'vg', 558: 'vh', 559: 'vi', 560: 'vj', 561: 'vk', 562: 'vl', 563: 'vm', 564: 'vn', 565: 'vo', 566: 'vr', 567: 'vs', 568: 'vt', 569: 'vu', 570: 'vv', 571: 'vy', 572: \"w'\", 573: 'wa', 574: 'wb', 575: 'wc', 576: 'wd', 577: 'we', 578: 'wf', 579: 'wg', 580: 'wh', 581: 'wi', 582: 'wk', 583: 'wl', 584: 'wm', 585: 'wn', 586: 'wo', 587: 'wp', 588: 'wr', 589: 'ws', 590: 'wt', 591: 'wu', 592: 'wv', 593: 'ww', 594: 'wy', 595: 'wz', 596: \"x'\", 597: 'xa', 598: 'xb', 599: 'xc', 600: 'xd', 601: 'xe', 602: 'xf', 603: 'xg', 604: 'xh', 605: 'xi', 606: 'xl', 607: 'xm', 608: 'xn', 609: 'xo', 610: 'xp', 611: 'xq', 612: 'xr', 613: 'xs', 614: 'xt', 615: 'xu', 616: 'xv', 617: 'xw', 618: 'xx', 619: 'xy', 620: \"y'\", 621: 'ya', 622: 'yb', 623: 'yc', 624: 'yd', 625: 'ye', 626: 'yf', 627: 'yg', 628: 'yh', 629: 'yi', 630: 'yj', 631: 'yk', 632: 'yl', 633: 'ym', 634: 'yn', 635: 'yo', 636: 'yp', 637: 'yq', 638: 'yr', 639: 'ys', 640: 'yt', 641: 'yu', 642: 'yv', 643: 'yw', 644: 'yx', 645: 'yy', 646: 'yz', 647: \"z'\", 648: 'za', 649: 'zb', 650: 'zc', 651: 'zd', 652: 'ze', 653: 'zf', 654: 'zg', 655: 'zh', 656: 'zi', 657: 'zk', 658: 'zl', 659: 'zm', 660: 'zn', 661: 'zo', 662: 'zp', 663: 'zq', 664: 'zr', 665: 'zs', 666: 'zt', 667: 'zu', 668: 'zv', 669: 'zw', 670: 'zy', 671: 'zz'}\n",
      "valid grp 672 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: \"a'\", 23: 'aa', 24: 'ab', 25: 'ac', 26: 'ad', 27: 'ae', 28: 'af', 29: 'ag', 30: 'ah', 31: 'ai', 32: 'aj', 33: 'ak', 34: 'al', 35: 'am', 36: 'an', 37: 'ao', 38: 'ap', 39: 'aq', 40: 'ar', 41: 'as', 42: 'at', 43: 'au', 44: 'av', 45: 'aw', 46: 'ax', 47: 'ay', 48: 'az', 49: \"b'\", 50: 'ba', 51: 'bb', 52: 'bc', 53: 'bd', 54: 'be', 55: 'bf', 56: 'bg', 57: 'bh', 58: 'bi', 59: 'bj', 60: 'bk', 61: 'bl', 62: 'bm', 63: 'bn', 64: 'bo', 65: 'bp', 66: 'br', 67: 'bs', 68: 'bt', 69: 'bu', 70: 'bv', 71: 'bw', 72: 'by', 73: 'bz', 74: \"c'\", 75: 'ca', 76: 'cb', 77: 'cc', 78: 'cd', 79: 'ce', 80: 'cf', 81: 'cg', 82: 'ch', 83: 'ci', 84: 'cj', 85: 'ck', 86: 'cl', 87: 'cm', 88: 'cn', 89: 'co', 90: 'cp', 91: 'cq', 92: 'cr', 93: 'cs', 94: 'ct', 95: 'cu', 96: 'cv', 97: 'cw', 98: 'cy', 99: 'cz', 100: \"d'\", 101: 'da', 102: 'db', 103: 'dc', 104: 'dd', 105: 'de', 106: 'df', 107: 'dg', 108: 'dh', 109: 'di', 110: 'dj', 111: 'dk', 112: 'dl', 113: 'dm', 114: 'dn', 115: 'do', 116: 'dp', 117: 'dq', 118: 'dr', 119: 'ds', 120: 'dt', 121: 'du', 122: 'dv', 123: 'dw', 124: 'dy', 125: 'dz', 126: \"e'\", 127: 'ea', 128: 'eb', 129: 'ec', 130: 'ed', 131: 'ee', 132: 'ef', 133: 'eg', 134: 'eh', 135: 'ei', 136: 'ej', 137: 'ek', 138: 'el', 139: 'em', 140: 'en', 141: 'eo', 142: 'ep', 143: 'eq', 144: 'er', 145: 'es', 146: 'et', 147: 'eu', 148: 'ev', 149: 'ew', 150: 'ex', 151: 'ey', 152: 'ez', 153: \"f'\", 154: 'fa', 155: 'fb', 156: 'fc', 157: 'fd', 158: 'fe', 159: 'ff', 160: 'fg', 161: 'fh', 162: 'fi', 163: 'fj', 164: 'fk', 165: 'fl', 166: 'fm', 167: 'fn', 168: 'fo', 169: 'fp', 170: 'fq', 171: 'fr', 172: 'fs', 173: 'ft', 174: 'fu', 175: 'fv', 176: 'fw', 177: 'fx', 178: 'fy', 179: 'fz', 180: \"g'\", 181: 'ga', 182: 'gb', 183: 'gc', 184: 'gd', 185: 'ge', 186: 'gf', 187: 'gg', 188: 'gh', 189: 'gi', 190: 'gj', 191: 'gk', 192: 'gl', 193: 'gm', 194: 'gn', 195: 'go', 196: 'gp', 197: 'gq', 198: 'gr', 199: 'gs', 200: 'gt', 201: 'gu', 202: 'gv', 203: 'gw', 204: 'gx', 205: 'gy', 206: 'gz', 207: \"h'\", 208: 'ha', 209: 'hb', 210: 'hc', 211: 'hd', 212: 'he', 213: 'hf', 214: 'hg', 215: 'hh', 216: 'hi', 217: 'hj', 218: 'hk', 219: 'hl', 220: 'hm', 221: 'hn', 222: 'ho', 223: 'hp', 224: 'hq', 225: 'hr', 226: 'hs', 227: 'ht', 228: 'hu', 229: 'hv', 230: 'hw', 231: 'hy', 232: 'i', 233: \"i'\", 234: 'ia', 235: 'ib', 236: 'ic', 237: 'id', 238: 'ie', 239: 'if', 240: 'ig', 241: 'ih', 242: 'ii', 243: 'ij', 244: 'ik', 245: 'il', 246: 'im', 247: 'in', 248: 'io', 249: 'ip', 250: 'iq', 251: 'ir', 252: 'is', 253: 'it', 254: 'iu', 255: 'iv', 256: 'iw', 257: 'ix', 258: 'iy', 259: 'iz', 260: \"j'\", 261: 'ja', 262: 'jc', 263: 'jd', 264: 'je', 265: 'jf', 266: 'jh', 267: 'ji', 268: 'jj', 269: 'jk', 270: 'jl', 271: 'jm', 272: 'jn', 273: 'jo', 274: 'js', 275: 'jt', 276: 'ju', 277: 'jv', 278: 'jy', 279: 'jz', 280: \"k'\", 281: 'ka', 282: 'kb', 283: 'kc', 284: 'kd', 285: 'ke', 286: 'kf', 287: 'kg', 288: 'kh', 289: 'ki', 290: 'kj', 291: 'kk', 292: 'kl', 293: 'km', 294: 'kn', 295: 'ko', 296: 'kp', 297: 'kr', 298: 'ks', 299: 'kt', 300: 'ku', 301: 'kv', 302: 'kw', 303: 'ky', 304: 'kz', 305: \"l'\", 306: 'la', 307: 'lb', 308: 'lc', 309: 'ld', 310: 'le', 311: 'lf', 312: 'lg', 313: 'lh', 314: 'li', 315: 'lj', 316: 'lk', 317: 'll', 318: 'lm', 319: 'ln', 320: 'lo', 321: 'lp', 322: 'lq', 323: 'lr', 324: 'ls', 325: 'lt', 326: 'lu', 327: 'lv', 328: 'lw', 329: 'lx', 330: 'ly', 331: 'lz', 332: \"m'\", 333: 'ma', 334: 'mb', 335: 'mc', 336: 'md', 337: 'me', 338: 'mf', 339: 'mg', 340: 'mh', 341: 'mi', 342: 'mj', 343: 'mk', 344: 'ml', 345: 'mm', 346: 'mn', 347: 'mo', 348: 'mp', 349: 'mq', 350: 'mr', 351: 'ms', 352: 'mt', 353: 'mu', 354: 'mv', 355: 'mw', 356: 'my', 357: 'mz', 358: \"n'\", 359: 'na', 360: 'nb', 361: 'nc', 362: 'nd', 363: 'ne', 364: 'nf', 365: 'ng', 366: 'nh', 367: 'ni', 368: 'nj', 369: 'nk', 370: 'nl', 371: 'nm', 372: 'nn', 373: 'no', 374: 'np', 375: 'nq', 376: 'nr', 377: 'ns', 378: 'nt', 379: 'nu', 380: 'nv', 381: 'nw', 382: 'nx', 383: 'ny', 384: 'nz', 385: 'o', 386: \"o'\", 387: 'oa', 388: 'ob', 389: 'oc', 390: 'od', 391: 'oe', 392: 'of', 393: 'og', 394: 'oh', 395: 'oi', 396: 'oj', 397: 'ok', 398: 'ol', 399: 'om', 400: 'on', 401: 'oo', 402: 'op', 403: 'oq', 404: 'or', 405: 'os', 406: 'ot', 407: 'ou', 408: 'ov', 409: 'ow', 410: 'ox', 411: 'oy', 412: 'oz', 413: \"p'\", 414: 'pa', 415: 'pb', 416: 'pc', 417: 'pd', 418: 'pe', 419: 'pf', 420: 'pg', 421: 'ph', 422: 'pi', 423: 'pj', 424: 'pk', 425: 'pl', 426: 'pm', 427: 'pn', 428: 'po', 429: 'pp', 430: 'pr', 431: 'ps', 432: 'pt', 433: 'pu', 434: 'pw', 435: 'py', 436: 'pz', 437: \"q'\", 438: 'qa', 439: 'qb', 440: 'qg', 441: 'qi', 442: 'qo', 443: 'qu', 444: 'qv', 445: \"r'\", 446: 'ra', 447: 'rb', 448: 'rc', 449: 'rd', 450: 're', 451: 'rf', 452: 'rg', 453: 'rh', 454: 'ri', 455: 'rj', 456: 'rk', 457: 'rl', 458: 'rm', 459: 'rn', 460: 'ro', 461: 'rp', 462: 'rq', 463: 'rr', 464: 'rs', 465: 'rt', 466: 'ru', 467: 'rv', 468: 'rw', 469: 'rx', 470: 'ry', 471: 'rz', 472: \"s'\", 473: 'sa', 474: 'sb', 475: 'sc', 476: 'sd', 477: 'se', 478: 'sf', 479: 'sg', 480: 'sh', 481: 'si', 482: 'sj', 483: 'sk', 484: 'sl', 485: 'sm', 486: 'sn', 487: 'so', 488: 'sp', 489: 'sq', 490: 'sr', 491: 'ss', 492: 'st', 493: 'su', 494: 'sv', 495: 'sw', 496: 'sx', 497: 'sy', 498: 'sz', 499: \"t'\", 500: 'ta', 501: 'tb', 502: 'tc', 503: 'td', 504: 'te', 505: 'tf', 506: 'tg', 507: 'th', 508: 'ti', 509: 'tj', 510: 'tk', 511: 'tl', 512: 'tm', 513: 'tn', 514: 'to', 515: 'tp', 516: 'tr', 517: 'ts', 518: 'tt', 519: 'tu', 520: 'tv', 521: 'tw', 522: 'tx', 523: 'ty', 524: 'tz', 525: \"u'\", 526: 'ua', 527: 'ub', 528: 'uc', 529: 'ud', 530: 'ue', 531: 'uf', 532: 'ug', 533: 'uh', 534: 'ui', 535: 'uj', 536: 'uk', 537: 'ul', 538: 'um', 539: 'un', 540: 'uo', 541: 'up', 542: 'uq', 543: 'ur', 544: 'us', 545: 'ut', 546: 'uu', 547: 'uv', 548: 'uw', 549: 'ux', 550: 'uy', 551: 'uz', 552: \"v'\", 553: 'va', 554: 'vc', 555: 'vd', 556: 've', 557: 'vg', 558: 'vh', 559: 'vi', 560: 'vj', 561: 'vk', 562: 'vl', 563: 'vm', 564: 'vn', 565: 'vo', 566: 'vr', 567: 'vs', 568: 'vt', 569: 'vu', 570: 'vv', 571: 'vy', 572: \"w'\", 573: 'wa', 574: 'wb', 575: 'wc', 576: 'wd', 577: 'we', 578: 'wf', 579: 'wg', 580: 'wh', 581: 'wi', 582: 'wk', 583: 'wl', 584: 'wm', 585: 'wn', 586: 'wo', 587: 'wp', 588: 'wr', 589: 'ws', 590: 'wt', 591: 'wu', 592: 'wv', 593: 'ww', 594: 'wy', 595: 'wz', 596: \"x'\", 597: 'xa', 598: 'xb', 599: 'xc', 600: 'xd', 601: 'xe', 602: 'xf', 603: 'xg', 604: 'xh', 605: 'xi', 606: 'xl', 607: 'xm', 608: 'xn', 609: 'xo', 610: 'xp', 611: 'xq', 612: 'xr', 613: 'xs', 614: 'xt', 615: 'xu', 616: 'xv', 617: 'xw', 618: 'xx', 619: 'xy', 620: \"y'\", 621: 'ya', 622: 'yb', 623: 'yc', 624: 'yd', 625: 'ye', 626: 'yf', 627: 'yg', 628: 'yh', 629: 'yi', 630: 'yj', 631: 'yk', 632: 'yl', 633: 'ym', 634: 'yn', 635: 'yo', 636: 'yp', 637: 'yq', 638: 'yr', 639: 'ys', 640: 'yt', 641: 'yu', 642: 'yv', 643: 'yw', 644: 'yx', 645: 'yy', 646: 'yz', 647: \"z'\", 648: 'za', 649: 'zb', 650: 'zc', 651: 'zd', 652: 'ze', 653: 'zf', 654: 'zg', 655: 'zh', 656: 'zi', 657: 'zk', 658: 'zl', 659: 'zm', 660: 'zn', 661: 'zo', 662: 'zp', 663: 'zq', 664: 'zr', 665: 'zs', 666: 'zt', 667: 'zu', 668: 'zv', 669: 'zw', 670: 'zy', 671: 'zz'}\n",
      "test grp 672 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: \"a'\", 23: 'aa', 24: 'ab', 25: 'ac', 26: 'ad', 27: 'ae', 28: 'af', 29: 'ag', 30: 'ah', 31: 'ai', 32: 'aj', 33: 'ak', 34: 'al', 35: 'am', 36: 'an', 37: 'ao', 38: 'ap', 39: 'aq', 40: 'ar', 41: 'as', 42: 'at', 43: 'au', 44: 'av', 45: 'aw', 46: 'ax', 47: 'ay', 48: 'az', 49: \"b'\", 50: 'ba', 51: 'bb', 52: 'bc', 53: 'bd', 54: 'be', 55: 'bf', 56: 'bg', 57: 'bh', 58: 'bi', 59: 'bj', 60: 'bk', 61: 'bl', 62: 'bm', 63: 'bn', 64: 'bo', 65: 'bp', 66: 'br', 67: 'bs', 68: 'bt', 69: 'bu', 70: 'bv', 71: 'bw', 72: 'by', 73: 'bz', 74: \"c'\", 75: 'ca', 76: 'cb', 77: 'cc', 78: 'cd', 79: 'ce', 80: 'cf', 81: 'cg', 82: 'ch', 83: 'ci', 84: 'cj', 85: 'ck', 86: 'cl', 87: 'cm', 88: 'cn', 89: 'co', 90: 'cp', 91: 'cq', 92: 'cr', 93: 'cs', 94: 'ct', 95: 'cu', 96: 'cv', 97: 'cw', 98: 'cy', 99: 'cz', 100: \"d'\", 101: 'da', 102: 'db', 103: 'dc', 104: 'dd', 105: 'de', 106: 'df', 107: 'dg', 108: 'dh', 109: 'di', 110: 'dj', 111: 'dk', 112: 'dl', 113: 'dm', 114: 'dn', 115: 'do', 116: 'dp', 117: 'dq', 118: 'dr', 119: 'ds', 120: 'dt', 121: 'du', 122: 'dv', 123: 'dw', 124: 'dy', 125: 'dz', 126: \"e'\", 127: 'ea', 128: 'eb', 129: 'ec', 130: 'ed', 131: 'ee', 132: 'ef', 133: 'eg', 134: 'eh', 135: 'ei', 136: 'ej', 137: 'ek', 138: 'el', 139: 'em', 140: 'en', 141: 'eo', 142: 'ep', 143: 'eq', 144: 'er', 145: 'es', 146: 'et', 147: 'eu', 148: 'ev', 149: 'ew', 150: 'ex', 151: 'ey', 152: 'ez', 153: \"f'\", 154: 'fa', 155: 'fb', 156: 'fc', 157: 'fd', 158: 'fe', 159: 'ff', 160: 'fg', 161: 'fh', 162: 'fi', 163: 'fj', 164: 'fk', 165: 'fl', 166: 'fm', 167: 'fn', 168: 'fo', 169: 'fp', 170: 'fq', 171: 'fr', 172: 'fs', 173: 'ft', 174: 'fu', 175: 'fv', 176: 'fw', 177: 'fx', 178: 'fy', 179: 'fz', 180: \"g'\", 181: 'ga', 182: 'gb', 183: 'gc', 184: 'gd', 185: 'ge', 186: 'gf', 187: 'gg', 188: 'gh', 189: 'gi', 190: 'gj', 191: 'gk', 192: 'gl', 193: 'gm', 194: 'gn', 195: 'go', 196: 'gp', 197: 'gq', 198: 'gr', 199: 'gs', 200: 'gt', 201: 'gu', 202: 'gv', 203: 'gw', 204: 'gx', 205: 'gy', 206: 'gz', 207: \"h'\", 208: 'ha', 209: 'hb', 210: 'hc', 211: 'hd', 212: 'he', 213: 'hf', 214: 'hg', 215: 'hh', 216: 'hi', 217: 'hj', 218: 'hk', 219: 'hl', 220: 'hm', 221: 'hn', 222: 'ho', 223: 'hp', 224: 'hq', 225: 'hr', 226: 'hs', 227: 'ht', 228: 'hu', 229: 'hv', 230: 'hw', 231: 'hy', 232: 'i', 233: \"i'\", 234: 'ia', 235: 'ib', 236: 'ic', 237: 'id', 238: 'ie', 239: 'if', 240: 'ig', 241: 'ih', 242: 'ii', 243: 'ij', 244: 'ik', 245: 'il', 246: 'im', 247: 'in', 248: 'io', 249: 'ip', 250: 'iq', 251: 'ir', 252: 'is', 253: 'it', 254: 'iu', 255: 'iv', 256: 'iw', 257: 'ix', 258: 'iy', 259: 'iz', 260: \"j'\", 261: 'ja', 262: 'jc', 263: 'jd', 264: 'je', 265: 'jf', 266: 'jh', 267: 'ji', 268: 'jj', 269: 'jk', 270: 'jl', 271: 'jm', 272: 'jn', 273: 'jo', 274: 'js', 275: 'jt', 276: 'ju', 277: 'jv', 278: 'jy', 279: 'jz', 280: \"k'\", 281: 'ka', 282: 'kb', 283: 'kc', 284: 'kd', 285: 'ke', 286: 'kf', 287: 'kg', 288: 'kh', 289: 'ki', 290: 'kj', 291: 'kk', 292: 'kl', 293: 'km', 294: 'kn', 295: 'ko', 296: 'kp', 297: 'kr', 298: 'ks', 299: 'kt', 300: 'ku', 301: 'kv', 302: 'kw', 303: 'ky', 304: 'kz', 305: \"l'\", 306: 'la', 307: 'lb', 308: 'lc', 309: 'ld', 310: 'le', 311: 'lf', 312: 'lg', 313: 'lh', 314: 'li', 315: 'lj', 316: 'lk', 317: 'll', 318: 'lm', 319: 'ln', 320: 'lo', 321: 'lp', 322: 'lq', 323: 'lr', 324: 'ls', 325: 'lt', 326: 'lu', 327: 'lv', 328: 'lw', 329: 'lx', 330: 'ly', 331: 'lz', 332: \"m'\", 333: 'ma', 334: 'mb', 335: 'mc', 336: 'md', 337: 'me', 338: 'mf', 339: 'mg', 340: 'mh', 341: 'mi', 342: 'mj', 343: 'mk', 344: 'ml', 345: 'mm', 346: 'mn', 347: 'mo', 348: 'mp', 349: 'mq', 350: 'mr', 351: 'ms', 352: 'mt', 353: 'mu', 354: 'mv', 355: 'mw', 356: 'my', 357: 'mz', 358: \"n'\", 359: 'na', 360: 'nb', 361: 'nc', 362: 'nd', 363: 'ne', 364: 'nf', 365: 'ng', 366: 'nh', 367: 'ni', 368: 'nj', 369: 'nk', 370: 'nl', 371: 'nm', 372: 'nn', 373: 'no', 374: 'np', 375: 'nq', 376: 'nr', 377: 'ns', 378: 'nt', 379: 'nu', 380: 'nv', 381: 'nw', 382: 'nx', 383: 'ny', 384: 'nz', 385: 'o', 386: \"o'\", 387: 'oa', 388: 'ob', 389: 'oc', 390: 'od', 391: 'oe', 392: 'of', 393: 'og', 394: 'oh', 395: 'oi', 396: 'oj', 397: 'ok', 398: 'ol', 399: 'om', 400: 'on', 401: 'oo', 402: 'op', 403: 'oq', 404: 'or', 405: 'os', 406: 'ot', 407: 'ou', 408: 'ov', 409: 'ow', 410: 'ox', 411: 'oy', 412: 'oz', 413: \"p'\", 414: 'pa', 415: 'pb', 416: 'pc', 417: 'pd', 418: 'pe', 419: 'pf', 420: 'pg', 421: 'ph', 422: 'pi', 423: 'pj', 424: 'pk', 425: 'pl', 426: 'pm', 427: 'pn', 428: 'po', 429: 'pp', 430: 'pr', 431: 'ps', 432: 'pt', 433: 'pu', 434: 'pw', 435: 'py', 436: 'pz', 437: \"q'\", 438: 'qa', 439: 'qb', 440: 'qg', 441: 'qi', 442: 'qo', 443: 'qu', 444: 'qv', 445: \"r'\", 446: 'ra', 447: 'rb', 448: 'rc', 449: 'rd', 450: 're', 451: 'rf', 452: 'rg', 453: 'rh', 454: 'ri', 455: 'rj', 456: 'rk', 457: 'rl', 458: 'rm', 459: 'rn', 460: 'ro', 461: 'rp', 462: 'rq', 463: 'rr', 464: 'rs', 465: 'rt', 466: 'ru', 467: 'rv', 468: 'rw', 469: 'rx', 470: 'ry', 471: 'rz', 472: \"s'\", 473: 'sa', 474: 'sb', 475: 'sc', 476: 'sd', 477: 'se', 478: 'sf', 479: 'sg', 480: 'sh', 481: 'si', 482: 'sj', 483: 'sk', 484: 'sl', 485: 'sm', 486: 'sn', 487: 'so', 488: 'sp', 489: 'sq', 490: 'sr', 491: 'ss', 492: 'st', 493: 'su', 494: 'sv', 495: 'sw', 496: 'sx', 497: 'sy', 498: 'sz', 499: \"t'\", 500: 'ta', 501: 'tb', 502: 'tc', 503: 'td', 504: 'te', 505: 'tf', 506: 'tg', 507: 'th', 508: 'ti', 509: 'tj', 510: 'tk', 511: 'tl', 512: 'tm', 513: 'tn', 514: 'to', 515: 'tp', 516: 'tr', 517: 'ts', 518: 'tt', 519: 'tu', 520: 'tv', 521: 'tw', 522: 'tx', 523: 'ty', 524: 'tz', 525: \"u'\", 526: 'ua', 527: 'ub', 528: 'uc', 529: 'ud', 530: 'ue', 531: 'uf', 532: 'ug', 533: 'uh', 534: 'ui', 535: 'uj', 536: 'uk', 537: 'ul', 538: 'um', 539: 'un', 540: 'uo', 541: 'up', 542: 'uq', 543: 'ur', 544: 'us', 545: 'ut', 546: 'uu', 547: 'uv', 548: 'uw', 549: 'ux', 550: 'uy', 551: 'uz', 552: \"v'\", 553: 'va', 554: 'vc', 555: 'vd', 556: 've', 557: 'vg', 558: 'vh', 559: 'vi', 560: 'vj', 561: 'vk', 562: 'vl', 563: 'vm', 564: 'vn', 565: 'vo', 566: 'vr', 567: 'vs', 568: 'vt', 569: 'vu', 570: 'vv', 571: 'vy', 572: \"w'\", 573: 'wa', 574: 'wb', 575: 'wc', 576: 'wd', 577: 'we', 578: 'wf', 579: 'wg', 580: 'wh', 581: 'wi', 582: 'wk', 583: 'wl', 584: 'wm', 585: 'wn', 586: 'wo', 587: 'wp', 588: 'wr', 589: 'ws', 590: 'wt', 591: 'wu', 592: 'wv', 593: 'ww', 594: 'wy', 595: 'wz', 596: \"x'\", 597: 'xa', 598: 'xb', 599: 'xc', 600: 'xd', 601: 'xe', 602: 'xf', 603: 'xg', 604: 'xh', 605: 'xi', 606: 'xl', 607: 'xm', 608: 'xn', 609: 'xo', 610: 'xp', 611: 'xq', 612: 'xr', 613: 'xs', 614: 'xt', 615: 'xu', 616: 'xv', 617: 'xw', 618: 'xx', 619: 'xy', 620: \"y'\", 621: 'ya', 622: 'yb', 623: 'yc', 624: 'yd', 625: 'ye', 626: 'yf', 627: 'yg', 628: 'yh', 629: 'yi', 630: 'yj', 631: 'yk', 632: 'yl', 633: 'ym', 634: 'yn', 635: 'yo', 636: 'yp', 637: 'yq', 638: 'yr', 639: 'ys', 640: 'yt', 641: 'yu', 642: 'yv', 643: 'yw', 644: 'yx', 645: 'yy', 646: 'yz', 647: \"z'\", 648: 'za', 649: 'zb', 650: 'zc', 651: 'zd', 652: 'ze', 653: 'zf', 654: 'zg', 655: 'zh', 656: 'zi', 657: 'zk', 658: 'zl', 659: 'zm', 660: 'zn', 661: 'zo', 662: 'zp', 663: 'zq', 664: 'zr', 665: 'zs', 666: 'zt', 667: 'zu', 668: 'zv', 669: 'zw', 670: 'zy', 671: 'zz'}\n",
      "train phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "valid phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "test phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "668 {\"'c\": 6, 'ca': 75, 'au': 43, 'us': 544, 'se': 477, 'co': 89, 'ou': 407, 'ur': 543, 'rs': 464, \"'e\": 8, 'em': 139, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 443, 'uo': 540, 'ot': 406, 'te': 504, \"'t\": 20, 'ti': 508, 'il': 245, 'is': 252, 'tw': 521, 'wa': 573, 'as': 41, 'aa': 23, 'ab': 24, 'be': 54, 'er': 144, 'rg': 452, 'ac': 25, 'ch': 82, 'he': 212, 'en': 140, 'ak': 33, 'ke': 285, 'al': 34, 'ls': 324, 'et': 146, 'th': 507, 'am': 35, 'mo': 347, 'od': 390, 'dt': 120, 'ar': 40, 'rd': 449, 'de': 105, 'ma': 333, 'dv': 122, 'va': 553, 'rk': 456, 'ro': 460, 'on': 400, \"n'\": 358, \"'s\": 19, 'ns': 377, 'so': 487, 'rt': 465, 'ba': 50, 'ha': 208, 'ck': 85, 'cu': 95, 'ad': 26, 'da': 101, 'ka': 281, 'di': 109, 'ie': 238, 'ai': 31, 'ir': 251, 'lk': 316, 'ki': 289, 'in': 247, 'lo': 320, 'os': 405, 'an': 36, 'nd': 362, 'do': 115, 'ne': 363, 'ed': 130, 'ni': 367, 'ng': 365, 'nm': 371, 'me': 337, 'nt': 378, 'to': 514, 'rc': 448, 're': 450, 'sc': 475, 'sh': 480, 'at': 42, 'ts': 517, 'es': 145, 'bb': 51, 'si': 481, 'el': 138, 'll': 317, 'tt': 518, 'ev': 148, 'vi': 559, 'le': 310, 'ey': 151, \"y'\": 620, 'bi': 58, 'it': 253, 'bo': 64, 'ud': 529, 'br': 66, 'ia': 234, 'io': 248, 'ru': 466, 'uz': 551, 'zz': 671, 'ze': 652, 'bs': 67, 'by': 72, 'bc': 52, 'ek': 137, 'bd': 53, 'la': 306, 'ah': 30, 'ic': 236, 'dn': 114, 'no': 373, 'or': 404, 'om': 399, 'mi': 341, 'na': 359, 'du': 121, 'uc': 528, 'ct': 94, 'ee': 131, 'ul': 537, 'az': 48, 'zi': 656, 'iz': 259, 'ow': 409, 'dr': 118, 'rf': 451, 'fo': 168, 'rl': 457, 'rm': 458, 'rn': 459, 'hy': 231, 'rr': 463, 'ra': 446, 'ex': 150, 'ya': 621, 'nc': 361, 'ce': 79, 'bh': 57, 'ho': 222, 'id': 237, 'li': 314, 'ty': 523, 'gt': 200, 'ol': 398, \"a'\": 22, 'ib': 235, 'bj': 59, 'je': 264, 'ec': 129, 'bk': 60, 'kh': 288, 'bl': 61, 'st': 492, 'ly': 330, 'bn': 63, \"o'\": 386, 'oa': 387, 'hi': 216, 'sm': 485, 'oo': 401, 'ri': 454, 'ig': 240, 'gi': 189, 'iv': 255, 've': 556, 'uh': 533, 'im': 246, 'un': 539, 'ds': 119, 'ut': 545, \"t'\": 499, 'ov': 408, \"e'\": 126, 'eb': 128, 'ms': 351, 'mc': 335, 'cz': 99, 'zy': 670, 'yk': 631, 'tz': 524, 'wi': 581, 'ea': 127, 'eg': 133, 'go': 195, 'dg': 107, 'ge': 185, 'og': 393, 'ga': 181, 'up': 541, 'pt': 432, 'tl': 511, 'tn': 513, 'ss': 491, 'yn': 634, 'zo': 661, 'sa': 473, 'lu': 326, 'lv': 327, 'rb': 447, 'ta': 500, 'tr': 516, 'su': 493, 'bt': 68, 'bu': 69, 'hm': 220, 'dz': 125, 'ys': 639, 'bz': 73, 'zu': 667, 'ug': 532, 'ci': 83, 'my': 356, 'mp': 348, 'po': 428, 'ap': 38, 'pu': 433, 'lc': 308, 'cc': 77, 'tu': 519, 'ua': 526, 'ep': 142, 'ry': 470, 'cl': 86, 'mm': 345, 'pa': 414, 'ny': 383, 'yi': 629, 'pl': 425, 'cy': 98, \"s'\": 472, 'cr': 92, 'ue': 530, 'ui': 534, 'um': 538, 'mu': 353, 'ay': 47, 'gl': 192, 'op': 402, 'ph': 421, 'yl': 632, 'nb': 360, 'ht': 227, 'if': 239, 'fi': 162, 'fy': 178, 'dl': 112, \"r'\": 445, 'nn': 372, 'kl': 292, 'kn': 294, 'wl': 583, 'gm': 193, 'kr': 297, 'oy': 411, 'yd': 624, \"d'\": 100, 'cm': 87, 'cn': 88, 'of': 392, 'ff': 159, 'yt': 640, 'cs': 93, 'cq': 91, 'av': 44, 'ag': 29, 'ob': 388, 'ym': 633, 'uf': 531, 'sy': 497, 'yc': 623, \"m'\": 332, 'ik': 244, 'mk': 343, 'ku': 300, 'sk': 483, 'wn': 585, 'za': 648, 'dc': 103, 'oc': 389, 'dd': 104, 'eo': 141, 'ks': 298, 'dw': 123, 'we': 577, 'dy': 124, 'lb': 307, \"l'\": 305, 'lm': 318, 'lp': 321, 'sb': 474, 'ei': 135, 'oi': 395, 'eq': 143, 'rh': 453, 'ld': 309, 'lt': 325, 'dh': 108, 'ip': 249, 'dj': 110, 'ja': 261, 'jo': 273, 'ju': 276, 'dk': 111, 'dm': 113, 'lf': 311, \"f'\": 153, 'hs': 226, 'gn': 194, 'ft': 173, 'ae': 27, 'sd': 476, 'vo': 565, 'fl': 165, 'xi': 605, \"h'\": 207, 'sp': 488, \"p'\": 413, 'af': 28, 'fa': 154, 'ye': 625, 'fe': 158, 'ix': 257, 'xe': 601, 'fr': 171, 'ax': 46, 'fg': 160, 'gh': 188, \"i'\": 233, 'fh': 161, 'fm': 166, 'ef': 132, 'fu': 174, 'mn': 346, 'gy': 205, 'pe': 418, 'gf': 186, 'gg': 187, 'rw': 468, 'gr': 198, 'eh': 134, 'iu': 254, 'ew': 149, 'aw': 45, 'xc': 599, 'pp': 429, 'ok': 397, 'ko': 295, 'gu': 201, 'yo': 635, 'hh': 215, 'hl': 219, 'lg': 312, 'lq': 322, 'hn': 221, 'hr': 225, 'hu': 228, 'km': 293, 'ml': 344, 'sl': 484, 'nl': 370, 'sw': 495, 'wo': 586, \"c'\": 74, 'gs': 199, 'ps': 431, 'nk': 369, 'rp': 461, 'tc': 502, 'tk': 510, 'iw': 256, 'aj': 32, \"j'\": 260, 'ji': 267, 'kb': 282, 'kc': 283, 'ih': 241, 'mb': 334, 'kk': 291, 'ky': 303, 'kz': 304, 'tv': 520, 'uq': 542, 'rq': 462, 'oh': 394, 'wy': 594, 'ej': 136, 'eu': 147, \"x'\": 596, 'xa': 597, 'xy': 619, 'nq': 375, 'lh': 313, 'nz': 384, 'ij': 243, 'iq': 250, 'nh': 366, 'nw': 381, 'yw': 643, 'ln': 319, 'tm': 512, 'ws': 589, 'lr': 323, 'uv': 547, 'lw': 328, 'ez': 152, 'mg': 339, 'mq': 349, 'oe': 391, 'fn': 167, 'nu': 379, 'pi': 422, \"k'\": 280, 'tf': 505, 'zh': 655, 'lz': 331, 'np': 374, 'xt': 614, 'md': 336, 'zc': 650, 'zq': 663, 'mf': 338, 'mh': 340, 'yv': 642, \"g'\": 180, 'pc': 416, 'mr': 350, 'mt': 352, 'mv': 354, 'mw': 355, 'yz': 646, 'sq': 489, 'nv': 380, 'oz': 412, 'rz': 471, 'uj': 535, 'cd': 78, 'nf': 364, 'nr': 376, 'sg': 479, 'pr': 430, 'ox': 410, 'ub': 527, 'bm': 62, \"u'\": 525, 'nx': 382, 'yb': 622, 'yh': 628, 'yp': 636, 'wh': 580, 'ao': 37, 'pf': 419, 'pg': 420, 'pn': 427, 'pk': 424, 'aq': 39, 'qa': 438, 'rv': 467, 'ux': 549, 'hb': 209, 'hd': 211, 'uk': 536, 'yr': 638, 'zm': 659, 'rj': 455, \"w'\": 572, 'wr': 588, 'rx': 469, 'zt': 666, 'ii': 242, 'hc': 210, 'hf': 213, 'hv': 229, 'hw': 230, \"v'\": 552, 'sn': 486, 'fd': 157, \"'h\": 10, 'uy': 550, 'vc': 554, 'vd': 555, 'vm': 563, 'vn': 564, 'vr': 566, 'vt': 568, 'wb': 574, 'wf': 578, 'wk': 582, 'kw': 302, 'wt': 590, 'xf': 602, 'xl': 606, 'xo': 609, 'xs': 613, 'yg': 627, 'yu': 641, 'yy': 645, 'zb': 649, 'zp': 662, \"b'\": 49, \"'r\": 18, \"'a\": 4, 'hk': 218, 'kd': 284, 'kf': 286, 'kg': 287, 'kp': 296, 'kt': 299, 'tj': 509, 'gb': 182, 'gd': 184, 'gp': 196, 'gw': 203, \"'i\": 11, 'sf': 478, 'zs': 665, 'sz': 498, 'gk': 191, 'nj': 368, 'kv': 301, 'xq': 611, 'fs': 172, 'sv': 494, 'vs': 567, 'wm': 584, 'tb': 501, 'sr': 490, 'td': 503, 'uw': 548, 'wd': 576, 'zl': 658, 'cv': 96, 'db': 102, 'df': 106, 'dp': 116, 'vu': 569, 'zr': 664, 'jy': 278, \"z'\": 647, 'wu': 591, 'gq': 197, 'hp': 223, 'vy': 571, 'zd': 651, 'zn': 660, 'lj': 315, 'fb': 155, 'xu': 615, 'xb': 598, 'kj': 290, 'zk': 657, 'xh': 604, 'tg': 506, 'sj': 482, 'oj': 396, 'gj': 190, 'oq': 403, 'wc': 575, 'xw': 617, 'xx': 618, 'yf': 626, 'jd': 263, 'tp': 515, 'fc': 156, 'fk': 164, 'jn': 272, 'py': 435, 'uu': 546, 'zw': 669, 'yx': 644, 'pb': 415, 'pw': 434, \"q'\": 437, \"'v\": 21, 'jk': 269, 'gc': 183, 'pd': 417, 'pm': 426, 'gx': 204, 'iy': 258, 'hg': 214, 'bw': 71, 'wg': 579, 'wp': 587, 'zf': 653, 'vl': 562, 'cw': 97, \"'o\": 16, 'mj': 342, 'vv': 570, 'xv': 616, 'bf': 55, 'hq': 224, 'dq': 117, 'vg': 557, 'lx': 329, \"'d\": 7, 'vj': 560, 'xp': 610, 'wv': 592, 'jv': 277, 'zg': 654, 'fj': 163, 'xm': 607, 'xn': 608, 'jt': 275, 'xg': 603, 'tx': 522, 'vh': 558, 'mz': 357, 'fp': 169, 'gv': 202, 'jj': 268, 'fw': 176, 'jl': 270, 'hj': 217, 'bg': 56, 'wz': 595, 'i': 232, \"'l\": 13, 'qb': 439, 'qg': 440, 'qi': 441, 'jf': 265, 'jh': 266, 'js': 274, 'jc': 262, 'bv': 70, 'pz': 436, 'fq': 170, \"'b\": 5, 'cb': 76, 'cf': 80, 'cg': 81, 'cp': 90, 'cj': 84, 'zv': 668, 'fx': 177, 'jz': 279, 'fz': 179, 'qv': 444, 'ww': 593, 'xr': 612, 'xd': 600, 'o': 385, \"'g\": 9, \"'k\": 12, 'vk': 561, 'qo': 442, 'jm': 271, 'yj': 630, 'pj': 423, 'fv': 175, 'bp': 65, 'sx': 496, 'yq': 637, 'gz': 206}\n",
      "668 {\"'c\": 6, 'ca': 75, 'au': 43, 'us': 544, 'se': 477, 'co': 89, 'ou': 407, 'ur': 543, 'rs': 464, \"'e\": 8, 'em': 139, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 443, 'uo': 540, 'ot': 406, 'te': 504, \"'t\": 20, 'ti': 508, 'il': 245, 'is': 252, 'tw': 521, 'wa': 573, 'as': 41, 'aa': 23, 'ab': 24, 'be': 54, 'er': 144, 'rg': 452, 'ac': 25, 'ch': 82, 'he': 212, 'en': 140, 'ak': 33, 'ke': 285, 'al': 34, 'ls': 324, 'et': 146, 'th': 507, 'am': 35, 'mo': 347, 'od': 390, 'dt': 120, 'ar': 40, 'rd': 449, 'de': 105, 'ma': 333, 'dv': 122, 'va': 553, 'rk': 456, 'ro': 460, 'on': 400, \"n'\": 358, \"'s\": 19, 'ns': 377, 'so': 487, 'rt': 465, 'ba': 50, 'ha': 208, 'ck': 85, 'cu': 95, 'ad': 26, 'da': 101, 'ka': 281, 'di': 109, 'ie': 238, 'ai': 31, 'ir': 251, 'lk': 316, 'ki': 289, 'in': 247, 'lo': 320, 'os': 405, 'an': 36, 'nd': 362, 'do': 115, 'ne': 363, 'ed': 130, 'ni': 367, 'ng': 365, 'nm': 371, 'me': 337, 'nt': 378, 'to': 514, 'rc': 448, 're': 450, 'sc': 475, 'sh': 480, 'at': 42, 'ts': 517, 'es': 145, 'bb': 51, 'si': 481, 'el': 138, 'll': 317, 'tt': 518, 'ev': 148, 'vi': 559, 'le': 310, 'ey': 151, \"y'\": 620, 'bi': 58, 'it': 253, 'bo': 64, 'ud': 529, 'br': 66, 'ia': 234, 'io': 248, 'ru': 466, 'uz': 551, 'zz': 671, 'ze': 652, 'bs': 67, 'by': 72, 'bc': 52, 'ek': 137, 'bd': 53, 'la': 306, 'ah': 30, 'ic': 236, 'dn': 114, 'no': 373, 'or': 404, 'om': 399, 'mi': 341, 'na': 359, 'du': 121, 'uc': 528, 'ct': 94, 'ee': 131, 'ul': 537, 'az': 48, 'zi': 656, 'iz': 259, 'ow': 409, 'dr': 118, 'rf': 451, 'fo': 168, 'rl': 457, 'rm': 458, 'rn': 459, 'hy': 231, 'rr': 463, 'ra': 446, 'ex': 150, 'ya': 621, 'nc': 361, 'ce': 79, 'bh': 57, 'ho': 222, 'id': 237, 'li': 314, 'ty': 523, 'gt': 200, 'ol': 398, \"a'\": 22, 'ib': 235, 'bj': 59, 'je': 264, 'ec': 129, 'bk': 60, 'kh': 288, 'bl': 61, 'st': 492, 'ly': 330, 'bn': 63, \"o'\": 386, 'oa': 387, 'hi': 216, 'sm': 485, 'oo': 401, 'ri': 454, 'ig': 240, 'gi': 189, 'iv': 255, 've': 556, 'uh': 533, 'im': 246, 'un': 539, 'ds': 119, 'ut': 545, \"t'\": 499, 'ov': 408, \"e'\": 126, 'eb': 128, 'ms': 351, 'mc': 335, 'cz': 99, 'zy': 670, 'yk': 631, 'tz': 524, 'wi': 581, 'ea': 127, 'eg': 133, 'go': 195, 'dg': 107, 'ge': 185, 'og': 393, 'ga': 181, 'up': 541, 'pt': 432, 'tl': 511, 'tn': 513, 'ss': 491, 'yn': 634, 'zo': 661, 'sa': 473, 'lu': 326, 'lv': 327, 'rb': 447, 'ta': 500, 'tr': 516, 'su': 493, 'bt': 68, 'bu': 69, 'hm': 220, 'dz': 125, 'ys': 639, 'bz': 73, 'zu': 667, 'ug': 532, 'ci': 83, 'my': 356, 'mp': 348, 'po': 428, 'ap': 38, 'pu': 433, 'lc': 308, 'cc': 77, 'tu': 519, 'ua': 526, 'ep': 142, 'ry': 470, 'cl': 86, 'mm': 345, 'pa': 414, 'ny': 383, 'yi': 629, 'pl': 425, 'cy': 98, \"s'\": 472, 'cr': 92, 'ue': 530, 'ui': 534, 'um': 538, 'mu': 353, 'ay': 47, 'gl': 192, 'op': 402, 'ph': 421, 'yl': 632, 'nb': 360, 'ht': 227, 'if': 239, 'fi': 162, 'fy': 178, 'dl': 112, \"r'\": 445, 'nn': 372, 'kl': 292, 'kn': 294, 'wl': 583, 'gm': 193, 'kr': 297, 'oy': 411, 'yd': 624, \"d'\": 100, 'cm': 87, 'cn': 88, 'of': 392, 'ff': 159, 'yt': 640, 'cs': 93, 'cq': 91, 'av': 44, 'ag': 29, 'ob': 388, 'ym': 633, 'uf': 531, 'sy': 497, 'yc': 623, \"m'\": 332, 'ik': 244, 'mk': 343, 'ku': 300, 'sk': 483, 'wn': 585, 'za': 648, 'dc': 103, 'oc': 389, 'dd': 104, 'eo': 141, 'ks': 298, 'dw': 123, 'we': 577, 'dy': 124, 'lb': 307, \"l'\": 305, 'lm': 318, 'lp': 321, 'sb': 474, 'ei': 135, 'oi': 395, 'eq': 143, 'rh': 453, 'ld': 309, 'lt': 325, 'dh': 108, 'ip': 249, 'dj': 110, 'ja': 261, 'jo': 273, 'ju': 276, 'dk': 111, 'dm': 113, 'lf': 311, \"f'\": 153, 'hs': 226, 'gn': 194, 'ft': 173, 'ae': 27, 'sd': 476, 'vo': 565, 'fl': 165, 'xi': 605, \"h'\": 207, 'sp': 488, \"p'\": 413, 'af': 28, 'fa': 154, 'ye': 625, 'fe': 158, 'ix': 257, 'xe': 601, 'fr': 171, 'ax': 46, 'fg': 160, 'gh': 188, \"i'\": 233, 'fh': 161, 'fm': 166, 'ef': 132, 'fu': 174, 'mn': 346, 'gy': 205, 'pe': 418, 'gf': 186, 'gg': 187, 'rw': 468, 'gr': 198, 'eh': 134, 'iu': 254, 'ew': 149, 'aw': 45, 'xc': 599, 'pp': 429, 'ok': 397, 'ko': 295, 'gu': 201, 'yo': 635, 'hh': 215, 'hl': 219, 'lg': 312, 'lq': 322, 'hn': 221, 'hr': 225, 'hu': 228, 'km': 293, 'ml': 344, 'sl': 484, 'nl': 370, 'sw': 495, 'wo': 586, \"c'\": 74, 'gs': 199, 'ps': 431, 'nk': 369, 'rp': 461, 'tc': 502, 'tk': 510, 'iw': 256, 'aj': 32, \"j'\": 260, 'ji': 267, 'kb': 282, 'kc': 283, 'ih': 241, 'mb': 334, 'kk': 291, 'ky': 303, 'kz': 304, 'tv': 520, 'uq': 542, 'rq': 462, 'oh': 394, 'wy': 594, 'ej': 136, 'eu': 147, \"x'\": 596, 'xa': 597, 'xy': 619, 'nq': 375, 'lh': 313, 'nz': 384, 'ij': 243, 'iq': 250, 'nh': 366, 'nw': 381, 'yw': 643, 'ln': 319, 'tm': 512, 'ws': 589, 'lr': 323, 'uv': 547, 'lw': 328, 'ez': 152, 'mg': 339, 'mq': 349, 'oe': 391, 'fn': 167, 'nu': 379, 'pi': 422, \"k'\": 280, 'tf': 505, 'zh': 655, 'lz': 331, 'np': 374, 'xt': 614, 'md': 336, 'zc': 650, 'zq': 663, 'mf': 338, 'mh': 340, 'yv': 642, \"g'\": 180, 'pc': 416, 'mr': 350, 'mt': 352, 'mv': 354, 'mw': 355, 'yz': 646, 'sq': 489, 'nv': 380, 'oz': 412, 'rz': 471, 'uj': 535, 'cd': 78, 'nf': 364, 'nr': 376, 'sg': 479, 'pr': 430, 'ox': 410, 'ub': 527, 'bm': 62, \"u'\": 525, 'nx': 382, 'yb': 622, 'yh': 628, 'yp': 636, 'wh': 580, 'ao': 37, 'pf': 419, 'pg': 420, 'pn': 427, 'pk': 424, 'aq': 39, 'qa': 438, 'rv': 467, 'ux': 549, 'hb': 209, 'hd': 211, 'uk': 536, 'yr': 638, 'zm': 659, 'rj': 455, \"w'\": 572, 'wr': 588, 'rx': 469, 'zt': 666, 'ii': 242, 'hc': 210, 'hf': 213, 'hv': 229, 'hw': 230, \"v'\": 552, 'sn': 486, 'fd': 157, \"'h\": 10, 'uy': 550, 'vc': 554, 'vd': 555, 'vm': 563, 'vn': 564, 'vr': 566, 'vt': 568, 'wb': 574, 'wf': 578, 'wk': 582, 'kw': 302, 'wt': 590, 'xf': 602, 'xl': 606, 'xo': 609, 'xs': 613, 'yg': 627, 'yu': 641, 'yy': 645, 'zb': 649, 'zp': 662, \"b'\": 49, \"'r\": 18, \"'a\": 4, 'hk': 218, 'kd': 284, 'kf': 286, 'kg': 287, 'kp': 296, 'kt': 299, 'tj': 509, 'gb': 182, 'gd': 184, 'gp': 196, 'gw': 203, \"'i\": 11, 'sf': 478, 'zs': 665, 'sz': 498, 'gk': 191, 'nj': 368, 'kv': 301, 'xq': 611, 'fs': 172, 'sv': 494, 'vs': 567, 'wm': 584, 'tb': 501, 'sr': 490, 'td': 503, 'uw': 548, 'wd': 576, 'zl': 658, 'cv': 96, 'db': 102, 'df': 106, 'dp': 116, 'vu': 569, 'zr': 664, 'jy': 278, \"z'\": 647, 'wu': 591, 'gq': 197, 'hp': 223, 'vy': 571, 'zd': 651, 'zn': 660, 'lj': 315, 'fb': 155, 'xu': 615, 'xb': 598, 'kj': 290, 'zk': 657, 'xh': 604, 'tg': 506, 'sj': 482, 'oj': 396, 'gj': 190, 'oq': 403, 'wc': 575, 'xw': 617, 'xx': 618, 'yf': 626, 'jd': 263, 'tp': 515, 'fc': 156, 'fk': 164, 'jn': 272, 'py': 435, 'uu': 546, 'zw': 669, 'yx': 644, 'pb': 415, 'pw': 434, \"q'\": 437, \"'v\": 21, 'jk': 269, 'gc': 183, 'pd': 417, 'pm': 426, 'gx': 204, 'iy': 258, 'hg': 214, 'bw': 71, 'wg': 579, 'wp': 587, 'zf': 653, 'vl': 562, 'cw': 97, \"'o\": 16, 'mj': 342, 'vv': 570, 'xv': 616, 'bf': 55, 'hq': 224, 'dq': 117, 'vg': 557, 'lx': 329, \"'d\": 7, 'vj': 560, 'xp': 610, 'wv': 592, 'jv': 277, 'zg': 654, 'fj': 163, 'xm': 607, 'xn': 608, 'jt': 275, 'xg': 603, 'tx': 522, 'vh': 558, 'mz': 357, 'fp': 169, 'gv': 202, 'jj': 268, 'fw': 176, 'jl': 270, 'hj': 217, 'bg': 56, 'wz': 595, 'i': 232, \"'l\": 13, 'qb': 439, 'qg': 440, 'qi': 441, 'jf': 265, 'jh': 266, 'js': 274, 'jc': 262, 'bv': 70, 'pz': 436, 'fq': 170, \"'b\": 5, 'cb': 76, 'cf': 80, 'cg': 81, 'cp': 90, 'cj': 84, 'zv': 668, 'fx': 177, 'jz': 279, 'fz': 179, 'qv': 444, 'ww': 593, 'xr': 612, 'xd': 600, 'o': 385, \"'g\": 9, \"'k\": 12, 'vk': 561, 'qo': 442, 'jm': 271, 'yj': 630, 'pj': 423, 'fv': 175, 'bp': 65, 'sx': 496, 'yq': 637, 'gz': 206}\n",
      "668 {\"'c\": 6, 'ca': 75, 'au': 43, 'us': 544, 'se': 477, 'co': 89, 'ou': 407, 'ur': 543, 'rs': 464, \"'e\": 8, 'em': 139, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 443, 'uo': 540, 'ot': 406, 'te': 504, \"'t\": 20, 'ti': 508, 'il': 245, 'is': 252, 'tw': 521, 'wa': 573, 'as': 41, 'aa': 23, 'ab': 24, 'be': 54, 'er': 144, 'rg': 452, 'ac': 25, 'ch': 82, 'he': 212, 'en': 140, 'ak': 33, 'ke': 285, 'al': 34, 'ls': 324, 'et': 146, 'th': 507, 'am': 35, 'mo': 347, 'od': 390, 'dt': 120, 'ar': 40, 'rd': 449, 'de': 105, 'ma': 333, 'dv': 122, 'va': 553, 'rk': 456, 'ro': 460, 'on': 400, \"n'\": 358, \"'s\": 19, 'ns': 377, 'so': 487, 'rt': 465, 'ba': 50, 'ha': 208, 'ck': 85, 'cu': 95, 'ad': 26, 'da': 101, 'ka': 281, 'di': 109, 'ie': 238, 'ai': 31, 'ir': 251, 'lk': 316, 'ki': 289, 'in': 247, 'lo': 320, 'os': 405, 'an': 36, 'nd': 362, 'do': 115, 'ne': 363, 'ed': 130, 'ni': 367, 'ng': 365, 'nm': 371, 'me': 337, 'nt': 378, 'to': 514, 'rc': 448, 're': 450, 'sc': 475, 'sh': 480, 'at': 42, 'ts': 517, 'es': 145, 'bb': 51, 'si': 481, 'el': 138, 'll': 317, 'tt': 518, 'ev': 148, 'vi': 559, 'le': 310, 'ey': 151, \"y'\": 620, 'bi': 58, 'it': 253, 'bo': 64, 'ud': 529, 'br': 66, 'ia': 234, 'io': 248, 'ru': 466, 'uz': 551, 'zz': 671, 'ze': 652, 'bs': 67, 'by': 72, 'bc': 52, 'ek': 137, 'bd': 53, 'la': 306, 'ah': 30, 'ic': 236, 'dn': 114, 'no': 373, 'or': 404, 'om': 399, 'mi': 341, 'na': 359, 'du': 121, 'uc': 528, 'ct': 94, 'ee': 131, 'ul': 537, 'az': 48, 'zi': 656, 'iz': 259, 'ow': 409, 'dr': 118, 'rf': 451, 'fo': 168, 'rl': 457, 'rm': 458, 'rn': 459, 'hy': 231, 'rr': 463, 'ra': 446, 'ex': 150, 'ya': 621, 'nc': 361, 'ce': 79, 'bh': 57, 'ho': 222, 'id': 237, 'li': 314, 'ty': 523, 'gt': 200, 'ol': 398, \"a'\": 22, 'ib': 235, 'bj': 59, 'je': 264, 'ec': 129, 'bk': 60, 'kh': 288, 'bl': 61, 'st': 492, 'ly': 330, 'bn': 63, \"o'\": 386, 'oa': 387, 'hi': 216, 'sm': 485, 'oo': 401, 'ri': 454, 'ig': 240, 'gi': 189, 'iv': 255, 've': 556, 'uh': 533, 'im': 246, 'un': 539, 'ds': 119, 'ut': 545, \"t'\": 499, 'ov': 408, \"e'\": 126, 'eb': 128, 'ms': 351, 'mc': 335, 'cz': 99, 'zy': 670, 'yk': 631, 'tz': 524, 'wi': 581, 'ea': 127, 'eg': 133, 'go': 195, 'dg': 107, 'ge': 185, 'og': 393, 'ga': 181, 'up': 541, 'pt': 432, 'tl': 511, 'tn': 513, 'ss': 491, 'yn': 634, 'zo': 661, 'sa': 473, 'lu': 326, 'lv': 327, 'rb': 447, 'ta': 500, 'tr': 516, 'su': 493, 'bt': 68, 'bu': 69, 'hm': 220, 'dz': 125, 'ys': 639, 'bz': 73, 'zu': 667, 'ug': 532, 'ci': 83, 'my': 356, 'mp': 348, 'po': 428, 'ap': 38, 'pu': 433, 'lc': 308, 'cc': 77, 'tu': 519, 'ua': 526, 'ep': 142, 'ry': 470, 'cl': 86, 'mm': 345, 'pa': 414, 'ny': 383, 'yi': 629, 'pl': 425, 'cy': 98, \"s'\": 472, 'cr': 92, 'ue': 530, 'ui': 534, 'um': 538, 'mu': 353, 'ay': 47, 'gl': 192, 'op': 402, 'ph': 421, 'yl': 632, 'nb': 360, 'ht': 227, 'if': 239, 'fi': 162, 'fy': 178, 'dl': 112, \"r'\": 445, 'nn': 372, 'kl': 292, 'kn': 294, 'wl': 583, 'gm': 193, 'kr': 297, 'oy': 411, 'yd': 624, \"d'\": 100, 'cm': 87, 'cn': 88, 'of': 392, 'ff': 159, 'yt': 640, 'cs': 93, 'cq': 91, 'av': 44, 'ag': 29, 'ob': 388, 'ym': 633, 'uf': 531, 'sy': 497, 'yc': 623, \"m'\": 332, 'ik': 244, 'mk': 343, 'ku': 300, 'sk': 483, 'wn': 585, 'za': 648, 'dc': 103, 'oc': 389, 'dd': 104, 'eo': 141, 'ks': 298, 'dw': 123, 'we': 577, 'dy': 124, 'lb': 307, \"l'\": 305, 'lm': 318, 'lp': 321, 'sb': 474, 'ei': 135, 'oi': 395, 'eq': 143, 'rh': 453, 'ld': 309, 'lt': 325, 'dh': 108, 'ip': 249, 'dj': 110, 'ja': 261, 'jo': 273, 'ju': 276, 'dk': 111, 'dm': 113, 'lf': 311, \"f'\": 153, 'hs': 226, 'gn': 194, 'ft': 173, 'ae': 27, 'sd': 476, 'vo': 565, 'fl': 165, 'xi': 605, \"h'\": 207, 'sp': 488, \"p'\": 413, 'af': 28, 'fa': 154, 'ye': 625, 'fe': 158, 'ix': 257, 'xe': 601, 'fr': 171, 'ax': 46, 'fg': 160, 'gh': 188, \"i'\": 233, 'fh': 161, 'fm': 166, 'ef': 132, 'fu': 174, 'mn': 346, 'gy': 205, 'pe': 418, 'gf': 186, 'gg': 187, 'rw': 468, 'gr': 198, 'eh': 134, 'iu': 254, 'ew': 149, 'aw': 45, 'xc': 599, 'pp': 429, 'ok': 397, 'ko': 295, 'gu': 201, 'yo': 635, 'hh': 215, 'hl': 219, 'lg': 312, 'lq': 322, 'hn': 221, 'hr': 225, 'hu': 228, 'km': 293, 'ml': 344, 'sl': 484, 'nl': 370, 'sw': 495, 'wo': 586, \"c'\": 74, 'gs': 199, 'ps': 431, 'nk': 369, 'rp': 461, 'tc': 502, 'tk': 510, 'iw': 256, 'aj': 32, \"j'\": 260, 'ji': 267, 'kb': 282, 'kc': 283, 'ih': 241, 'mb': 334, 'kk': 291, 'ky': 303, 'kz': 304, 'tv': 520, 'uq': 542, 'rq': 462, 'oh': 394, 'wy': 594, 'ej': 136, 'eu': 147, \"x'\": 596, 'xa': 597, 'xy': 619, 'nq': 375, 'lh': 313, 'nz': 384, 'ij': 243, 'iq': 250, 'nh': 366, 'nw': 381, 'yw': 643, 'ln': 319, 'tm': 512, 'ws': 589, 'lr': 323, 'uv': 547, 'lw': 328, 'ez': 152, 'mg': 339, 'mq': 349, 'oe': 391, 'fn': 167, 'nu': 379, 'pi': 422, \"k'\": 280, 'tf': 505, 'zh': 655, 'lz': 331, 'np': 374, 'xt': 614, 'md': 336, 'zc': 650, 'zq': 663, 'mf': 338, 'mh': 340, 'yv': 642, \"g'\": 180, 'pc': 416, 'mr': 350, 'mt': 352, 'mv': 354, 'mw': 355, 'yz': 646, 'sq': 489, 'nv': 380, 'oz': 412, 'rz': 471, 'uj': 535, 'cd': 78, 'nf': 364, 'nr': 376, 'sg': 479, 'pr': 430, 'ox': 410, 'ub': 527, 'bm': 62, \"u'\": 525, 'nx': 382, 'yb': 622, 'yh': 628, 'yp': 636, 'wh': 580, 'ao': 37, 'pf': 419, 'pg': 420, 'pn': 427, 'pk': 424, 'aq': 39, 'qa': 438, 'rv': 467, 'ux': 549, 'hb': 209, 'hd': 211, 'uk': 536, 'yr': 638, 'zm': 659, 'rj': 455, \"w'\": 572, 'wr': 588, 'rx': 469, 'zt': 666, 'ii': 242, 'hc': 210, 'hf': 213, 'hv': 229, 'hw': 230, \"v'\": 552, 'sn': 486, 'fd': 157, \"'h\": 10, 'uy': 550, 'vc': 554, 'vd': 555, 'vm': 563, 'vn': 564, 'vr': 566, 'vt': 568, 'wb': 574, 'wf': 578, 'wk': 582, 'kw': 302, 'wt': 590, 'xf': 602, 'xl': 606, 'xo': 609, 'xs': 613, 'yg': 627, 'yu': 641, 'yy': 645, 'zb': 649, 'zp': 662, \"b'\": 49, \"'r\": 18, \"'a\": 4, 'hk': 218, 'kd': 284, 'kf': 286, 'kg': 287, 'kp': 296, 'kt': 299, 'tj': 509, 'gb': 182, 'gd': 184, 'gp': 196, 'gw': 203, \"'i\": 11, 'sf': 478, 'zs': 665, 'sz': 498, 'gk': 191, 'nj': 368, 'kv': 301, 'xq': 611, 'fs': 172, 'sv': 494, 'vs': 567, 'wm': 584, 'tb': 501, 'sr': 490, 'td': 503, 'uw': 548, 'wd': 576, 'zl': 658, 'cv': 96, 'db': 102, 'df': 106, 'dp': 116, 'vu': 569, 'zr': 664, 'jy': 278, \"z'\": 647, 'wu': 591, 'gq': 197, 'hp': 223, 'vy': 571, 'zd': 651, 'zn': 660, 'lj': 315, 'fb': 155, 'xu': 615, 'xb': 598, 'kj': 290, 'zk': 657, 'xh': 604, 'tg': 506, 'sj': 482, 'oj': 396, 'gj': 190, 'oq': 403, 'wc': 575, 'xw': 617, 'xx': 618, 'yf': 626, 'jd': 263, 'tp': 515, 'fc': 156, 'fk': 164, 'jn': 272, 'py': 435, 'uu': 546, 'zw': 669, 'yx': 644, 'pb': 415, 'pw': 434, \"q'\": 437, \"'v\": 21, 'jk': 269, 'gc': 183, 'pd': 417, 'pm': 426, 'gx': 204, 'iy': 258, 'hg': 214, 'bw': 71, 'wg': 579, 'wp': 587, 'zf': 653, 'vl': 562, 'cw': 97, \"'o\": 16, 'mj': 342, 'vv': 570, 'xv': 616, 'bf': 55, 'hq': 224, 'dq': 117, 'vg': 557, 'lx': 329, \"'d\": 7, 'vj': 560, 'xp': 610, 'wv': 592, 'jv': 277, 'zg': 654, 'fj': 163, 'xm': 607, 'xn': 608, 'jt': 275, 'xg': 603, 'tx': 522, 'vh': 558, 'mz': 357, 'fp': 169, 'gv': 202, 'jj': 268, 'fw': 176, 'jl': 270, 'hj': 217, 'bg': 56, 'wz': 595, 'i': 232, \"'l\": 13, 'qb': 439, 'qg': 440, 'qi': 441, 'jf': 265, 'jh': 266, 'js': 274, 'jc': 262, 'bv': 70, 'pz': 436, 'fq': 170, \"'b\": 5, 'cb': 76, 'cf': 80, 'cg': 81, 'cp': 90, 'cj': 84, 'zv': 668, 'fx': 177, 'jz': 279, 'fz': 179, 'qv': 444, 'ww': 593, 'xr': 612, 'xd': 600, 'o': 385, \"'g\": 9, \"'k\": 12, 'vk': 561, 'qo': 442, 'jm': 271, 'yj': 630, 'pj': 423, 'fv': 175, 'bp': 65, 'sx': 496, 'yq': 637, 'gz': 206}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'L': 19, 'AA': 3, 'B': 8, 'G': 14, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'AW': 5, 'UW': 30, 'F': 13, 'HH': 15, 'AY': 7, 'JH': 17, 'Y': 33, 'CH': 9, 'P': 24, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'L': 19, 'AA': 3, 'B': 8, 'G': 14, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'AW': 5, 'UW': 30, 'F': 13, 'HH': 15, 'AY': 7, 'JH': 17, 'Y': 33, 'CH': 9, 'P': 24, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'L': 19, 'AA': 3, 'B': 8, 'G': 14, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'AW': 5, 'UW': 30, 'F': 13, 'HH': 15, 'AY': 7, 'JH': 17, 'Y': 33, 'CH': 9, 'P': 24, 'OY': 23}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "      )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 128\n",
      "hidden_size: 50\n",
      "n_layers: 1\n",
      "Encoder has a total number of 113016 parameters\n",
      "Decoder has a total number of 42515 parameters\n",
      "Total number of all parameters is 155531\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 1 finished in 0m 34.09s (- 56m 15.31s) (1 1.0%). train avg loss: 1.7454, val avg loss: 1.6166\n",
      "Training for epoch 2 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 2 finished in 1m 7.0s (- 54m 42.78s) (2 2.0%). train avg loss: 0.8519, val avg loss: 1.1419\n",
      "Training for epoch 3 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 3 finished in 1m 39.74s (- 53m 44.88s) (3 3.0%). train avg loss: 0.6041, val avg loss: 1.0353\n",
      "Training for epoch 4 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 4 finished in 2m 13.04s (- 53m 12.87s) (4 4.0%). train avg loss: 0.498, val avg loss: 0.964\n",
      "Training for epoch 5 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 5 finished in 2m 48.36s (- 53m 18.75s) (5 5.0%). train avg loss: 0.4413, val avg loss: 0.8732\n",
      "Training for epoch 6 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 6 finished in 3m 22.59s (- 52m 53.85s) (6 6.0%). train avg loss: 0.4131, val avg loss: 0.9135\n",
      "Training for epoch 7 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 7 finished in 4m 1.59s (- 53m 29.73s) (7 7.0%). train avg loss: 0.3859, val avg loss: 0.9356\n",
      "Training for epoch 8 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 8 finished in 4m 34.67s (- 52m 38.73s) (8 8.0%). train avg loss: 0.379, val avg loss: 0.8236\n",
      "Training for epoch 9 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 9 finished in 5m 6.41s (- 51m 38.11s) (9 9.0%). train avg loss: 0.3549, val avg loss: 0.8597\n",
      "Training for epoch 10 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 10 finished in 5m 38.63s (- 50m 47.67s) (10 10.0%). train avg loss: 0.3359, val avg loss: 0.8281\n",
      "Training for epoch 11 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 11 finished in 6m 10.01s (- 49m 53.7s) (11 11.0%). train avg loss: 0.329, val avg loss: 0.8294\n",
      "Training for epoch 12 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 12 finished in 6m 41.46s (- 49m 4.05s) (12 12.0%). train avg loss: 0.35, val avg loss: 0.8663\n",
      "Training for epoch 13 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 13 finished in 7m 15.05s (- 48m 31.48s) (13 13.0%). train avg loss: 0.3129, val avg loss: 0.7764\n",
      "Training for epoch 14 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 14 finished in 7m 48.13s (- 47m 55.64s) (14 14.0%). train avg loss: 0.3116, val avg loss: 0.7619\n",
      "Training for epoch 15 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 15 finished in 8m 21.77s (- 47m 23.36s) (15 15.0%). train avg loss: 0.3093, val avg loss: 0.7724\n",
      "Training for epoch 16 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 16 finished in 8m 55.12s (- 46m 49.35s) (16 16.0%). train avg loss: 0.3035, val avg loss: 0.7848\n",
      "Training for epoch 17 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 17 finished in 9m 27.44s (- 46m 10.44s) (17 17.0%). train avg loss: 0.2977, val avg loss: 0.7773\n",
      "Training for epoch 18 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 18 finished in 9m 59.51s (- 45m 31.11s) (18 18.0%). train avg loss: 0.2834, val avg loss: 0.7106\n",
      "Training for epoch 19 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 19 finished in 10m 33.21s (- 44m 59.45s) (19 19.0%). train avg loss: 0.3218, val avg loss: 0.8388\n",
      "Training for epoch 20 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 20 finished in 11m 6.71s (- 44m 26.85s) (20 20.0%). train avg loss: 0.288, val avg loss: 0.7209\n",
      "Training for epoch 21 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 21 finished in 11m 39.95s (- 43m 53.14s) (21 21.0%). train avg loss: 0.2681, val avg loss: 0.7413\n",
      "Training for epoch 22 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 22 finished in 12m 14.1s (- 43m 22.71s) (22 22.0%). train avg loss: 0.277, val avg loss: 0.7248\n",
      "Training for epoch 23 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 23 finished in 12m 48.1s (- 42m 51.46s) (23 23.0%). train avg loss: 0.2862, val avg loss: 0.7222\n",
      "Training for epoch 24 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 24 finished in 13m 20.55s (- 42m 15.07s) (24 24.0%). train avg loss: 0.2604, val avg loss: 0.6703\n",
      "Training for epoch 25 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 25 finished in 13m 53.22s (- 41m 39.66s) (25 25.0%). train avg loss: 0.2658, val avg loss: 0.6587\n",
      "Training for epoch 26 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 26 finished in 14m 26.43s (- 41m 5.99s) (26 26.0%). train avg loss: 0.2509, val avg loss: 0.7048\n",
      "Training for epoch 27 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 27 finished in 15m 5.23s (- 40m 47.49s) (27 27.0%). train avg loss: 0.2603, val avg loss: 0.6964\n",
      "Training for epoch 28 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 28 finished in 15m 39.54s (- 40m 15.96s) (28 28.0%). train avg loss: 0.253, val avg loss: 0.7034\n",
      "Training for epoch 29 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 29 finished in 16m 12.96s (- 39m 42.07s) (29 29.0%). train avg loss: 0.2435, val avg loss: 0.7069\n",
      "Training for epoch 30 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 30 finished in 16m 46.1s (- 39m 7.57s) (30 30.0%). train avg loss: 0.263, val avg loss: 0.6816\n",
      "Training for epoch 31 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 31 finished in 17m 19.07s (- 38m 32.77s) (31 31.0%). train avg loss: 0.2502, val avg loss: 0.7408\n",
      "Training for epoch 32 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 32 finished in 17m 52.39s (- 37m 58.83s) (32 32.0%). train avg loss: 0.2488, val avg loss: 0.6578\n",
      "Training for epoch 33 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 33 finished in 18m 25.98s (- 37m 25.48s) (33 33.0%). train avg loss: 0.242, val avg loss: 0.6583\n",
      "Training for epoch 34 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 34 finished in 19m 0.67s (- 36m 54.24s) (34 34.0%). train avg loss: 0.2384, val avg loss: 0.6626\n",
      "Training for epoch 35 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 35 finished in 19m 34.64s (- 36m 21.47s) (35 35.0%). train avg loss: 0.2613, val avg loss: 0.7223\n",
      "Training for epoch 36 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 36 finished in 20m 8.29s (- 35m 48.07s) (36 36.0%). train avg loss: 0.2291, val avg loss: 0.7031\n",
      "Training for epoch 37 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 37 finished in 20m 42.02s (- 35m 14.8s) (37 37.0%). train avg loss: 0.2263, val avg loss: 0.6756\n",
      "Training for epoch 38 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 38 finished in 21m 15.42s (- 34m 40.94s) (38 38.0%). train avg loss: 0.2348, val avg loss: 0.6322\n",
      "Training for epoch 39 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 39 finished in 21m 49.03s (- 34m 7.46s) (39 39.0%). train avg loss: 0.2293, val avg loss: 0.6569\n",
      "Training for epoch 40 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 40 finished in 22m 21.34s (- 33m 32.01s) (40 40.0%). train avg loss: 0.2267, val avg loss: 0.6459\n",
      "Training for epoch 41 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 41 finished in 22m 53.65s (- 32m 56.71s) (41 41.0%). train avg loss: 0.2249, val avg loss: 0.6492\n",
      "Training for epoch 42 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 42 finished in 23m 27.66s (- 32m 23.92s) (42 42.0%). train avg loss: 0.2183, val avg loss: 0.6266\n",
      "Training for epoch 43 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 43 finished in 24m 0.29s (- 31m 49.22s) (43 43.0%). train avg loss: 0.2229, val avg loss: 0.6328\n",
      "Training for epoch 44 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 44 finished in 24m 32.64s (- 31m 14.26s) (44 44.0%). train avg loss: 0.2335, val avg loss: 0.6591\n",
      "Training for epoch 45 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 45 finished in 25m 3.21s (- 30m 37.26s) (45 45.0%). train avg loss: 0.2073, val avg loss: 0.6481\n",
      "Training for epoch 46 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 46 finished in 25m 34.96s (- 30m 1.91s) (46 46.0%). train avg loss: 0.2139, val avg loss: 0.6557\n",
      "Training for epoch 47 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 47 finished in 26m 6.75s (- 29m 26.77s) (47 47.0%). train avg loss: 0.2087, val avg loss: 0.6522\n",
      "Training for epoch 48 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 48 finished in 26m 38.91s (- 28m 52.15s) (48 48.0%). train avg loss: 0.2397, val avg loss: 0.9018\n",
      "Training for epoch 49 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 49 finished in 27m 11.81s (- 28m 18.41s) (49 49.0%). train avg loss: 0.2233, val avg loss: 0.6462\n",
      "Training for epoch 50 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 50 finished in 27m 43.52s (- 27m 43.52s) (50 50.0%). train avg loss: 0.2114, val avg loss: 0.6531\n",
      "Training for epoch 51 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 51 finished in 28m 14.58s (- 27m 8.12s) (51 51.0%). train avg loss: 0.2164, val avg loss: 0.6149\n",
      "Training for epoch 52 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 52 finished in 28m 45.82s (- 26m 33.07s) (52 52.0%). train avg loss: 0.2191, val avg loss: 0.6667\n",
      "Training for epoch 53 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 53 finished in 29m 17.63s (- 25m 58.65s) (53 53.0%). train avg loss: 0.2181, val avg loss: 0.6843\n",
      "Training for epoch 54 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 54 finished in 29m 49.28s (- 25m 24.2s) (54 54.0%). train avg loss: 0.2058, val avg loss: 0.6046\n",
      "Training for epoch 55 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 55 finished in 30m 20.72s (- 24m 49.68s) (55 55.0%). train avg loss: 0.2085, val avg loss: 0.6104\n",
      "Training for epoch 56 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 56 finished in 30m 52.62s (- 24m 15.63s) (56 56.0%). train avg loss: 0.2164, val avg loss: 0.6159\n",
      "Training for epoch 57 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 57 finished in 31m 23.82s (- 23m 41.13s) (57 57.0%). train avg loss: 0.2076, val avg loss: 0.6369\n",
      "Training for epoch 58 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 58 finished in 31m 55.43s (- 23m 7.03s) (58 58.0%). train avg loss: 0.2053, val avg loss: 0.6205\n",
      "Training for epoch 59 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 59 finished in 32m 26.84s (- 22m 32.89s) (59 59.0%). train avg loss: 0.2129, val avg loss: 0.6189\n",
      "Training for epoch 60 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 60 finished in 32m 58.71s (- 21m 59.14s) (60 60.0%). train avg loss: 0.2085, val avg loss: 0.6104\n",
      "Training for epoch 61 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 61 finished in 33m 30.67s (- 21m 25.51s) (61 61.0%). train avg loss: 0.2044, val avg loss: 0.6567\n",
      "Training for epoch 62 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 62 finished in 34m 1.19s (- 20m 51.05s) (62 62.0%). train avg loss: 0.2035, val avg loss: 0.6041\n",
      "Training for epoch 63 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 63 finished in 34m 32.77s (- 20m 17.34s) (63 63.0%). train avg loss: 0.2031, val avg loss: 0.6467\n",
      "Training for epoch 64 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 64 finished in 35m 4.52s (- 19m 43.79s) (64 64.0%). train avg loss: 0.1997, val avg loss: 0.6362\n",
      "Training for epoch 65 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 65 finished in 35m 36.44s (- 19m 10.39s) (65 65.0%). train avg loss: 0.1995, val avg loss: 0.6023\n",
      "Training for epoch 66 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 66 finished in 36m 9.09s (- 18m 37.41s) (66 66.0%). train avg loss: 0.1955, val avg loss: 0.5799\n",
      "Training for epoch 67 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 67 finished in 36m 40.38s (- 18m 3.77s) (67 67.0%). train avg loss: 0.1957, val avg loss: 0.6132\n",
      "Training for epoch 68 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 68 finished in 37m 11.7s (- 17m 30.21s) (68 68.0%). train avg loss: 0.1965, val avg loss: 0.6267\n",
      "Training for epoch 69 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 69 finished in 37m 43.58s (- 16m 56.97s) (69 69.0%). train avg loss: 0.1943, val avg loss: 0.5981\n",
      "Training for epoch 70 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 70 finished in 38m 15.3s (- 16m 23.7s) (70 70.0%). train avg loss: 0.2015, val avg loss: 0.6799\n",
      "Training for epoch 71 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 71 finished in 38m 47.22s (- 15m 50.55s) (71 71.0%). train avg loss: 0.1926, val avg loss: 0.5999\n",
      "Training for epoch 72 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 72 finished in 39m 18.84s (- 15m 17.33s) (72 72.0%). train avg loss: 0.2049, val avg loss: 0.6052\n",
      "Training for epoch 73 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 73 finished in 39m 50.56s (- 14m 44.18s) (73 73.0%). train avg loss: 0.1991, val avg loss: 0.6061\n",
      "Training for epoch 74 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 74 finished in 40m 21.81s (- 14m 10.91s) (74 74.0%). train avg loss: 0.2082, val avg loss: 0.6029\n",
      "Training for epoch 75 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 75 finished in 40m 52.87s (- 13m 37.62s) (75 75.0%). train avg loss: 0.2057, val avg loss: 0.644\n",
      "Training for epoch 76 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 76 finished in 41m 28.57s (- 13m 5.86s) (76 76.0%). train avg loss: 0.203, val avg loss: 0.5915\n",
      "Training for epoch 77 has started (lr=0.001). Found 1580 batch(es).\n",
      "Epoch 77 finished in 42m 5.79s (- 12m 34.46s) (77 77.0%). train avg loss: 0.1854, val avg loss: 0.5974\n",
      "Training for epoch 78 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 78 finished in 42m 37.79s (- 12m 1.43s) (78 78.0%). train avg loss: 0.1731, val avg loss: 0.5653\n",
      "Training for epoch 79 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 79 finished in 43m 11.55s (- 11m 28.89s) (79 79.0%). train avg loss: 0.1687, val avg loss: 0.5808\n",
      "Training for epoch 80 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 80 finished in 43m 47.59s (- 10m 56.9s) (80 80.0%). train avg loss: 0.1646, val avg loss: 0.5804\n",
      "Training for epoch 81 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 81 finished in 44m 19.46s (- 10m 23.83s) (81 81.0%). train avg loss: 0.1704, val avg loss: 0.6066\n",
      "Training for epoch 82 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 82 finished in 44m 51.18s (- 9m 50.75s) (82 82.0%). train avg loss: 0.1674, val avg loss: 0.5925\n",
      "Training for epoch 83 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 83 finished in 45m 22.44s (- 9m 17.61s) (83 83.0%). train avg loss: 0.1667, val avg loss: 0.5783\n",
      "Training for epoch 84 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 84 finished in 45m 58.51s (- 8m 45.43s) (84 84.0%). train avg loss: 0.1625, val avg loss: 0.5667\n",
      "Training for epoch 85 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 85 finished in 46m 35.5s (- 8m 13.32s) (85 85.0%). train avg loss: 0.1604, val avg loss: 0.5784\n",
      "Training for epoch 86 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 86 finished in 47m 12.37s (- 7m 41.08s) (86 86.0%). train avg loss: 0.1632, val avg loss: 0.579\n",
      "Training for epoch 87 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 87 finished in 47m 51.17s (- 7m 9.03s) (87 87.0%). train avg loss: 0.1601, val avg loss: 0.5826\n",
      "Training for epoch 88 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 88 finished in 48m 25.18s (- 6m 36.16s) (88 88.0%). train avg loss: 0.1674, val avg loss: 0.5627\n",
      "Training for epoch 89 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 89 finished in 48m 56.67s (- 6m 2.96s) (89 89.0%). train avg loss: 0.1602, val avg loss: 0.5825\n",
      "Training for epoch 90 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 90 finished in 49m 31.14s (- 5m 30.13s) (90 90.0%). train avg loss: 0.1586, val avg loss: 0.6025\n",
      "Training for epoch 91 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 91 finished in 50m 8.36s (- 4m 57.53s) (91 91.0%). train avg loss: 0.1581, val avg loss: 0.5689\n",
      "Training for epoch 92 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 92 finished in 50m 39.31s (- 4m 24.29s) (92 92.0%). train avg loss: 0.1605, val avg loss: 0.5712\n",
      "Training for epoch 93 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 93 finished in 51m 11.04s (- 3m 51.15s) (93 93.0%). train avg loss: 0.1603, val avg loss: 0.5839\n",
      "Training for epoch 94 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 94 finished in 51m 42.02s (- 3m 18.0s) (94 94.0%). train avg loss: 0.1591, val avg loss: 0.5954\n",
      "Training for epoch 95 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 95 finished in 52m 13.13s (- 2m 44.9s) (95 95.0%). train avg loss: 0.1603, val avg loss: 0.5973\n",
      "Training for epoch 96 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 96 finished in 52m 44.39s (- 2m 11.85s) (96 96.0%). train avg loss: 0.1604, val avg loss: 0.5732\n",
      "Training for epoch 97 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 97 finished in 53m 16.08s (- 1m 38.85s) (97 97.0%). train avg loss: 0.1541, val avg loss: 0.5809\n",
      "Training for epoch 98 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 98 finished in 53m 48.17s (- 1m 5.88s) (98 98.0%). train avg loss: 0.1589, val avg loss: 0.5903\n",
      "Training for epoch 99 has started (lr=0.0005). Found 1580 batch(es).\n",
      "Epoch 99 finished in 54m 19.77s (- 0m 32.93s) (99 99.0%). train avg loss: 0.16, val avg loss: 0.5873\n",
      "Training for epoch 100 has started (lr=0.00025). Found 1580 batch(es).\n",
      "Epoch 100 finished in 54m 51.65s (- 0m 0.0s) (100 100.0%). train avg loss: 0.1501, val avg loss: 0.5456\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-7.5, min_value-.125, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 13.57576755209145%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> DISTASIO\n",
      "= ['D', 'IY', 'S', 'T', 'AA', 'S', 'IY', 'AO', 'W']\n",
      "< D IY S T AA S IY AO W ['D', 'IY', 'S', 'T', 'AA', 'S', 'IY', 'AO', 'W']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f744f5ffdf0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x571.429 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
