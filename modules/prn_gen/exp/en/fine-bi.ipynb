{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/exp/en\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"128\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"50\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"validation_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369, 166, 398, 229, 578, 394, 107, 277, 416, 665, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f44ff4504f0> ([6, 99, 67, 583, 514, 1], [18, 6, 34, 1])\n",
      "([6, 99, 67, 583, 514, 1], [18, 6, 34, 1])\n",
      "([6, 99, 67, 583, 514, 1], [18, 6, 34, 1])\n",
      "train grp 716 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: '-a', 23: '-b', 24: '-c', 25: '-d', 26: '-e', 27: '-f', 28: '-g', 29: '-h', 30: '-i', 31: '-k', 32: '-l', 33: '-m', 34: '-n', 35: '-o', 36: '-p', 37: '-q', 38: '-r', 39: '-s', 40: '-t', 41: '-u', 42: '-v', 43: '-w', 44: '-y', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'd-', 126: 'da', 127: 'db', 128: 'dc', 129: 'dd', 130: 'de', 131: 'df', 132: 'dg', 133: 'dh', 134: 'di', 135: 'dj', 136: 'dk', 137: 'dl', 138: 'dm', 139: 'dn', 140: 'do', 141: 'dp', 142: 'dq', 143: 'dr', 144: 'ds', 145: 'dt', 146: 'du', 147: 'dv', 148: 'dw', 149: 'dy', 150: 'dz', 151: \"e'\", 152: 'e-', 153: 'ea', 154: 'eb', 155: 'ec', 156: 'ed', 157: 'ee', 158: 'ef', 159: 'eg', 160: 'eh', 161: 'ei', 162: 'ej', 163: 'ek', 164: 'el', 165: 'em', 166: 'en', 167: 'eo', 168: 'ep', 169: 'eq', 170: 'er', 171: 'es', 172: 'et', 173: 'eu', 174: 'ev', 175: 'ew', 176: 'ex', 177: 'ey', 178: 'ez', 179: \"f'\", 180: 'f-', 181: 'fa', 182: 'fb', 183: 'fc', 184: 'fd', 185: 'fe', 186: 'ff', 187: 'fg', 188: 'fh', 189: 'fi', 190: 'fj', 191: 'fk', 192: 'fl', 193: 'fm', 194: 'fn', 195: 'fo', 196: 'fp', 197: 'fq', 198: 'fr', 199: 'fs', 200: 'ft', 201: 'fu', 202: 'fv', 203: 'fw', 204: 'fx', 205: 'fy', 206: 'fz', 207: \"g'\", 208: 'g-', 209: 'ga', 210: 'gb', 211: 'gc', 212: 'gd', 213: 'ge', 214: 'gf', 215: 'gg', 216: 'gh', 217: 'gi', 218: 'gj', 219: 'gk', 220: 'gl', 221: 'gm', 222: 'gn', 223: 'go', 224: 'gp', 225: 'gq', 226: 'gr', 227: 'gs', 228: 'gt', 229: 'gu', 230: 'gv', 231: 'gw', 232: 'gx', 233: 'gy', 234: 'gz', 235: \"h'\", 236: 'h-', 237: 'ha', 238: 'hb', 239: 'hc', 240: 'hd', 241: 'he', 242: 'hf', 243: 'hg', 244: 'hh', 245: 'hi', 246: 'hj', 247: 'hk', 248: 'hl', 249: 'hm', 250: 'hn', 251: 'ho', 252: 'hp', 253: 'hq', 254: 'hr', 255: 'hs', 256: 'ht', 257: 'hu', 258: 'hv', 259: 'hw', 260: 'hy', 261: 'i', 262: \"i'\", 263: 'i-', 264: 'ia', 265: 'ib', 266: 'ic', 267: 'id', 268: 'ie', 269: 'if', 270: 'ig', 271: 'ih', 272: 'ii', 273: 'ij', 274: 'ik', 275: 'il', 276: 'im', 277: 'in', 278: 'io', 279: 'ip', 280: 'iq', 281: 'ir', 282: 'is', 283: 'it', 284: 'iu', 285: 'iv', 286: 'iw', 287: 'ix', 288: 'iy', 289: 'iz', 290: \"j'\", 291: 'ja', 292: 'jc', 293: 'jd', 294: 'je', 295: 'jf', 296: 'jh', 297: 'ji', 298: 'jj', 299: 'jk', 300: 'jm', 301: 'jn', 302: 'jo', 303: 'jr', 304: 'js', 305: 'jt', 306: 'ju', 307: 'jv', 308: 'jy', 309: \"k'\", 310: 'k-', 311: 'ka', 312: 'kb', 313: 'kc', 314: 'kd', 315: 'ke', 316: 'kf', 317: 'kg', 318: 'kh', 319: 'ki', 320: 'kj', 321: 'kk', 322: 'kl', 323: 'km', 324: 'kn', 325: 'ko', 326: 'kp', 327: 'kr', 328: 'ks', 329: 'kt', 330: 'ku', 331: 'kv', 332: 'kw', 333: 'ky', 334: 'kz', 335: \"l'\", 336: 'l-', 337: 'la', 338: 'lb', 339: 'lc', 340: 'ld', 341: 'le', 342: 'lf', 343: 'lg', 344: 'lh', 345: 'li', 346: 'lj', 347: 'lk', 348: 'll', 349: 'lm', 350: 'ln', 351: 'lo', 352: 'lp', 353: 'lq', 354: 'lr', 355: 'ls', 356: 'lt', 357: 'lu', 358: 'lv', 359: 'lw', 360: 'lx', 361: 'ly', 362: 'lz', 363: \"m'\", 364: 'm-', 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'p-', 449: 'pa', 450: 'pb', 451: 'pc', 452: 'pd', 453: 'pe', 454: 'pf', 455: 'pg', 456: 'ph', 457: 'pi', 458: 'pj', 459: 'pk', 460: 'pl', 461: 'pm', 462: 'pn', 463: 'po', 464: 'pp', 465: 'pr', 466: 'ps', 467: 'pt', 468: 'pu', 469: 'pw', 470: 'py', 471: 'pz', 472: \"q'\", 473: 'qa', 474: 'qb', 475: 'qg', 476: 'qi', 477: 'qo', 478: 'qu', 479: 'qv', 480: \"r'\", 481: 'r-', 482: 'ra', 483: 'rb', 484: 'rc', 485: 'rd', 486: 're', 487: 'rf', 488: 'rg', 489: 'rh', 490: 'ri', 491: 'rj', 492: 'rk', 493: 'rl', 494: 'rm', 495: 'rn', 496: 'ro', 497: 'rp', 498: 'rq', 499: 'rr', 500: 'rs', 501: 'rt', 502: 'ru', 503: 'rv', 504: 'rw', 505: 'rx', 506: 'ry', 507: 'rz', 508: \"s'\", 509: 's-', 510: 'sa', 511: 'sb', 512: 'sc', 513: 'sd', 514: 'se', 515: 'sf', 516: 'sg', 517: 'sh', 518: 'si', 519: 'sj', 520: 'sk', 521: 'sl', 522: 'sm', 523: 'sn', 524: 'so', 525: 'sp', 526: 'sq', 527: 'sr', 528: 'ss', 529: 'st', 530: 'su', 531: 'sv', 532: 'sw', 533: 'sx', 534: 'sy', 535: 'sz', 536: \"t'\", 537: 't-', 538: 'ta', 539: 'tb', 540: 'tc', 541: 'td', 542: 'te', 543: 'tf', 544: 'tg', 545: 'th', 546: 'ti', 547: 'tj', 548: 'tk', 549: 'tl', 550: 'tm', 551: 'tn', 552: 'to', 553: 'tp', 554: 'tr', 555: 'ts', 556: 'tt', 557: 'tu', 558: 'tv', 559: 'tw', 560: 'tx', 561: 'ty', 562: 'tz', 563: \"u'\", 564: 'u-', 565: 'ua', 566: 'ub', 567: 'uc', 568: 'ud', 569: 'ue', 570: 'uf', 571: 'ug', 572: 'uh', 573: 'ui', 574: 'uj', 575: 'uk', 576: 'ul', 577: 'um', 578: 'un', 579: 'uo', 580: 'up', 581: 'uq', 582: 'ur', 583: 'us', 584: 'ut', 585: 'uu', 586: 'uv', 587: 'uw', 588: 'ux', 589: 'uy', 590: 'uz', 591: \"v'\", 592: 'va', 593: 'vc', 594: 'vd', 595: 've', 596: 'vg', 597: 'vh', 598: 'vi', 599: 'vj', 600: 'vk', 601: 'vl', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: 'vz', 611: \"w'\", 612: 'w-', 613: 'wa', 614: 'wb', 615: 'wc', 616: 'wd', 617: 'we', 618: 'wf', 619: 'wg', 620: 'wh', 621: 'wi', 622: 'wk', 623: 'wl', 624: 'wm', 625: 'wn', 626: 'wo', 627: 'wp', 628: 'wq', 629: 'wr', 630: 'ws', 631: 'wt', 632: 'wu', 633: 'wv', 634: 'ww', 635: 'wy', 636: 'wz', 637: \"x'\", 638: 'x-', 639: 'xa', 640: 'xb', 641: 'xc', 642: 'xd', 643: 'xe', 644: 'xf', 645: 'xg', 646: 'xh', 647: 'xi', 648: 'xl', 649: 'xm', 650: 'xn', 651: 'xo', 652: 'xp', 653: 'xq', 654: 'xr', 655: 'xs', 656: 'xt', 657: 'xu', 658: 'xv', 659: 'xw', 660: 'xx', 661: 'xy', 662: 'xz', 663: \"y'\", 664: 'y-', 665: 'ya', 666: 'yb', 667: 'yc', 668: 'yd', 669: 'ye', 670: 'yf', 671: 'yg', 672: 'yh', 673: 'yi', 674: 'yj', 675: 'yk', 676: 'yl', 677: 'ym', 678: 'yn', 679: 'yo', 680: 'yp', 681: 'yq', 682: 'yr', 683: 'ys', 684: 'yt', 685: 'yu', 686: 'yv', 687: 'yw', 688: 'yx', 689: 'yy', 690: 'yz', 691: \"z'\", 692: 'za', 693: 'zb', 694: 'zc', 695: 'zd', 696: 'ze', 697: 'zf', 698: 'zg', 699: 'zh', 700: 'zi', 701: 'zk', 702: 'zl', 703: 'zm', 704: 'zn', 705: 'zo', 706: 'zp', 707: 'zq', 708: 'zr', 709: 'zs', 710: 'zt', 711: 'zu', 712: 'zv', 713: 'zw', 714: 'zy', 715: 'zz'}\n",
      "valid grp 716 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: '-a', 23: '-b', 24: '-c', 25: '-d', 26: '-e', 27: '-f', 28: '-g', 29: '-h', 30: '-i', 31: '-k', 32: '-l', 33: '-m', 34: '-n', 35: '-o', 36: '-p', 37: '-q', 38: '-r', 39: '-s', 40: '-t', 41: '-u', 42: '-v', 43: '-w', 44: '-y', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'd-', 126: 'da', 127: 'db', 128: 'dc', 129: 'dd', 130: 'de', 131: 'df', 132: 'dg', 133: 'dh', 134: 'di', 135: 'dj', 136: 'dk', 137: 'dl', 138: 'dm', 139: 'dn', 140: 'do', 141: 'dp', 142: 'dq', 143: 'dr', 144: 'ds', 145: 'dt', 146: 'du', 147: 'dv', 148: 'dw', 149: 'dy', 150: 'dz', 151: \"e'\", 152: 'e-', 153: 'ea', 154: 'eb', 155: 'ec', 156: 'ed', 157: 'ee', 158: 'ef', 159: 'eg', 160: 'eh', 161: 'ei', 162: 'ej', 163: 'ek', 164: 'el', 165: 'em', 166: 'en', 167: 'eo', 168: 'ep', 169: 'eq', 170: 'er', 171: 'es', 172: 'et', 173: 'eu', 174: 'ev', 175: 'ew', 176: 'ex', 177: 'ey', 178: 'ez', 179: \"f'\", 180: 'f-', 181: 'fa', 182: 'fb', 183: 'fc', 184: 'fd', 185: 'fe', 186: 'ff', 187: 'fg', 188: 'fh', 189: 'fi', 190: 'fj', 191: 'fk', 192: 'fl', 193: 'fm', 194: 'fn', 195: 'fo', 196: 'fp', 197: 'fq', 198: 'fr', 199: 'fs', 200: 'ft', 201: 'fu', 202: 'fv', 203: 'fw', 204: 'fx', 205: 'fy', 206: 'fz', 207: \"g'\", 208: 'g-', 209: 'ga', 210: 'gb', 211: 'gc', 212: 'gd', 213: 'ge', 214: 'gf', 215: 'gg', 216: 'gh', 217: 'gi', 218: 'gj', 219: 'gk', 220: 'gl', 221: 'gm', 222: 'gn', 223: 'go', 224: 'gp', 225: 'gq', 226: 'gr', 227: 'gs', 228: 'gt', 229: 'gu', 230: 'gv', 231: 'gw', 232: 'gx', 233: 'gy', 234: 'gz', 235: \"h'\", 236: 'h-', 237: 'ha', 238: 'hb', 239: 'hc', 240: 'hd', 241: 'he', 242: 'hf', 243: 'hg', 244: 'hh', 245: 'hi', 246: 'hj', 247: 'hk', 248: 'hl', 249: 'hm', 250: 'hn', 251: 'ho', 252: 'hp', 253: 'hq', 254: 'hr', 255: 'hs', 256: 'ht', 257: 'hu', 258: 'hv', 259: 'hw', 260: 'hy', 261: 'i', 262: \"i'\", 263: 'i-', 264: 'ia', 265: 'ib', 266: 'ic', 267: 'id', 268: 'ie', 269: 'if', 270: 'ig', 271: 'ih', 272: 'ii', 273: 'ij', 274: 'ik', 275: 'il', 276: 'im', 277: 'in', 278: 'io', 279: 'ip', 280: 'iq', 281: 'ir', 282: 'is', 283: 'it', 284: 'iu', 285: 'iv', 286: 'iw', 287: 'ix', 288: 'iy', 289: 'iz', 290: \"j'\", 291: 'ja', 292: 'jc', 293: 'jd', 294: 'je', 295: 'jf', 296: 'jh', 297: 'ji', 298: 'jj', 299: 'jk', 300: 'jm', 301: 'jn', 302: 'jo', 303: 'jr', 304: 'js', 305: 'jt', 306: 'ju', 307: 'jv', 308: 'jy', 309: \"k'\", 310: 'k-', 311: 'ka', 312: 'kb', 313: 'kc', 314: 'kd', 315: 'ke', 316: 'kf', 317: 'kg', 318: 'kh', 319: 'ki', 320: 'kj', 321: 'kk', 322: 'kl', 323: 'km', 324: 'kn', 325: 'ko', 326: 'kp', 327: 'kr', 328: 'ks', 329: 'kt', 330: 'ku', 331: 'kv', 332: 'kw', 333: 'ky', 334: 'kz', 335: \"l'\", 336: 'l-', 337: 'la', 338: 'lb', 339: 'lc', 340: 'ld', 341: 'le', 342: 'lf', 343: 'lg', 344: 'lh', 345: 'li', 346: 'lj', 347: 'lk', 348: 'll', 349: 'lm', 350: 'ln', 351: 'lo', 352: 'lp', 353: 'lq', 354: 'lr', 355: 'ls', 356: 'lt', 357: 'lu', 358: 'lv', 359: 'lw', 360: 'lx', 361: 'ly', 362: 'lz', 363: \"m'\", 364: 'm-', 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'p-', 449: 'pa', 450: 'pb', 451: 'pc', 452: 'pd', 453: 'pe', 454: 'pf', 455: 'pg', 456: 'ph', 457: 'pi', 458: 'pj', 459: 'pk', 460: 'pl', 461: 'pm', 462: 'pn', 463: 'po', 464: 'pp', 465: 'pr', 466: 'ps', 467: 'pt', 468: 'pu', 469: 'pw', 470: 'py', 471: 'pz', 472: \"q'\", 473: 'qa', 474: 'qb', 475: 'qg', 476: 'qi', 477: 'qo', 478: 'qu', 479: 'qv', 480: \"r'\", 481: 'r-', 482: 'ra', 483: 'rb', 484: 'rc', 485: 'rd', 486: 're', 487: 'rf', 488: 'rg', 489: 'rh', 490: 'ri', 491: 'rj', 492: 'rk', 493: 'rl', 494: 'rm', 495: 'rn', 496: 'ro', 497: 'rp', 498: 'rq', 499: 'rr', 500: 'rs', 501: 'rt', 502: 'ru', 503: 'rv', 504: 'rw', 505: 'rx', 506: 'ry', 507: 'rz', 508: \"s'\", 509: 's-', 510: 'sa', 511: 'sb', 512: 'sc', 513: 'sd', 514: 'se', 515: 'sf', 516: 'sg', 517: 'sh', 518: 'si', 519: 'sj', 520: 'sk', 521: 'sl', 522: 'sm', 523: 'sn', 524: 'so', 525: 'sp', 526: 'sq', 527: 'sr', 528: 'ss', 529: 'st', 530: 'su', 531: 'sv', 532: 'sw', 533: 'sx', 534: 'sy', 535: 'sz', 536: \"t'\", 537: 't-', 538: 'ta', 539: 'tb', 540: 'tc', 541: 'td', 542: 'te', 543: 'tf', 544: 'tg', 545: 'th', 546: 'ti', 547: 'tj', 548: 'tk', 549: 'tl', 550: 'tm', 551: 'tn', 552: 'to', 553: 'tp', 554: 'tr', 555: 'ts', 556: 'tt', 557: 'tu', 558: 'tv', 559: 'tw', 560: 'tx', 561: 'ty', 562: 'tz', 563: \"u'\", 564: 'u-', 565: 'ua', 566: 'ub', 567: 'uc', 568: 'ud', 569: 'ue', 570: 'uf', 571: 'ug', 572: 'uh', 573: 'ui', 574: 'uj', 575: 'uk', 576: 'ul', 577: 'um', 578: 'un', 579: 'uo', 580: 'up', 581: 'uq', 582: 'ur', 583: 'us', 584: 'ut', 585: 'uu', 586: 'uv', 587: 'uw', 588: 'ux', 589: 'uy', 590: 'uz', 591: \"v'\", 592: 'va', 593: 'vc', 594: 'vd', 595: 've', 596: 'vg', 597: 'vh', 598: 'vi', 599: 'vj', 600: 'vk', 601: 'vl', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: 'vz', 611: \"w'\", 612: 'w-', 613: 'wa', 614: 'wb', 615: 'wc', 616: 'wd', 617: 'we', 618: 'wf', 619: 'wg', 620: 'wh', 621: 'wi', 622: 'wk', 623: 'wl', 624: 'wm', 625: 'wn', 626: 'wo', 627: 'wp', 628: 'wq', 629: 'wr', 630: 'ws', 631: 'wt', 632: 'wu', 633: 'wv', 634: 'ww', 635: 'wy', 636: 'wz', 637: \"x'\", 638: 'x-', 639: 'xa', 640: 'xb', 641: 'xc', 642: 'xd', 643: 'xe', 644: 'xf', 645: 'xg', 646: 'xh', 647: 'xi', 648: 'xl', 649: 'xm', 650: 'xn', 651: 'xo', 652: 'xp', 653: 'xq', 654: 'xr', 655: 'xs', 656: 'xt', 657: 'xu', 658: 'xv', 659: 'xw', 660: 'xx', 661: 'xy', 662: 'xz', 663: \"y'\", 664: 'y-', 665: 'ya', 666: 'yb', 667: 'yc', 668: 'yd', 669: 'ye', 670: 'yf', 671: 'yg', 672: 'yh', 673: 'yi', 674: 'yj', 675: 'yk', 676: 'yl', 677: 'ym', 678: 'yn', 679: 'yo', 680: 'yp', 681: 'yq', 682: 'yr', 683: 'ys', 684: 'yt', 685: 'yu', 686: 'yv', 687: 'yw', 688: 'yx', 689: 'yy', 690: 'yz', 691: \"z'\", 692: 'za', 693: 'zb', 694: 'zc', 695: 'zd', 696: 'ze', 697: 'zf', 698: 'zg', 699: 'zh', 700: 'zi', 701: 'zk', 702: 'zl', 703: 'zm', 704: 'zn', 705: 'zo', 706: 'zp', 707: 'zq', 708: 'zr', 709: 'zs', 710: 'zt', 711: 'zu', 712: 'zv', 713: 'zw', 714: 'zy', 715: 'zz'}\n",
      "test grp 716 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'k\", 13: \"'l\", 14: \"'m\", 15: \"'n\", 16: \"'o\", 17: \"'q\", 18: \"'r\", 19: \"'s\", 20: \"'t\", 21: \"'v\", 22: '-a', 23: '-b', 24: '-c', 25: '-d', 26: '-e', 27: '-f', 28: '-g', 29: '-h', 30: '-i', 31: '-k', 32: '-l', 33: '-m', 34: '-n', 35: '-o', 36: '-p', 37: '-q', 38: '-r', 39: '-s', 40: '-t', 41: '-u', 42: '-v', 43: '-w', 44: '-y', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'd-', 126: 'da', 127: 'db', 128: 'dc', 129: 'dd', 130: 'de', 131: 'df', 132: 'dg', 133: 'dh', 134: 'di', 135: 'dj', 136: 'dk', 137: 'dl', 138: 'dm', 139: 'dn', 140: 'do', 141: 'dp', 142: 'dq', 143: 'dr', 144: 'ds', 145: 'dt', 146: 'du', 147: 'dv', 148: 'dw', 149: 'dy', 150: 'dz', 151: \"e'\", 152: 'e-', 153: 'ea', 154: 'eb', 155: 'ec', 156: 'ed', 157: 'ee', 158: 'ef', 159: 'eg', 160: 'eh', 161: 'ei', 162: 'ej', 163: 'ek', 164: 'el', 165: 'em', 166: 'en', 167: 'eo', 168: 'ep', 169: 'eq', 170: 'er', 171: 'es', 172: 'et', 173: 'eu', 174: 'ev', 175: 'ew', 176: 'ex', 177: 'ey', 178: 'ez', 179: \"f'\", 180: 'f-', 181: 'fa', 182: 'fb', 183: 'fc', 184: 'fd', 185: 'fe', 186: 'ff', 187: 'fg', 188: 'fh', 189: 'fi', 190: 'fj', 191: 'fk', 192: 'fl', 193: 'fm', 194: 'fn', 195: 'fo', 196: 'fp', 197: 'fq', 198: 'fr', 199: 'fs', 200: 'ft', 201: 'fu', 202: 'fv', 203: 'fw', 204: 'fx', 205: 'fy', 206: 'fz', 207: \"g'\", 208: 'g-', 209: 'ga', 210: 'gb', 211: 'gc', 212: 'gd', 213: 'ge', 214: 'gf', 215: 'gg', 216: 'gh', 217: 'gi', 218: 'gj', 219: 'gk', 220: 'gl', 221: 'gm', 222: 'gn', 223: 'go', 224: 'gp', 225: 'gq', 226: 'gr', 227: 'gs', 228: 'gt', 229: 'gu', 230: 'gv', 231: 'gw', 232: 'gx', 233: 'gy', 234: 'gz', 235: \"h'\", 236: 'h-', 237: 'ha', 238: 'hb', 239: 'hc', 240: 'hd', 241: 'he', 242: 'hf', 243: 'hg', 244: 'hh', 245: 'hi', 246: 'hj', 247: 'hk', 248: 'hl', 249: 'hm', 250: 'hn', 251: 'ho', 252: 'hp', 253: 'hq', 254: 'hr', 255: 'hs', 256: 'ht', 257: 'hu', 258: 'hv', 259: 'hw', 260: 'hy', 261: 'i', 262: \"i'\", 263: 'i-', 264: 'ia', 265: 'ib', 266: 'ic', 267: 'id', 268: 'ie', 269: 'if', 270: 'ig', 271: 'ih', 272: 'ii', 273: 'ij', 274: 'ik', 275: 'il', 276: 'im', 277: 'in', 278: 'io', 279: 'ip', 280: 'iq', 281: 'ir', 282: 'is', 283: 'it', 284: 'iu', 285: 'iv', 286: 'iw', 287: 'ix', 288: 'iy', 289: 'iz', 290: \"j'\", 291: 'ja', 292: 'jc', 293: 'jd', 294: 'je', 295: 'jf', 296: 'jh', 297: 'ji', 298: 'jj', 299: 'jk', 300: 'jm', 301: 'jn', 302: 'jo', 303: 'jr', 304: 'js', 305: 'jt', 306: 'ju', 307: 'jv', 308: 'jy', 309: \"k'\", 310: 'k-', 311: 'ka', 312: 'kb', 313: 'kc', 314: 'kd', 315: 'ke', 316: 'kf', 317: 'kg', 318: 'kh', 319: 'ki', 320: 'kj', 321: 'kk', 322: 'kl', 323: 'km', 324: 'kn', 325: 'ko', 326: 'kp', 327: 'kr', 328: 'ks', 329: 'kt', 330: 'ku', 331: 'kv', 332: 'kw', 333: 'ky', 334: 'kz', 335: \"l'\", 336: 'l-', 337: 'la', 338: 'lb', 339: 'lc', 340: 'ld', 341: 'le', 342: 'lf', 343: 'lg', 344: 'lh', 345: 'li', 346: 'lj', 347: 'lk', 348: 'll', 349: 'lm', 350: 'ln', 351: 'lo', 352: 'lp', 353: 'lq', 354: 'lr', 355: 'ls', 356: 'lt', 357: 'lu', 358: 'lv', 359: 'lw', 360: 'lx', 361: 'ly', 362: 'lz', 363: \"m'\", 364: 'm-', 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'p-', 449: 'pa', 450: 'pb', 451: 'pc', 452: 'pd', 453: 'pe', 454: 'pf', 455: 'pg', 456: 'ph', 457: 'pi', 458: 'pj', 459: 'pk', 460: 'pl', 461: 'pm', 462: 'pn', 463: 'po', 464: 'pp', 465: 'pr', 466: 'ps', 467: 'pt', 468: 'pu', 469: 'pw', 470: 'py', 471: 'pz', 472: \"q'\", 473: 'qa', 474: 'qb', 475: 'qg', 476: 'qi', 477: 'qo', 478: 'qu', 479: 'qv', 480: \"r'\", 481: 'r-', 482: 'ra', 483: 'rb', 484: 'rc', 485: 'rd', 486: 're', 487: 'rf', 488: 'rg', 489: 'rh', 490: 'ri', 491: 'rj', 492: 'rk', 493: 'rl', 494: 'rm', 495: 'rn', 496: 'ro', 497: 'rp', 498: 'rq', 499: 'rr', 500: 'rs', 501: 'rt', 502: 'ru', 503: 'rv', 504: 'rw', 505: 'rx', 506: 'ry', 507: 'rz', 508: \"s'\", 509: 's-', 510: 'sa', 511: 'sb', 512: 'sc', 513: 'sd', 514: 'se', 515: 'sf', 516: 'sg', 517: 'sh', 518: 'si', 519: 'sj', 520: 'sk', 521: 'sl', 522: 'sm', 523: 'sn', 524: 'so', 525: 'sp', 526: 'sq', 527: 'sr', 528: 'ss', 529: 'st', 530: 'su', 531: 'sv', 532: 'sw', 533: 'sx', 534: 'sy', 535: 'sz', 536: \"t'\", 537: 't-', 538: 'ta', 539: 'tb', 540: 'tc', 541: 'td', 542: 'te', 543: 'tf', 544: 'tg', 545: 'th', 546: 'ti', 547: 'tj', 548: 'tk', 549: 'tl', 550: 'tm', 551: 'tn', 552: 'to', 553: 'tp', 554: 'tr', 555: 'ts', 556: 'tt', 557: 'tu', 558: 'tv', 559: 'tw', 560: 'tx', 561: 'ty', 562: 'tz', 563: \"u'\", 564: 'u-', 565: 'ua', 566: 'ub', 567: 'uc', 568: 'ud', 569: 'ue', 570: 'uf', 571: 'ug', 572: 'uh', 573: 'ui', 574: 'uj', 575: 'uk', 576: 'ul', 577: 'um', 578: 'un', 579: 'uo', 580: 'up', 581: 'uq', 582: 'ur', 583: 'us', 584: 'ut', 585: 'uu', 586: 'uv', 587: 'uw', 588: 'ux', 589: 'uy', 590: 'uz', 591: \"v'\", 592: 'va', 593: 'vc', 594: 'vd', 595: 've', 596: 'vg', 597: 'vh', 598: 'vi', 599: 'vj', 600: 'vk', 601: 'vl', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: 'vz', 611: \"w'\", 612: 'w-', 613: 'wa', 614: 'wb', 615: 'wc', 616: 'wd', 617: 'we', 618: 'wf', 619: 'wg', 620: 'wh', 621: 'wi', 622: 'wk', 623: 'wl', 624: 'wm', 625: 'wn', 626: 'wo', 627: 'wp', 628: 'wq', 629: 'wr', 630: 'ws', 631: 'wt', 632: 'wu', 633: 'wv', 634: 'ww', 635: 'wy', 636: 'wz', 637: \"x'\", 638: 'x-', 639: 'xa', 640: 'xb', 641: 'xc', 642: 'xd', 643: 'xe', 644: 'xf', 645: 'xg', 646: 'xh', 647: 'xi', 648: 'xl', 649: 'xm', 650: 'xn', 651: 'xo', 652: 'xp', 653: 'xq', 654: 'xr', 655: 'xs', 656: 'xt', 657: 'xu', 658: 'xv', 659: 'xw', 660: 'xx', 661: 'xy', 662: 'xz', 663: \"y'\", 664: 'y-', 665: 'ya', 666: 'yb', 667: 'yc', 668: 'yd', 669: 'ye', 670: 'yf', 671: 'yg', 672: 'yh', 673: 'yi', 674: 'yj', 675: 'yk', 676: 'yl', 677: 'ym', 678: 'yn', 679: 'yo', 680: 'yp', 681: 'yq', 682: 'yr', 683: 'ys', 684: 'yt', 685: 'yu', 686: 'yv', 687: 'yw', 688: 'yx', 689: 'yy', 690: 'yz', 691: \"z'\", 692: 'za', 693: 'zb', 694: 'zc', 695: 'zd', 696: 'ze', 697: 'zf', 698: 'zg', 699: 'zh', 700: 'zi', 701: 'zk', 702: 'zl', 703: 'zm', 704: 'zn', 705: 'zo', 706: 'zp', 707: 'zq', 708: 'zr', 709: 'zs', 710: 'zt', 711: 'zu', 712: 'zv', 713: 'zw', 714: 'zy', 715: 'zz'}\n",
      "train phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "valid phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "test phn 35 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'OY', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'SH', 29: 'T', 30: 'UW', 31: 'V', 32: 'W', 33: 'Y', 34: 'Z'}\n",
      "712 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 583, 'se': 514, 'co': 113, 'ou': 441, 'ur': 582, 'rs': 500, \"'e\": 8, 'em': 165, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 478, 'uo': 579, 'ot': 440, 'te': 542, \"'t\": 20, 'ti': 546, 'is': 282, 'aa': 47, 'ab': 48, 'be': 78, 'er': 170, 'rg': 488, 'ac': 49, 'ch': 106, 'he': 241, 'en': 166, 'ak': 57, 'ke': 315, 'al': 58, 'ls': 355, 'et': 172, 'th': 545, 'am': 59, 'mo': 379, 'od': 424, 'dt': 145, 'an': 60, 'nc': 394, 'or': 438, 'ar': 64, 'rd': 485, 'de': 130, 'ma': 365, 'dv': 147, 'va': 592, 'rk': 492, 'ro': 496, 'on': 434, \"n'\": 390, \"'s\": 19, 'ns': 410, 'so': 524, 'rt': 501, 'as': 65, 'ba': 74, 'ha': 237, 'ck': 109, 'cu': 119, 'ad': 50, 'da': 126, 'ka': 311, 'di': 134, 'ai': 55, 'ir': 281, 'lk': 347, 'ki': 319, 'in': 277, 'lo': 351, 'ne': 396, 'os': 439, 'nd': 395, 'do': 140, 'ni': 400, 'ng': 398, 'nm': 404, 'me': 369, 'nt': 411, 'ts': 555, 'to': 552, 'rc': 484, 're': 486, 'sc': 512, 'sh': 517, 'ed': 156, 'at': 66, 'es': 171, 'bb': 75, 'si': 518, 'ie': 268, 'el': 164, 'll': 348, 'nh': 399, 'tt': 556, 'ev': 174, 'vi': 598, 'il': 275, 'le': 341, 'ey': 177, \"y'\": 663, 'bi': 82, 'it': 283, 'bo': 88, \"t'\": 536, 'ud': 568, 'br': 90, 'ia': 264, 'io': 278, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 163, 'bd': 77, 'la': 337, 'ah': 54, 'ic': 266, 'dn': 139, 'no': 406, 'ol': 432, 'om': 433, 'mi': 373, 'na': 392, 'du': 146, 'uc': 567, 'ct': 118, 'ee': 157, 'ul': 576, 'az': 72, 'zi': 700, 'iz': 289, 'ln': 350, 'ow': 443, 'dr': 143, 'cr': 116, 'mb': 366, 'rl': 493, 'rm': 494, 'rr': 499, 'ra': 482, 'ya': 665, 'ce': 103, 'yt': 684, 'ta': 538, 'bh': 81, 'ho': 251, 'id': 267, 'ig': 270, 'ga': 209, 'li': 345, 'ty': 561, 'gt': 228, \"a'\": 45, 'ib': 265, 'tz': 562, 'bj': 83, 'je': 294, 'ec': 155, 'bk': 84, 'kh': 318, 'bl': 85, 'ze': 696, 'e-': 152, '-b': 23, 'st': 529, 'oo': 435, 'ly': 361, 'bn': 87, \"o'\": 419, 'oa': 421, 'hi': 245, 'sm': 522, 'ri': 490, 'gi': 217, 'rn': 495, 'if': 269, 'fa': 181, 'ci': 107, 'iv': 285, 've': 595, 'uh': 572, 'im': 276, 'un': 578, 'ds': 144, 'ut': 584, 'ov': 442, \"e'\": 151, 'bp': 89, 'pl': 460, 'lp': 352, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 714, 'yk': 675, 'wi': 621, 'ea': 153, 'eg': 159, 'go': 223, 'dg': 132, 'ge': 213, 'og': 427, 'ru': 502, 'up': 580, 'pt': 467, 'tl': 549, 'tn': 551, 'ss': 528, 'yn': 678, 'uz': 590, 'zz': 715, 'zo': 705, 'sa': 510, 'ei': 161, 'lu': 357, 'lv': 358, 'rb': 483, 'rp': 497, 'tr': 554, 'su': 530, 'dl': 137, 'bt': 92, 'bu': 93, 'hm': 249, 'dz': 150, 'ys': 683, 'cs': 117, 'my': 388, 'mp': 380, 'po': 463, 'ap': 62, 'pu': 468, 'lc': 339, 'cc': 101, 'tu': 557, 'ua': 565, 'ep': 168, 'ry': 506, 't-': 537, '-p': 36, 'pr': 465, 'cl': 110, 'mm': 377, 'pa': 449, 'ny': 416, 'yi': 673, \"r'\": 480, 'gl': 220, 'cy': 122, \"s'\": 508, 'ps': 466, 'ue': 569, 'ui': 573, 'um': 577, 'mu': 385, 'ay': 71, 'op': 436, 'ph': 456, 'yl': 676, 'eb': 154, 'nb': 393, 'ht': 256, 'hy': 260, 'fi': 189, 'fy': 205, 'kl': 322, 'km': 323, 'kn': 324, 'wl': 623, 'gm': 221, 'kr': 327, 'oy': 445, 'yd': 668, \"d'\": 124, 'cm': 111, 'cn': 112, 'oc': 423, 'of': 426, 'ff': 186, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 677, 'tm': 550, 'uf': 570, 'sy': 534, 'yc': 667, \"m'\": 363, 'ik': 274, 'za': 692, 'sk': 520, 'wn': 625, 'ex': 176, 'dc': 128, 'dd': 129, 'eo': 167, 'ks': 328, 'dw': 148, 'we': 617, 'dy': 149, 'lb': 338, \"l'\": 335, 'lm': 349, 'nn': 405, 'sb': 511, 'oi': 429, 'eq': 169, 'rh': 489, 'ld': 340, 'lt': 356, 'dh': 133, 'eu': 173, 'lf': 342, 'ip': 279, 'dj': 135, 'ja': 291, 'jo': 302, 'ju': 306, 'dk': 136, 'dm': 138, \"f'\": 179, 'fo': 195, 'hs': 255, '-f': 27, '-m': 33, '-s': 39, 'gn': 222, 'ae': 51, 'sd': 513, \"g'\": 207, 'vo': 603, 'ft': 200, 'oq': 437, \"h'\": 235, 'sp': 525, 'hl': 248, \"p'\": 447, 'af': 52, 'ye': 669, 'fe': 185, 'ix': 287, 'xe': 643, 'fl': 192, 'fr': 198, 'ax': 70, 'fg': 187, 'gh': 216, \"i'\": 262, 'fh': 188, 'ox': 444, 'xi': 647, 'fm': 193, 'fs': 199, 'ef': 158, 'ug': 571, 'rw': 504, 'wa': 613, 'mn': 378, 'gy': 233, 'pe': 453, 'gc': 211, 'gf': 214, 'gg': 215, 'gr': 226, 'iu': 284, 'ew': 175, 'aw': 69, 'xc': 641, 'fu': 201, 'pp': 464, 'ok': 431, 'ko': 325, 'gu': 229, 'hh': 244, 'lg': 343, 'lq': 353, 'hn': 250, 'hr': 254, 'hu': 257, 'ml': 376, 'sl': 521, 'sw': 532, 'wo': 626, \"c'\": 98, 'gs': 227, 'rf': 487, 'nk': 402, 'tc': 540, 'iw': 286, 'aj': 56, \"x'\": 637, 'ji': 297, 'kb': 312, 'kc': 313, 'iy': 288, 'kk': 321, 'ky': 333, 'kz': 334, 'tv': 558, 'uq': 581, 'rq': 498, 'oh': 428, 'eh': 160, 'ej': 162, 'xa': 639, 'xo': 651, 'xy': 661, 'nq': 408, 'lh': 344, 'nz': 417, 'ij': 273, 'iq': 280, 'l-': 336, '-i': 30, 'n-': 391, '-o': 35, 'nw': 414, 'yw': 687, 'nu': 412, 'lr': 354, 'pi': 457, 'uv': 586, 'lw': 359, 'ez': 178, 'mq': 381, 'fn': 194, \"k'\": 309, 'tf': 543, 'zh': 699, 'wy': 635, 'lz': 362, 'ku': 330, 'np': 407, 'xt': 656, 'md': 368, 'zc': 694, 'zq': 707, 'mf': 370, 'mg': 371, 'mh': 372, 'yv': 686, 'oe': 425, 'pc': 451, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yo': 679, 'yx': 688, 'yz': 690, 'sq': 526, 'nv': 413, 'oz': 446, 'ws': 630, 'uj': 574, 'cd': 102, 'nf': 397, 'nj': 401, 'ih': 271, 'nr': 409, 'sg': 516, 'i-': 263, 'ub': 566, 'bm': 86, 'tk': 548, \"u'\": 563, 'tw': 559, 'nx': 415, 'yb': 666, 'yh': 672, 'yp': 680, 'wh': 620, 'ao': 61, 'pf': 454, 'pg': 455, 'pk': 459, 'aq': 63, 'qa': 473, 'rv': 503, 'ux': 588, 'hb': 238, 'hd': 240, 'uk': 575, 'zm': 703, 'rj': 491, 'kw': 332, 'wr': 629, 'd-': 125, '-t': 40, '-c': 24, \"w'\": 611, 'zt': 710, 'zu': 711, 'rx': 505, 'rz': 507, 'ii': 272, 'hc': 239, 'hf': 242, 'hw': 259, \"v'\": 591, 'pn': 462, 'yr': 682, 'fd': 184, \"'h\": 10, '-l': 32, 'uy': 589, 'vd': 594, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 614, 'wf': 618, 'wk': 622, 'wt': 631, 'xf': 644, 'xl': 648, 'xs': 655, 'yg': 671, 'yu': 685, 'yy': 689, 'zb': 693, \"b'\": 73, \"'r\": 18, 'hk': 247, 'k-': 310, '-a': 22, 'kd': 314, 'kf': 316, 'kg': 317, 'kp': 326, 'kt': 329, 'y-': 664, 'tj': 547, 'gb': 210, 'gd': 212, 'gp': 224, 'gw': 231, \"'i\": 11, 'sz': 535, 'gk': 219, 'kv': 331, 'xq': 653, 'fk': 191, 'sv': 531, 'vs': 605, 'wm': 624, 'sn': 523, 'sf': 515, 'tb': 539, 'sr': 527, 'td': 541, 'hg': 243, 'uw': 587, 'wd': 616, 'zl': 702, 'cv': 120, 'db': 127, 'df': 131, 'dp': 141, 'vu': 607, 'zr': 708, 'nl': 403, 'jy': 308, \"z'\": 691, 'wu': 632, 'gq': 225, '-k': 31, 'hp': 252, 'vy': 609, 'zd': 695, 'zn': 704, 'lj': 346, 'fb': 182, 'xu': 657, 'xb': 640, 'kj': 320, 'mk': 375, '-r': 38, '-g': 28, '-v': 42, 'bz': 97, 'tg': 544, 'sj': 519, 'oj': 430, 'gj': 218, 'qi': 476, 'wc': 615, 'xw': 659, 'xx': 660, 'yf': 670, 'jd': 293, '-n': 34, 'zk': 701, 'tp': 553, 'fc': 183, '-d': 25, 'r-': 481, 'uu': 585, '-u': 41, 'py': 470, 'zw': 713, 'pb': 450, 'pw': 469, \"q'\": 472, \"'v\": 21, 'jk': 299, 'pd': 452, 'pm': 461, 'gx': 232, 'jn': 301, 'zp': 706, 'o-': 420, 'bw': 95, '-e': 26, 'wg': 619, 'zf': 697, 's-': 509, 'vl': 601, 'g-': 208, 'cw': 121, \"'a\": 4, \"'o\": 16, 'hv': 258, 'vc': 593, 'zs': 709, 'mj': 374, 'xh': 646, 'vv': 608, 'xv': 658, 'mz': 389, 'bf': 79, 'hq': 253, 'dq': 142, 'lx': 360, '-h': 29, \"'d\": 7, 'vj': 599, 'x-': 638, '-w': 43, 'xn': 650, 'xp': 652, 'jv': 307, 'zg': 698, '-y': 44, 'fj': 190, 'jt': 305, 'w-': 612, 'xg': 645, 'xm': 649, 'tx': 560, 'gz': 234, 'gv': 230, 'jj': 298, 'f-': 180, 'fw': 203, 'wv': 633, \"'l\": 13, 'h-': 236, 'hj': 246, 'fp': 196, 'js': 304, 'vh': 597, 'wz': 636, 'i': 261, 'qb': 474, 'qg': 475, 'zv': 712, 'jf': 295, 'jh': 296, 'jc': 292, 'wp': 627, 'bv': 94, 'pz': 471, 'fq': 197, 'vg': 596, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, \"j'\": 290, 'jr': 303, 'xz': 662, 'cj': 108, 'fx': 204, 'fz': 206, 'qv': 479, 'wq': 628, 'ww': 634, 'xr': 654, 'xd': 642, 'o': 418, \"'b\": 5, \"'g\": 9, \"'k\": 12, 'a-': 46, '-q': 37, 'vk': 600, 'qo': 477, 'vz': 610, 'jm': 300, 'yj': 674, 'pj': 458, 'p-': 448, 'fv': 202, 'bg': 80, 'u-': 564, 'sx': 533, 'm-': 364, 'yq': 681}\n",
      "712 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 583, 'se': 514, 'co': 113, 'ou': 441, 'ur': 582, 'rs': 500, \"'e\": 8, 'em': 165, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 478, 'uo': 579, 'ot': 440, 'te': 542, \"'t\": 20, 'ti': 546, 'is': 282, 'aa': 47, 'ab': 48, 'be': 78, 'er': 170, 'rg': 488, 'ac': 49, 'ch': 106, 'he': 241, 'en': 166, 'ak': 57, 'ke': 315, 'al': 58, 'ls': 355, 'et': 172, 'th': 545, 'am': 59, 'mo': 379, 'od': 424, 'dt': 145, 'an': 60, 'nc': 394, 'or': 438, 'ar': 64, 'rd': 485, 'de': 130, 'ma': 365, 'dv': 147, 'va': 592, 'rk': 492, 'ro': 496, 'on': 434, \"n'\": 390, \"'s\": 19, 'ns': 410, 'so': 524, 'rt': 501, 'as': 65, 'ba': 74, 'ha': 237, 'ck': 109, 'cu': 119, 'ad': 50, 'da': 126, 'ka': 311, 'di': 134, 'ai': 55, 'ir': 281, 'lk': 347, 'ki': 319, 'in': 277, 'lo': 351, 'ne': 396, 'os': 439, 'nd': 395, 'do': 140, 'ni': 400, 'ng': 398, 'nm': 404, 'me': 369, 'nt': 411, 'ts': 555, 'to': 552, 'rc': 484, 're': 486, 'sc': 512, 'sh': 517, 'ed': 156, 'at': 66, 'es': 171, 'bb': 75, 'si': 518, 'ie': 268, 'el': 164, 'll': 348, 'nh': 399, 'tt': 556, 'ev': 174, 'vi': 598, 'il': 275, 'le': 341, 'ey': 177, \"y'\": 663, 'bi': 82, 'it': 283, 'bo': 88, \"t'\": 536, 'ud': 568, 'br': 90, 'ia': 264, 'io': 278, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 163, 'bd': 77, 'la': 337, 'ah': 54, 'ic': 266, 'dn': 139, 'no': 406, 'ol': 432, 'om': 433, 'mi': 373, 'na': 392, 'du': 146, 'uc': 567, 'ct': 118, 'ee': 157, 'ul': 576, 'az': 72, 'zi': 700, 'iz': 289, 'ln': 350, 'ow': 443, 'dr': 143, 'cr': 116, 'mb': 366, 'rl': 493, 'rm': 494, 'rr': 499, 'ra': 482, 'ya': 665, 'ce': 103, 'yt': 684, 'ta': 538, 'bh': 81, 'ho': 251, 'id': 267, 'ig': 270, 'ga': 209, 'li': 345, 'ty': 561, 'gt': 228, \"a'\": 45, 'ib': 265, 'tz': 562, 'bj': 83, 'je': 294, 'ec': 155, 'bk': 84, 'kh': 318, 'bl': 85, 'ze': 696, 'e-': 152, '-b': 23, 'st': 529, 'oo': 435, 'ly': 361, 'bn': 87, \"o'\": 419, 'oa': 421, 'hi': 245, 'sm': 522, 'ri': 490, 'gi': 217, 'rn': 495, 'if': 269, 'fa': 181, 'ci': 107, 'iv': 285, 've': 595, 'uh': 572, 'im': 276, 'un': 578, 'ds': 144, 'ut': 584, 'ov': 442, \"e'\": 151, 'bp': 89, 'pl': 460, 'lp': 352, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 714, 'yk': 675, 'wi': 621, 'ea': 153, 'eg': 159, 'go': 223, 'dg': 132, 'ge': 213, 'og': 427, 'ru': 502, 'up': 580, 'pt': 467, 'tl': 549, 'tn': 551, 'ss': 528, 'yn': 678, 'uz': 590, 'zz': 715, 'zo': 705, 'sa': 510, 'ei': 161, 'lu': 357, 'lv': 358, 'rb': 483, 'rp': 497, 'tr': 554, 'su': 530, 'dl': 137, 'bt': 92, 'bu': 93, 'hm': 249, 'dz': 150, 'ys': 683, 'cs': 117, 'my': 388, 'mp': 380, 'po': 463, 'ap': 62, 'pu': 468, 'lc': 339, 'cc': 101, 'tu': 557, 'ua': 565, 'ep': 168, 'ry': 506, 't-': 537, '-p': 36, 'pr': 465, 'cl': 110, 'mm': 377, 'pa': 449, 'ny': 416, 'yi': 673, \"r'\": 480, 'gl': 220, 'cy': 122, \"s'\": 508, 'ps': 466, 'ue': 569, 'ui': 573, 'um': 577, 'mu': 385, 'ay': 71, 'op': 436, 'ph': 456, 'yl': 676, 'eb': 154, 'nb': 393, 'ht': 256, 'hy': 260, 'fi': 189, 'fy': 205, 'kl': 322, 'km': 323, 'kn': 324, 'wl': 623, 'gm': 221, 'kr': 327, 'oy': 445, 'yd': 668, \"d'\": 124, 'cm': 111, 'cn': 112, 'oc': 423, 'of': 426, 'ff': 186, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 677, 'tm': 550, 'uf': 570, 'sy': 534, 'yc': 667, \"m'\": 363, 'ik': 274, 'za': 692, 'sk': 520, 'wn': 625, 'ex': 176, 'dc': 128, 'dd': 129, 'eo': 167, 'ks': 328, 'dw': 148, 'we': 617, 'dy': 149, 'lb': 338, \"l'\": 335, 'lm': 349, 'nn': 405, 'sb': 511, 'oi': 429, 'eq': 169, 'rh': 489, 'ld': 340, 'lt': 356, 'dh': 133, 'eu': 173, 'lf': 342, 'ip': 279, 'dj': 135, 'ja': 291, 'jo': 302, 'ju': 306, 'dk': 136, 'dm': 138, \"f'\": 179, 'fo': 195, 'hs': 255, '-f': 27, '-m': 33, '-s': 39, 'gn': 222, 'ae': 51, 'sd': 513, \"g'\": 207, 'vo': 603, 'ft': 200, 'oq': 437, \"h'\": 235, 'sp': 525, 'hl': 248, \"p'\": 447, 'af': 52, 'ye': 669, 'fe': 185, 'ix': 287, 'xe': 643, 'fl': 192, 'fr': 198, 'ax': 70, 'fg': 187, 'gh': 216, \"i'\": 262, 'fh': 188, 'ox': 444, 'xi': 647, 'fm': 193, 'fs': 199, 'ef': 158, 'ug': 571, 'rw': 504, 'wa': 613, 'mn': 378, 'gy': 233, 'pe': 453, 'gc': 211, 'gf': 214, 'gg': 215, 'gr': 226, 'iu': 284, 'ew': 175, 'aw': 69, 'xc': 641, 'fu': 201, 'pp': 464, 'ok': 431, 'ko': 325, 'gu': 229, 'hh': 244, 'lg': 343, 'lq': 353, 'hn': 250, 'hr': 254, 'hu': 257, 'ml': 376, 'sl': 521, 'sw': 532, 'wo': 626, \"c'\": 98, 'gs': 227, 'rf': 487, 'nk': 402, 'tc': 540, 'iw': 286, 'aj': 56, \"x'\": 637, 'ji': 297, 'kb': 312, 'kc': 313, 'iy': 288, 'kk': 321, 'ky': 333, 'kz': 334, 'tv': 558, 'uq': 581, 'rq': 498, 'oh': 428, 'eh': 160, 'ej': 162, 'xa': 639, 'xo': 651, 'xy': 661, 'nq': 408, 'lh': 344, 'nz': 417, 'ij': 273, 'iq': 280, 'l-': 336, '-i': 30, 'n-': 391, '-o': 35, 'nw': 414, 'yw': 687, 'nu': 412, 'lr': 354, 'pi': 457, 'uv': 586, 'lw': 359, 'ez': 178, 'mq': 381, 'fn': 194, \"k'\": 309, 'tf': 543, 'zh': 699, 'wy': 635, 'lz': 362, 'ku': 330, 'np': 407, 'xt': 656, 'md': 368, 'zc': 694, 'zq': 707, 'mf': 370, 'mg': 371, 'mh': 372, 'yv': 686, 'oe': 425, 'pc': 451, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yo': 679, 'yx': 688, 'yz': 690, 'sq': 526, 'nv': 413, 'oz': 446, 'ws': 630, 'uj': 574, 'cd': 102, 'nf': 397, 'nj': 401, 'ih': 271, 'nr': 409, 'sg': 516, 'i-': 263, 'ub': 566, 'bm': 86, 'tk': 548, \"u'\": 563, 'tw': 559, 'nx': 415, 'yb': 666, 'yh': 672, 'yp': 680, 'wh': 620, 'ao': 61, 'pf': 454, 'pg': 455, 'pk': 459, 'aq': 63, 'qa': 473, 'rv': 503, 'ux': 588, 'hb': 238, 'hd': 240, 'uk': 575, 'zm': 703, 'rj': 491, 'kw': 332, 'wr': 629, 'd-': 125, '-t': 40, '-c': 24, \"w'\": 611, 'zt': 710, 'zu': 711, 'rx': 505, 'rz': 507, 'ii': 272, 'hc': 239, 'hf': 242, 'hw': 259, \"v'\": 591, 'pn': 462, 'yr': 682, 'fd': 184, \"'h\": 10, '-l': 32, 'uy': 589, 'vd': 594, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 614, 'wf': 618, 'wk': 622, 'wt': 631, 'xf': 644, 'xl': 648, 'xs': 655, 'yg': 671, 'yu': 685, 'yy': 689, 'zb': 693, \"b'\": 73, \"'r\": 18, 'hk': 247, 'k-': 310, '-a': 22, 'kd': 314, 'kf': 316, 'kg': 317, 'kp': 326, 'kt': 329, 'y-': 664, 'tj': 547, 'gb': 210, 'gd': 212, 'gp': 224, 'gw': 231, \"'i\": 11, 'sz': 535, 'gk': 219, 'kv': 331, 'xq': 653, 'fk': 191, 'sv': 531, 'vs': 605, 'wm': 624, 'sn': 523, 'sf': 515, 'tb': 539, 'sr': 527, 'td': 541, 'hg': 243, 'uw': 587, 'wd': 616, 'zl': 702, 'cv': 120, 'db': 127, 'df': 131, 'dp': 141, 'vu': 607, 'zr': 708, 'nl': 403, 'jy': 308, \"z'\": 691, 'wu': 632, 'gq': 225, '-k': 31, 'hp': 252, 'vy': 609, 'zd': 695, 'zn': 704, 'lj': 346, 'fb': 182, 'xu': 657, 'xb': 640, 'kj': 320, 'mk': 375, '-r': 38, '-g': 28, '-v': 42, 'bz': 97, 'tg': 544, 'sj': 519, 'oj': 430, 'gj': 218, 'qi': 476, 'wc': 615, 'xw': 659, 'xx': 660, 'yf': 670, 'jd': 293, '-n': 34, 'zk': 701, 'tp': 553, 'fc': 183, '-d': 25, 'r-': 481, 'uu': 585, '-u': 41, 'py': 470, 'zw': 713, 'pb': 450, 'pw': 469, \"q'\": 472, \"'v\": 21, 'jk': 299, 'pd': 452, 'pm': 461, 'gx': 232, 'jn': 301, 'zp': 706, 'o-': 420, 'bw': 95, '-e': 26, 'wg': 619, 'zf': 697, 's-': 509, 'vl': 601, 'g-': 208, 'cw': 121, \"'a\": 4, \"'o\": 16, 'hv': 258, 'vc': 593, 'zs': 709, 'mj': 374, 'xh': 646, 'vv': 608, 'xv': 658, 'mz': 389, 'bf': 79, 'hq': 253, 'dq': 142, 'lx': 360, '-h': 29, \"'d\": 7, 'vj': 599, 'x-': 638, '-w': 43, 'xn': 650, 'xp': 652, 'jv': 307, 'zg': 698, '-y': 44, 'fj': 190, 'jt': 305, 'w-': 612, 'xg': 645, 'xm': 649, 'tx': 560, 'gz': 234, 'gv': 230, 'jj': 298, 'f-': 180, 'fw': 203, 'wv': 633, \"'l\": 13, 'h-': 236, 'hj': 246, 'fp': 196, 'js': 304, 'vh': 597, 'wz': 636, 'i': 261, 'qb': 474, 'qg': 475, 'zv': 712, 'jf': 295, 'jh': 296, 'jc': 292, 'wp': 627, 'bv': 94, 'pz': 471, 'fq': 197, 'vg': 596, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, \"j'\": 290, 'jr': 303, 'xz': 662, 'cj': 108, 'fx': 204, 'fz': 206, 'qv': 479, 'wq': 628, 'ww': 634, 'xr': 654, 'xd': 642, 'o': 418, \"'b\": 5, \"'g\": 9, \"'k\": 12, 'a-': 46, '-q': 37, 'vk': 600, 'qo': 477, 'vz': 610, 'jm': 300, 'yj': 674, 'pj': 458, 'p-': 448, 'fv': 202, 'bg': 80, 'u-': 564, 'sx': 533, 'm-': 364, 'yq': 681}\n",
      "712 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 583, 'se': 514, 'co': 113, 'ou': 441, 'ur': 582, 'rs': 500, \"'e\": 8, 'em': 165, \"'m\": 14, \"'n\": 15, \"'q\": 17, 'qu': 478, 'uo': 579, 'ot': 440, 'te': 542, \"'t\": 20, 'ti': 546, 'is': 282, 'aa': 47, 'ab': 48, 'be': 78, 'er': 170, 'rg': 488, 'ac': 49, 'ch': 106, 'he': 241, 'en': 166, 'ak': 57, 'ke': 315, 'al': 58, 'ls': 355, 'et': 172, 'th': 545, 'am': 59, 'mo': 379, 'od': 424, 'dt': 145, 'an': 60, 'nc': 394, 'or': 438, 'ar': 64, 'rd': 485, 'de': 130, 'ma': 365, 'dv': 147, 'va': 592, 'rk': 492, 'ro': 496, 'on': 434, \"n'\": 390, \"'s\": 19, 'ns': 410, 'so': 524, 'rt': 501, 'as': 65, 'ba': 74, 'ha': 237, 'ck': 109, 'cu': 119, 'ad': 50, 'da': 126, 'ka': 311, 'di': 134, 'ai': 55, 'ir': 281, 'lk': 347, 'ki': 319, 'in': 277, 'lo': 351, 'ne': 396, 'os': 439, 'nd': 395, 'do': 140, 'ni': 400, 'ng': 398, 'nm': 404, 'me': 369, 'nt': 411, 'ts': 555, 'to': 552, 'rc': 484, 're': 486, 'sc': 512, 'sh': 517, 'ed': 156, 'at': 66, 'es': 171, 'bb': 75, 'si': 518, 'ie': 268, 'el': 164, 'll': 348, 'nh': 399, 'tt': 556, 'ev': 174, 'vi': 598, 'il': 275, 'le': 341, 'ey': 177, \"y'\": 663, 'bi': 82, 'it': 283, 'bo': 88, \"t'\": 536, 'ud': 568, 'br': 90, 'ia': 264, 'io': 278, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 163, 'bd': 77, 'la': 337, 'ah': 54, 'ic': 266, 'dn': 139, 'no': 406, 'ol': 432, 'om': 433, 'mi': 373, 'na': 392, 'du': 146, 'uc': 567, 'ct': 118, 'ee': 157, 'ul': 576, 'az': 72, 'zi': 700, 'iz': 289, 'ln': 350, 'ow': 443, 'dr': 143, 'cr': 116, 'mb': 366, 'rl': 493, 'rm': 494, 'rr': 499, 'ra': 482, 'ya': 665, 'ce': 103, 'yt': 684, 'ta': 538, 'bh': 81, 'ho': 251, 'id': 267, 'ig': 270, 'ga': 209, 'li': 345, 'ty': 561, 'gt': 228, \"a'\": 45, 'ib': 265, 'tz': 562, 'bj': 83, 'je': 294, 'ec': 155, 'bk': 84, 'kh': 318, 'bl': 85, 'ze': 696, 'e-': 152, '-b': 23, 'st': 529, 'oo': 435, 'ly': 361, 'bn': 87, \"o'\": 419, 'oa': 421, 'hi': 245, 'sm': 522, 'ri': 490, 'gi': 217, 'rn': 495, 'if': 269, 'fa': 181, 'ci': 107, 'iv': 285, 've': 595, 'uh': 572, 'im': 276, 'un': 578, 'ds': 144, 'ut': 584, 'ov': 442, \"e'\": 151, 'bp': 89, 'pl': 460, 'lp': 352, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 714, 'yk': 675, 'wi': 621, 'ea': 153, 'eg': 159, 'go': 223, 'dg': 132, 'ge': 213, 'og': 427, 'ru': 502, 'up': 580, 'pt': 467, 'tl': 549, 'tn': 551, 'ss': 528, 'yn': 678, 'uz': 590, 'zz': 715, 'zo': 705, 'sa': 510, 'ei': 161, 'lu': 357, 'lv': 358, 'rb': 483, 'rp': 497, 'tr': 554, 'su': 530, 'dl': 137, 'bt': 92, 'bu': 93, 'hm': 249, 'dz': 150, 'ys': 683, 'cs': 117, 'my': 388, 'mp': 380, 'po': 463, 'ap': 62, 'pu': 468, 'lc': 339, 'cc': 101, 'tu': 557, 'ua': 565, 'ep': 168, 'ry': 506, 't-': 537, '-p': 36, 'pr': 465, 'cl': 110, 'mm': 377, 'pa': 449, 'ny': 416, 'yi': 673, \"r'\": 480, 'gl': 220, 'cy': 122, \"s'\": 508, 'ps': 466, 'ue': 569, 'ui': 573, 'um': 577, 'mu': 385, 'ay': 71, 'op': 436, 'ph': 456, 'yl': 676, 'eb': 154, 'nb': 393, 'ht': 256, 'hy': 260, 'fi': 189, 'fy': 205, 'kl': 322, 'km': 323, 'kn': 324, 'wl': 623, 'gm': 221, 'kr': 327, 'oy': 445, 'yd': 668, \"d'\": 124, 'cm': 111, 'cn': 112, 'oc': 423, 'of': 426, 'ff': 186, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 677, 'tm': 550, 'uf': 570, 'sy': 534, 'yc': 667, \"m'\": 363, 'ik': 274, 'za': 692, 'sk': 520, 'wn': 625, 'ex': 176, 'dc': 128, 'dd': 129, 'eo': 167, 'ks': 328, 'dw': 148, 'we': 617, 'dy': 149, 'lb': 338, \"l'\": 335, 'lm': 349, 'nn': 405, 'sb': 511, 'oi': 429, 'eq': 169, 'rh': 489, 'ld': 340, 'lt': 356, 'dh': 133, 'eu': 173, 'lf': 342, 'ip': 279, 'dj': 135, 'ja': 291, 'jo': 302, 'ju': 306, 'dk': 136, 'dm': 138, \"f'\": 179, 'fo': 195, 'hs': 255, '-f': 27, '-m': 33, '-s': 39, 'gn': 222, 'ae': 51, 'sd': 513, \"g'\": 207, 'vo': 603, 'ft': 200, 'oq': 437, \"h'\": 235, 'sp': 525, 'hl': 248, \"p'\": 447, 'af': 52, 'ye': 669, 'fe': 185, 'ix': 287, 'xe': 643, 'fl': 192, 'fr': 198, 'ax': 70, 'fg': 187, 'gh': 216, \"i'\": 262, 'fh': 188, 'ox': 444, 'xi': 647, 'fm': 193, 'fs': 199, 'ef': 158, 'ug': 571, 'rw': 504, 'wa': 613, 'mn': 378, 'gy': 233, 'pe': 453, 'gc': 211, 'gf': 214, 'gg': 215, 'gr': 226, 'iu': 284, 'ew': 175, 'aw': 69, 'xc': 641, 'fu': 201, 'pp': 464, 'ok': 431, 'ko': 325, 'gu': 229, 'hh': 244, 'lg': 343, 'lq': 353, 'hn': 250, 'hr': 254, 'hu': 257, 'ml': 376, 'sl': 521, 'sw': 532, 'wo': 626, \"c'\": 98, 'gs': 227, 'rf': 487, 'nk': 402, 'tc': 540, 'iw': 286, 'aj': 56, \"x'\": 637, 'ji': 297, 'kb': 312, 'kc': 313, 'iy': 288, 'kk': 321, 'ky': 333, 'kz': 334, 'tv': 558, 'uq': 581, 'rq': 498, 'oh': 428, 'eh': 160, 'ej': 162, 'xa': 639, 'xo': 651, 'xy': 661, 'nq': 408, 'lh': 344, 'nz': 417, 'ij': 273, 'iq': 280, 'l-': 336, '-i': 30, 'n-': 391, '-o': 35, 'nw': 414, 'yw': 687, 'nu': 412, 'lr': 354, 'pi': 457, 'uv': 586, 'lw': 359, 'ez': 178, 'mq': 381, 'fn': 194, \"k'\": 309, 'tf': 543, 'zh': 699, 'wy': 635, 'lz': 362, 'ku': 330, 'np': 407, 'xt': 656, 'md': 368, 'zc': 694, 'zq': 707, 'mf': 370, 'mg': 371, 'mh': 372, 'yv': 686, 'oe': 425, 'pc': 451, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yo': 679, 'yx': 688, 'yz': 690, 'sq': 526, 'nv': 413, 'oz': 446, 'ws': 630, 'uj': 574, 'cd': 102, 'nf': 397, 'nj': 401, 'ih': 271, 'nr': 409, 'sg': 516, 'i-': 263, 'ub': 566, 'bm': 86, 'tk': 548, \"u'\": 563, 'tw': 559, 'nx': 415, 'yb': 666, 'yh': 672, 'yp': 680, 'wh': 620, 'ao': 61, 'pf': 454, 'pg': 455, 'pk': 459, 'aq': 63, 'qa': 473, 'rv': 503, 'ux': 588, 'hb': 238, 'hd': 240, 'uk': 575, 'zm': 703, 'rj': 491, 'kw': 332, 'wr': 629, 'd-': 125, '-t': 40, '-c': 24, \"w'\": 611, 'zt': 710, 'zu': 711, 'rx': 505, 'rz': 507, 'ii': 272, 'hc': 239, 'hf': 242, 'hw': 259, \"v'\": 591, 'pn': 462, 'yr': 682, 'fd': 184, \"'h\": 10, '-l': 32, 'uy': 589, 'vd': 594, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 614, 'wf': 618, 'wk': 622, 'wt': 631, 'xf': 644, 'xl': 648, 'xs': 655, 'yg': 671, 'yu': 685, 'yy': 689, 'zb': 693, \"b'\": 73, \"'r\": 18, 'hk': 247, 'k-': 310, '-a': 22, 'kd': 314, 'kf': 316, 'kg': 317, 'kp': 326, 'kt': 329, 'y-': 664, 'tj': 547, 'gb': 210, 'gd': 212, 'gp': 224, 'gw': 231, \"'i\": 11, 'sz': 535, 'gk': 219, 'kv': 331, 'xq': 653, 'fk': 191, 'sv': 531, 'vs': 605, 'wm': 624, 'sn': 523, 'sf': 515, 'tb': 539, 'sr': 527, 'td': 541, 'hg': 243, 'uw': 587, 'wd': 616, 'zl': 702, 'cv': 120, 'db': 127, 'df': 131, 'dp': 141, 'vu': 607, 'zr': 708, 'nl': 403, 'jy': 308, \"z'\": 691, 'wu': 632, 'gq': 225, '-k': 31, 'hp': 252, 'vy': 609, 'zd': 695, 'zn': 704, 'lj': 346, 'fb': 182, 'xu': 657, 'xb': 640, 'kj': 320, 'mk': 375, '-r': 38, '-g': 28, '-v': 42, 'bz': 97, 'tg': 544, 'sj': 519, 'oj': 430, 'gj': 218, 'qi': 476, 'wc': 615, 'xw': 659, 'xx': 660, 'yf': 670, 'jd': 293, '-n': 34, 'zk': 701, 'tp': 553, 'fc': 183, '-d': 25, 'r-': 481, 'uu': 585, '-u': 41, 'py': 470, 'zw': 713, 'pb': 450, 'pw': 469, \"q'\": 472, \"'v\": 21, 'jk': 299, 'pd': 452, 'pm': 461, 'gx': 232, 'jn': 301, 'zp': 706, 'o-': 420, 'bw': 95, '-e': 26, 'wg': 619, 'zf': 697, 's-': 509, 'vl': 601, 'g-': 208, 'cw': 121, \"'a\": 4, \"'o\": 16, 'hv': 258, 'vc': 593, 'zs': 709, 'mj': 374, 'xh': 646, 'vv': 608, 'xv': 658, 'mz': 389, 'bf': 79, 'hq': 253, 'dq': 142, 'lx': 360, '-h': 29, \"'d\": 7, 'vj': 599, 'x-': 638, '-w': 43, 'xn': 650, 'xp': 652, 'jv': 307, 'zg': 698, '-y': 44, 'fj': 190, 'jt': 305, 'w-': 612, 'xg': 645, 'xm': 649, 'tx': 560, 'gz': 234, 'gv': 230, 'jj': 298, 'f-': 180, 'fw': 203, 'wv': 633, \"'l\": 13, 'h-': 236, 'hj': 246, 'fp': 196, 'js': 304, 'vh': 597, 'wz': 636, 'i': 261, 'qb': 474, 'qg': 475, 'zv': 712, 'jf': 295, 'jh': 296, 'jc': 292, 'wp': 627, 'bv': 94, 'pz': 471, 'fq': 197, 'vg': 596, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, \"j'\": 290, 'jr': 303, 'xz': 662, 'cj': 108, 'fx': 204, 'fz': 206, 'qv': 479, 'wq': 628, 'ww': 634, 'xr': 654, 'xd': 642, 'o': 418, \"'b\": 5, \"'g\": 9, \"'k\": 12, 'a-': 46, '-q': 37, 'vk': 600, 'qo': 477, 'vz': 610, 'jm': 300, 'yj': 674, 'pj': 458, 'p-': 448, 'fv': 202, 'bg': 80, 'u-': 564, 'sx': 533, 'm-': 364, 'yq': 681}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n",
      "32 {'K': 18, 'AX': 6, 'Z': 34, 'AO': 4, 'R': 26, 'S': 27, 'M': 20, 'N': 21, 'W': 32, 'T': 29, 'IY': 16, 'AA': 3, 'B': 8, 'G': 14, 'L': 19, 'EH': 11, 'D': 10, 'V': 31, 'Q': 25, 'NG': 22, 'SH': 28, 'EY': 12, 'HH': 15, 'AW': 5, 'UW': 30, 'AY': 7, 'JH': 17, 'Y': 33, 'F': 13, 'P': 24, 'CH': 9, 'OY': 23}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "      )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 128\n",
      "hidden_size: 50\n",
      "n_layers: 1\n",
      "Encoder has a total number of 118648 parameters\n",
      "Decoder has a total number of 42515 parameters\n",
      "Total number of all parameters is 161163\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 1 finished in 0m 38.42s (- 63m 23.51s) (1 1.0%). train avg loss: 1.6976, val avg loss: 1.5937\n",
      "Training for epoch 2 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 2 finished in 1m 13.92s (- 60m 22.26s) (2 2.0%). train avg loss: 0.8537, val avg loss: 1.1694\n",
      "Training for epoch 3 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 3 finished in 1m 48.01s (- 58m 12.48s) (3 3.0%). train avg loss: 0.5948, val avg loss: 0.9845\n",
      "Training for epoch 4 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 4 finished in 2m 22.07s (- 56m 49.68s) (4 4.0%). train avg loss: 0.4958, val avg loss: 1.0382\n",
      "Training for epoch 5 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 5 finished in 2m 56.43s (- 55m 52.22s) (5 5.0%). train avg loss: 0.4327, val avg loss: 0.8806\n",
      "Training for epoch 6 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 6 finished in 3m 32.66s (- 55m 31.61s) (6 6.0%). train avg loss: 0.4021, val avg loss: 0.9315\n",
      "Training for epoch 7 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 7 finished in 4m 6.83s (- 54m 39.35s) (7 7.0%). train avg loss: 0.3985, val avg loss: 0.8203\n",
      "Training for epoch 8 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 8 finished in 4m 45.59s (- 54m 44.31s) (8 8.0%). train avg loss: 0.3702, val avg loss: 0.8449\n",
      "Training for epoch 9 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 9 finished in 5m 23.82s (- 54m 34.22s) (9 9.0%). train avg loss: 0.3607, val avg loss: 0.8943\n",
      "Training for epoch 10 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 10 finished in 6m 3.17s (- 54m 28.54s) (10 10.0%). train avg loss: 0.3376, val avg loss: 0.7777\n",
      "Training for epoch 11 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 11 finished in 6m 39.3s (- 53m 50.7s) (11 11.0%). train avg loss: 0.3334, val avg loss: 0.8459\n",
      "Training for epoch 12 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 12 finished in 7m 18.73s (- 53m 37.37s) (12 12.0%). train avg loss: 0.3328, val avg loss: 0.7567\n",
      "Training for epoch 13 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 13 finished in 7m 55.98s (- 53m 5.39s) (13 13.0%). train avg loss: 0.3136, val avg loss: 0.7772\n",
      "Training for epoch 14 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 14 finished in 8m 29.59s (- 52m 10.34s) (14 14.0%). train avg loss: 0.3103, val avg loss: 0.7972\n",
      "Training for epoch 15 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 15 finished in 9m 4.48s (- 51m 25.36s) (15 15.0%). train avg loss: 0.3063, val avg loss: 0.7507\n",
      "Training for epoch 16 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 16 finished in 9m 39.23s (- 50m 40.94s) (16 16.0%). train avg loss: 0.2954, val avg loss: 0.6909\n",
      "Training for epoch 17 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 17 finished in 10m 12.82s (- 49m 52.0s) (17 17.0%). train avg loss: 0.3076, val avg loss: 0.7518\n",
      "Training for epoch 18 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 18 finished in 10m 48.51s (- 49m 14.32s) (18 18.0%). train avg loss: 0.2891, val avg loss: 0.7205\n",
      "Training for epoch 19 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 19 finished in 11m 26.58s (- 48m 46.99s) (19 19.0%). train avg loss: 0.2864, val avg loss: 0.701\n",
      "Training for epoch 20 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 20 finished in 12m 5.4s (- 48m 21.59s) (20 20.0%). train avg loss: 0.2742, val avg loss: 0.7484\n",
      "Training for epoch 21 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 21 finished in 12m 44.19s (- 47m 54.8s) (21 21.0%). train avg loss: 0.2731, val avg loss: 0.6951\n",
      "Training for epoch 22 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 22 finished in 13m 22.93s (- 47m 26.75s) (22 22.0%). train avg loss: 0.2598, val avg loss: 0.7026\n",
      "Training for epoch 23 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 23 finished in 13m 59.18s (- 46m 49.43s) (23 23.0%). train avg loss: 0.262, val avg loss: 0.719\n",
      "Training for epoch 24 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 24 finished in 14m 37.84s (- 46m 19.82s) (24 24.0%). train avg loss: 0.2708, val avg loss: 0.7367\n",
      "Training for epoch 25 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 25 finished in 15m 18.44s (- 45m 55.33s) (25 25.0%). train avg loss: 0.2609, val avg loss: 0.9114\n",
      "Training for epoch 26 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 26 finished in 15m 58.56s (- 45m 28.21s) (26 26.0%). train avg loss: 0.2751, val avg loss: 0.7255\n",
      "Training for epoch 27 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 27 finished in 16m 34.93s (- 44m 50.0s) (27 27.0%). train avg loss: 0.2499, val avg loss: 0.6904\n",
      "Training for epoch 28 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 28 finished in 17m 13.78s (- 44m 18.29s) (28 28.0%). train avg loss: 0.2432, val avg loss: 0.661\n",
      "Training for epoch 29 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 29 finished in 17m 51.56s (- 43m 43.48s) (29 29.0%). train avg loss: 0.2437, val avg loss: 0.6852\n",
      "Training for epoch 30 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 30 finished in 18m 28.59s (- 43m 6.71s) (30 30.0%). train avg loss: 0.2572, val avg loss: 0.7265\n",
      "Training for epoch 31 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 31 finished in 19m 8.38s (- 42m 36.06s) (31 31.0%). train avg loss: 0.2398, val avg loss: 0.6833\n",
      "Training for epoch 32 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 32 finished in 19m 44.79s (- 41m 57.67s) (32 32.0%). train avg loss: 0.2501, val avg loss: 0.6704\n",
      "Training for epoch 33 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 33 finished in 20m 16.48s (- 41m 9.82s) (33 33.0%). train avg loss: 0.2411, val avg loss: 0.6617\n",
      "Training for epoch 34 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 34 finished in 20m 49.38s (- 40m 25.26s) (34 34.0%). train avg loss: 0.2422, val avg loss: 0.6654\n",
      "Training for epoch 35 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 35 finished in 21m 24.01s (- 39m 44.59s) (35 35.0%). train avg loss: 0.2315, val avg loss: 0.6549\n",
      "Training for epoch 36 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 36 finished in 21m 57.07s (- 39m 1.45s) (36 36.0%). train avg loss: 0.2327, val avg loss: 0.6516\n",
      "Training for epoch 37 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 37 finished in 22m 32.25s (- 38m 22.49s) (37 37.0%). train avg loss: 0.2417, val avg loss: 0.6872\n",
      "Training for epoch 38 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 38 finished in 23m 7.5s (- 37m 43.82s) (38 38.0%). train avg loss: 0.2344, val avg loss: 0.6882\n",
      "Training for epoch 39 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 39 finished in 23m 42.96s (- 37m 5.65s) (39 39.0%). train avg loss: 0.2304, val avg loss: 0.6611\n",
      "Training for epoch 40 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 40 finished in 24m 15.57s (- 36m 23.35s) (40 40.0%). train avg loss: 0.2238, val avg loss: 0.659\n",
      "Training for epoch 41 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 41 finished in 24m 47.79s (- 35m 40.97s) (41 41.0%). train avg loss: 0.2198, val avg loss: 0.6667\n",
      "Training for epoch 42 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 42 finished in 25m 22.12s (- 35m 1.98s) (42 42.0%). train avg loss: 0.2157, val avg loss: 0.6424\n",
      "Training for epoch 43 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 43 finished in 25m 53.85s (- 34m 19.75s) (43 43.0%). train avg loss: 0.2159, val avg loss: 0.7175\n",
      "Training for epoch 44 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 44 finished in 26m 24.41s (- 33m 36.52s) (44 44.0%). train avg loss: 0.2224, val avg loss: 0.63\n",
      "Training for epoch 45 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 45 finished in 26m 58.61s (- 32m 58.3s) (45 45.0%). train avg loss: 0.2172, val avg loss: 0.7171\n",
      "Training for epoch 46 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 46 finished in 27m 33.16s (- 32m 20.67s) (46 46.0%). train avg loss: 0.2139, val avg loss: 0.6322\n",
      "Training for epoch 47 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 47 finished in 28m 8.38s (- 31m 43.92s) (47 47.0%). train avg loss: 0.2269, val avg loss: 0.6406\n",
      "Training for epoch 48 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 48 finished in 28m 45.4s (- 31m 9.18s) (48 48.0%). train avg loss: 0.2059, val avg loss: 0.6174\n",
      "Training for epoch 49 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 49 finished in 29m 19.23s (- 30m 31.04s) (49 49.0%). train avg loss: 0.2086, val avg loss: 0.6448\n",
      "Training for epoch 50 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 50 finished in 29m 52.55s (- 29m 52.55s) (50 50.0%). train avg loss: 0.2119, val avg loss: 0.6798\n",
      "Training for epoch 51 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 51 finished in 30m 26.97s (- 29m 15.32s) (51 51.0%). train avg loss: 0.2112, val avg loss: 0.614\n",
      "Training for epoch 52 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 52 finished in 31m 2.75s (- 28m 39.46s) (52 52.0%). train avg loss: 0.2274, val avg loss: 0.6613\n",
      "Training for epoch 53 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 53 finished in 31m 35.92s (- 28m 1.29s) (53 53.0%). train avg loss: 0.2136, val avg loss: 0.6184\n",
      "Training for epoch 54 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 54 finished in 32m 9.58s (- 27m 23.72s) (54 54.0%). train avg loss: 0.199, val avg loss: 0.631\n",
      "Training for epoch 55 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 55 finished in 32m 41.36s (- 26m 44.75s) (55 55.0%). train avg loss: 0.2092, val avg loss: 0.6203\n",
      "Training for epoch 56 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 56 finished in 33m 12.99s (- 26m 5.92s) (56 56.0%). train avg loss: 0.208, val avg loss: 0.6108\n",
      "Training for epoch 57 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 57 finished in 33m 46.01s (- 25m 28.39s) (57 57.0%). train avg loss: 0.2016, val avg loss: 0.6911\n",
      "Training for epoch 58 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 58 finished in 34m 17.86s (- 24m 50.17s) (58 58.0%). train avg loss: 0.2102, val avg loss: 0.5915\n",
      "Training for epoch 59 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 59 finished in 34m 51.48s (- 24m 13.4s) (59 59.0%). train avg loss: 0.2135, val avg loss: 0.6097\n",
      "Training for epoch 60 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 60 finished in 35m 22.83s (- 23m 35.22s) (60 60.0%). train avg loss: 0.1943, val avg loss: 0.6051\n",
      "Training for epoch 61 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 61 finished in 35m 55.44s (- 22m 58.07s) (61 61.0%). train avg loss: 0.2028, val avg loss: 0.6119\n",
      "Training for epoch 62 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 62 finished in 36m 36.31s (- 22m 26.13s) (62 62.0%). train avg loss: 0.2031, val avg loss: 0.5959\n",
      "Training for epoch 63 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 63 finished in 37m 15.0s (- 21m 52.62s) (63 63.0%). train avg loss: 0.1984, val avg loss: 0.6034\n",
      "Training for epoch 64 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 64 finished in 37m 54.41s (- 21m 19.36s) (64 64.0%). train avg loss: 0.2034, val avg loss: 0.5884\n",
      "Training for epoch 65 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 65 finished in 38m 32.71s (- 20m 45.31s) (65 65.0%). train avg loss: 0.1923, val avg loss: 0.6209\n",
      "Training for epoch 66 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 66 finished in 39m 12.73s (- 20m 12.01s) (66 66.0%). train avg loss: 0.2086, val avg loss: 0.6056\n",
      "Training for epoch 67 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 67 finished in 39m 52.58s (- 19m 38.44s) (67 67.0%). train avg loss: 0.2018, val avg loss: 0.6321\n",
      "Training for epoch 68 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 68 finished in 40m 32.17s (- 19m 4.55s) (68 68.0%). train avg loss: 0.1933, val avg loss: 0.6082\n",
      "Training for epoch 69 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 69 finished in 41m 8.03s (- 18m 28.83s) (69 69.0%). train avg loss: 0.2025, val avg loss: 0.596\n",
      "Training for epoch 70 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 70 finished in 41m 40.47s (- 17m 51.63s) (70 70.0%). train avg loss: 0.1976, val avg loss: 0.5925\n",
      "Training for epoch 71 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 71 finished in 42m 14.08s (- 17m 15.05s) (71 71.0%). train avg loss: 0.1921, val avg loss: 0.5999\n",
      "Training for epoch 72 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 72 finished in 42m 47.78s (- 16m 38.58s) (72 72.0%). train avg loss: 0.1936, val avg loss: 0.6284\n",
      "Training for epoch 73 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 73 finished in 43m 21.63s (- 16m 2.25s) (73 73.0%). train avg loss: 0.1998, val avg loss: 0.6007\n",
      "Training for epoch 74 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 74 finished in 43m 55.66s (- 15m 26.04s) (74 74.0%). train avg loss: 0.1932, val avg loss: 0.5699\n",
      "Training for epoch 75 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 75 finished in 44m 27.75s (- 14m 49.25s) (75 75.0%). train avg loss: 0.191, val avg loss: 0.6181\n",
      "Training for epoch 76 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 76 finished in 45m 0.38s (- 14m 12.75s) (76 76.0%). train avg loss: 0.1888, val avg loss: 0.6538\n",
      "Training for epoch 77 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 77 finished in 45m 33.41s (- 13m 36.47s) (77 77.0%). train avg loss: 0.1866, val avg loss: 0.5636\n",
      "Training for epoch 78 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 78 finished in 46m 10.13s (- 13m 1.32s) (78 78.0%). train avg loss: 0.1843, val avg loss: 0.5842\n",
      "Training for epoch 79 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 79 finished in 46m 49.01s (- 12m 26.7s) (79 79.0%). train avg loss: 0.1975, val avg loss: 0.6537\n",
      "Training for epoch 80 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 80 finished in 47m 21.26s (- 11m 50.31s) (80 80.0%). train avg loss: 0.1873, val avg loss: 0.6436\n",
      "Training for epoch 81 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 81 finished in 47m 53.05s (- 11m 13.93s) (81 81.0%). train avg loss: 0.1881, val avg loss: 0.5849\n",
      "Training for epoch 82 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 82 finished in 48m 26.72s (- 10m 38.06s) (82 82.0%). train avg loss: 0.1906, val avg loss: 0.5745\n",
      "Training for epoch 83 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 83 finished in 49m 0.14s (- 10m 2.2s) (83 83.0%). train avg loss: 0.1845, val avg loss: 0.632\n",
      "Training for epoch 84 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 84 finished in 49m 34.1s (- 9m 26.5s) (84 84.0%). train avg loss: 0.2055, val avg loss: 0.6847\n",
      "Training for epoch 85 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 85 finished in 50m 7.89s (- 8m 50.8s) (85 85.0%). train avg loss: 0.1926, val avg loss: 0.592\n",
      "Training for epoch 86 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 86 finished in 50m 40.31s (- 8m 14.93s) (86 86.0%). train avg loss: 0.1811, val avg loss: 0.6008\n",
      "Training for epoch 87 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 87 finished in 51m 13.23s (- 7m 39.22s) (87 87.0%). train avg loss: 0.1944, val avg loss: 0.6391\n",
      "Training for epoch 88 has started (lr=0.001). Found 1586 batch(es).\n",
      "Epoch 88 finished in 51m 46.66s (- 7m 3.64s) (88 88.0%). train avg loss: 0.1835, val avg loss: 0.5873\n",
      "Training for epoch 89 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 89 finished in 52m 20.49s (- 6m 28.15s) (89 89.0%). train avg loss: 0.1669, val avg loss: 0.5588\n",
      "Training for epoch 90 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 90 finished in 52m 53.62s (- 5m 52.62s) (90 90.0%). train avg loss: 0.1628, val avg loss: 0.5634\n",
      "Training for epoch 91 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 91 finished in 53m 26.77s (- 5m 17.15s) (91 91.0%). train avg loss: 0.1584, val avg loss: 0.5631\n",
      "Training for epoch 92 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 92 finished in 53m 59.16s (- 4m 41.67s) (92 92.0%). train avg loss: 0.1621, val avg loss: 0.5501\n",
      "Training for epoch 93 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 93 finished in 54m 35.11s (- 4m 6.51s) (93 93.0%). train avg loss: 0.16, val avg loss: 0.5696\n",
      "Training for epoch 94 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 94 finished in 55m 10.23s (- 3m 31.29s) (94 94.0%). train avg loss: 0.1597, val avg loss: 0.5789\n",
      "Training for epoch 95 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 95 finished in 55m 43.76s (- 2m 55.99s) (95 95.0%). train avg loss: 0.157, val avg loss: 0.5894\n",
      "Training for epoch 96 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 96 finished in 56m 17.92s (- 2m 20.75s) (96 96.0%). train avg loss: 0.1611, val avg loss: 0.5887\n",
      "Training for epoch 97 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 97 finished in 56m 51.26s (- 1m 45.5s) (97 97.0%). train avg loss: 0.1571, val avg loss: 0.5873\n",
      "Training for epoch 98 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 98 finished in 57m 24.75s (- 1m 10.3s) (98 98.0%). train avg loss: 0.1559, val avg loss: 0.5914\n",
      "Training for epoch 99 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 99 finished in 57m 58.56s (- 0m 35.14s) (99 99.0%). train avg loss: 0.1547, val avg loss: 0.5822\n",
      "Training for epoch 100 has started (lr=0.0005). Found 1586 batch(es).\n",
      "Epoch 100 finished in 58m 33.25s (- 0m 0.0s) (100 100.0%). train avg loss: 0.153, val avg loss: 0.5799\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5.5, min_value-.125, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 15.357355839445674%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> WEISSENBORN\n",
      "= ['W', 'AY', 'S', 'AX', 'N', 'B', 'AX', 'R', 'N']\n",
      "< W AY S AX N B AX R N ['W', 'AY', 'S', 'AX', 'N', 'B', 'AX', 'R', 'N']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4358e33520>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
