{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn_gen/exp/id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL =\"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"50\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models_fallback\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  if split_name == \"train+val\" :\n",
    "    print(f\"Merging train and val entries ..\")\n",
    "    with open(os.path.join(DATA_DIR, f\"train.csv\"), encoding=\"utf-8\") as f_train_csv, \\\n",
    "         open(os.path.join(DATA_DIR, f\"val.csv\"), encoding=\"utf-8\") as f_val_csv :\n",
    "      next(f_train_csv, None)\n",
    "      next(f_val_csv, None)\n",
    "      train_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_train_csv]\n",
    "      val_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_val_csv]\n",
    "      pairs = train_pairs + val_pairs\n",
    "      graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "      phonemes_list = [extract_arpabet_phonemes(pair[-1]) for pair in pairs]\n",
    "      g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "    return g2p_dataset, pairs\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[-1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging train and val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train+val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 29, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f83ba61b7c0> ([5, 5, 19, 29, 6, 1], [23, 3, 1])\n",
      "([5, 5, 19, 29, 6, 1], [23, 3, 1])\n",
      "([5, 5, 19, 29, 6, 1], [23, 3, 1])\n",
      "train grp 31 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'y', 30: 'z'}\n",
      "test grp 31 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'y', 30: 'z'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "27 {'-': 5, 'n': 19, 'y': 29, 'a': 6, 'd': 9, 'b': 7, 'e': 10, 'l': 17, 's': 24, 'c': 8, 'o': 20, 'm': 18, 'p': 21, 'g': 12, 'k': 16, 'u': 26, 'h': 13, 'i': 14, 'w': 28, 't': 25, \"'\": 4, 'r': 23, 'j': 15, 'v': 27, 'f': 11, 'z': 30, 'q': 22}\n",
      "27 {'-': 5, 'n': 19, 'y': 29, 'a': 6, 'd': 9, 'b': 7, 'e': 10, 'l': 17, 's': 24, 'c': 8, 'o': 20, 'm': 18, 'p': 21, 'g': 12, 'k': 16, 'u': 26, 'h': 13, 'i': 14, 'w': 28, 't': 25, \"'\": 4, 'r': 23, 'j': 15, 'v': 27, 'f': 11, 'z': 30, 'q': 22}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 50\n",
      "n_layers: 1\n",
      "Encoder has a total number of 19384 parameters\n",
      "Decoder has a total number of 30840 parameters\n",
      "Total number of all parameters is 50224\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 1 finished in 0m 9.84s (- 16m 14.19s) (1 1.0%). train avg loss: 1.8928\n",
      "Training for epoch 2 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 2 finished in 0m 19.18s (- 15m 39.8s) (2 2.0%). train avg loss: 0.9124\n",
      "Training for epoch 3 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 3 finished in 0m 27.95s (- 15m 3.64s) (3 3.0%). train avg loss: 0.38\n",
      "Training for epoch 4 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 4 finished in 0m 37.22s (- 14m 53.37s) (4 4.0%). train avg loss: 0.2278\n",
      "Training for epoch 5 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 5 finished in 0m 47.13s (- 14m 55.54s) (5 5.0%). train avg loss: 0.1774\n",
      "Training for epoch 6 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 6 finished in 0m 56.67s (- 14m 47.9s) (6 6.0%). train avg loss: 0.1327\n",
      "Training for epoch 7 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 7 finished in 1m 4.13s (- 14m 12.05s) (7 7.0%). train avg loss: 0.1202\n",
      "Training for epoch 8 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 8 finished in 1m 11.7s (- 13m 44.56s) (8 8.0%). train avg loss: 0.1111\n",
      "Training for epoch 9 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 9 finished in 1m 19.53s (- 13m 24.17s) (9 9.0%). train avg loss: 0.1142\n",
      "Training for epoch 10 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 10 finished in 1m 27.92s (- 13m 11.24s) (10 10.0%). train avg loss: 0.0894\n",
      "Training for epoch 11 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 11 finished in 1m 36.46s (- 13m 0.46s) (11 11.0%). train avg loss: 0.0706\n",
      "Training for epoch 12 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 12 finished in 1m 44.33s (- 12m 45.06s) (12 12.0%). train avg loss: 0.0949\n",
      "Training for epoch 13 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 13 finished in 1m 52.22s (- 12m 30.98s) (13 13.0%). train avg loss: 0.0867\n",
      "Training for epoch 14 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 14 finished in 1m 59.61s (- 12m 14.73s) (14 14.0%). train avg loss: 0.0678\n",
      "Training for epoch 15 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 15 finished in 2m 8.04s (- 12m 5.55s) (15 15.0%). train avg loss: 0.0547\n",
      "Training for epoch 16 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 16 finished in 2m 16.28s (- 11m 55.45s) (16 16.0%). train avg loss: 0.092\n",
      "Training for epoch 17 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 17 finished in 2m 23.74s (- 11m 41.78s) (17 17.0%). train avg loss: 0.0671\n",
      "Training for epoch 18 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 18 finished in 2m 31.31s (- 11m 29.29s) (18 18.0%). train avg loss: 0.0568\n",
      "Training for epoch 19 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 19 finished in 2m 39.25s (- 11m 18.91s) (19 19.0%). train avg loss: 0.0576\n",
      "Training for epoch 20 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 20 finished in 2m 48.07s (- 11m 12.29s) (20 20.0%). train avg loss: 0.0611\n",
      "Training for epoch 21 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 21 finished in 2m 56.48s (- 11m 3.9s) (21 21.0%). train avg loss: 0.0496\n",
      "Training for epoch 22 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 22 finished in 3m 4.4s (- 10m 53.77s) (22 22.0%). train avg loss: 0.0604\n",
      "Training for epoch 23 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 23 finished in 3m 12.98s (- 10m 46.06s) (23 23.0%). train avg loss: 0.0849\n",
      "Training for epoch 24 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 24 finished in 3m 21.68s (- 10m 38.64s) (24 24.0%). train avg loss: 0.087\n",
      "Training for epoch 25 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 25 finished in 3m 30.76s (- 10m 32.29s) (25 25.0%). train avg loss: 0.061\n",
      "Training for epoch 26 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 26 finished in 3m 39.73s (- 10m 25.39s) (26 26.0%). train avg loss: 0.0589\n",
      "Training for epoch 27 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 27 finished in 3m 48.22s (- 10m 17.04s) (27 27.0%). train avg loss: 0.0499\n",
      "Training for epoch 28 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 28 finished in 3m 57.03s (- 10m 9.51s) (28 28.0%). train avg loss: 0.0793\n",
      "Training for epoch 29 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 29 finished in 4m 4.81s (- 9m 59.35s) (29 29.0%). train avg loss: 0.0795\n",
      "Training for epoch 30 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 30 finished in 4m 12.67s (- 9m 49.57s) (30 30.0%). train avg loss: 0.0465\n",
      "Training for epoch 31 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 31 finished in 4m 20.01s (- 9m 38.74s) (31 31.0%). train avg loss: 0.0489\n",
      "Training for epoch 32 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 32 finished in 4m 27.05s (- 9m 27.47s) (32 32.0%). train avg loss: 0.0514\n",
      "Training for epoch 33 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 33 finished in 4m 34.31s (- 9m 16.92s) (33 33.0%). train avg loss: 0.0391\n",
      "Training for epoch 34 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 34 finished in 4m 42.58s (- 9m 8.54s) (34 34.0%). train avg loss: 0.0403\n",
      "Training for epoch 35 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 35 finished in 4m 50.22s (- 8m 58.99s) (35 35.0%). train avg loss: 0.0367\n",
      "Training for epoch 36 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 36 finished in 4m 57.99s (- 8m 49.77s) (36 36.0%). train avg loss: 0.0377\n",
      "Training for epoch 37 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 37 finished in 5m 5.23s (- 8m 39.71s) (37 37.0%). train avg loss: 0.0365\n",
      "Training for epoch 38 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 38 finished in 5m 12.42s (- 8m 29.73s) (38 38.0%). train avg loss: 0.034\n",
      "Training for epoch 39 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 39 finished in 5m 19.8s (- 8m 20.2s) (39 39.0%). train avg loss: 0.0365\n",
      "Training for epoch 40 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 40 finished in 5m 27.31s (- 8m 10.96s) (40 40.0%). train avg loss: 0.0448\n",
      "Training for epoch 41 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 41 finished in 5m 34.69s (- 8m 1.63s) (41 41.0%). train avg loss: 0.1122\n",
      "Training for epoch 42 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 42 finished in 5m 42.36s (- 7m 52.78s) (42 42.0%). train avg loss: 0.0458\n",
      "Training for epoch 43 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 43 finished in 5m 50.44s (- 7m 44.54s) (43 43.0%). train avg loss: 0.035\n",
      "Training for epoch 44 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 44 finished in 5m 57.72s (- 7m 35.28s) (44 44.0%). train avg loss: 0.0475\n",
      "Training for epoch 45 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 45 finished in 6m 5.39s (- 7m 26.58s) (45 45.0%). train avg loss: 0.0434\n",
      "Training for epoch 46 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 46 finished in 6m 13.76s (- 7m 18.76s) (46 46.0%). train avg loss: 0.0346\n",
      "Training for epoch 47 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 47 finished in 6m 21.08s (- 7m 9.73s) (47 47.0%). train avg loss: 0.0345\n",
      "Training for epoch 48 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 48 finished in 6m 28.37s (- 7m 0.73s) (48 48.0%). train avg loss: 0.0344\n",
      "Training for epoch 49 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 49 finished in 6m 35.77s (- 6m 51.92s) (49 49.0%). train avg loss: 0.0396\n",
      "Training for epoch 50 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 50 finished in 6m 43.25s (- 6m 43.25s) (50 50.0%). train avg loss: 0.0323\n",
      "Training for epoch 51 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 51 finished in 6m 50.84s (- 6m 34.73s) (51 51.0%). train avg loss: 0.0274\n",
      "Training for epoch 52 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 52 finished in 6m 59.09s (- 6m 26.85s) (52 52.0%). train avg loss: 0.0291\n",
      "Training for epoch 53 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 53 finished in 7m 7.17s (- 6m 18.81s) (53 53.0%). train avg loss: 0.0287\n",
      "Training for epoch 54 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 54 finished in 7m 14.74s (- 6m 10.34s) (54 54.0%). train avg loss: 0.0286\n",
      "Training for epoch 55 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 55 finished in 7m 22.46s (- 6m 2.01s) (55 55.0%). train avg loss: 0.027\n",
      "Training for epoch 56 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 56 finished in 7m 30.61s (- 5m 54.05s) (56 56.0%). train avg loss: 0.0289\n",
      "Training for epoch 57 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 57 finished in 7m 39.59s (- 5m 46.71s) (57 57.0%). train avg loss: 0.0267\n",
      "Training for epoch 58 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 58 finished in 7m 47.49s (- 5m 38.53s) (58 58.0%). train avg loss: 0.027\n",
      "Training for epoch 59 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 59 finished in 7m 55.21s (- 5m 30.23s) (59 59.0%). train avg loss: 0.0249\n",
      "Training for epoch 60 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 60 finished in 8m 2.81s (- 5m 21.87s) (60 60.0%). train avg loss: 0.026\n",
      "Training for epoch 61 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 61 finished in 8m 10.81s (- 5m 13.8s) (61 61.0%). train avg loss: 0.0273\n",
      "Training for epoch 62 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 62 finished in 8m 20.0s (- 5m 6.45s) (62 62.0%). train avg loss: 0.0322\n",
      "Training for epoch 63 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 63 finished in 8m 27.69s (- 4m 58.17s) (63 63.0%). train avg loss: 0.0268\n",
      "Training for epoch 64 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 64 finished in 8m 35.26s (- 4m 49.84s) (64 64.0%). train avg loss: 0.0237\n",
      "Training for epoch 65 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 65 finished in 8m 43.01s (- 4m 41.62s) (65 65.0%). train avg loss: 0.024\n",
      "Training for epoch 66 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 66 finished in 8m 51.02s (- 4m 33.56s) (66 66.0%). train avg loss: 0.0246\n",
      "Training for epoch 67 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 67 finished in 8m 59.82s (- 4m 25.88s) (67 67.0%). train avg loss: 0.0238\n",
      "Training for epoch 68 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 68 finished in 9m 7.65s (- 4m 17.72s) (68 68.0%). train avg loss: 0.0238\n",
      "Training for epoch 69 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 69 finished in 9m 15.21s (- 4m 9.44s) (69 69.0%). train avg loss: 0.0252\n",
      "Training for epoch 70 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 70 finished in 9m 22.67s (- 4m 1.15s) (70 70.0%). train avg loss: 0.024\n",
      "Training for epoch 71 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 71 finished in 9m 30.84s (- 3m 53.16s) (71 71.0%). train avg loss: 0.0249\n",
      "Training for epoch 72 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 72 finished in 9m 39.69s (- 3m 45.43s) (72 72.0%). train avg loss: 0.0234\n",
      "Training for epoch 73 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 73 finished in 9m 46.94s (- 3m 37.09s) (73 73.0%). train avg loss: 0.0276\n",
      "Training for epoch 74 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 74 finished in 9m 54.06s (- 3m 28.73s) (74 74.0%). train avg loss: 0.0329\n",
      "Training for epoch 75 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 75 finished in 10m 1.25s (- 3m 20.42s) (75 75.0%). train avg loss: 0.0277\n",
      "Training for epoch 76 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 76 finished in 10m 8.88s (- 3m 12.28s) (76 76.0%). train avg loss: 0.0233\n",
      "Training for epoch 77 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 77 finished in 10m 16.57s (- 3m 4.17s) (77 77.0%). train avg loss: 0.0268\n",
      "Training for epoch 78 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 78 finished in 10m 24.57s (- 2m 56.16s) (78 78.0%). train avg loss: 0.0219\n",
      "Training for epoch 79 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 79 finished in 10m 31.65s (- 2m 47.91s) (79 79.0%). train avg loss: 0.0236\n",
      "Training for epoch 80 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 80 finished in 10m 38.67s (- 2m 39.67s) (80 80.0%). train avg loss: 0.0213\n",
      "Training for epoch 81 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 81 finished in 10m 46.33s (- 2m 31.61s) (81 81.0%). train avg loss: 0.0233\n",
      "Training for epoch 82 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 82 finished in 10m 53.83s (- 2m 23.52s) (82 82.0%). train avg loss: 0.0203\n",
      "Training for epoch 83 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 83 finished in 11m 1.35s (- 2m 15.46s) (83 83.0%). train avg loss: 0.0241\n",
      "Training for epoch 84 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 84 finished in 11m 8.68s (- 2m 7.37s) (84 84.0%). train avg loss: 0.0223\n",
      "Training for epoch 85 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 85 finished in 11m 16.17s (- 1m 59.32s) (85 85.0%). train avg loss: 0.0214\n",
      "Training for epoch 86 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 86 finished in 11m 23.47s (- 1m 51.26s) (86 86.0%). train avg loss: 0.0222\n",
      "Training for epoch 87 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 87 finished in 11m 31.2s (- 1m 43.28s) (87 87.0%). train avg loss: 0.0239\n",
      "Training for epoch 88 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 88 finished in 11m 39.1s (- 1m 35.33s) (88 88.0%). train avg loss: 0.023\n",
      "Training for epoch 89 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 89 finished in 11m 46.01s (- 1m 27.26s) (89 89.0%). train avg loss: 0.0221\n",
      "Training for epoch 90 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 90 finished in 11m 53.56s (- 1m 19.28s) (90 90.0%). train avg loss: 0.0202\n",
      "Training for epoch 91 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 91 finished in 12m 0.99s (- 1m 11.31s) (91 91.0%). train avg loss: 0.0218\n",
      "Training for epoch 92 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 92 finished in 12m 8.42s (- 1m 3.34s) (92 92.0%). train avg loss: 0.0204\n",
      "Training for epoch 93 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 93 finished in 12m 15.97s (- 0m 55.4s) (93 93.0%). train avg loss: 0.0201\n",
      "Training for epoch 94 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 94 finished in 12m 23.44s (- 0m 47.45s) (94 94.0%). train avg loss: 0.0217\n",
      "Training for epoch 95 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 95 finished in 12m 30.86s (- 0m 39.52s) (95 95.0%). train avg loss: 0.0294\n",
      "Training for epoch 96 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 96 finished in 12m 37.9s (- 0m 31.58s) (96 96.0%). train avg loss: 0.0228\n",
      "Training for epoch 97 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 97 finished in 12m 45.35s (- 0m 23.67s) (97 97.0%). train avg loss: 0.0213\n",
      "Training for epoch 98 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 98 finished in 12m 53.04s (- 0m 15.78s) (98 98.0%). train avg loss: 0.019\n",
      "Training for epoch 99 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 99 finished in 13m 0.52s (- 0m 7.88s) (99 99.0%). train avg loss: 0.0292\n",
      "Training for epoch 100 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 100 finished in 13m 8.24s (- 0m 0.0s) (100 100.0%). train avg loss: 0.0523\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "best_train_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on train loss\n",
    "  encoder_scheduler.step(avg_train_loss)\n",
    "  decoder_scheduler.step(avg_train_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  # Save the model if the train loss is better than the previous iterations' train loss\n",
    "  if avg_train_loss < best_train_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_train_loss = avg_train_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"train\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-7.5, min_value+.1, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 3.3482933225669176%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in test_pairs :\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(test_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> fungsikan\n",
      "= ['F', 'UW', 'NG', 'S', 'IY', 'K', 'AA', 'N']\n",
      "< F UW NG S IY K AA N ['F', 'UW', 'NG', 'S', 'IY', 'K', 'AA', 'N']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f82e256e730>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
