{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn_gen/exp/id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"32\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[-1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[272, 119, 293, 164, 436, 289, 82, 200, 308, 464, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f109d817e20> ([12, 25, 308, 464, 1], [23, 3, 1])\n",
      "([12, 25, 308, 464, 1], [23, 3, 1])\n",
      "([12, 25, 308, 464, 1], [23, 3, 1])\n",
      "train grp 482 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'d\", 7: \"'h\", 8: \"'j\", 9: \"'o\", 10: \"'r\", 11: \"'u\", 12: '--', 13: '-a', 14: '-b', 15: '-c', 16: '-d', 17: '-e', 18: '-g', 19: '-h', 20: '-i', 21: '-j', 22: '-k', 23: '-l', 24: '-m', 25: '-n', 26: '-p', 27: '-q', 28: '-r', 29: '-s', 30: '-t', 31: '-u', 32: '-w', 33: \"a'\", 34: 'a-', 35: 'aa', 36: 'ab', 37: 'ac', 38: 'ad', 39: 'ae', 40: 'af', 41: 'ag', 42: 'ah', 43: 'ai', 44: 'aj', 45: 'ak', 46: 'al', 47: 'am', 48: 'an', 49: 'ao', 50: 'ap', 51: 'aq', 52: 'ar', 53: 'as', 54: 'at', 55: 'au', 56: 'av', 57: 'aw', 58: 'ax', 59: 'ay', 60: 'az', 61: \"b'\", 62: 'ba', 63: 'bb', 64: 'bd', 65: 'be', 66: 'bi', 67: 'bj', 68: 'bk', 69: 'bl', 70: 'bm', 71: 'bn', 72: 'bo', 73: 'br', 74: 'bs', 75: 'bt', 76: 'bu', 77: 'bv', 78: 'by', 79: 'ca', 80: 'ce', 81: 'ch', 82: 'ci', 83: 'cm', 84: 'co', 85: 'cr', 86: 'cu', 87: \"d'\", 88: 'da', 89: 'db', 90: 'de', 91: 'dh', 92: 'di', 93: 'dj', 94: 'dk', 95: 'dm', 96: 'dn', 97: 'do', 98: 'dr', 99: 'ds', 100: 'du', 101: 'dv', 102: 'dw', 103: 'dy', 104: \"e'\", 105: 'e-', 106: 'ea', 107: 'eb', 108: 'ec', 109: 'ed', 110: 'ee', 111: 'ef', 112: 'eg', 113: 'eh', 114: 'ei', 115: 'ej', 116: 'ek', 117: 'el', 118: 'em', 119: 'en', 120: 'eo', 121: 'ep', 122: 'er', 123: 'es', 124: 'et', 125: 'eu', 126: 'ev', 127: 'ew', 128: 'ex', 129: 'ey', 130: 'ez', 131: 'f-', 132: 'fa', 133: 'fd', 134: 'fe', 135: 'fi', 136: 'fk', 137: 'fl', 138: 'fo', 139: 'fr', 140: 'fs', 141: 'ft', 142: 'fu', 143: 'fw', 144: 'g-', 145: 'ga', 146: 'gb', 147: 'gc', 148: 'gd', 149: 'ge', 150: 'gf', 151: 'gg', 152: 'gh', 153: 'gi', 154: 'gj', 155: 'gk', 156: 'gl', 157: 'gm', 158: 'gn', 159: 'go', 160: 'gp', 161: 'gr', 162: 'gs', 163: 'gt', 164: 'gu', 165: 'h-', 166: 'ha', 167: 'hb', 168: 'hd', 169: 'he', 170: 'hf', 171: 'hg', 172: 'hi', 173: 'hj', 174: 'hk', 175: 'hl', 176: 'hm', 177: 'hn', 178: 'ho', 179: 'hp', 180: 'hr', 181: 'hs', 182: 'ht', 183: 'hu', 184: 'hw', 185: 'hy', 186: 'i-', 187: 'ia', 188: 'ib', 189: 'ic', 190: 'id', 191: 'ie', 192: 'if', 193: 'ig', 194: 'ih', 195: 'ii', 196: 'ij', 197: 'ik', 198: 'il', 199: 'im', 200: 'in', 201: 'io', 202: 'ip', 203: 'ir', 204: 'is', 205: 'it', 206: 'iu', 207: 'iv', 208: 'iw', 209: 'ix', 210: 'iy', 211: 'iz', 212: 'ja', 213: 'je', 214: 'ji', 215: 'jl', 216: 'jn', 217: 'jo', 218: 'jr', 219: 'jt', 220: 'ju', 221: 'jw', 222: 'k-', 223: 'ka', 224: 'kb', 225: 'kc', 226: 'kd', 227: 'ke', 228: 'kh', 229: 'ki', 230: 'kj', 231: 'kk', 232: 'kl', 233: 'km', 234: 'kn', 235: 'ko', 236: 'kp', 237: 'kr', 238: 'ks', 239: 'kt', 240: 'ku', 241: 'kw', 242: 'ky', 243: 'kz', 244: 'l-', 245: 'la', 246: 'lb', 247: 'ld', 248: 'le', 249: 'lf', 250: 'lg', 251: 'lh', 252: 'li', 253: 'lj', 254: 'lk', 255: 'll', 256: 'lm', 257: 'ln', 258: 'lo', 259: 'lp', 260: 'lq', 261: 'lr', 262: 'ls', 263: 'lt', 264: 'lu', 265: 'lv', 266: 'lw', 267: 'ly', 268: \"m'\", 269: 'ma', 270: 'mb', 271: 'mc', 272: 'me', 273: 'mf', 274: 'mi', 275: 'mk', 276: 'ml', 277: 'mm', 278: 'mn', 279: 'mo', 280: 'mp', 281: 'mr', 282: 'ms', 283: 'mt', 284: 'mu', 285: 'mz', 286: 'n-', 287: 'na', 288: 'nb', 289: 'nc', 290: 'nd', 291: 'ne', 292: 'nf', 293: 'ng', 294: 'nh', 295: 'ni', 296: 'nj', 297: 'nk', 298: 'nl', 299: 'nm', 300: 'nn', 301: 'no', 302: 'np', 303: 'ns', 304: 'nt', 305: 'nu', 306: 'nv', 307: 'nw', 308: 'ny', 309: 'nz', 310: \"o'\", 311: 'o-', 312: 'oa', 313: 'ob', 314: 'oc', 315: 'od', 316: 'oe', 317: 'of', 318: 'og', 319: 'oh', 320: 'oi', 321: 'oj', 322: 'ok', 323: 'ol', 324: 'om', 325: 'on', 326: 'oo', 327: 'op', 328: 'or', 329: 'os', 330: 'ot', 331: 'ov', 332: 'ow', 333: 'ox', 334: 'oy', 335: 'oz', 336: 'pa', 337: 'pc', 338: 'pe', 339: 'pg', 340: 'pi', 341: 'pj', 342: 'pk', 343: 'pl', 344: 'pm', 345: 'pn', 346: 'po', 347: 'pr', 348: 'ps', 349: 'pt', 350: 'pu', 351: 'py', 352: 'qi', 353: 'qu', 354: \"r'\", 355: 'r-', 356: 'ra', 357: 'rb', 358: 'rc', 359: 'rd', 360: 're', 361: 'rf', 362: 'rg', 363: 'rh', 364: 'ri', 365: 'rj', 366: 'rk', 367: 'rl', 368: 'rm', 369: 'rn', 370: 'ro', 371: 'rp', 372: 'rr', 373: 'rs', 374: 'rt', 375: 'ru', 376: 'rv', 377: 'rw', 378: 'ry', 379: 'rz', 380: 's-', 381: 'sa', 382: 'sb', 383: 'sc', 384: 'sd', 385: 'se', 386: 'sf', 387: 'sh', 388: 'si', 389: 'sj', 390: 'sk', 391: 'sl', 392: 'sm', 393: 'sn', 394: 'so', 395: 'sp', 396: 'sr', 397: 'ss', 398: 'st', 399: 'su', 400: 'sw', 401: 'sy', 402: 't-', 403: 'ta', 404: 'tb', 405: 'te', 406: 'tf', 407: 'tg', 408: 'th', 409: 'ti', 410: 'tk', 411: 'tl', 412: 'tm', 413: 'tn', 414: 'to', 415: 'tp', 416: 'tr', 417: 'ts', 418: 'tt', 419: 'tu', 420: 'tw', 421: \"u'\", 422: 'u-', 423: 'ua', 424: 'ub', 425: 'uc', 426: 'ud', 427: 'ue', 428: 'uf', 429: 'ug', 430: 'uh', 431: 'ui', 432: 'uj', 433: 'uk', 434: 'ul', 435: 'um', 436: 'un', 437: 'uo', 438: 'up', 439: 'ur', 440: 'us', 441: 'ut', 442: 'uv', 443: 'uw', 444: 'uy', 445: 'uz', 446: 'va', 447: 've', 448: 'vg', 449: 'vi', 450: 'vo', 451: 'vu', 452: 'wa', 453: 'we', 454: 'wi', 455: 'wo', 456: 'wt', 457: 'wu', 458: 'xa', 459: 'xe', 460: 'xi', 461: 'xp', 462: 'xt', 463: 'xu', 464: 'ya', 465: 'ye', 466: 'yg', 467: 'yh', 468: 'yi', 469: 'yo', 470: 'yr', 471: 'yt', 472: 'yu', 473: 'za', 474: 'zb', 475: 'ze', 476: 'zh', 477: 'zi', 478: 'zm', 479: 'zo', 480: 'zu', 481: 'zz'}\n",
      "valid grp 482 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'d\", 7: \"'h\", 8: \"'j\", 9: \"'o\", 10: \"'r\", 11: \"'u\", 12: '--', 13: '-a', 14: '-b', 15: '-c', 16: '-d', 17: '-e', 18: '-g', 19: '-h', 20: '-i', 21: '-j', 22: '-k', 23: '-l', 24: '-m', 25: '-n', 26: '-p', 27: '-q', 28: '-r', 29: '-s', 30: '-t', 31: '-u', 32: '-w', 33: \"a'\", 34: 'a-', 35: 'aa', 36: 'ab', 37: 'ac', 38: 'ad', 39: 'ae', 40: 'af', 41: 'ag', 42: 'ah', 43: 'ai', 44: 'aj', 45: 'ak', 46: 'al', 47: 'am', 48: 'an', 49: 'ao', 50: 'ap', 51: 'aq', 52: 'ar', 53: 'as', 54: 'at', 55: 'au', 56: 'av', 57: 'aw', 58: 'ax', 59: 'ay', 60: 'az', 61: \"b'\", 62: 'ba', 63: 'bb', 64: 'bd', 65: 'be', 66: 'bi', 67: 'bj', 68: 'bk', 69: 'bl', 70: 'bm', 71: 'bn', 72: 'bo', 73: 'br', 74: 'bs', 75: 'bt', 76: 'bu', 77: 'bv', 78: 'by', 79: 'ca', 80: 'ce', 81: 'ch', 82: 'ci', 83: 'cm', 84: 'co', 85: 'cr', 86: 'cu', 87: \"d'\", 88: 'da', 89: 'db', 90: 'de', 91: 'dh', 92: 'di', 93: 'dj', 94: 'dk', 95: 'dm', 96: 'dn', 97: 'do', 98: 'dr', 99: 'ds', 100: 'du', 101: 'dv', 102: 'dw', 103: 'dy', 104: \"e'\", 105: 'e-', 106: 'ea', 107: 'eb', 108: 'ec', 109: 'ed', 110: 'ee', 111: 'ef', 112: 'eg', 113: 'eh', 114: 'ei', 115: 'ej', 116: 'ek', 117: 'el', 118: 'em', 119: 'en', 120: 'eo', 121: 'ep', 122: 'er', 123: 'es', 124: 'et', 125: 'eu', 126: 'ev', 127: 'ew', 128: 'ex', 129: 'ey', 130: 'ez', 131: 'f-', 132: 'fa', 133: 'fd', 134: 'fe', 135: 'fi', 136: 'fk', 137: 'fl', 138: 'fo', 139: 'fr', 140: 'fs', 141: 'ft', 142: 'fu', 143: 'fw', 144: 'g-', 145: 'ga', 146: 'gb', 147: 'gc', 148: 'gd', 149: 'ge', 150: 'gf', 151: 'gg', 152: 'gh', 153: 'gi', 154: 'gj', 155: 'gk', 156: 'gl', 157: 'gm', 158: 'gn', 159: 'go', 160: 'gp', 161: 'gr', 162: 'gs', 163: 'gt', 164: 'gu', 165: 'h-', 166: 'ha', 167: 'hb', 168: 'hd', 169: 'he', 170: 'hf', 171: 'hg', 172: 'hi', 173: 'hj', 174: 'hk', 175: 'hl', 176: 'hm', 177: 'hn', 178: 'ho', 179: 'hp', 180: 'hr', 181: 'hs', 182: 'ht', 183: 'hu', 184: 'hw', 185: 'hy', 186: 'i-', 187: 'ia', 188: 'ib', 189: 'ic', 190: 'id', 191: 'ie', 192: 'if', 193: 'ig', 194: 'ih', 195: 'ii', 196: 'ij', 197: 'ik', 198: 'il', 199: 'im', 200: 'in', 201: 'io', 202: 'ip', 203: 'ir', 204: 'is', 205: 'it', 206: 'iu', 207: 'iv', 208: 'iw', 209: 'ix', 210: 'iy', 211: 'iz', 212: 'ja', 213: 'je', 214: 'ji', 215: 'jl', 216: 'jn', 217: 'jo', 218: 'jr', 219: 'jt', 220: 'ju', 221: 'jw', 222: 'k-', 223: 'ka', 224: 'kb', 225: 'kc', 226: 'kd', 227: 'ke', 228: 'kh', 229: 'ki', 230: 'kj', 231: 'kk', 232: 'kl', 233: 'km', 234: 'kn', 235: 'ko', 236: 'kp', 237: 'kr', 238: 'ks', 239: 'kt', 240: 'ku', 241: 'kw', 242: 'ky', 243: 'kz', 244: 'l-', 245: 'la', 246: 'lb', 247: 'ld', 248: 'le', 249: 'lf', 250: 'lg', 251: 'lh', 252: 'li', 253: 'lj', 254: 'lk', 255: 'll', 256: 'lm', 257: 'ln', 258: 'lo', 259: 'lp', 260: 'lq', 261: 'lr', 262: 'ls', 263: 'lt', 264: 'lu', 265: 'lv', 266: 'lw', 267: 'ly', 268: \"m'\", 269: 'ma', 270: 'mb', 271: 'mc', 272: 'me', 273: 'mf', 274: 'mi', 275: 'mk', 276: 'ml', 277: 'mm', 278: 'mn', 279: 'mo', 280: 'mp', 281: 'mr', 282: 'ms', 283: 'mt', 284: 'mu', 285: 'mz', 286: 'n-', 287: 'na', 288: 'nb', 289: 'nc', 290: 'nd', 291: 'ne', 292: 'nf', 293: 'ng', 294: 'nh', 295: 'ni', 296: 'nj', 297: 'nk', 298: 'nl', 299: 'nm', 300: 'nn', 301: 'no', 302: 'np', 303: 'ns', 304: 'nt', 305: 'nu', 306: 'nv', 307: 'nw', 308: 'ny', 309: 'nz', 310: \"o'\", 311: 'o-', 312: 'oa', 313: 'ob', 314: 'oc', 315: 'od', 316: 'oe', 317: 'of', 318: 'og', 319: 'oh', 320: 'oi', 321: 'oj', 322: 'ok', 323: 'ol', 324: 'om', 325: 'on', 326: 'oo', 327: 'op', 328: 'or', 329: 'os', 330: 'ot', 331: 'ov', 332: 'ow', 333: 'ox', 334: 'oy', 335: 'oz', 336: 'pa', 337: 'pc', 338: 'pe', 339: 'pg', 340: 'pi', 341: 'pj', 342: 'pk', 343: 'pl', 344: 'pm', 345: 'pn', 346: 'po', 347: 'pr', 348: 'ps', 349: 'pt', 350: 'pu', 351: 'py', 352: 'qi', 353: 'qu', 354: \"r'\", 355: 'r-', 356: 'ra', 357: 'rb', 358: 'rc', 359: 'rd', 360: 're', 361: 'rf', 362: 'rg', 363: 'rh', 364: 'ri', 365: 'rj', 366: 'rk', 367: 'rl', 368: 'rm', 369: 'rn', 370: 'ro', 371: 'rp', 372: 'rr', 373: 'rs', 374: 'rt', 375: 'ru', 376: 'rv', 377: 'rw', 378: 'ry', 379: 'rz', 380: 's-', 381: 'sa', 382: 'sb', 383: 'sc', 384: 'sd', 385: 'se', 386: 'sf', 387: 'sh', 388: 'si', 389: 'sj', 390: 'sk', 391: 'sl', 392: 'sm', 393: 'sn', 394: 'so', 395: 'sp', 396: 'sr', 397: 'ss', 398: 'st', 399: 'su', 400: 'sw', 401: 'sy', 402: 't-', 403: 'ta', 404: 'tb', 405: 'te', 406: 'tf', 407: 'tg', 408: 'th', 409: 'ti', 410: 'tk', 411: 'tl', 412: 'tm', 413: 'tn', 414: 'to', 415: 'tp', 416: 'tr', 417: 'ts', 418: 'tt', 419: 'tu', 420: 'tw', 421: \"u'\", 422: 'u-', 423: 'ua', 424: 'ub', 425: 'uc', 426: 'ud', 427: 'ue', 428: 'uf', 429: 'ug', 430: 'uh', 431: 'ui', 432: 'uj', 433: 'uk', 434: 'ul', 435: 'um', 436: 'un', 437: 'uo', 438: 'up', 439: 'ur', 440: 'us', 441: 'ut', 442: 'uv', 443: 'uw', 444: 'uy', 445: 'uz', 446: 'va', 447: 've', 448: 'vg', 449: 'vi', 450: 'vo', 451: 'vu', 452: 'wa', 453: 'we', 454: 'wi', 455: 'wo', 456: 'wt', 457: 'wu', 458: 'xa', 459: 'xe', 460: 'xi', 461: 'xp', 462: 'xt', 463: 'xu', 464: 'ya', 465: 'ye', 466: 'yg', 467: 'yh', 468: 'yi', 469: 'yo', 470: 'yr', 471: 'yt', 472: 'yu', 473: 'za', 474: 'zb', 475: 'ze', 476: 'zh', 477: 'zi', 478: 'zm', 479: 'zo', 480: 'zu', 481: 'zz'}\n",
      "test grp 482 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'d\", 7: \"'h\", 8: \"'j\", 9: \"'o\", 10: \"'r\", 11: \"'u\", 12: '--', 13: '-a', 14: '-b', 15: '-c', 16: '-d', 17: '-e', 18: '-g', 19: '-h', 20: '-i', 21: '-j', 22: '-k', 23: '-l', 24: '-m', 25: '-n', 26: '-p', 27: '-q', 28: '-r', 29: '-s', 30: '-t', 31: '-u', 32: '-w', 33: \"a'\", 34: 'a-', 35: 'aa', 36: 'ab', 37: 'ac', 38: 'ad', 39: 'ae', 40: 'af', 41: 'ag', 42: 'ah', 43: 'ai', 44: 'aj', 45: 'ak', 46: 'al', 47: 'am', 48: 'an', 49: 'ao', 50: 'ap', 51: 'aq', 52: 'ar', 53: 'as', 54: 'at', 55: 'au', 56: 'av', 57: 'aw', 58: 'ax', 59: 'ay', 60: 'az', 61: \"b'\", 62: 'ba', 63: 'bb', 64: 'bd', 65: 'be', 66: 'bi', 67: 'bj', 68: 'bk', 69: 'bl', 70: 'bm', 71: 'bn', 72: 'bo', 73: 'br', 74: 'bs', 75: 'bt', 76: 'bu', 77: 'bv', 78: 'by', 79: 'ca', 80: 'ce', 81: 'ch', 82: 'ci', 83: 'cm', 84: 'co', 85: 'cr', 86: 'cu', 87: \"d'\", 88: 'da', 89: 'db', 90: 'de', 91: 'dh', 92: 'di', 93: 'dj', 94: 'dk', 95: 'dm', 96: 'dn', 97: 'do', 98: 'dr', 99: 'ds', 100: 'du', 101: 'dv', 102: 'dw', 103: 'dy', 104: \"e'\", 105: 'e-', 106: 'ea', 107: 'eb', 108: 'ec', 109: 'ed', 110: 'ee', 111: 'ef', 112: 'eg', 113: 'eh', 114: 'ei', 115: 'ej', 116: 'ek', 117: 'el', 118: 'em', 119: 'en', 120: 'eo', 121: 'ep', 122: 'er', 123: 'es', 124: 'et', 125: 'eu', 126: 'ev', 127: 'ew', 128: 'ex', 129: 'ey', 130: 'ez', 131: 'f-', 132: 'fa', 133: 'fd', 134: 'fe', 135: 'fi', 136: 'fk', 137: 'fl', 138: 'fo', 139: 'fr', 140: 'fs', 141: 'ft', 142: 'fu', 143: 'fw', 144: 'g-', 145: 'ga', 146: 'gb', 147: 'gc', 148: 'gd', 149: 'ge', 150: 'gf', 151: 'gg', 152: 'gh', 153: 'gi', 154: 'gj', 155: 'gk', 156: 'gl', 157: 'gm', 158: 'gn', 159: 'go', 160: 'gp', 161: 'gr', 162: 'gs', 163: 'gt', 164: 'gu', 165: 'h-', 166: 'ha', 167: 'hb', 168: 'hd', 169: 'he', 170: 'hf', 171: 'hg', 172: 'hi', 173: 'hj', 174: 'hk', 175: 'hl', 176: 'hm', 177: 'hn', 178: 'ho', 179: 'hp', 180: 'hr', 181: 'hs', 182: 'ht', 183: 'hu', 184: 'hw', 185: 'hy', 186: 'i-', 187: 'ia', 188: 'ib', 189: 'ic', 190: 'id', 191: 'ie', 192: 'if', 193: 'ig', 194: 'ih', 195: 'ii', 196: 'ij', 197: 'ik', 198: 'il', 199: 'im', 200: 'in', 201: 'io', 202: 'ip', 203: 'ir', 204: 'is', 205: 'it', 206: 'iu', 207: 'iv', 208: 'iw', 209: 'ix', 210: 'iy', 211: 'iz', 212: 'ja', 213: 'je', 214: 'ji', 215: 'jl', 216: 'jn', 217: 'jo', 218: 'jr', 219: 'jt', 220: 'ju', 221: 'jw', 222: 'k-', 223: 'ka', 224: 'kb', 225: 'kc', 226: 'kd', 227: 'ke', 228: 'kh', 229: 'ki', 230: 'kj', 231: 'kk', 232: 'kl', 233: 'km', 234: 'kn', 235: 'ko', 236: 'kp', 237: 'kr', 238: 'ks', 239: 'kt', 240: 'ku', 241: 'kw', 242: 'ky', 243: 'kz', 244: 'l-', 245: 'la', 246: 'lb', 247: 'ld', 248: 'le', 249: 'lf', 250: 'lg', 251: 'lh', 252: 'li', 253: 'lj', 254: 'lk', 255: 'll', 256: 'lm', 257: 'ln', 258: 'lo', 259: 'lp', 260: 'lq', 261: 'lr', 262: 'ls', 263: 'lt', 264: 'lu', 265: 'lv', 266: 'lw', 267: 'ly', 268: \"m'\", 269: 'ma', 270: 'mb', 271: 'mc', 272: 'me', 273: 'mf', 274: 'mi', 275: 'mk', 276: 'ml', 277: 'mm', 278: 'mn', 279: 'mo', 280: 'mp', 281: 'mr', 282: 'ms', 283: 'mt', 284: 'mu', 285: 'mz', 286: 'n-', 287: 'na', 288: 'nb', 289: 'nc', 290: 'nd', 291: 'ne', 292: 'nf', 293: 'ng', 294: 'nh', 295: 'ni', 296: 'nj', 297: 'nk', 298: 'nl', 299: 'nm', 300: 'nn', 301: 'no', 302: 'np', 303: 'ns', 304: 'nt', 305: 'nu', 306: 'nv', 307: 'nw', 308: 'ny', 309: 'nz', 310: \"o'\", 311: 'o-', 312: 'oa', 313: 'ob', 314: 'oc', 315: 'od', 316: 'oe', 317: 'of', 318: 'og', 319: 'oh', 320: 'oi', 321: 'oj', 322: 'ok', 323: 'ol', 324: 'om', 325: 'on', 326: 'oo', 327: 'op', 328: 'or', 329: 'os', 330: 'ot', 331: 'ov', 332: 'ow', 333: 'ox', 334: 'oy', 335: 'oz', 336: 'pa', 337: 'pc', 338: 'pe', 339: 'pg', 340: 'pi', 341: 'pj', 342: 'pk', 343: 'pl', 344: 'pm', 345: 'pn', 346: 'po', 347: 'pr', 348: 'ps', 349: 'pt', 350: 'pu', 351: 'py', 352: 'qi', 353: 'qu', 354: \"r'\", 355: 'r-', 356: 'ra', 357: 'rb', 358: 'rc', 359: 'rd', 360: 're', 361: 'rf', 362: 'rg', 363: 'rh', 364: 'ri', 365: 'rj', 366: 'rk', 367: 'rl', 368: 'rm', 369: 'rn', 370: 'ro', 371: 'rp', 372: 'rr', 373: 'rs', 374: 'rt', 375: 'ru', 376: 'rv', 377: 'rw', 378: 'ry', 379: 'rz', 380: 's-', 381: 'sa', 382: 'sb', 383: 'sc', 384: 'sd', 385: 'se', 386: 'sf', 387: 'sh', 388: 'si', 389: 'sj', 390: 'sk', 391: 'sl', 392: 'sm', 393: 'sn', 394: 'so', 395: 'sp', 396: 'sr', 397: 'ss', 398: 'st', 399: 'su', 400: 'sw', 401: 'sy', 402: 't-', 403: 'ta', 404: 'tb', 405: 'te', 406: 'tf', 407: 'tg', 408: 'th', 409: 'ti', 410: 'tk', 411: 'tl', 412: 'tm', 413: 'tn', 414: 'to', 415: 'tp', 416: 'tr', 417: 'ts', 418: 'tt', 419: 'tu', 420: 'tw', 421: \"u'\", 422: 'u-', 423: 'ua', 424: 'ub', 425: 'uc', 426: 'ud', 427: 'ue', 428: 'uf', 429: 'ug', 430: 'uh', 431: 'ui', 432: 'uj', 433: 'uk', 434: 'ul', 435: 'um', 436: 'un', 437: 'uo', 438: 'up', 439: 'ur', 440: 'us', 441: 'ut', 442: 'uv', 443: 'uw', 444: 'uy', 445: 'uz', 446: 'va', 447: 've', 448: 'vg', 449: 'vi', 450: 'vo', 451: 'vu', 452: 'wa', 453: 'we', 454: 'wi', 455: 'wo', 456: 'wt', 457: 'wu', 458: 'xa', 459: 'xe', 460: 'xi', 461: 'xp', 462: 'xt', 463: 'xu', 464: 'ya', 465: 'ye', 466: 'yg', 467: 'yh', 468: 'yi', 469: 'yo', 470: 'yr', 471: 'yt', 472: 'yu', 473: 'za', 474: 'zb', 475: 'ze', 476: 'zh', 477: 'zi', 478: 'zm', 479: 'zo', 480: 'zu', 481: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "478 {'--': 12, '-n': 25, 'ny': 308, 'ya': 464, '-b': 14, 'ba': 62, 'au': 55, 'ur': 439, 'be': 65, 'el': 117, 'la': 245, 'as': 53, '-c': 15, 'co': 84, 'om': 324, 'mp': 280, 'pe': 338, 'en': 119, 'ng': 293, '-k': 22, 'ka': 223, 'an': 48, '-l': 23, 'ah': 42, 'le': 248, 'gg': 151, 'ga': 145, '-m': 24, 'ma': 269, 'ha': 166, 'si': 388, 'is': 204, 'sw': 400, 'wa': 452, '-s': 29, 'se': 385, 'ed': 109, 'da': 88, 'ar': 52, 'ra': 356, 'ep': 121, 'pi': 340, 'it': 205, '-t': 30, 'ta': 403, 'ak': 45, '-w': 32, 'at': 54, 'ti': 409, \"a'\": 33, \"'b\": 5, \"'d\": 6, 'du': 100, \"'h\": 7, 'ad': 38, 'ab': 36, \"b'\": 61, \"'a\": 4, 'a-': 34, '-a': 13, 'di': 92, 'ia': 187, 'ik': 197, 'h-': 165, 'ai': 43, 'im': 199, 'na': 287, 'kt': 239, 'in': 200, 'al': 46, 'ku': 240, 'us': 440, 'gn': 158, 'r-': 355, 'bd': 64, 'do': 97, 'me': 272, 'mi': 274, 'uk': 433, 'ks': 238, 'to': 414, 'or': 328, 'ul': 434, 'er': 122, 'et': 124, 'bi': 66, 'id': 190, 'il': 198, 'io': 201, 'og': 318, 'ge': 149, 'ne': 291, 'es': 123, 'os': 329, 'sf': 386, 'fe': 134, 'ot': 330, 'sa': 381, 'ek': 116, 'tu': 419, 'ri': 364, 'ie': 191, 'bj': 67, 'ja': 212, 'bl': 69, 'ut': 441, 'ps': 348, 'lu': 264, 'bn': 71, 'no': 301, 'rm': 368, 'nu': 305, 'bo': 72, 'oi': 320, 'ol': 323, 'li': 252, 'on': 325, 'em': 118, 'rs': 373, 'rt': 374, 'br': 73, 're': 360, 'ko': 235, 'bs': 74, 'ns': 303, 'nt': 304, 'te': 405, 'ei': 114, 'sm': 392, 'so': 394, 'rb': 357, 'rp': 371, 'pt': 349, 'if': 192, 'st': 398, 'tr': 416, 'su': 399, 'rd': 359, 'bt': 75, 'bu': 76, 'ua': 423, 'uh': 430, 'lh': 251, 'ay': 59, 'un': 436, 'ac': 37, 'ca': 79, 'aw': 57, 'wi': 454, 'ce': 80, 'ci': 82, 'cu': 86, 'um': 435, 'gk': 155, 'aa': 35, 'ag': 41, 'gi': 153, 'iu': 206, 'am': 47, 'ap': 50, 'de': 90, 'eg': 112, 'eh': 113, 'pa': 336, 'uz': 445, 'ze': 475, 'dh': 91, 'he': 169, 'ib': 188, 'ic': 189, 'ig': 193, 'gu': 164, 'kk': 231, 'km': 233, 'mu': 284, 'kn': 234, 'od': 315, 'dr': 98, 'hu': 183, 'rg': 362, 'nd': 290, 'ip': 202, 'po': 346, 'pu': 350, 'ir': 203, 'aj': 44, 'tn': 413, 'ok': 322, 'oh': 319, 'iw': 208, 'gs': 162, 'dj': 93, 'je': 213, 'iv': 207, 'va': 446, 'dm': 95, 'ni': 295, 'op': 327, 'ds': 99, 'dv': 101, 've': 447, 'vi': 449, 'vo': 450, 'ae': 39, 'ro': 370, 'ob': 313, 'gr': 161, 'lo': 258, 'mo': 279, 'ov': 331, 'af': 40, 'fa': 132, 'fd': 133, 'fi': 135, 'fr': 139, 'fs': 140, 'fw': 143, 'ih': 194, 'gl': 156, 'go': 159, 'o-': 311, '-g': 18, 'hi': 172, 'ms': 282, 'hk': 174, 'hl': 175, 'lb': 246, 'lk': 254, 'ki': 229, 'ub': 424, 'hm': 176, 'ho': 178, 'hw': 184, 'lf': 249, 'rn': 369, 'nm': 299, 'ji': 214, 'jn': 216, 'ju': 220, 'uj': 432, 'k-': 222, 'ln': 257, 'kb': 224, 'kh': 228, 'ru': 375, 'ht': 182, 'kl': 232, 'eo': 120, 'kr': 237, 'lt': 263, 'ui': 431, 'up': 438, 'nk': 297, 'kw': 241, 'l-': 244, '-q': 27, 'qu': 353, \"r'\": 354, 'mn': 278, 'lg': 250, 'oj': 321, 'jo': 217, 'tm': 412, 'f-': 131, 'mb': 270, 'ea': 106, 'sy': 401, 'ke': 227, 'll': 255, 'lm': 256, 'rh': 363, 'az': 60, 'za': 473, 'of': 317, 'fo': 138, 'rf': 361, 'lp': 259, 'lq': 260, 'uv': 442, 'lv': 265, 'lw': 266, 'nn': 300, 'hn': 177, 'tl': 411, 'by': 78, 'eb': 107, 'mf': 273, 'mk': 275, 'pl': 343, 'ud': 426, 'mr': 281, 'rk': 366, 'nb': 288, 'nc': 289, 'ew': 127, 'nf': 292, 'gh': 152, 'gp': 160, 'nj': 296, 'ry': 378, 'jl': 215, 'oa': 312, 'pr': 347, 'rr': 372, 'rw': 377, 'i-': 186, 'nz': 309, 'zi': 477, 'ij': 196, 'yi': 468, 'ao': 49, 'th': 408, 'aq': 51, 'qi': 352, 'rc': 358, 'rj': 365, 'rl': 367, 'sb': 382, 'ex': 128, 'xu': 463, 'sh': 387, 'mt': 283, 'sk': 390, 'sl': 391, 'sn': 393, 'sp': 395, 'sr': 396, 'ss': 397, 'yu': 472, 'av': 56, 'vg': 448, 'we': 453, 'wu': 457, 'ax': 58, 'xi': 460, 'ye': 465, 'yo': 469, 'zh': 476, 'iz': 211, 'zm': 478, 'uy': 444, 'ft': 141, 'eu': 125, 'hr': 180, 'kd': 226, 'kp': 236, 'ls': 262, 'rz': 379, 'wo': 455, 'zo': 479, 'ec': 108, 'ej': 115, \"o'\": 310, \"'o\": 9, 'ef': 111, 'tk': 410, 'gt': 163, 'pk': 342, 's-': 380, 'oy': 334, 'uc': 425, 'ug': 429, 'kc': 225, 'gd': 148, 'ue': 427, 'dw': 102, 'fu': 142, 'fl': 137, 'ev': 126, 'jr': 218, 'hf': 170, 'oc': 314, 'tb': 404, 'fk': 136, 'hy': 185, 'rv': 376, 'ez': 130, \"d'\": 87, 'nv': 306, 'ly': 267, 'zu': 480, 'ox': 333, 'oz': 335, 'nw': 307, 'nl': 298, 'ld': 247, 'mc': 271, 'gc': 147, 'pc': 337, 'pg': 339, 'pj': 341, \"e'\": 104, 'oo': 326, 'ch': 81, 'cm': 83, 'g-': 144, 'ow': 332, 'cr': 85, '-d': 16, 'hs': 181, 'ee': 110, 'gm': 157, 'ii': 195, 'ts': 417, 'dk': 94, '-j': 21, 'bk': 68, 'gj': 154, 'iy': 210, 'gb': 146, 'ix': 209, 'tp': 415, 'xp': 461, 'xt': 462, 'mz': 285, 'tt': 418, 'pm': 344, 'pn': 345, 'u-': 422, '-h': 19, 'uf': 428, 'jt': 219, 'uw': 443, 'nh': 294, 'yt': 471, 'zz': 481, 'ml': 276, '-r': 28, 'yh': 467, 'ky': 242, 'py': 351, 'hp': 179, '-p': 26, 'ey': 129, 'oe': 316, 'gf': 150, 'uo': 437, 'dy': 103, 'dn': 96, 'tw': 420, 'sj': 389, 'yg': 466, 'yr': 470, 'zb': 474, 't-': 402, 'hb': 167, 'kj': 230, '-i': 20, '-e': 17, 'hg': 171, 'mm': 277, \"u'\": 421, \"'j\": 8, 'hd': 168, 'bm': 70, 'sc': 383, 'tf': 406, 'e-': 105, 'hj': 173, 'n-': 286, 'lr': 261, 'tg': 407, 'lj': 253, 'bb': 63, 'bv': 77, 'wt': 456, \"'r\": 10, \"'u\": 11, 'db': 89, 'jw': 221, 'kz': 243, 'np': 302, '-u': 31, \"m'\": 268, 'vu': 451, 'xa': 458, 'xe': 459, 'sd': 384}\n",
      "478 {'--': 12, '-n': 25, 'ny': 308, 'ya': 464, '-b': 14, 'ba': 62, 'au': 55, 'ur': 439, 'be': 65, 'el': 117, 'la': 245, 'as': 53, '-c': 15, 'co': 84, 'om': 324, 'mp': 280, 'pe': 338, 'en': 119, 'ng': 293, '-k': 22, 'ka': 223, 'an': 48, '-l': 23, 'ah': 42, 'le': 248, 'gg': 151, 'ga': 145, '-m': 24, 'ma': 269, 'ha': 166, 'si': 388, 'is': 204, 'sw': 400, 'wa': 452, '-s': 29, 'se': 385, 'ed': 109, 'da': 88, 'ar': 52, 'ra': 356, 'ep': 121, 'pi': 340, 'it': 205, '-t': 30, 'ta': 403, 'ak': 45, '-w': 32, 'at': 54, 'ti': 409, \"a'\": 33, \"'b\": 5, \"'d\": 6, 'du': 100, \"'h\": 7, 'ad': 38, 'ab': 36, \"b'\": 61, \"'a\": 4, 'a-': 34, '-a': 13, 'di': 92, 'ia': 187, 'ik': 197, 'h-': 165, 'ai': 43, 'im': 199, 'na': 287, 'kt': 239, 'in': 200, 'al': 46, 'ku': 240, 'us': 440, 'gn': 158, 'r-': 355, 'bd': 64, 'do': 97, 'me': 272, 'mi': 274, 'uk': 433, 'ks': 238, 'to': 414, 'or': 328, 'ul': 434, 'er': 122, 'et': 124, 'bi': 66, 'id': 190, 'il': 198, 'io': 201, 'og': 318, 'ge': 149, 'ne': 291, 'es': 123, 'os': 329, 'sf': 386, 'fe': 134, 'ot': 330, 'sa': 381, 'ek': 116, 'tu': 419, 'ri': 364, 'ie': 191, 'bj': 67, 'ja': 212, 'bl': 69, 'ut': 441, 'ps': 348, 'lu': 264, 'bn': 71, 'no': 301, 'rm': 368, 'nu': 305, 'bo': 72, 'oi': 320, 'ol': 323, 'li': 252, 'on': 325, 'em': 118, 'rs': 373, 'rt': 374, 'br': 73, 're': 360, 'ko': 235, 'bs': 74, 'ns': 303, 'nt': 304, 'te': 405, 'ei': 114, 'sm': 392, 'so': 394, 'rb': 357, 'rp': 371, 'pt': 349, 'if': 192, 'st': 398, 'tr': 416, 'su': 399, 'rd': 359, 'bt': 75, 'bu': 76, 'ua': 423, 'uh': 430, 'lh': 251, 'ay': 59, 'un': 436, 'ac': 37, 'ca': 79, 'aw': 57, 'wi': 454, 'ce': 80, 'ci': 82, 'cu': 86, 'um': 435, 'gk': 155, 'aa': 35, 'ag': 41, 'gi': 153, 'iu': 206, 'am': 47, 'ap': 50, 'de': 90, 'eg': 112, 'eh': 113, 'pa': 336, 'uz': 445, 'ze': 475, 'dh': 91, 'he': 169, 'ib': 188, 'ic': 189, 'ig': 193, 'gu': 164, 'kk': 231, 'km': 233, 'mu': 284, 'kn': 234, 'od': 315, 'dr': 98, 'hu': 183, 'rg': 362, 'nd': 290, 'ip': 202, 'po': 346, 'pu': 350, 'ir': 203, 'aj': 44, 'tn': 413, 'ok': 322, 'oh': 319, 'iw': 208, 'gs': 162, 'dj': 93, 'je': 213, 'iv': 207, 'va': 446, 'dm': 95, 'ni': 295, 'op': 327, 'ds': 99, 'dv': 101, 've': 447, 'vi': 449, 'vo': 450, 'ae': 39, 'ro': 370, 'ob': 313, 'gr': 161, 'lo': 258, 'mo': 279, 'ov': 331, 'af': 40, 'fa': 132, 'fd': 133, 'fi': 135, 'fr': 139, 'fs': 140, 'fw': 143, 'ih': 194, 'gl': 156, 'go': 159, 'o-': 311, '-g': 18, 'hi': 172, 'ms': 282, 'hk': 174, 'hl': 175, 'lb': 246, 'lk': 254, 'ki': 229, 'ub': 424, 'hm': 176, 'ho': 178, 'hw': 184, 'lf': 249, 'rn': 369, 'nm': 299, 'ji': 214, 'jn': 216, 'ju': 220, 'uj': 432, 'k-': 222, 'ln': 257, 'kb': 224, 'kh': 228, 'ru': 375, 'ht': 182, 'kl': 232, 'eo': 120, 'kr': 237, 'lt': 263, 'ui': 431, 'up': 438, 'nk': 297, 'kw': 241, 'l-': 244, '-q': 27, 'qu': 353, \"r'\": 354, 'mn': 278, 'lg': 250, 'oj': 321, 'jo': 217, 'tm': 412, 'f-': 131, 'mb': 270, 'ea': 106, 'sy': 401, 'ke': 227, 'll': 255, 'lm': 256, 'rh': 363, 'az': 60, 'za': 473, 'of': 317, 'fo': 138, 'rf': 361, 'lp': 259, 'lq': 260, 'uv': 442, 'lv': 265, 'lw': 266, 'nn': 300, 'hn': 177, 'tl': 411, 'by': 78, 'eb': 107, 'mf': 273, 'mk': 275, 'pl': 343, 'ud': 426, 'mr': 281, 'rk': 366, 'nb': 288, 'nc': 289, 'ew': 127, 'nf': 292, 'gh': 152, 'gp': 160, 'nj': 296, 'ry': 378, 'jl': 215, 'oa': 312, 'pr': 347, 'rr': 372, 'rw': 377, 'i-': 186, 'nz': 309, 'zi': 477, 'ij': 196, 'yi': 468, 'ao': 49, 'th': 408, 'aq': 51, 'qi': 352, 'rc': 358, 'rj': 365, 'rl': 367, 'sb': 382, 'ex': 128, 'xu': 463, 'sh': 387, 'mt': 283, 'sk': 390, 'sl': 391, 'sn': 393, 'sp': 395, 'sr': 396, 'ss': 397, 'yu': 472, 'av': 56, 'vg': 448, 'we': 453, 'wu': 457, 'ax': 58, 'xi': 460, 'ye': 465, 'yo': 469, 'zh': 476, 'iz': 211, 'zm': 478, 'uy': 444, 'ft': 141, 'eu': 125, 'hr': 180, 'kd': 226, 'kp': 236, 'ls': 262, 'rz': 379, 'wo': 455, 'zo': 479, 'ec': 108, 'ej': 115, \"o'\": 310, \"'o\": 9, 'ef': 111, 'tk': 410, 'gt': 163, 'pk': 342, 's-': 380, 'oy': 334, 'uc': 425, 'ug': 429, 'kc': 225, 'gd': 148, 'ue': 427, 'dw': 102, 'fu': 142, 'fl': 137, 'ev': 126, 'jr': 218, 'hf': 170, 'oc': 314, 'tb': 404, 'fk': 136, 'hy': 185, 'rv': 376, 'ez': 130, \"d'\": 87, 'nv': 306, 'ly': 267, 'zu': 480, 'ox': 333, 'oz': 335, 'nw': 307, 'nl': 298, 'ld': 247, 'mc': 271, 'gc': 147, 'pc': 337, 'pg': 339, 'pj': 341, \"e'\": 104, 'oo': 326, 'ch': 81, 'cm': 83, 'g-': 144, 'ow': 332, 'cr': 85, '-d': 16, 'hs': 181, 'ee': 110, 'gm': 157, 'ii': 195, 'ts': 417, 'dk': 94, '-j': 21, 'bk': 68, 'gj': 154, 'iy': 210, 'gb': 146, 'ix': 209, 'tp': 415, 'xp': 461, 'xt': 462, 'mz': 285, 'tt': 418, 'pm': 344, 'pn': 345, 'u-': 422, '-h': 19, 'uf': 428, 'jt': 219, 'uw': 443, 'nh': 294, 'yt': 471, 'zz': 481, 'ml': 276, '-r': 28, 'yh': 467, 'ky': 242, 'py': 351, 'hp': 179, '-p': 26, 'ey': 129, 'oe': 316, 'gf': 150, 'uo': 437, 'dy': 103, 'dn': 96, 'tw': 420, 'sj': 389, 'yg': 466, 'yr': 470, 'zb': 474, 't-': 402, 'hb': 167, 'kj': 230, '-i': 20, '-e': 17, 'hg': 171, 'mm': 277, \"u'\": 421, \"'j\": 8, 'hd': 168, 'bm': 70, 'sc': 383, 'tf': 406, 'e-': 105, 'hj': 173, 'n-': 286, 'lr': 261, 'tg': 407, 'lj': 253, 'bb': 63, 'bv': 77, 'wt': 456, \"'r\": 10, \"'u\": 11, 'db': 89, 'jw': 221, 'kz': 243, 'np': 302, '-u': 31, \"m'\": 268, 'vu': 451, 'xa': 458, 'xe': 459, 'sd': 384}\n",
      "478 {'--': 12, '-n': 25, 'ny': 308, 'ya': 464, '-b': 14, 'ba': 62, 'au': 55, 'ur': 439, 'be': 65, 'el': 117, 'la': 245, 'as': 53, '-c': 15, 'co': 84, 'om': 324, 'mp': 280, 'pe': 338, 'en': 119, 'ng': 293, '-k': 22, 'ka': 223, 'an': 48, '-l': 23, 'ah': 42, 'le': 248, 'gg': 151, 'ga': 145, '-m': 24, 'ma': 269, 'ha': 166, 'si': 388, 'is': 204, 'sw': 400, 'wa': 452, '-s': 29, 'se': 385, 'ed': 109, 'da': 88, 'ar': 52, 'ra': 356, 'ep': 121, 'pi': 340, 'it': 205, '-t': 30, 'ta': 403, 'ak': 45, '-w': 32, 'at': 54, 'ti': 409, \"a'\": 33, \"'b\": 5, \"'d\": 6, 'du': 100, \"'h\": 7, 'ad': 38, 'ab': 36, \"b'\": 61, \"'a\": 4, 'a-': 34, '-a': 13, 'di': 92, 'ia': 187, 'ik': 197, 'h-': 165, 'ai': 43, 'im': 199, 'na': 287, 'kt': 239, 'in': 200, 'al': 46, 'ku': 240, 'us': 440, 'gn': 158, 'r-': 355, 'bd': 64, 'do': 97, 'me': 272, 'mi': 274, 'uk': 433, 'ks': 238, 'to': 414, 'or': 328, 'ul': 434, 'er': 122, 'et': 124, 'bi': 66, 'id': 190, 'il': 198, 'io': 201, 'og': 318, 'ge': 149, 'ne': 291, 'es': 123, 'os': 329, 'sf': 386, 'fe': 134, 'ot': 330, 'sa': 381, 'ek': 116, 'tu': 419, 'ri': 364, 'ie': 191, 'bj': 67, 'ja': 212, 'bl': 69, 'ut': 441, 'ps': 348, 'lu': 264, 'bn': 71, 'no': 301, 'rm': 368, 'nu': 305, 'bo': 72, 'oi': 320, 'ol': 323, 'li': 252, 'on': 325, 'em': 118, 'rs': 373, 'rt': 374, 'br': 73, 're': 360, 'ko': 235, 'bs': 74, 'ns': 303, 'nt': 304, 'te': 405, 'ei': 114, 'sm': 392, 'so': 394, 'rb': 357, 'rp': 371, 'pt': 349, 'if': 192, 'st': 398, 'tr': 416, 'su': 399, 'rd': 359, 'bt': 75, 'bu': 76, 'ua': 423, 'uh': 430, 'lh': 251, 'ay': 59, 'un': 436, 'ac': 37, 'ca': 79, 'aw': 57, 'wi': 454, 'ce': 80, 'ci': 82, 'cu': 86, 'um': 435, 'gk': 155, 'aa': 35, 'ag': 41, 'gi': 153, 'iu': 206, 'am': 47, 'ap': 50, 'de': 90, 'eg': 112, 'eh': 113, 'pa': 336, 'uz': 445, 'ze': 475, 'dh': 91, 'he': 169, 'ib': 188, 'ic': 189, 'ig': 193, 'gu': 164, 'kk': 231, 'km': 233, 'mu': 284, 'kn': 234, 'od': 315, 'dr': 98, 'hu': 183, 'rg': 362, 'nd': 290, 'ip': 202, 'po': 346, 'pu': 350, 'ir': 203, 'aj': 44, 'tn': 413, 'ok': 322, 'oh': 319, 'iw': 208, 'gs': 162, 'dj': 93, 'je': 213, 'iv': 207, 'va': 446, 'dm': 95, 'ni': 295, 'op': 327, 'ds': 99, 'dv': 101, 've': 447, 'vi': 449, 'vo': 450, 'ae': 39, 'ro': 370, 'ob': 313, 'gr': 161, 'lo': 258, 'mo': 279, 'ov': 331, 'af': 40, 'fa': 132, 'fd': 133, 'fi': 135, 'fr': 139, 'fs': 140, 'fw': 143, 'ih': 194, 'gl': 156, 'go': 159, 'o-': 311, '-g': 18, 'hi': 172, 'ms': 282, 'hk': 174, 'hl': 175, 'lb': 246, 'lk': 254, 'ki': 229, 'ub': 424, 'hm': 176, 'ho': 178, 'hw': 184, 'lf': 249, 'rn': 369, 'nm': 299, 'ji': 214, 'jn': 216, 'ju': 220, 'uj': 432, 'k-': 222, 'ln': 257, 'kb': 224, 'kh': 228, 'ru': 375, 'ht': 182, 'kl': 232, 'eo': 120, 'kr': 237, 'lt': 263, 'ui': 431, 'up': 438, 'nk': 297, 'kw': 241, 'l-': 244, '-q': 27, 'qu': 353, \"r'\": 354, 'mn': 278, 'lg': 250, 'oj': 321, 'jo': 217, 'tm': 412, 'f-': 131, 'mb': 270, 'ea': 106, 'sy': 401, 'ke': 227, 'll': 255, 'lm': 256, 'rh': 363, 'az': 60, 'za': 473, 'of': 317, 'fo': 138, 'rf': 361, 'lp': 259, 'lq': 260, 'uv': 442, 'lv': 265, 'lw': 266, 'nn': 300, 'hn': 177, 'tl': 411, 'by': 78, 'eb': 107, 'mf': 273, 'mk': 275, 'pl': 343, 'ud': 426, 'mr': 281, 'rk': 366, 'nb': 288, 'nc': 289, 'ew': 127, 'nf': 292, 'gh': 152, 'gp': 160, 'nj': 296, 'ry': 378, 'jl': 215, 'oa': 312, 'pr': 347, 'rr': 372, 'rw': 377, 'i-': 186, 'nz': 309, 'zi': 477, 'ij': 196, 'yi': 468, 'ao': 49, 'th': 408, 'aq': 51, 'qi': 352, 'rc': 358, 'rj': 365, 'rl': 367, 'sb': 382, 'ex': 128, 'xu': 463, 'sh': 387, 'mt': 283, 'sk': 390, 'sl': 391, 'sn': 393, 'sp': 395, 'sr': 396, 'ss': 397, 'yu': 472, 'av': 56, 'vg': 448, 'we': 453, 'wu': 457, 'ax': 58, 'xi': 460, 'ye': 465, 'yo': 469, 'zh': 476, 'iz': 211, 'zm': 478, 'uy': 444, 'ft': 141, 'eu': 125, 'hr': 180, 'kd': 226, 'kp': 236, 'ls': 262, 'rz': 379, 'wo': 455, 'zo': 479, 'ec': 108, 'ej': 115, \"o'\": 310, \"'o\": 9, 'ef': 111, 'tk': 410, 'gt': 163, 'pk': 342, 's-': 380, 'oy': 334, 'uc': 425, 'ug': 429, 'kc': 225, 'gd': 148, 'ue': 427, 'dw': 102, 'fu': 142, 'fl': 137, 'ev': 126, 'jr': 218, 'hf': 170, 'oc': 314, 'tb': 404, 'fk': 136, 'hy': 185, 'rv': 376, 'ez': 130, \"d'\": 87, 'nv': 306, 'ly': 267, 'zu': 480, 'ox': 333, 'oz': 335, 'nw': 307, 'nl': 298, 'ld': 247, 'mc': 271, 'gc': 147, 'pc': 337, 'pg': 339, 'pj': 341, \"e'\": 104, 'oo': 326, 'ch': 81, 'cm': 83, 'g-': 144, 'ow': 332, 'cr': 85, '-d': 16, 'hs': 181, 'ee': 110, 'gm': 157, 'ii': 195, 'ts': 417, 'dk': 94, '-j': 21, 'bk': 68, 'gj': 154, 'iy': 210, 'gb': 146, 'ix': 209, 'tp': 415, 'xp': 461, 'xt': 462, 'mz': 285, 'tt': 418, 'pm': 344, 'pn': 345, 'u-': 422, '-h': 19, 'uf': 428, 'jt': 219, 'uw': 443, 'nh': 294, 'yt': 471, 'zz': 481, 'ml': 276, '-r': 28, 'yh': 467, 'ky': 242, 'py': 351, 'hp': 179, '-p': 26, 'ey': 129, 'oe': 316, 'gf': 150, 'uo': 437, 'dy': 103, 'dn': 96, 'tw': 420, 'sj': 389, 'yg': 466, 'yr': 470, 'zb': 474, 't-': 402, 'hb': 167, 'kj': 230, '-i': 20, '-e': 17, 'hg': 171, 'mm': 277, \"u'\": 421, \"'j\": 8, 'hd': 168, 'bm': 70, 'sc': 383, 'tf': 406, 'e-': 105, 'hj': 173, 'n-': 286, 'lr': 261, 'tg': 407, 'lj': 253, 'bb': 63, 'bv': 77, 'wt': 456, \"'r\": 10, \"'u\": 11, 'db': 89, 'jw': 221, 'kz': 243, 'np': 302, '-u': 31, \"m'\": 268, 'vu': 451, 'xa': 458, 'xe': 459, 'sd': 384}\n",
      "33 {'NY': 23, 'AA': 3, 'B': 8, 'UW': 31, 'R': 27, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'N': 21, 'HH': 15, 'G': 14, 'IY': 16, 'W': 33, 'D': 10, 'T': 30, 'Q': 26, 'AY': 7, 'AW': 5, 'F': 13, 'JH': 17, 'OY': 24, 'Y': 34, 'Z': 35, 'V': 32, 'SH': 29, 'EY': 12}\n",
      "33 {'NY': 23, 'AA': 3, 'B': 8, 'UW': 31, 'R': 27, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'N': 21, 'HH': 15, 'G': 14, 'IY': 16, 'W': 33, 'D': 10, 'T': 30, 'Q': 26, 'AY': 7, 'AW': 5, 'F': 13, 'JH': 17, 'OY': 24, 'Y': 34, 'Z': 35, 'V': 32, 'SH': 29, 'EY': 12}\n",
      "33 {'NY': 23, 'AA': 3, 'B': 8, 'UW': 31, 'R': 27, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'N': 21, 'HH': 15, 'G': 14, 'IY': 16, 'W': 33, 'D': 10, 'T': 30, 'Q': 26, 'AY': 7, 'AW': 5, 'F': 13, 'JH': 17, 'OY': 24, 'Y': 34, 'Z': 35, 'V': 32, 'SH': 29, 'EY': 12}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "      )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 32\n",
      "n_layers: 1\n",
      "Encoder has a total number of 40256 parameters\n",
      "Decoder has a total number of 17124 parameters\n",
      "Total number of all parameters is 57380\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 1 finished in 0m 8.8s (- 14m 30.96s) (1 1.0%). train avg loss: 2.1988, val avg loss: 2.2006\n",
      "Training for epoch 2 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 2 finished in 0m 16.68s (- 13m 37.17s) (2 2.0%). train avg loss: 1.6632, val avg loss: 1.898\n",
      "Training for epoch 3 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 3 finished in 0m 24.93s (- 13m 26.0s) (3 3.0%). train avg loss: 1.4092, val avg loss: 1.6973\n",
      "Training for epoch 4 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 4 finished in 0m 33.48s (- 13m 23.51s) (4 4.0%). train avg loss: 1.1734, val avg loss: 1.5838\n",
      "Training for epoch 5 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 5 finished in 0m 41.73s (- 13m 12.85s) (5 5.0%). train avg loss: 0.9599, val avg loss: 1.3679\n",
      "Training for epoch 6 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 6 finished in 0m 51.32s (- 13m 24.0s) (6 6.0%). train avg loss: 0.7742, val avg loss: 1.2319\n",
      "Training for epoch 7 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 7 finished in 0m 59.81s (- 13m 14.62s) (7 7.0%). train avg loss: 0.6519, val avg loss: 1.0425\n",
      "Training for epoch 8 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 8 finished in 1m 7.98s (- 13m 1.79s) (8 8.0%). train avg loss: 0.5402, val avg loss: 0.9864\n",
      "Training for epoch 9 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 9 finished in 1m 16.38s (- 12m 52.27s) (9 9.0%). train avg loss: 0.4675, val avg loss: 0.9121\n",
      "Training for epoch 10 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 10 finished in 1m 25.07s (- 12m 45.62s) (10 10.0%). train avg loss: 0.4135, val avg loss: 0.8573\n",
      "Training for epoch 11 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 11 finished in 1m 34.53s (- 12m 44.85s) (11 11.0%). train avg loss: 0.3521, val avg loss: 0.8572\n",
      "Training for epoch 12 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 12 finished in 1m 44.65s (- 12m 47.42s) (12 12.0%). train avg loss: 0.3165, val avg loss: 0.7729\n",
      "Training for epoch 13 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 13 finished in 1m 54.05s (- 12m 43.25s) (13 13.0%). train avg loss: 0.2739, val avg loss: 0.8345\n",
      "Training for epoch 14 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 14 finished in 2m 2.56s (- 12m 32.88s) (14 14.0%). train avg loss: 0.2572, val avg loss: 0.6755\n",
      "Training for epoch 15 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 15 finished in 2m 12.51s (- 12m 30.9s) (15 15.0%). train avg loss: 0.2405, val avg loss: 0.618\n",
      "Training for epoch 16 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 16 finished in 2m 20.69s (- 12m 18.64s) (16 16.0%). train avg loss: 0.2198, val avg loss: 0.6089\n",
      "Training for epoch 17 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 17 finished in 2m 29.28s (- 12m 8.83s) (17 17.0%). train avg loss: 0.2052, val avg loss: 0.6646\n",
      "Training for epoch 18 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 18 finished in 2m 37.9s (- 11m 59.32s) (18 18.0%). train avg loss: 0.1856, val avg loss: 0.6241\n",
      "Training for epoch 19 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 19 finished in 2m 46.3s (- 11m 48.94s) (19 19.0%). train avg loss: 0.1672, val avg loss: 0.6855\n",
      "Training for epoch 20 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 20 finished in 2m 53.96s (- 11m 35.84s) (20 20.0%). train avg loss: 0.1639, val avg loss: 0.6457\n",
      "Training for epoch 21 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 21 finished in 3m 2.52s (- 11m 26.64s) (21 21.0%). train avg loss: 0.1478, val avg loss: 0.5794\n",
      "Training for epoch 22 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 22 finished in 3m 11.55s (- 11m 19.12s) (22 22.0%). train avg loss: 0.1394, val avg loss: 0.5174\n",
      "Training for epoch 23 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 23 finished in 3m 19.5s (- 11m 7.88s) (23 23.0%). train avg loss: 0.1339, val avg loss: 0.5613\n",
      "Training for epoch 24 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 24 finished in 3m 28.03s (- 10m 58.77s) (24 24.0%). train avg loss: 0.1389, val avg loss: 0.613\n",
      "Training for epoch 25 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 25 finished in 3m 36.41s (- 10m 49.22s) (25 25.0%). train avg loss: 0.1278, val avg loss: 0.4826\n",
      "Training for epoch 26 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 26 finished in 3m 45.16s (- 10m 40.84s) (26 26.0%). train avg loss: 0.1195, val avg loss: 0.5129\n",
      "Training for epoch 27 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 27 finished in 3m 54.32s (- 10m 33.54s) (27 27.0%). train avg loss: 0.1168, val avg loss: 0.5829\n",
      "Training for epoch 28 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 28 finished in 4m 3.43s (- 10m 25.95s) (28 28.0%). train avg loss: 0.0963, val avg loss: 0.432\n",
      "Training for epoch 29 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 29 finished in 4m 12.33s (- 10m 17.76s) (29 29.0%). train avg loss: 0.0858, val avg loss: 0.4389\n",
      "Training for epoch 30 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 30 finished in 4m 20.51s (- 10m 7.87s) (30 30.0%). train avg loss: 0.0821, val avg loss: 0.4742\n",
      "Training for epoch 31 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 31 finished in 4m 30.92s (- 10m 3.0s) (31 31.0%). train avg loss: 0.1253, val avg loss: 0.415\n",
      "Training for epoch 32 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 32 finished in 4m 39.52s (- 9m 53.99s) (32 32.0%). train avg loss: 0.1135, val avg loss: 0.4182\n",
      "Training for epoch 33 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 33 finished in 4m 47.9s (- 9m 44.53s) (33 33.0%). train avg loss: 0.0806, val avg loss: 0.3899\n",
      "Training for epoch 34 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 34 finished in 4m 56.95s (- 9m 36.44s) (34 34.0%). train avg loss: 0.0749, val avg loss: 0.5341\n",
      "Training for epoch 35 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 35 finished in 5m 5.86s (- 9m 28.02s) (35 35.0%). train avg loss: 0.0773, val avg loss: 0.3988\n",
      "Training for epoch 36 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 36 finished in 5m 15.12s (- 9m 20.21s) (36 36.0%). train avg loss: 0.0621, val avg loss: 0.4062\n",
      "Training for epoch 37 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 37 finished in 5m 24.1s (- 9m 11.84s) (37 37.0%). train avg loss: 0.0948, val avg loss: 0.3962\n",
      "Training for epoch 38 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 38 finished in 5m 32.32s (- 9m 2.21s) (38 38.0%). train avg loss: 0.0774, val avg loss: 0.3998\n",
      "Training for epoch 39 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 39 finished in 5m 40.52s (- 8m 52.6s) (39 39.0%). train avg loss: 0.0721, val avg loss: 0.4566\n",
      "Training for epoch 40 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 40 finished in 5m 49.84s (- 8m 44.77s) (40 40.0%). train avg loss: 0.0667, val avg loss: 0.5523\n",
      "Training for epoch 41 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 41 finished in 5m 59.88s (- 8m 37.88s) (41 41.0%). train avg loss: 0.1065, val avg loss: 0.4334\n",
      "Training for epoch 42 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 42 finished in 6m 8.18s (- 8m 28.43s) (42 42.0%). train avg loss: 0.0778, val avg loss: 0.3271\n",
      "Training for epoch 43 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 43 finished in 6m 16.59s (- 8m 19.2s) (43 43.0%). train avg loss: 0.0535, val avg loss: 0.3542\n",
      "Training for epoch 44 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 44 finished in 6m 25.5s (- 8m 10.64s) (44 44.0%). train avg loss: 0.0507, val avg loss: 0.337\n",
      "Training for epoch 45 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 45 finished in 6m 34.63s (- 8m 2.32s) (45 45.0%). train avg loss: 0.0554, val avg loss: 0.3743\n",
      "Training for epoch 46 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 46 finished in 6m 43.6s (- 7m 53.79s) (46 46.0%). train avg loss: 0.0851, val avg loss: 0.4601\n",
      "Training for epoch 47 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 47 finished in 6m 52.01s (- 7m 44.61s) (47 47.0%). train avg loss: 0.0833, val avg loss: 0.3223\n",
      "Training for epoch 48 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 48 finished in 7m 1.38s (- 7m 36.49s) (48 48.0%). train avg loss: 0.0509, val avg loss: 0.4061\n",
      "Training for epoch 49 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 49 finished in 7m 10.83s (- 7m 28.42s) (49 49.0%). train avg loss: 0.1051, val avg loss: 0.4487\n",
      "Training for epoch 50 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 50 finished in 7m 20.39s (- 7m 20.39s) (50 50.0%). train avg loss: 0.0608, val avg loss: 0.3396\n",
      "Training for epoch 51 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 51 finished in 7m 29.73s (- 7m 12.09s) (51 51.0%). train avg loss: 0.0466, val avg loss: 0.3775\n",
      "Training for epoch 52 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 52 finished in 7m 39.06s (- 7m 3.75s) (52 52.0%). train avg loss: 0.0511, val avg loss: 0.4104\n",
      "Training for epoch 53 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 53 finished in 7m 48.81s (- 6m 55.73s) (53 53.0%). train avg loss: 0.0469, val avg loss: 0.3278\n",
      "Training for epoch 54 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 54 finished in 7m 58.64s (- 6m 47.73s) (54 54.0%). train avg loss: 0.044, val avg loss: 0.3602\n",
      "Training for epoch 55 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 55 finished in 8m 8.62s (- 6m 39.78s) (55 55.0%). train avg loss: 0.0406, val avg loss: 0.3549\n",
      "Training for epoch 56 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 56 finished in 8m 18.08s (- 6m 31.35s) (56 56.0%). train avg loss: 0.0444, val avg loss: 0.331\n",
      "Training for epoch 57 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 57 finished in 8m 27.58s (- 6m 22.91s) (57 57.0%). train avg loss: 0.0407, val avg loss: 0.3381\n",
      "Training for epoch 58 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 58 finished in 8m 36.65s (- 6m 14.12s) (58 58.0%). train avg loss: 0.057, val avg loss: 0.3101\n",
      "Training for epoch 59 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 59 finished in 8m 46.39s (- 6m 5.8s) (59 59.0%). train avg loss: 0.0521, val avg loss: 0.4043\n",
      "Training for epoch 60 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 60 finished in 8m 56.1s (- 5m 57.4s) (60 60.0%). train avg loss: 0.0494, val avg loss: 0.2949\n",
      "Training for epoch 61 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 61 finished in 9m 5.61s (- 5m 48.83s) (61 61.0%). train avg loss: 0.039, val avg loss: 0.5038\n",
      "Training for epoch 62 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 62 finished in 9m 14.95s (- 5m 40.13s) (62 62.0%). train avg loss: 0.0848, val avg loss: 0.3532\n",
      "Training for epoch 63 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 63 finished in 9m 24.56s (- 5m 31.57s) (63 63.0%). train avg loss: 0.0492, val avg loss: 0.2891\n",
      "Training for epoch 64 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 64 finished in 9m 34.28s (- 5m 23.03s) (64 64.0%). train avg loss: 0.0549, val avg loss: 0.3389\n",
      "Training for epoch 65 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 65 finished in 9m 43.81s (- 5m 14.36s) (65 65.0%). train avg loss: 0.0445, val avg loss: 0.2919\n",
      "Training for epoch 66 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 66 finished in 9m 53.1s (- 5m 5.54s) (66 66.0%). train avg loss: 0.0364, val avg loss: 0.2961\n",
      "Training for epoch 67 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 67 finished in 10m 2.67s (- 4m 56.84s) (67 67.0%). train avg loss: 0.0331, val avg loss: 0.3571\n",
      "Training for epoch 68 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 68 finished in 10m 13.47s (- 4m 48.69s) (68 68.0%). train avg loss: 0.0333, val avg loss: 0.3941\n",
      "Training for epoch 69 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 69 finished in 10m 22.38s (- 4m 39.62s) (69 69.0%). train avg loss: 0.0363, val avg loss: 0.3481\n",
      "Training for epoch 70 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 70 finished in 10m 31.21s (- 4m 30.52s) (70 70.0%). train avg loss: 0.0854, val avg loss: 0.4348\n",
      "Training for epoch 71 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 71 finished in 10m 40.49s (- 4m 21.61s) (71 71.0%). train avg loss: 0.0768, val avg loss: 0.4475\n",
      "Training for epoch 72 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 72 finished in 10m 48.57s (- 4m 12.22s) (72 72.0%). train avg loss: 0.0918, val avg loss: 0.3615\n",
      "Training for epoch 73 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 73 finished in 10m 58.14s (- 4m 3.42s) (73 73.0%). train avg loss: 0.0484, val avg loss: 0.3377\n",
      "Training for epoch 74 has started (lr=0.001). Found 358 batch(es).\n",
      "Epoch 74 finished in 11m 6.77s (- 3m 54.27s) (74 74.0%). train avg loss: 0.038, val avg loss: 0.3383\n",
      "Training for epoch 75 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 75 finished in 11m 15.54s (- 3m 45.18s) (75 75.0%). train avg loss: 0.0342, val avg loss: 0.3185\n",
      "Training for epoch 76 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 76 finished in 11m 24.11s (- 3m 36.03s) (76 76.0%). train avg loss: 0.0315, val avg loss: 0.3084\n",
      "Training for epoch 77 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 77 finished in 11m 32.88s (- 3m 26.96s) (77 77.0%). train avg loss: 0.0341, val avg loss: 0.2916\n",
      "Training for epoch 78 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 78 finished in 11m 40.52s (- 3m 17.58s) (78 78.0%). train avg loss: 0.0319, val avg loss: 0.3445\n",
      "Training for epoch 79 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 79 finished in 11m 48.89s (- 3m 8.44s) (79 79.0%). train avg loss: 0.0302, val avg loss: 0.3159\n",
      "Training for epoch 80 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 80 finished in 11m 58.18s (- 2m 59.54s) (80 80.0%). train avg loss: 0.028, val avg loss: 0.3282\n",
      "Training for epoch 81 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 81 finished in 12m 6.11s (- 2m 50.32s) (81 81.0%). train avg loss: 0.0284, val avg loss: 0.3288\n",
      "Training for epoch 82 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 82 finished in 12m 13.84s (- 2m 41.09s) (82 82.0%). train avg loss: 0.0275, val avg loss: 0.3202\n",
      "Training for epoch 83 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 83 finished in 12m 22.5s (- 2m 32.08s) (83 83.0%). train avg loss: 0.0288, val avg loss: 0.3159\n",
      "Training for epoch 84 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 84 finished in 12m 31.11s (- 2m 23.07s) (84 84.0%). train avg loss: 0.0262, val avg loss: 0.2951\n",
      "Training for epoch 85 has started (lr=0.0005). Found 358 batch(es).\n",
      "Epoch 85 finished in 12m 39.61s (- 2m 14.05s) (85 85.0%). train avg loss: 0.028, val avg loss: 0.3482\n",
      "Training for epoch 86 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 86 finished in 12m 48.63s (- 2m 5.13s) (86 86.0%). train avg loss: 0.0266, val avg loss: 0.3036\n",
      "Training for epoch 87 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 87 finished in 12m 57.0s (- 1m 56.1s) (87 87.0%). train avg loss: 0.0251, val avg loss: 0.3293\n",
      "Training for epoch 88 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 88 finished in 13m 4.96s (- 1m 47.04s) (88 88.0%). train avg loss: 0.0251, val avg loss: 0.3389\n",
      "Training for epoch 89 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 89 finished in 13m 12.84s (- 1m 37.99s) (89 89.0%). train avg loss: 0.0266, val avg loss: 0.3739\n",
      "Training for epoch 90 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 90 finished in 13m 21.56s (- 1m 29.06s) (90 90.0%). train avg loss: 0.0266, val avg loss: 0.3426\n",
      "Training for epoch 91 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 91 finished in 13m 30.51s (- 1m 20.16s) (91 91.0%). train avg loss: 0.0236, val avg loss: 0.2987\n",
      "Training for epoch 92 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 92 finished in 13m 38.76s (- 1m 11.2s) (92 92.0%). train avg loss: 0.0244, val avg loss: 0.3363\n",
      "Training for epoch 93 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 93 finished in 13m 46.86s (- 1m 2.24s) (93 93.0%). train avg loss: 0.0237, val avg loss: 0.3183\n",
      "Training for epoch 94 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 94 finished in 13m 55.67s (- 0m 53.34s) (94 94.0%). train avg loss: 0.0249, val avg loss: 0.3339\n",
      "Training for epoch 95 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 95 finished in 14m 5.24s (- 0m 44.49s) (95 95.0%). train avg loss: 0.0237, val avg loss: 0.3315\n",
      "Training for epoch 96 has started (lr=0.00025). Found 358 batch(es).\n",
      "Epoch 96 finished in 14m 14.71s (- 0m 35.61s) (96 96.0%). train avg loss: 0.0242, val avg loss: 0.2972\n",
      "Training for epoch 97 has started (lr=0.000125). Found 358 batch(es).\n",
      "Epoch 97 finished in 14m 23.75s (- 0m 26.71s) (97 97.0%). train avg loss: 0.0222, val avg loss: 0.311\n",
      "Training for epoch 98 has started (lr=0.000125). Found 358 batch(es).\n",
      "Epoch 98 finished in 14m 32.71s (- 0m 17.81s) (98 98.0%). train avg loss: 0.0223, val avg loss: 0.3006\n",
      "Training for epoch 99 has started (lr=0.000125). Found 358 batch(es).\n",
      "Epoch 99 finished in 14m 41.84s (- 0m 8.91s) (99 99.0%). train avg loss: 0.0228, val avg loss: 0.3047\n",
      "Training for epoch 100 has started (lr=0.000125). Found 358 batch(es).\n",
      "Epoch 100 finished in 14m 49.66s (- 0m 0.0s) (100 100.0%). train avg loss: 0.0242, val avg loss: 0.329\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"train-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"train-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"train-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"train-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHFCAYAAAAg3/mzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdeXxU9b3/8ddkmyRkI4SEBAIJIAKiEFmURQsiUMDUpRYVWgqC1QpSxaWm1N1rWquIlILenyIXRUpVVFQKYi2rGyBxY1EgkAAJISzZyTq/P05mkpBtJpkly/v5eMxjzpw5Z85ncnudN9/tmCwWiwURERERD/HydAEiIiLSvimMiIiIiEcpjIiIiIhHKYyIiIiIRymMiIiIiEcpjIiIiIhHKYyIiIiIRymMiIiIiEcpjIiIiIhHKYyISLOsWLECk8nErl27PF2KiLRSCiMiIiLiUQojIiIi4lEKIyLicmlpafz6178mMjISs9lMv379eP7556moqKhx3LJlyxg4cCBBQUEEBwfTt29f/vSnP9neLyws5IEHHiA+Ph5/f3/Cw8MZMmQIq1evdvdXEhEn8vF0ASLStp06dYoRI0ZQUlLCU089RVxcHB9++CEPPPAAhw4dYunSpQD885//5O677+aee+7hueeew8vLi4MHD7J3717bZ82fP5/XX3+dp59+moSEBAoKCvj+++85ffq0p76eiDiBwoiIuNTChQs5fvw4X375JcOGDQNgwoQJlJeX89JLL3HvvffSp08fduzYQVhYGIsXL7adO3bs2BqftWPHDsaPH899991n2zd58mT3fBERcRl104iIS3366af079/fFkSsZsyYgcVi4dNPPwVg2LBhnDt3jttuu43333+f7OzsWp81bNgw/v3vf/Pwww+zefNmioqK3PIdRMS1FEZExKVOnz5NdHR0rf0xMTG29wF+85vfsHz5co4ePcovf/lLIiMjueKKK9i0aZPtnMWLF/PHP/6R9957jzFjxhAeHs4NN9zATz/95J4vIyIuoTAiIi7VqVMnMjIyau0/ceIEABEREbZ9M2fO5LPPPiMnJ4ePPvoIi8XCddddx9GjRwHo0KEDTzzxBPv37yczM5Nly5bxxRdfkJiY6J4vIyIuoTAiIi41duxY9u7dy9dff11j/8qVKzGZTIwZM6bWOR06dGDixIksWLCAkpISfvjhh1rHREVFMWPGDG677TYOHDhAYWGhy76DiLiWBrCKiFN8+umnHDlypNb+O++8k5UrVzJ58mSefPJJevTowUcffcTSpUv5/e9/T58+fQC44447CAgIYOTIkURHR5OZmUlycjKhoaEMHToUgCuuuILrrruOyy67jI4dO7Jv3z5ef/11hg8fTmBgoDu/rog4kclisVg8XYSItF4rVqxg5syZ9b6fmpqKl5cXSUlJbNy4kdzcXHr27Mns2bOZP38+Xl5GA+3KlStZsWIFe/fu5ezZs0RERDBq1Cj+/Oc/c+mllwKQlJTEJ598wqFDhygsLKRr165cf/31LFiwgE6dOrnl+4qI8ymMiIiIiEdpzIiIiIh4lMKIiIiIeJTCiIiIiHiUwoiIiIh4lMKIiIiIeJTCiIiIiHhUq1j0rKKighMnThAcHIzJZPJ0OSIiImIHi8VCXl4eMTExtjWF6tIqwsiJEyeIjY31dBkiIiLSBOnp6XTr1q3e91tFGAkODgaMLxMSEuLhakRERMQeubm5xMbG2n7H69Mqwoi1ayYkJERhREREpJVpbIiFBrCKiIiIRymMiIiIiEcpjIiIiIhHtYoxIyIiIq5SXl5OaWmpp8tolXx9ffH29m725yiMiIhIu2SxWMjMzOTcuXOeLqVVCwsLo0uXLs1aB0xhRERE2iVrEImMjCQwMFCLajrIYrFQWFhIVlYWANHR0U3+LIURERFpd8rLy21BpFOnTp4up9UKCAgAICsri8jIyCZ32WgAq4iItDvWMSKBgYEerqT1s/4NmzPuRmFERETaLXXNNJ8z/oYKIyIiIuJRDoWR5ORkhg4dSnBwMJGRkdxwww0cOHCgwXPWrl3LuHHj6Ny5MyEhIQwfPpyNGzc2q2gRERFpvri4OBYtWuTpMhwLI1u2bGHOnDl88cUXbNq0ibKyMsaPH09BQUG952zdupVx48axfv16du/ezZgxY0hMTGTPnj3NLl5ERKS9GT16NPfee69TPmvnzp387ne/c8pnNYdDs2k2bNhQ4/Vrr71GZGQku3fv5uqrr67znAsT1zPPPMP777/PBx98QEJCgoPlOllJIeRngn8YBIZ7thYREREnsFgslJeX4+PT+E98586d3VBR45o1ZiQnJweA8HD7f8grKirIy8tr8Jzi4mJyc3NrPFzhx3/cDIsT2LNxhUs+X0RExJlmzJjBli1bePHFFzGZTJhMJlasWIHJZGLjxo0MGTIEs9nMtm3bOHToENdffz1RUVEEBQUxdOhQPvnkkxqfd2E3jclk4pVXXuHGG28kMDCQiy66iHXr1rn8ezU5jFgsFubPn8+oUaMYMGCA3ec9//zzFBQUMGXKlHqPSU5OJjQ01PaIjY1tapkNOm3qCEBF7kmXfL6IiLQeFouFwpIyjzwsFotdNb744osMHz6cO+64g4yMDDIyMmy/kQ899BDJycns27ePyy67jPz8fCZNmsQnn3zCnj17mDBhAomJiaSlpTV4jSeeeIIpU6bw7bffMmnSJKZNm8aZM2ea/fdtSJMXPZs7dy7ffvst27dvt/uc1atX8/jjj/P+++8TGRlZ73FJSUnMnz/f9jo3N9clgeS8v9E85VOoMCIi0t4VlZbT/1HPTLDY++QEAv0a/0kODQ3Fz8+PwMBAunTpAsD+/fsBePLJJxk3bpzt2E6dOjFw4EDb66effpp3332XdevWMXfu3HqvMWPGDG677TbAGFrx97//na+++oqf//znTfpu9mhSGLnnnntYt24dW7dupVu3bnads2bNGmbNmsVbb73Ftdde2+CxZrMZs9nclNIcUhpgBCLz+VMuv5aIiIgrDRkypMbrgoICnnjiCT788ENOnDhBWVkZRUVFjbaMXHbZZbbtDh06EBwcbFvy3VUcCiMWi4V77rmHd999l82bNxMfH2/XeatXr+b2229n9erVTJ48uUmFukJZByOMBJZke7gSERHxtABfb/Y+OcFj126uDh061Hj94IMPsnHjRp577jl69+5NQEAAN998MyUlJQ1+jq+vb43XJpOJioqKZtfXEIfCyJw5c3jzzTd5//33CQ4OJjMzEzCajazr0yclJXH8+HFWrlwJGEFk+vTpvPjii1x55ZW2cwICAggNDXXmd3FchygAgkpPe7YOERHxOJPJZFdXiaf5+flRXl7e6HHbtm1jxowZ3HjjjQDk5+dz5MgRF1fXNA4NYF22bBk5OTmMHj2a6Oho22PNmjW2YzIyMmo0Ab388suUlZUxZ86cGuf84Q9/cN63aCKvUKO/LaTsDLg49YmIiDhDXFwcX375JUeOHCE7O7veVovevXuzdu1aUlJS+Oabb5g6darLWziayuFumsasWLGixuvNmzc7cgm38gk2wogP5VB0BjpEeLgiERGRhj3wwAP89re/pX///hQVFfHaa6/VedwLL7zA7bffzogRI4iIiOCPf/yjy5bKaC6Txd75RB6Um5tLaGgoOTk5hISEOO1zPz90mj4rB9LJlAd37YAu9k9RFhGR1uv8+fOkpqYSHx+Pv7+/p8tp1Rr6W9r7+92ub5QX7O9DliXMeJGf6dliRERE2ql2H0ZOWcNIntYaERER8YR2HUaCzD5kUbkKa55aRkRERDyhfYeRat00pTkZHq5GRESkfWrXYcTs413t/jQKIyIiIp7QrsMIQJ5vJwAsea5d6lZERETq1u7DSIGvsbaIV4EGsIqIiHhCuw8jRWbjzr2+hVnQ8pdcERERaXPafRgpCTTCiHd5ERTnebgaERGR9qfdhxH/gCByLcZN/shXV42IiLRtcXFxLFq0yNNl1NDuw0iQufrCZ1prRERExN0URvx9yLIY03vVMiIiIuJ+CiNmX7JQy4iIiLR8L7/8Ml27dqWioqLG/l/84hf89re/5dChQ1x//fVERUURFBTE0KFD+eSTTzxUrf3afRjRzfJERAQwZlSWFHjmYedszl/96ldkZ2fz3//+17bv7NmzbNy4kWnTppGfn8+kSZP45JNP2LNnDxMmTCAxMZG0tDRX/dWcwsfTBXhakNmHY7pZnoiIlBbCMzGeufafToBfh0YPCw8P5+c//zlvvvkmY8eOBeCtt94iPDycsWPH4u3tzcCBA23HP/3007z77rusW7eOuXPnuqz85mr3LSNBZrWMiIhI6zFt2jTeeecdiouLAVi1ahW33nor3t7eFBQU8NBDD9G/f3/CwsIICgpi//79ahlp6YL8q+7cq5YREZF2zDfQaKHw1LXtlJiYSEVFBR999BFDhw5l27ZtLFy4EIAHH3yQjRs38txzz9G7d28CAgK4+eabKSkpcVXlTtHuw0iwWkZERATAZLKrq8TTAgICuOmmm1i1ahUHDx6kT58+DB48GIBt27YxY8YMbrzxRgDy8/M5cuSIB6u1j8KIv2/VOiPnc6C0CHwDPFuUiIhIA6ZNm0ZiYiI//PADv/71r237e/fuzdq1a0lMTMRkMvHII4/UmnnTEjk0ZiQ5OZmhQ4cSHBxMZGQkN9xwAwcOHGj0vC1btjB48GD8/f3p2bMnL730UpMLdrYgfx9yCeQ8vsYOrTUiIiIt3DXXXEN4eDgHDhxg6tSptv0vvPACHTt2ZMSIESQmJjJhwgQuv/xyD1ZqH4daRrZs2cKcOXMYOnQoZWVlLFiwgPHjx7N37146dKi7aSs1NZVJkyZxxx138MYbb7Bjxw7uvvtuOnfuzC9/+UunfInmCDL7ACayLGF0N50yxo10jPN0WSIiIvXy9vbmxIna41vi4uL49NNPa+ybM2dOjdctsdvGoTCyYcOGGq9fe+01IiMj2b17N1dffXWd57z00kt0797dtg5+v3792LVrF88991yLCCPB/safIMvS0QgjGjciIiLiVs2a2puTkwMY857r8/nnnzN+/Pga+yZMmMCuXbsoLS2t85zi4mJyc3NrPFzF7OOFj5eJU5ZQY4dm1IiIiLhVk8OIxWJh/vz5jBo1igEDBtR7XGZmJlFRUTX2RUVFUVZWRnZ2dp3nJCcnExoaanvExsY2tcxGmUymyvvTaEaNiIiIJzQ5jMydO5dvv/2W1atXN3qsyWSq8dpSuezthfutkpKSyMnJsT3S09ObWqZdjIXPtNaIiIiIJzRpau8999zDunXr2Lp1K926dWvw2C5dupCZWbO1ISsrCx8fHzp16lTnOWazGbPZ3JTSmiTI7FN1szy1jIiItBsWO+8JI/Vzxt/QoZYRi8XC3LlzWbt2LZ9++inx8fGNnjN8+HA2bdpUY9/HH3/MkCFD8PX1daxaFwn296laa0QtIyIibZ7196ewsNDDlbR+1r9hc37THWoZmTNnDm+++Sbvv/8+wcHBthaP0NBQAgKMhcKSkpI4fvw4K1euBOCuu+5iyZIlzJ8/nzvuuIPPP/+cV1991a7uHXcJ9vclU2NGRETaDW9vb8LCwsjKygIgMDCw3qEDUjeLxUJhYSFZWVmEhYXh7e3d5M9yKIwsW7YMgNGjR9fY/9prrzFjxgwAMjIyatyQJz4+nvXr13Pffffxj3/8g5iYGBYvXtwipvVa1RgzUpAN5WXg3e4XpxURadO6dOkCYAsk0jRhYWG2v2VTOfSLa0+/0IoVK2rt+9nPfsbXX3/tyKXcKsjfh9MEU4E3XpRDQRaEeOg20iIi4hYmk4no6GgiIyPrXWpCGubr69usFhEr/fMf42Z5Frwo8O1IcGk25GUqjIiItBPe3t5O+UGVpmvWomdthbEkPOT4VM7u0f1pRERE3EZhBKObBuCsV+VKsnkaxCoiIuIuCiNUtYycNlUOYlXLiIiIiNsojFD9ZnnWtUbUMiIiIuIuCiMY64wAZFZY1xpRy4iIiIi7KIxQ1U1zvCzE2KGWEREREbdRGKFqAOux0sowopYRERERt1EYwVhnBOBISbUwUlHuwYpERETaD4URqlpGTlSEYfE2Q0UZ5KR7uCoREZH2QWEECPD1xssEFrwoD4szdp4+6NGaRERE2guFEYz7E1gHsZ4P7WnsPH3IgxWJiIi0HwojlazTews79DB2KIyIiIi4hcJIJdv9aWxhRN00IiIi7qAwUsm6CusZc3djh8KIiIiIWyiMVLLOqMny62rsyEmHsmIPViQiItI+KIxUst0szxIKfsFgqYCzRzxblIiISDugMFLJ2k2TV1wOnXoZO9VVIyIi4nIKI5WsLSP5xWXVwohm1IiIiLiaw2Fk69atJCYmEhMTg8lk4r333mv0nFWrVjFw4EACAwOJjo5m5syZnD59ukkFu0qQ2Zjam1dcBp16GzvVMiIiIuJyDoeRgoICBg4cyJIlS+w6fvv27UyfPp1Zs2bxww8/8NZbb7Fz505mz57tcLGuZB3Amn++ehhRy4iIiIir+Th6wsSJE5k4caLdx3/xxRfExcUxb948AOLj47nzzjt59tlnHb20SwVX76YJr+ymOaMwIiIi4mouHzMyYsQIjh07xvr167FYLJw8eZK3336byZMnu/rSDqnZMlK5JHxeBhTne7AqERGRts8tYWTVqlXccsst+Pn50aVLF8LCwvj73/9e7znFxcXk5ubWeLha1WyaMgjoCIERxhtqHREREXEpl4eRvXv3Mm/ePB599FF2797Nhg0bSE1N5a677qr3nOTkZEJDQ22P2NhYV5dZbTZNqbFD40ZERETcwuVhJDk5mZEjR/Lggw9y2WWXMWHCBJYuXcry5cvJyMio85ykpCRycnJsj/T0dFeXaWsZyT9fZuzQ9F4RERG3cHgAq6MKCwvx8al5GW9vbwAsFkud55jNZsxms6tLq8E2tfd8GRaLBZMWPhMREXELh1tG8vPzSUlJISUlBYDU1FRSUlJIS0sDjFaN6dOn245PTExk7dq1LFu2jMOHD7Njxw7mzZvHsGHDiImJcdLXaD7rANayCgvFZRVV3TQaMyIiIuJSDreM7Nq1izFjxthez58/H4Df/va3rFixgoyMDFswAZgxYwZ5eXksWbKE+++/n7CwMK655hr++te/OqF85wn09cZkAovFaB3xD1fLiIiIiDuYLPX1lbQgubm5hIaGkpOTQ0hIiMuuc+ljG8krLuO/D4wmPsQEz0QbbzyUCoHhLruuiIhIW2Tv77fuTVNNjbVG/AIhpJvxhgaxioiIuIzCSDXW6b15tum9lYufqatGRETEZRRGqqk9vVc3zBMREXE1hZFqgvyN6b35xReEEc2oERERcRmFkWpq3CwPqm6Yp5YRERERl1EYqcY2ZqRWN81hY86viIiIOJ3CSDXW2TS2MNKxB5i8obQA8jI9WJmIiEjbpTBSTa2b5Xn7GoEE1FUjIiLiIgoj1dSaTQOaUSMiIuJiCiPVBF04gBWqwkjWPg9UJCIi0vYpjFRTa8wIQLehxvORbR6oSEREpO1TGKkm+MJ1RgDirzaes/ZCfpYHqhIREWnbFEaqqbObpkMERF1qbKdu9UBVIiIibZvCSDV1DmCFqtaR1C1urkhERKTtUxippvqiZ5bqi5z1/JnxrJYRERERp1MYqaZTkB8AJeUVnCssrXqjxwhj8bOzR+DsUc8UJyIi0kYpjFRj9vEmIsgMwPFzRdXeCIaug41tddWIiIg4lcLIBbqG+QOQkXO+5hvqqhEREXEJhZELxIQFAHCiessIQHy1MKKb5omIiDiNwsgFokPrCSPdhoKPP+SfhFP7PVCZiIhI2+RwGNm6dSuJiYnExMRgMpl47733Gj2nuLiYBQsW0KNHD8xmM7169WL58uVNKtjVYiq7aY5fGEZ8/aH7lca2umpEREScxuEwUlBQwMCBA1myZInd50yZMoX//Oc/vPrqqxw4cIDVq1fTt29fRy/tFl3r66aBqq6awxrEKiIi4iw+jp4wceJEJk6caPfxGzZsYMuWLRw+fJjw8HAA4uLiHL2s21jHjNQawApVYeTIdigvA2+H/3wiIiJyAZePGVm3bh1Dhgzh2WefpWvXrvTp04cHHniAoqI6Wh5agOjKbpqTuecpLa+o+WbMIDCHQnEOZHzjgepERETaHpf/0/7w4cNs374df39/3n33XbKzs7n77rs5c+ZMveNGiouLKS4utr3Ozc11dZk2ER3M+Hl7UVJewcnc83TrGFj1ppc3xI2CAx8Z6410G+y2ukRERNoql7eMVFRUYDKZWLVqFcOGDWPSpEksXLiQFStW1Ns6kpycTGhoqO0RGxvr6jJtvLxMttaRE+fq6qrRfWpEREScyeVhJDo6mq5duxIaGmrb169fPywWC8eOHavznKSkJHJycmyP9PR0V5dZQ0yoddxIHWHJuvhZ2hdQVlz7fREREXGIy8PIyJEjOXHiBPn5+bZ9P/74I15eXnTr1q3Oc8xmMyEhITUe7hRd3/RegM59jXEjZefh9CG31iUiItIWORxG8vPzSUlJISUlBYDU1FRSUlJIS0sDjFaN6dOn246fOnUqnTp1YubMmezdu5etW7fy4IMPcvvttxMQEOCkr+FcDU7vNZkgPM7YPpvqvqJERETaKIfDyK5du0hISCAhIQGA+fPnk5CQwKOPPgpARkaGLZgABAUFsWnTJs6dO8eQIUOYNm0aiYmJLF682ElfwfmqloSvY8wIQMd44/mMwoiIiEhzOTybZvTo0VgauDfLihUrau3r27cvmzZtcvRSHlPv/WmswivDiFpGREREmk33pqlDTKh1Nk09YUQtIyIiIk6jMFKH6MqWkdzzZeSdL619gFpGREREnEZhpA5BZh9CA3yBepaFt7aMnEszloUXERGRJlMYqUeD40ZCYsDbDyrKILfutVJERETEPgoj9agaN1JHy4iXN4T1MLY1bkRERKRZFEbqoRk1IiIi7qEwUo9Gw4hm1IiIiDiFwkg9Yqw3y6vr/jSglhEREREnURiph/2rsB5xT0EiIiJtlMJIPaxhJCOniIqKOlacrd4y0sCKtCIiItIwhZF6RAWb8TJBabmF7Pzi2geE9QBMUJIPBdlur09ERKStUBiph4+3F11CrONG6uiq8fU31hsBjRsRERFpBoWRBkTbO6Pm7BH3FCQiItIGKYw0oPG1RuKMZ03vFRERaTKFkQZYp/ceb7RlRGFERESkqRRGGtDVOqOmvum94Vr4TEREpLkURhoQHVrZTVPfwmdqGREREWk2hZEG2FZhbez+NPknoaTATVWJiIi0LQojDbB202Tnl3C+tLz2AQEdwT/M2NaMGhERkSZRGGlAaIAvgX7eAGTWtdYIaNyIiIhIMzkcRrZu3UpiYiIxMTGYTCbee+89u8/dsWMHPj4+DBo0yNHLeoTJZLL/7r0aNyIiItIkDoeRgoICBg4cyJIlSxw6Lycnh+nTpzN27FhHL+lR0aGNTO9Vy4iIiEiz+Dh6wsSJE5k4caLDF7rzzjuZOnUq3t7eDrWmeFpXe+/eq5YRERGRJnHLmJHXXnuNQ4cO8dhjj9l1fHFxMbm5uTUenmINI8fPFdZ9gFpGREREmsXlYeSnn37i4YcfZtWqVfj42NcQk5ycTGhoqO0RGxvr4irr1y3cCCPHzjYyZiQnHcrL3FSViIhI2+HSMFJeXs7UqVN54okn6NOnj93nJSUlkZOTY3ukp6e7sMqGdesYCDQQRoKjwccfKsqMQCIiIiIOcXjMiCPy8vLYtWsXe/bsYe7cuQBUVFRgsVjw8fHh448/5pprrql1ntlsxmw2u7I0u3XrWDWbprzCgreXqeYBXl7QMQ5O7TfGjVi7bURERMQuLg0jISEhfPfddzX2LV26lE8//ZS3336b+PiW/8MdGeyPr7eJ0nILJ3PP26b61tAx3ggjZ1Khl/trFBERac0cDiP5+fkcPHjQ9jo1NZWUlBTCw8Pp3r07SUlJHD9+nJUrV+Ll5cWAAQNqnB8ZGYm/v3+t/S2Vt5ex1sjR04UcO1tUdxgJ14waERGRpnJ4zMiuXbtISEggISEBgPnz55OQkMCjjz4KQEZGBmlpac6t0sOsXTXHztY3o6an8XzqgJsqEhERaTscbhkZPXo0Foul3vdXrFjR4PmPP/44jz/+uKOX9ahuYYHA6foHsXa93Hg+tgssFjCZ6j5OREREatG9aezQaMtI1KXgbYaiM3DmsBsrExERaf0URuzQ6FojPn4QPdDYPrbLTVWJiIi0DQojdmh0rRGAbkOM5+MKIyIiIo5QGLHDhWuN1H1QZRg5ttNNVYmIiLQNCiN2sK41UlZhITO3nhvmdRtqPGd+B6UNtKCIiIhIDQojdrCuNQJw7Ew9g1hDY6FDpLEsfMa3bqxORESkdVMYsVPVjJp6Wj1MpqrWEXXViIiI2E1hxE7GWiONDWIdbDxrEKuIiIjdFEbs1OhaI1CtZURhRERExF4KI3aKDbejZSQmATBBTjrkZbqnMBERkVZOYcROtpaRcw20jJiDIbK/sa3WEREREbsojNjJuvBZxrnzlJVXNHCg1hsRERFxhMKInSKDzba1Rk7mFdd/oC2MqGVERETEHgojdvLyMtG1sbVGoGoQ64k9UF7mhspERERaN4URB9h1j5qIPuAXDKUFcGqfmyoTERFpvRRGHNDowmcAXt7Q9XJjW101IiIijVIYcYBda42Axo2IiIg4QGHEAXZ104CWhRcREXGAwogDrC0j6Y21jHStbBnJPgBF51xclYiISOumMOIA21ojOY2sNRLUGcK6G9uZ37mhMhERkdbL4TCydetWEhMTiYmJwWQy8d577zV4/Nq1axk3bhydO3cmJCSE4cOHs3HjxiYX7EnWtUbKKyxk5p5v+ODOfY3n0z+5vjAREZFWzOEwUlBQwMCBA1myZIldx2/dupVx48axfv16du/ezZgxY0hMTGTPnj0OF+tpNdYaaWzcSEQf4zlbYURERKQhPo6eMHHiRCZOnGj38YsWLarx+plnnuH999/ngw8+ICEhwdHLe1y3joEcOV3YeBjp1C4zAdIAACAASURBVNt4zv7R9UWJiIi0Yg6HkeaqqKggLy+P8PDweo8pLi6muLhqyfXc3Fx3lGYXu6f32lpGFEZEREQa4vYBrM8//zwFBQVMmTKl3mOSk5MJDQ21PWJjY91YYcPsWvgMqsLIuXQobeRYERGRdsytYWT16tU8/vjjrFmzhsjIyHqPS0pKIicnx/ZIT093Y5UNq1prpJGWkQ4R4B8GWOD0IdcXJiIi0kq5LYysWbOGWbNm8a9//Ytrr722wWPNZjMhISE1Hi2F3S0jJpO6akREROzgljCyevVqZsyYwZtvvsnkyZPdcUmXsXutEdCMGhERETs4PIA1Pz+fgwcP2l6npqaSkpJCeHg43bt3JykpiePHj7Ny5UrACCLTp0/nxRdf5MorryQzMxOAgIAAQkNDnfQ13Ccy2Iyftxcl5RVk5JwnNjyw/oMjLjKe1TIiIiJSL4dbRnbt2kVCQoJtWu78+fNJSEjg0UcfBSAjI4O0tDTb8S+//DJlZWXMmTOH6Oho2+MPf/iDk76Ce3l5mYgNN7pqjp7WjBoREZHmcrhlZPTo0VgslnrfX7FiRY3XmzdvdvQSLV6PTh04dKqAo2cKGEVE/Qdaw8jpg1BRAV5afV9ERORC+nVsgu6VXTONtox07AFevlBaCLnH3VCZiIhI66Mw0gRxnaxhpKDhA719Ibynsa2uGhERkTopjDRBj04dADtaRqDaIFbNqBEREamLwkgT9KhsGUk7U9jg+BmgKozo7r0iIiJ1Uhhpgm4dA/EyQWFJOafyixs+WDNqREREGqQw0gR+Pl7EhDk6vVctIyIiInVRGGmiHp3snFHTqbfxnJcB51vO3YdFRERaCoWRJuoebgxiTWtsRk1AGARFGdsaNyIiIlKLwkgTWaf3HrFrRo26akREROqjMNJEtm6aM45M79UgVhERkQspjDRR1VojjXTTgGbUiIiINEBhpImsS8KfKywlp6i04YO18JmIiEi9FEaaqIPZh87BZgDS7J3ee+YwlJe5uDIREZHWRWGkGXqEWwexNtJVE9INfAKgvATOHXVDZSIiIq2HwkgzdK+2LHyDvLyq1htRV42IiEgNCiPNEFc5iPVItj2DWDWjRkREpC4KI83g2PRezagRERGpi8JIM1in9zY6gBWg88XGc+pWKG9k9o2IiEg7ojDSDNYBrJm55zlfWt7wwReNh8AIYwDrnjfcUJ2IiEjroDDSDGGBvoT4+wB2DGI1B8HVDxjbW/4KpUUurk5ERKR1cDiMbN26lcTERGJiYjCZTLz33nuNnrNlyxYGDx6Mv78/PXv25KWXXmpSsS2NyWSyddXYNYh1yO0QGmvcwXfnKy6uTkREpHVwOIwUFBQwcOBAlixZYtfxqampTJo0iauuuoo9e/bwpz/9iXnz5vHOO+84XGxLZPf0XgAfM4x+2NjethDO57qwMhERkdbBx9ETJk6cyMSJE+0+/qWXXqJ79+4sWrQIgH79+rFr1y6ee+45fvnLXzp6+RbHevfeo/YMYgW47FbY8aIxq+bzf8CYJBdWJyIi0vK5fMzI559/zvjx42vsmzBhArt27aK0tO5ZJcXFxeTm5tZ4tFQ9wiu7aey5YR6Atw+MWWBsf74ECrJdVJmIiEjr4PIwkpmZSVRUVI19UVFRlJWVkZ1d9w9xcnIyoaGhtkdsbKyry2yyHo5001j1vx6iB0FJvtFdIyIi0o65ZTaNyWSq8dpisdS53yopKYmcnBzbIz093eU1NpV1AOuxs0WUllfYd5LJBGMfMbZ3vgI5x11UnYiISMvn8jDSpUsXMjMza+zLysrCx8eHTp061XmO2WwmJCSkxqOligw24+/rRXmFhRPnHJiu22ssdB8B5cWw+zXXFSgiItLCuTyMDB8+nE2bNtXY9/HHHzNkyBB8fX1dfXmX8/Iy0T3cwUGsYLSOXPE7Y3vPG1Be5oLqREREWj6Hw0h+fj4pKSmkpKQAxtTdlJQU0tLSAKOLZfr06bbj77rrLo4ePcr8+fPZt28fy5cv59VXX+WBBx5w0lfwvO6Vg1iP2juI1eriycaqrHkZ8NPHLqhMRESk5XM4jOzatYuEhAQSEhIAmD9/PgkJCTz66KMAZGRk2IIJQHx8POvXr2fz5s0MGjSIp556isWLF7eJab1WDk/vtfLxg0FTje3dK5xblIiISCvh8Dojo0ePtg1ArcuKFStq7fvZz37G119/7eilWo0eEUbLSKo9q7Be6PLfwmeL4eAmOJcOYS135pCIiIgr6N40TtCrsxFGDp3Kd/zkiN4QdxVYKnQDPRERaZcURpygd2QQYKw10ujde+syeIbxvOd1DWQVEZF2R2HECToHmQnx96HC4sBKrNX1S4SAcMg9Dgc/cX6BIiIiLZjCiBOYTCZ6VbaOHMxqQleNj7lqIOvX/+fEykRERFo+hREn6d3ZCCOHsprQMgLGQFaAHzdA7gknVSUiItLyKYw4iXXcyMGmDGIF6NwHeozUQFYREWl3FEacpFfnZnTTWFkHsu5aDiUOrlkiIiLSSimMOIm1ZeTwqXwqKupfh6VB/a+H0FhjRdavXnZidSIiIi2XwoiTxIYH4ufjRXFZBccduWFedT5mGLPA2N72AhSecV6BIiIiLZTCiJN4e5noWbkSa7O6ai6bApGXQHEObH/BSdWJiIi0XAojTmQdN9KklVitvLzh2seN7S9fhpxjza5LRESkJVMYcaJmrTVS3UXjoMcoKC+G/yY7oTIREZGWS2HEiXo7K4yYTDDuCWP7mzfh5N5mViYiItJyKYw4kfWGeQdP5Td4Z2O7dBsC/X5hrDvynyedUJ2IiEjL5OPpAtqSXp2DMJngXGEpZwpK6BRkbt4Hjn0U9n8EP/4b/hpvtJhgMp4vmgA3/MMpdYuIiHiSWkacyN/Xm24dAwAndNUARFwEw35nbBedgcLTUJgNBacg5Q1134iISJugMOJk1nvUNHlZ+AtNeAbm7YE5X8HdX8DvP4deY433vn/bOdcQERHxIIURJ+vV3BvmXcjLC8J7QueLIbIfRPWHhGnGe9+9DfWNTcnaDyf2OKcGERERF1IYcbJm3zDPHn0mgm8HOHcUju2q/X7+KXhlLCyfqFVcRUSkxVMYcTJrGDnkjDEj9fELhH7XGdvfvVX7/S+WQkk+lBVBRorr6hAREXGCJoWRpUuXEh8fj7+/P4MHD2bbtm0NHr9q1SoGDhxIYGAg0dHRzJw5k9OnTzep4JbO2k1z/FwRBcVlrrvQgJuN5x/WQnm16xSdg52vVL3O/N51NYiIiDiBw2FkzZo13HvvvSxYsIA9e/Zw1VVXMXHiRNLS0uo8fvv27UyfPp1Zs2bxww8/8NZbb7Fz505mz57d7OJboo4d/OjUwQ+A1GwnjRupS68xEBBuzKxJ3VK1/6v/B8W5Va8zv3NdDSIiIk7gcBhZuHAhs2bNYvbs2fTr149FixYRGxvLsmXL6jz+iy++IC4ujnnz5hEfH8+oUaO488472bWrjrEObYTTloVviLcvXHKjsf39O8Zzcb7RRQNw6a+M55NqGRERkZbNoTBSUlLC7t27GT9+fI3948eP57PPPqvznBEjRnDs2DHWr1+PxWLh5MmTvP3220yePLne6xQXF5Obm1vj0Zo4bVn4xlgDx951UFoEX/+fsR5JeE+45hHjvVMHoPS8a+sQERFpBofCSHZ2NuXl5URFRdXYHxUVRWZmZp3njBgxglWrVnHLLbfg5+dHly5dCAsL4+9//3u910lOTiY0NNT2iI2NdaRMj3PK3XvtEXsFhHSDkjzY9yF8Vvk3HXkvhHWHgI5gKYdT+11bh4iISDM0aQCryWSq8dpisdTaZ7V3717mzZvHo48+yu7du9mwYQOpqancdddd9X5+UlISOTk5tkd6enpTyvQYt7WMeHnBpb80ttffD3kZENIVBt5mLBkfNcB4T+NGRESkBXPo3jQRERF4e3vXagXJysqq1VpilZyczMiRI3nwwQcBuOyyy+jQoQNXXXUVTz/9NNHR0bXOMZvNmM3NvK+LB1nDyJHTBZSVV+Dj7cIZ1Jf+Cna8COdzjNcj5oGPMYCWLpfBkW0aNyIiIi2aQ7+Sfn5+DB48mE2bNtXYv2nTJkaMGFHnOYWFhXh51byMt7c3QPPvbNtCRYf4E+DrTWm5hSOnC117sagB0LmvsR0YAZdPr3qvi1pGRESk5XP4n+zz58/nlVdeYfny5ezbt4/77ruPtLQ0W7dLUlIS06dX/SAmJiaydu1ali1bxuHDh9mxYwfz5s1j2LBhxMTEOO+btCBeXib6x4QAkJJ+zrUXM5lg2B3G9uiHjQXRrLpcajxnfl//svEiIiIe5lA3DcAtt9zC6dOnefLJJ8nIyGDAgAGsX7+eHj16AJCRkVFjzZEZM2aQl5fHkiVLuP/++wkLC+Oaa67hr3/9q/O+RQs0LD6c3UfP8lXqaW4e3M21FxsyC/pdD0Gda+6PuBi8fKE4B86lQccerq1DRESkCUyWVtBXkpubS2hoKDk5OYSEhHi6HLv890AWM1/bSVynQDY/OMZzhSwbBSe/g1vfhL71T6cWERFxNnt/v3VvGhcZ3KMjJhMcOV3IyVwPrvNh66rRuBEREWmZFEZcJMTfl/7RRgr8KtWDd87VIFYREWnhFEZcaFh8OODpMKKWERERadkURlzoisowsvOIB8OIdeGzc0er1iIRERFpQRRGXGhonBFG9mfmca6wxDNFBIYbS8YDnPzBMzWIiIg0QGHEhToFmenVuQMAO4+c9VwhtnEjWolVRERaHoURFxsW3wmAr1JPe64I27iRbz1Xg4iISD0URlzsipYwiNU6bkT3qBERkRZIYcTFrDNqvj+RS0FxmWeKsLaMnNwL5R6qQUREpB4KIy4WExZAt44BlFdY+DrNQ+NGOsaDXxCUF8Ppg87//ILT8MO7UF7q/M8WEZE2T2HEDYbFebirxssLoi4xtl2x3sjaO+CtGbDhYed/toiItHkKI25g7ar5skUsfvaNcz/32G449B9je+cr8MN7zv18ERFp8xRG3MAaRlLSz3G+tNwzRcReYTx/swZKCu0/r6wY8jLrf3/bc8ZzYITxvO4eOJPatBpFRKRdUhhxg/iIDkQEmSkpq+DbYx5aBfWSGyGsBxRkwc7/Z/95/5oOC/vD/o9qv3fyBziwHjDBjA+NwFOcC2/fDmUeWuRNRERaHYURNzCZTJ5fGt7bF0ZXjunYvgjO5zZ+zqH/wo8bwFIO790NOcdqvr/teeO5//UQ2Q9++Sr4h8GJr+E/Tzi3fhERabMURtxkaFxHAL447MHFzy6dAp0ugqIz8OVLDR9rsVQFCi9fOH8O3rkDKiq7mU4fMmbQAFx1v/EcFgs3LDO2P18CB/7t/O8gIiJtjsKImwzvZYyp2HnkDMVlHho34u1T1Try2RIoamCq8b51cGIP+HaAmeuNqcFpn8HWvxnvb18Ilgq4aAJEX1Z1Xt9JcOXdxvb7cx0bnyIiIu2Swoib9IkKIiLIzPnSCvaknfNcIZfcBJH9oTjHCCR1KS+D/zxlbI+YC7HD4LoXjNdb/moMgv3mn8brqx+off61TxjjUwqz4dt/Ov87iIhIm6Iw4iYmk4mRvY371Ow4mO25Qry8YMwCY/uLZVBQRy3fvAmnf4KAcBg+19h32RQYeJvRGvLu76CiDOKvNoLKhXz84MrfG9ufL4WKCtd8FxERaRMURtxoZG+jq2a7J8MIQN/JED0ISgtgx6Ka75UWwea/GNtXPwD+IVXvTXoOwntVvb6qjlYRq4RfgznECDUHNzmvdhERaXOaFEaWLl1KfHw8/v7+DB48mG3btjV4fHFxMQsWLKBHjx6YzWZ69erF8uXLm1Rwa2YNI98eyyH3vAeXTjeZ4Jo/G9tfvARvzYTv3oaic8bCZbnHIaQbDJlV8zxzENy83AgZva4xWkbqYw6Gwb81tj+vpzvIHudzIfdE088XEZEWz8fRE9asWcO9997L0qVLGTlyJC+//DITJ05k7969dO/evc5zpkyZwsmTJ3n11Vfp3bs3WVlZlJW1vxu2dQ0LID6iA6nZBXx5+Azj+kd5rpje10Lf62D/h/DDWuPh5WM8wBjo6utf+7yYQXD/AfAxG6GmIcPuNLppUrdCxrc1B7rao6wYXh0HZ4/CPbsgtJtj54uISKvgcMvIwoULmTVrFrNnz6Zfv34sWrSI2NhYli1bVufxGzZsYMuWLaxfv55rr72WuLg4hg0bxogRI5pdfGvUIsaNgBEkprwOs/8Do+ZD577GOJCy8xDRxxgfUh+/QPDybvwaYbFwyQ3G9hdLHa/xy5fh1H4oK4LDWxw/X0REWgWHwkhJSQm7d+9m/PjxNfaPHz+ezz77rM5z1q1bx5AhQ3j22Wfp2rUrffr04YEHHqCoqKje6xQXF5Obm1vj0VaMquyq8XgYAWMwa7chcO1jMOdLuOdr+MXfYdrbxjRgZ7hyjvH83duQm2H/efmnqqYRA6R/6Zx6RESkxXEojGRnZ1NeXk5UVM3uhaioKDIz675/yeHDh9m+fTvff/897777LosWLeLtt99mzpw59V4nOTmZ0NBQ2yM2NtaRMlu0K3t2wmSCn7LyOZl73tPl1NSpF1w+HTr2cN5ndhsM3YdDRaljy9D/92ljaXm/ION1+lfOq0lERFqUJg1gNV0wVsBisdTaZ1VRUYHJZGLVqlUMGzaMSZMmsXDhQlasWFFv60hSUhI5OTm2R3p6elPKbJHCAv24tGso0EJaR9zBOj1413IoKWj8+Mzv4OuVxvaNlSvFntpnDLC1R0WFMW35yA7HaxUREbdzKIxERETg7e1dqxUkKyurVmuJVXR0NF27diU0NNS2r1+/flgsFo4dO1bnOWazmZCQkBqPtmSkravGg0vDu9PFE6FjvLHia8qbDR9rscCGJGM9k0tuhH6JEN7TeO/YLvuu98Na2PAwvHtX8+oWERG3cCiM+Pn5MXjwYDZtqrluxKZNm+odkDpy5EhOnDhBfn6+bd+PP/6Il5cX3bq1z9kRI3tVjRuxWCwersYNvLxheGW33GeLobyBac37P4Ij28DbbKzkCsbdgMH+cSNf/a/xnJNmjD0REZEWzeFumvnz5/PKK6+wfPly9u3bx3333UdaWhp33WX8KzQpKYnp06fbjp86dSqdOnVi5syZ7N27l61bt/Lggw9y++23ExAQ4Lxv0ooMieuIn48XmbnnOXTKjm6LtiDh19ChM5xLg+/fqfuYsmL4uHL9kxH3VI1dsa7yak8YOZFS87jMb5pes4iIuIXDYeSWW25h0aJFPPnkkwwaNIitW7eyfv16evQwfjgyMjJIS0uzHR8UFMSmTZs4d+4cQ4YMYdq0aSQmJrJ48WLnfYtWxt/X23YX388OtZNxI74BVTfQ27aw7iXid7wIZ1MhKApG3Ve139oycny3cd+chlw4SDZDYUREpKUzWVpBP0Fubi6hoaHk5OS0mfEjSzcf5NkNBxjfP4r/nT7E0+W4x/kceOFS4yZ9t6yCftdVvZf5HfzvGGPWzU2vwGW/qnqvohz+GmfMrrlzW/2LpxWegYX9jLVSrAu69b8epqx06dfyKIsFPrwPykvgF0uM6doiIi2Evb/f+i+Xh1jHjXx++DRl5e3kRnL+oTBstrG97XnjhxSgrATe+70RRPpeB5feXPM8L29jPRRouKtmzxtGEOlyKQytvE5bbxnJ2gu7X4OUVZD1g6erERFpEoURDxnQNZQQfx/yzpfxzbEcT5fjPlf8HnwC4MTXcHizsW/b80bLSEA4XPdC3cvMW7tqju2s+3Mryo376gAM+x1EDzS2zx4xZvG0VfvXV21rKrOItFIKIx7i7WVi9MWRAKz9uu4pzm1SUOeqG+hte94YcLrtOeP15OcgKLLu8xobxPrTJjh3FPzDYMDNEBgOYZX3Ssr41nn12yP7J6NbyXr3Y1c68FHV9pGGb1gpItJSKYx40K3DjJVl39tznPzidnTjwBH3GDfkO7IN/jnVuCdO/+vhkpvqP6frEMBktHTknaz9vnU67+W/Me6dA1WtI+7uqtn7vtEas2eVa6+TewJO7Kl6ffSzugcGi4i0cE66AYk0xfCenegZ0YHD2QW8n3KcaVc4cRn2liy0Gwy81RjjkXscAiNg8sKG7wLsHwJRl8DJ7+HYV8ZiaFbZB+HQfwATDJlVtT96EOz7wP1hxBoQctIgLxOCu7jmOgcqu2hiEuDUj1B0xlipNuoS11yvDcjJyaGwsNDTZbhNYGBgjQUnRVoqhREPMplMTL2iO09/tI83vkhj6rDu9S6r3+aMvK+y5cAC1y2EDhGNnxM7zAgj6V/WDCOfLzGe+0yA8Piq/dGDjGdPhREwVo2tPmvImazjRfpfb9zV+PB/jXEjCiN1ysnJYcmSJZSWNrDoXhvj6+vL3LlzFUikxVMY8bCbB3fj2Y0H2JeRS0r6ORK6d/R0Se4R0Rum/B8U5xs/pvaIvcK4v031m+Z99ndjNgnAFRcs/26dAnz6IBTngTm4+XU3Ju+k0dpjdWyna8LI+VxI3WpsXzzZGMB7+L9G19cVv3P+9dqAwsJCSktLuemmm+jcubNzPtRiMboOg6KqugdbiFOnTrF27VoKCwsVRqTFUxjxsLBAP667NJq1e46z6su09hNGwP4QYmUdxHpij7Fa6+4VVSu2jlkAvcbUPD4oEoJjIO8EZH4PPYY3u+RGVW8VAfvvp+Oog58YU6E79YbOfaBolLH/6A7jB7K9tLA1QefOnYmOjnbOh536EU5tAfrBJTc45zNF2iENYG0Bpl1pzPr44JsT5BS2nyZkh3WMN8aXlJfAR/fDvx8y9l/9IPzsobrPcfcgVmsYiUmofP1146vGNoV1vMjFkyqvd7kxZbrwNJza7/zrSd3yKwdT57SjGXEiLqAw0gJc3r0jfbsEU1xWwTvtaZqvo0ymqvVG9rxuPI/8g9EqUh+3h5GvjeeBt4E5BEoLjUGlzlReCj99bGz3nWw8+/hVtRwd2e7c60n9rGvYlOQZXY4i0iQKIy2AyWRi2hVG68ibX6W1jzv5NpX1BxeMe91c+0TDXRLuDCMWS1XLSNfB0PVyY7u+hdqa6ugOY2n9wAjoNrRqf9xVxrPCiPtUX1Avv44p5yJiF4WRFuKGhK4E+nlzMCufr1LPeLqclmvATdDpIuNGehOeaXxshDWMnNoPpUUNH3voU3htknFDvqbIOQYFp4w1VKIGVAUFZ48bsc6iufjnxlL5VnEjjWfruBFxvephJC/Tc3WItHIKIy1EsL8v1w+KAeD1L456uJoWLKw73LMLrn3cvkGaITFGC4KlHE7urf+4zO9hzW+MH/L1Dzbtx9zaKhLZH3z9q4URJ7aMWCzVxotMrvle18Hg428EouwfnXfNdmbp0qXEx8fj7+/P4MGD2batnpVtS4tY++lOxj2wnM43PE3IgHEMHz6cjRs31jp00aJFXHzxxQQEBBAbG8t9993H+fPnbe/n5eVx77330qNHDwICAhgxYgQ7d9b8383atWuZMGECERERmEwmUlJSnPq9RTxJYaQFsS56tv67DH48mefhatoIk6laV82euo/Jz4LVt0JJZZ//8d3GbBVHWceLWAevdq28uV/2j867P07md5CTbgxW7Tm65ns+5qoA5Kql4TO+hY0LoKTANZ/vYWvWrOHee+9lwYIF7Nmzh6uuuoqJEyeSlpZW++Cis2z99gjjBvdm/V9msHv5Q4wZM4bExET27Kn639qqVat4+OGHeeyxx9i3bx+vvvoqa9asISkpyXbM7Nmz2bRpE6+//jrfffcd48eP59prr+X48app4gUFBYwcOZK//MUNtxkQcTOFkRZkQNdQfn5JFyos8Jd/a0aE0zQ0bqT0PPxzmvEDH94LEn5j7N/8F8dbR2zjRSrHinToBOE9je2mdv1caO97xnOvMXWva2EbN+Kim+Z9MM9YZG73/7nm8z1s4cKFzJo1i9mzZ9OvXz8WLVpEbGwsy5Ytq31w0VkWzb2Oh+74FUP7duOiSDPPPPEIF110ER988IHtsM8//5yRI0cydepU4uLiGD9+PLfddhu7dhndd0VFRbzzzjs8++yzXH311fTu3ZvHH3+c+Pj4Gtf9zW9+w6OPPsq1117r8r+DiLspjLQwD/38Yry9THy6P4vPDmV7upy2ob4wYrEYP67HvgL/UJj6Lxj7qNHqcHwXHPyP/deoPnjV2jICVa0jzhg3UngGvvp/xvalv6r7mLjK9UaObHf+uJHTh6q+oyNdT+Wl8PYseOeOFn3vnJKSEnbv3s348eNr7B8/fjyfffZZ7ROsrV1BXSDAWB+oIucEeXl5hIeH2w4bNWoUu3fv5quvjMX6Dh8+zPr165k82ehmKysro7y8HH9//xofHxAQwPbtLhiMnJsB/02Gf//RWK9HpAVQGGlhenYOYuowY2bNX/69n4oKDURstpjKZeFP7oWyEmM77yT85wn4dg2YvGHKSmNV2KBIGFp5f5vNyfb/oJ85bMxw8TYbY0asnDmI9bPFUJxrDI7tX88CW10HGzUUZBkrzzrTD2urto878H22Pgffvw3f/ctYzr+Fys7Opry8nKioqBr7o6KiyMysY3CqNYwEdDQCCfD8whcoKChgypQptsNuvfVWnnrqKUaNGoWvry+9evVizJgxPPzwwwAEBwczfPhwnnrqKU6cOEF5eTlvvPEGX375JRkZGc77guk7jVC4aABs+Qt8+RJ895bzPl+kGRRGWqB5Yy+ig5833x7L4cPvnPgfo/YqrIfR8lFRCm/9FhYnwPN9YPsLxvuT/lZz/MXIPzjeOmJtMehyKXj7Vu3vZm0Z2dm8loq8k/Dly8b2mAXgVc//6/r6V1tvxMnjRr6vFkbOpUH+qcbPObYbtv6t6vWhT51bkwtceH8oi8VS9z2jqoeR4C6s/s83PL7oFdasv/gttgAAIABJREFUWUNkZKTtsM2bN/M///M/LF26lK+//pq1a9fy4Ycf8tRTT9mOef3117FYLHTt2hWz2czixYuZOnUq3t7eF17VcYVn4NUJ8Oq1RiisKLOFJ75/p/mfL+IECiMtUOdgM3f9rBcAf9u4n+Kycg9X1MpVH8R6YL3RioHJaGH4+V+rWkKsmtI6cuF4EauoAcYMl/PnjG6Optq+0FhAretguHhiw8f2qJzia713jTOc3AtZe8HbD0JjjX2NtY6UFMK7vzNmMgV2MvYd/q/zanKyiIgIvL29a7WCZGVl1WotAWqEkTWbdjLrb2v519Ozao3peOSRR/jNb37D7NmzufTSS7nxxht55plnSE5OpqKy26pXr15s2bKF/Px80tPT+eqrrygtLSU+Pv7Cqzpu9/9B+hfG/+0GTYPfbYGZlTOyDm+xL1SKuFiTwojdU98usGPHDnx8fBg0aFBTLtuuzLoqnshgM+lninjjizpG8otjfvYw9L0OrnoApr0DfzwCv98BV95V9/Ej5lW1jhyqbB2pqDD+w30mtXZAqWu8CBgro1rvHtzUKb7n0o0bBAJc80jjU5qt9+g5vNm4gZ4zWLtoeo+D+KuN7ca6njY9anQVBcfArauNfUc/b3y9Fw/x8/Nj8ODBbNq0qcb+TZs2MWLEiJoHl543wiGw+v2PmTH3Id788y1MvrwblJ2vcWhhYSFeF7RkeXt7Y7FYai1w2KFDB6Kjozl79iwbN27k+usdvH/ThapPBb9+Kdyw1Oi27NTL+N+qpRz2vd+8a4g4gcNhxKGpb9Xk5OQwffp0xo4d2+Ri25NAPx/mj+sDwN8//YmcIt2zplniRsKtq2DsI3DRtRAQ1vDxwVFVrSNrfweLLoWnI+G53rB4EPxzqvGDBMYP/onKNR9iLq/9WdW7appi67PG/Xjirqo9nbcuXYeAOdT4l/uFN+5rCoulqjl/wE1G6ww03DJy8BPYWTnY9oZ/GF1HwTFQXgxH6xgM2kLMnz+fV155heXLl7Nv3z7uu+8+0tLSuOsuI7QmJSUxffp0OG+0iqzecoDpM2/n+eef58pB/ck8k0fmwe/IycmxfWZiYiLLli3jn//8J6mpqWzatIlHHnmEX/ziF7ZumI0bN7Jhwwbb+2PGjOHiiy9m5syZts85c+YMKSkp7N1rrJdz4MABUlJS6h7PYlV0Bs4dNYL1hS1ql9xkPFfvfhPxEIfDiENT36q58847mTp1KsOHu+HOqW3EzYO7cVFkEOcKS3l122FPl9P+jJgHvh2Mm8+dSzPGnGACk5fxr803pxjrbWT/BKUFxrERF9X+nLoWPys6a/xgZzVy35rTh2DPKmPbnlYRAG8f6FnZeuHIjKD6nNhjdG35Bho/aNZwdfzrumfHFJ6B9+YY28PuhF7XGHXbWmzs7Ko5vAWe71c1tscNbrnlFhYtWsSTTz7JoEGD2Lp1K+vXr6dHD2MNoIyMDOMfXpVdNC9/8CVlZWXMmTOH6MQ/Ef3LZKL7DeMPf/iD7TP//Oc/c//99/PnP/+Z/v37M2vWLCZMmMDLL79sOyYnJ4c5c+bQt29fpk+fzqhRo/j444/x9a0af7Ru3ToSEhJss3BuvfVWEhISeOmll+r/QmcrF1DsMx7MQTXfu+RG4/noZ5B7oql/MhGn8HHkYOvUN+socKt6p75Veu211zh06BBvvPEGTz/9dNMqbYd8vL24b1wf7l71NSs+O8IdV/ck2N+38RPFOYKjYPYnkH0AQrpCcDQEd4H0L+HNWyB1C7x+U9V/1KMH1lye3coaRk7+ABv+BEe3G4uHUdlE37mv8a/UATcZYcZiMX7sctJh81+NpvSLxkP3K+yvvddY2PeB0cU0+o/N+jPYWkX6/Bz8OhizhXwCjJk9pw9C5z41j9/6N8jPhIg+xkq5tpqugZRVcGhz49fM+MZY/6UkD3a8CFfOMbq83ODuu+/m7rvvNrrjvn8H+nS0vbdixQpjo3Idl81vPFd1s8KjO4xxOpGXQP9f2M7x8fHhscce47HHHqv3mlOmTKkxA6cuM2bMYMaMGfZ/EYvFCNEhVLWCVPf/27vzuKir9YHjn9kYdhQQEEXFfUvLJa9bbuWaXdM2s9Tq1s9K07yVdaubdW/X7m2z7k0r0zYrzTQz01LLvUxFcSVX3BAEXNhhmJnz++PA4AgoIDIiz/v1moDvfGfmzJH4PnPOc55TKwqi/qTzSfYshq6Plf25hahk5QpGyr30DThw4ADPPvss69evx2wu28vl5eWRl1e0/j09Pb08zbymDGwTQZM6fhxKyeLzTUd5rHdTTzepZglvrW/na9QD7l8MX4zQf8hP6PoRxZJXCwXV01MUGSdh03tFx2s1hIxEvW/Omn/pW1CUDkRsF+wA2/eF8rW7acF06ImtkHPu0tNSpXE6Yc+3+vu2I/RXk0UHXsc36ama84MRh71ouWj/f7oXZovupb+e2qWr3voXrThxc/YIzL1DByJQNIrUcnDF3kNFJcTo0bCT2/UqqfO5klfP69fCFSpVuWGe3QbJe3RAa/Fxvy8jqWjErln/kh/ftuB3ePdCCUaER1UogbWsS98cDgf33nsvL7/8Ms2bNy92f2mmTZtGUFCQ6xYVFVWRZl4TjEaDKwCZvT6eHJusrLkqRHWGMUv1KhFVMFVxYfLq+XpOhoh20GEMDP8IJv8Bk3bCUwd0YmHTm/UGe2nHiwIRvzo6B+WWfxStBiqrWg30yIRy6BGcijr+O6QngDVQt7FQ/VKKucWv0Xvj+IbokZDz+dcpuqgfXlPy62Wl6tGmrGS9EumG+/TxXV9X/D1UhN2mR0ZAT2Hkuyelui3rLRRQEIxkny6qZ3MlKadOLN7/ow4YL0yqPn1Af23at+RqvQCt/6ynHRO26iBQCA8pVzBS3qVvGRkZbN26lfHjx2M2mzGbzbzyyivs2LEDs9nML7+UXHPgueeeIy0tzXU7fvx4eZp5zbnt+kjq1/bhdJaNrzbLypqrRt12MHaZ/kRsskLDbqWfe+PDMG493PYutLsTAuvq4z614IZRcN9CHZiMWQrjt8LzSfD0QXhkNXR/omLta1IwOnI5eSOFUzQtb9U1TAqVlsS66xv9tc3t7vVWXG0qCFAOlZA3YsvSeThnDukRolHfQOeH9X37lkNuFY6Qnj0Myl7wg9JJoOdzBSNFlVbx8gOvAH1+VvKVb2P8OjhbEDCdO+peYVgpSC0oetf8IiNKAeFFWwhIIqvwoHIFI+Va+gYEBgaya9cuYmNjXbdx48bRokULYmNj6dKl5Dlwq9VKYGCg260ms5iMPNpb1x35cN1hqTtyNQlrqXcRfmKb3iH4cvgGQ3RPnTdy4ZB7RRRO1Rz6pWIF1xz2or1wCqdoChWOjJzaU7RUNz9H56kAXFdK/kPjPiW3yenU1UETYvRow32LdMBWt70e4bHnFj13VUgtGFUwFOQAFV70QZdQzy/YKPDC6a/C0ZGMi6xwqQzJcXDsN/19SMHU7aHVkFcwqpZ2HPIzwegF0T0u/lxtZVVNhWWdrrx9p2q4ck/TlHnpG2A0Gmnbtq3bLSwsDG9vb9q2bYufn1/lvptr2B0d6xMeaCUpPZdF2xIu/QBRdawBEFTf060ormF3PWKTdrzo4loev7+vp1x8gqFxL/f7gqLAL0xX8yz8RL7/Rz3FVKtBURXYCzXoqovAZSbpXJlC616H/cv1ffd+XZSHYjBAu4LAZuf88r+HinA6ikrpFwZdZ44U3V84KmLx1e09n3/BCPGVzBvJTIE/CmqHRHWBNiN0crUjFw6s0McLV2kF1de7OV9Mq9v0FOGpXZCy78q1+1qTdgJmdoNZfWHDdE+3ptordzBS5qVvolJZzSYe7ql3gJ255hB2x9W74Zi4Snj5QsOCpfQHV5XvsfHrddEygD5/Kz7lYjCct8S34JNh4RRN2ztKX4Js8S6aziqcqjmwSle6Bbj17eKBTOGmgPHr9CZvF0o9qJdAV9YmfGnH9UiMxRcadNM5FblndSIwlJwvUuhKj4zk5+ipM6cNajWC6N56a4AWg3Q7U/dB8h9FQUXthpd+Tt/goukzKQ9fNnmZ8NU9OqgGWPUSbJnt2TZVc+VaTVPItfStBK6lb6WYOnUqU6dOrcjL1nj3dmnAjDWHOHYmm+93nuT2G67CT+Pi6tKkn04WPfSz+2oJpXRyakgz8Atxf0zaCVgwVie/trsHOv+l5Oeu10HXWzmxVV+gCz+Vl7ajcKHGffQ0zaFf9LLYhQ8BCjo9CNffW/z82o2KlqDu/ga6TSi6b8c8+HacfrzFVy87Dm+jc1ra31PiqEBKyiXKnx/+DU6nQ50oSD0LeX6QkQB/bNYJuCcO6PuNkXDhRna5St93OhPqHis5b6ailIK4pTo/xCsQom+EU+eNwFibQcIW2PgFOO2kZDiKVvhcStsR+t9v4zsQ3ATa31157b7WOB2w6GFI2qWTzFsN1RWSf/irHiVtd/El2qJkFQpGhGf4epl5sHsj3lixn//+fJABbSLw9ZJ/QnERTfvByhd1XYz8XD0y4bDD0omwfa6u1Nr7WZ1ga7Loc+bfD9mp+sI7dHrpoxz1CkdGtsLeJbpKbHjb4kuhL9SkD6xE1+X4+n69b0+9jjDwtdIf0+5OHYzs/LooGNm/AhY/Big9zZCfrduSsBW2faprmtz1uU7SBHx9fbFYLCxadJHcCKV0nkx+NkSnQtBBfdFJ2glBm3VOz7FNOsk2oh1ExBd//O6lesrkt7OlL1+uiPREOPyLzmNp1h9iPne/3+mAfSt0/ReA4KZYmvTA17eUlTTnaztCr8jZ/6PeTyhpJ9z8si6gdy06uV33Y9125X/sqpd0EG6y6m0O6nfSz7Vllg6MvfyKas+IMjOoCzdHuAqlp6cTFBREWlpajU9mTc/Np+8ba0jNtDGwTQQzRnXAaCxDVU5RMykFb7XS9Uzu/1ZPOyx8CP5Y6n5eaAsY9Jq+IG37TE9BPLL24sP8uWnwWkNA6UJfyXt0kbMeT168TU6n3jU5q2CEwicY/m+dLsJVmuwz8EYznaPy2CbIy4BPbwN7DrS7G/78nl6Ke2q3Dh62zIa8tIJ9cea6Vv+kpaWRnZ1d+usk7oQv7tCjLI9t0sFbwnb46m4duD2+Cb4eravpDnkLWt1a/DmWPaMDmhaDdTBXWeaN0q/bYQz0fb7kc45vhvkFy6Hv/ATfVjcTFBRUtud3OmD1v2D9G/rnxn3gjjl6Gqeyrfm3ruo7cNqVef6LSdmvcz0MBv17F9aq7I+N+RS+L1jdNmI2XHeH/t7phO8egx1f6Q0J7/26qOJwDVfW67cEI9XQ1iNnGDlrE/kOxRN9mzK5fwtPN0lczRY/DrFz9UXs9CFdAdZkhREf6b1Lfn5F18YoZDDqpcYX1gkpyf9u1HkKhSbt0gmsl7LwYV07xGDUK2fK8of7q5H6E2nrYXrqKfec3rhv5FfFp0NSD+r9g1L36fc6dLqeckrdp8ufH/1VXwz7vej+Pn/+h74Yt/4z3PWZPuaww38a6+Dm4V/gq3t1rsDDvxQtcT7fqT0FFzsjTNgGwSXsvLvxXdjyEdz16cXr0xQ6+it8PEhf6J6I1YX0ShPzqa6N0muKzicprz2LYfGjenTIP1znwTjy9ciXw6aTkPu/quvGVMThtfBZQYXa8Otg9GLwC63Yc1VE4e8R6Do+D628+AiQLQsOrIS4JbD3Ox0Q9/5b8crGDjt8M1av+vIJhsd/L3lkzOmE7Z/pWjwtby195DHnrF6t4xsM3rUq9m95FZBg5Bq3YOtxnv5mJwD/HXkDQ9tf5rJSce3avRC+ebDoZ68AfQGPLqgvkXNWf1Ld/KHOE+n3ki7SVhaLH9PTIaAvUg/+WLbHHdmgp4N6TSl95+Ri72MRfFO0cRz1O8Po7/SweEly0+Hb/yu68HgH6dGc85m89Kf/VkP1z+/9CVLiYPgs97n/eaP0aFLPv8L6N/WxKUdKTmIFmDtCJw13fhiGvOF+X8p+mNlVX9TC2sD/rb10bsnnw3XeT8cHKne0pTRJu3Uwd2F9lUK+ITDkzaKtEMrK6YQPb9KjV4XqtITRS1zTaS5pJ/R0isN2XjCUr8+PurFi+Tjx6+HTW/W0ipefntK6eWrJo3mHVuuA8eDPegSuUPuRMGxmyUGEPQ9m9dMrk1reCnfPLX7eujfgl3/o7xv11P1Y57wPlFmnYcNbsHmW3lgSdHt9g3X5gJtfrvxRl9SDujr0oNcrfWpOgpEa4F/L4vhw3WGsZiNf/19X2kdVsOS3uLZln9Gf7FE64e6+hSVXdE09oKtwNr25bBvygZ4O+aEgcBnyZunJrpUhPwdeb6bLxIe20IHPpYb4nU5Y+xqs/bf+2eyjq+c27K4viH8s1X/oh83Uc///7aDzT54+6B5obJ4Fy57Sy5mzkvV9U46U/rrx6+DTofr1ntxd9MlfKfj8dvfNAm9+GXpMKv25EmL08lGDSdezqd3o4u+5suRl6PwYDPrCb/LS9VVWTtUXW9DByOA3iydBlyb2Sz3qYg2CUQt0onTGSV0rZfQSvUT58Gr9e7V/eVF14wtZg3Rl2WYDdF5UWXJznE6Y1QcSY/XvaWQHPbVi8oJxG4oCAqXgt//Bihdx7R9Vu5FeAt3qtoIckYv8/5G4U7+O066rLbc7L6H74M86UEWB0aK3GzBaoNt46PIoxHwMv/6vaCsEi19RTZtCFj94cHn5qzKX5uDPsOABPfLX61no81zlPG8BCUZqAIdT8fBnW/nlj2TCAqwsGd+DiCDvSz9Q1DzLntarXkZ8BCFNKu95E3fCBz31Bfyv+8t+UaqorXN0NdZb3y5fbZfUg3paJ6Jd0YZ7DjssmQA7vgQMemTn2K/QuLcecTnf6UM6UClUr6OepimNUvqCdHK7+x/4uO91TofJC7pPgnX/0QHL45tKDzK+uhf2/QDt74XbL747epWw2/RU1ro39EiaXx3oOFYnwV4s/8KWDf/tqIOPW16B7hN1ns+nt0HaMQhqoIOeM4eKHhPRTo9ombz0DaXzYnLOuD+3TzAEN9a/28FNoPkAiLze/ZydX+tVMF4B8MR2HSB+cSccXKmTsR8qWA22fIpORgW4/j7o8n86mbusATrokcY1/9LTK4//rqe6zh6FD3vpkcgOo/Uo2/IpOmn4QhHt9Ahl0356NCj7tL6teEEHawF14S8/X3y67lKUgk0zYcXzOuiL6qJHcioz6RoJRmqMjNx8hs/4lQPJmXRrEsLch7pIQquoOkrBr//VfxzbXWJJ79XI6YTlzxRdfAAGv6FXF51PKZjeTl80QVeYHTGLi9rzrf7kb60Nnd6HpFOwaSqEnIZeT+vNDz8dCkfW6yXY9y0sfsFL2g3vdwcMMH6Lrs57tTi5Hb59VE9rFQprrSu6XndX8eTntf+B1a/qnKLHtxRtL3DumA5ICqvcWgP1VEjnh9ynLwo5HXq0aP9Pejly0s7i5xiM0PMp6PVMwSqxHPhfZ11Dpt/fdSAAeipoRlc9XdP7b/o97V8OGGDAq/Cnx8oXhBRy5MNH/XRBwOaD4M6PYXZ/3dbIG+CBH4ve/x/LdFCSdkwvte/7QkEhuhJyRHLTYPYA3efh1+kREmuAvk8pnVu0d7EeaWpze+mBhT0Plk7WuWSgg65b37p0gbwKkGCkBolPzWLwO+vJyXfw4q2teahHCQlzQoiSKaWTeDe8BRjgyT0lf+JcMkGvNIKyDWc7HfCXJrDwGKSf92c2yAwffgp33aunxmZ20/kQ56/OKLTgAb0ZXpvb4c5PLuddXhn2PL2se88ineTpzNfHjRa48REddPnUhoxT8O4NesrhjjnFtxdIT9SjLWGt9eooq3/Z22DL0snIpw/pUZXjWwoCCvQI1vBZOvH055chsB5MiHHfbmHbZ/rftpDZG4Z/qJOYL8epvfDBTbpPwq/TU1u+IXqV2oUrx2zZetqwXsdL52ycO6bzUrKSixK4D6yEDW8X7SAOelqvcW9d96fBn3TgdeawDvoO/aIDJYNR765d0aCrDCQYqWE+33SUFxfvxsts5IcJPWgWHuDpJglRvez6Rn+KLu0idH4C7e0fXrow2KJFMGJE8eOGgv988w0MH140pO9XR49+eNfS5eRPxuoqnygYtxEi2l7Gm6sCOWfhjx90Iboj6/Uxn2BdwTcxVte1qdcJ/rLqil34XHYvgqWT9EiCpSDBOT8Lbv9AF8M7n1Iwd7i+QPuGwMh5pW9nUF7r39SBLugL//2Li2+tUBEJMfDxEJ1Y6xNcNG1lsurA9fSBS++ZYw2CO+e478Z9BUgwUsMopRj78RbW7k+hbb1AFj3aHS9z9VwKJsRV6fxE4IdW6UTY0jgc0KgRnDhR8v0GA9SvD/Hxenfg93tA6n5dMTUvwz1pscVg/em3Ojm4Cn563n3/IYAHV0CDkjdIrXRpJ3QRssLAqG57eHhNydMf2Wd0cm2roWUroV9WDjvMGaAL8V0qUbm84r7XK9JQemqr80M6CbZwVdLpQ3ol3a4FOjenVpTOq6kdrZebt7rt4rV9KokEIzVQcnou/aev41x2PuP7NOWpAVJ/RIhKtWqqnlq585OLLy1dswb6lGH55erV0Lt3QR2RwbhWbxiMOkE3tIWuTBva9LKbXuUcdr06ZPW/9Cf31sN0XZWq5HTqJatxS2HQv4sntVaF3DS9nPtSq3AqYt+Petqm/d060bc0Sl350ahSSDBSQy3blchjX2zDaIAF47rRsWEpdRCEEFfOV1/BvSXss3OhL7+EkSP19ye26hUTwU10kmfhqp/qLuesLlLXbIDevFHUKGW9fss4/jVm8HV1uf2GejgVTPhyG2v2JXu6SULUPHXrlv+8+p30ktTQptdOIAI6gbXN7RKIiIuSYOQaNPW2NjQI9uVkWi5jP97CmDmbOXAqw9PNEqLm6NlT54SUNjRuMEBUlD5PCCHByLUoyMfC9+N78FCPaCwmA2v3pzDwnfW8uHg3Z7Jsnm6eENc+kwneeUd/f2FAUvjz9On6PCGEBCPXqiBfCy/e2poVT/aif+twHE7F55uO0uv11Xy0/jA2eyllloUQlWP4cL18t94FNUvq1y9a1iuEACSBtcb49VAq/1wax97EdAAahfjy3OBW9G8djsFDWdZC1AgOB6xfD4mJOkekZ08ZERE1hqymEcU4nIqFMSd4fcU+UjL0bpDdmoTw7xHtiAqW5DIhhBCVS1bTiGJMRgN3dY5i9VO9Gd+nKVazkV8PnWbQO+tZsPU41SAuFUIIcQ2SYKQG8reaeWpAC1ZN7kXnRrXJzLPz9Dc7eXTuNklwFUIIUeUqFIzMmDGD6OhovL296dixI+vXry/13EWLFnHLLbdQp04dAgMD6dq1Kz/99FOFGywqT1SwL/Me6cozA1tgNhr4cU8SA6avY+nOkzidMkoihBCiapQ7GJk/fz6TJk3i+eefZ/v27fTs2ZNBgwZx7NixEs9ft24dt9xyC8uWLSMmJoY+ffowdOhQtm/fftmNF5fPZDTwWO+mLH68O03D/EnJyGP8l9sZMH0dS3acxCFBiRBCiCus3AmsXbp0oUOHDsycOdN1rFWrVgwbNoxp06aV6TnatGnD3Xffzd///vcynS8JrFUjN9/BzDWHmLMxnoxcOwBN6vjxRL9mDG0XidEoq26EEEKU3RVJYLXZbMTExNC/f3+34/379+fXX38t03M4nU4yMjIIDg4uz0uLKuBtMfHkLc3ZMKUvk29pTpCPhUMpWUycF8t9s3/n+JlsTzdRCCHENahcwUhqaioOh4Pw8HC34+Hh4SQlJZXpOd58802ysrK46667Sj0nLy+P9PR0t5uoOkE+Fp7o14wNU/rw11ua423Rq24GTF/H55uOSj6JEEKISlWhBNYLi2QppcpUOOurr75i6tSpzJ8/n7CwsFLPmzZtGkFBQa5bVFRURZopLlOAt4UJ/ZqxfOJNdG5Um2ybgxcX72bUR79z7LSMkgghhKgc5QpGQkNDMZlMxUZBkpOTi42WXGj+/Pk89NBDfP3119x8880XPfe5554jLS3NdTt+/Hh5mikqWXSoH/Mf6cpLQ1vjbTHy2+HT9J++lplrDpHvkLLyQgghLk+5ghEvLy86duzIypUr3Y6vXLmSbt26lfq4r776irFjx/Lll18yZMiQS76O1WolMDDQ7SY8y2g08ED3aH6ceBN/ahxMbr6Tf//4B7e+u4GYo2c83TwhhBDVWLlX08yfP5/777+f999/n65du/Lhhx8ya9Ys9uzZQ8OGDXnuuedISEjgs88+A3QgMnr0aN555x2Gn7cxlI+PD0FBQWV6TVlNc3VRSrFwWwKv/rCXs9n5ANzZsT5/ahxC3VreRAb5EBHkjbdF9t8QQoia7IruTTNjxgz+85//kJiYSNu2bXn77be56aabABg7dixHjhxhzZo1APTu3Zu1a9cWe44xY8bwySefVOqbEVXrbJaNacvj+HrriRLvvz6qFs8MaEG3pqFV3DIhhBBXA9koT1SZzfFnWBhzgoRzOZxMy+HkuRxy84tySXo1r8Ozg1rSqq782wkhRE0iwYjwGKUUSem5fLD2MHM3HcXuVBgMMPyG+tzVqT4dGtbGYpJtkYQQ4lonwYi4KhxJzeL1Ffv4YWei61iAt5keTUPp3aIO3ZqEUr+2T6lLw3PzHSgFPl6SfyKEENWNBCPiqhJ7/Bwfb4xn3f4UV9JroUBvM63qBtKqbiBNwvxJTs9l/6kM9p/K5OjpLExGA/1ahnNHx/r0alFHRlWEEKKakGBEXJUcTsXOE+dYsy+FtftT2HMyjXxH2X8FQ/29+PP19Rh8XQTt69fCLIGJEEJctSQYEdWCze7kYHImcYnp7E1M53BKJhFB3jQLC6B5eADNw/05nWVjYcwJFscmkJppcz020NtMz2Z1uKl5KF0bhxIeZMVqrrzpnNx8B/fjIRfXAAAcdUlEQVR99DuJabkM71CPuzpFERXsW2nPL4QQ1zoJRsQ1J9/hZN3+FBbHnmTd/hTScvKLnRPkY6FOgJU6/lZuaR3OmG6NMFVwt+FXvt/LnI3xrp8NBujZrA4jO0fRv01EhZ9XCCFqCglGxDXN4VTsOHGOtZeY7rmxUTBv3tW+3CMavx06zchZmwCY2K8ZMUfPsuFgquv+QW0j+N+9Ha7agCQ33yFF54QQHifBiKhRlFKk5eSTkpFHSkYeexPTeXvlfrJsDvy8TLw0tA13dqpfpg0dM3LzGTh9PQnnchh5YxTThrcD4OjpLOZtOc7s9fHYHE7u/1NDXvlzmzI9Z1Wav+UYLyzezbDr6zFt+HUez6txOBUT523HZnfyv3s74GWWPB8haoqyXr/lr4K4JhgMBmr5etEsPIBuTUP5S8/Grt2Gs2wOnlm4k4c/i2H7sbNcKv7+59I4Es7lEBXsw/NDWruONwzxY8rAlrx1d3sMBvh801FmrDlUae8hx+Zgy5EzOJwV/3yw52QaL363h3yHYkHMCSbOj/X4Zobfbk9g6c5EVuw9xf9+OeDRtgghrk4SjIhrVoMQX+Y90pVnB7XEYjKwKu4Ut8/4lT5vrGH6qv0cSc0q9phVe08xf+txDAZ4887r8beai51za7tI/n6rDlJe/2kfX2+9/F2lM/Ps3PnBr9z5/m/c9r8NbD1S/s0Hs/LsTPhSj0BcVy8Ii8nADzsTGf/lNmx2zwQkufkO3lqxz/Xze2sOsTshzSNtESVzOhVLd57kng9/Y9a6w55ujqihZJpG1Ahxiem8v/YQK/acIiff4TreINiX+rV9qF/bh3q1fPl801FSM/N4uGe026hISV5b/gfvrz2EyWhg1uiO9G0ZXqG22exOHvxki1tOCsDtN9Tj2UEtCQ/0vuRzKKWY/PUOvt2eQN0gb5Y90ZPtx88ybq4ORPq2DGPGqA6XnUfy7fYTfPbbUSwmI4HeZgK8LQR4m7m5VTg3Na9T7Pz31x7iteV/EBnkzXX1g/hpzylaRgSwZHyPa3a6Zt7mY6w/mMo//tyWYD8vTzeH+NQsnlqwg4hAb3q3qEPvFmHUCbDidCpW7E3i7ZUH2Hcqw3X+lw93oVsT2U9KVA7JGRGiBFl5dlbsTeLb7SfZcCCFkmZEmoX58/2EHpe8cCul+OuCHSzalgBA3SBv2kQG0joyiNZ1AwHFmax8zmbbOJNlw2o2MvLGBm7JtE6nYtL8WJbsOImvl4kZozrw054k5m05jlLg52Xizk5RXFcviJZ1A2ga5l/i8uUFW4/z9Dc7MRkNzHvkT3RuFAzAuv0pPPzZVvLsTno2C2XGqA4EeFsq1Hdf/H6U57/dXeJ9BgO8d28HBl9X13XsXLaNm/6zmvRcO2/c2Z4+Lepwy9vrOJNl44m+TZncv0WF2nE123Myjdv+txGHU3FjdDBzH+ri0aBLKcXIWZvYdNh9pK1d/SDyHYq4xHRAV0VuFubPtmPnqFfLh5+evKnEUUEhykuCESEu4UyWjQOnMkg4l8OJszkknM0hPTefybc0p1l4QJmeI9/hZPLXO/h+x8kynW8xGbi7cxTj+zQjPNDKP5bGMWdjPGajgTljO7tGF3aeOMdLS/aw/dg5t8ebjQaa1PGnVd0AWhZUrfX1MjF69mZy8h08PaAFj/dp6vaY3w6d5qFPt5Btc9A83J+PRnemQUj5Vhd9tfkYzy3aBcDorg3pEh1CRm4+6bn5xBw9y097TuFlMvLJA51duzS/+sNeZq2Pp2VEAD880ROTUU8bPf7lNkxGA9893p229YLK1Y6rmd3hZNiMjexOSHcdu6dzFNOGX+exJOfvYhOYOC8Wb4uRsd2i2XAwxa19/lYzD3ZvxEM9GmMyGRg4fR0nzuZwT+coXhvRziNtFtcWCUaEqEIZufnEJWaw52Qae06ms/9UBhaTkdq+XgT7Wajt58XuhDQ2HjwNgJfZSNfGIazdnwLA9LuvZ9gN9dyes3AYfdPhM8QlphOXmE56rr3UNvRsFsqnD9yIsYTlxjuOn+Phz7aSnJFHLV8LM0Z1KPNQ/Pwtx5iyUAciD/WI5oUhrdwurg6nYsJX21i2Kwl/q5l5j/yJIB8L/d5ci83h5JMHOtO7RZjr/Me/2MYPuxKvuemaD9YeYtryPwj0NvPCra2ZsnAnSsFLQ1vzQPfoKm9PRm4+/d5cS3JGHk/1b874vs0ASE7PZc3+FLLy7Ay7vh61z5tK2nRYL2lXCj4e25k+LcNKe3ohykSCESGuQpsOn+atFfvZfF6C6vODW/HwTY0v+VilFIlpua7AJC4xg7ikdOJTs4gM8mHx492pE2At9fGn0nN55LOt7DiRhtlo4KXb2nD/nxqWen5Wnp1F2xP4+3e7UQoe6N6Iv9/ausRP+Xl2B2PnbOG3w6cJ9bfSJjKQtftT6NYkhC/+0sXtMacz81zTNTc2Cmbizc3o1iTEo0uknU7FqrhTnErPpUmYP83DAwj1L70vL3QkNYsB09eRZ3fynzvacVenKGatO8yry+IwGuCTB250y6nJttlJz7ETEXTpfKCK+ufSvXy0IZ5GIb789ORNZa5O/I+le5m9IZ6wACsrnryJWr6ez3sR1ZcEI0JcpZRSbDx4mo83xtM5OphxvZpc1vPl2ByYjIYyjTDk5juYsnAn38XqaaV6tXx0xdoAK6H+VsxGA/GpWRxKySQxLdf1uLHdGvHS0JIDkUIZufnc/cEm9iYWTQN8P74H19UvPhWzcu8pHvsixlWo7vqoWozv05S+LcM4lZHL4ZQsDqdkcuxMNrV8vWgU4kejUF8ahvhVei7DrwdT+dfyOLfpC4BgPy+a1vEnIsibUH8roQFehPpbaRzqR4cGtV0jUEop7p31O78dPk2PpqF8/tCNGAwGlFI8/c1Ovok5QYC3mUd7N+HAqUx2J6RxKCUTp4KWEQH8+fp6DG1fl/q1K2+rgT+S0hny7gYcTlVsZOpScvMdDH53PYdTsvjz9ZG8c88NldYuUfNIMCKEKJFSiplrD/HGT/tKTOA9X4ifF/d2acDkW5qXaeQiOSOXO2b+xrEz2dzWPpJ3R5Z+IUs4l8OHaw8xb8tx8gqWHltMhktunBji50V4oDfhgVbCAvTX8CBvIoN8qFvLm7pBPvhbzSSl53L8TDbHz2Rz4mwOFpOB+rULV0/5ci7Hxr+X/8HqfXqqzN9qplOj2sSnZnHsTDYX+8sYEejNbddHclv7SHYnpPHsol34WEz8NOkmt3ycPLuDe2f9TszRs8Wew2DA7TU6N6rNwLZ16dU8lCZ1/Cs8UqSU4u4PNrH5yBkGtong/fs7lvs5Yo+fY/iMjTgVtKobyJ+vj2Ro+0jq1fKpUJtEzSXBiBDiolIz8zh+JltXrc3UlWvz7E6iQ/1oUsefJnX8KjREf/JcDt9uT2BUlwZlenxKRh6zN8Qzd9NRMvPsmIwGGgT70jjUjwYhvqRl5xN/Ooujp7M5k2W75PNB8Qv9xZiNBkZ1acAT/ZoRUjA1k2NzcCglk0Mpma7+Sc2wkZKZx/ZjZ8k4L3en8LVeGNKKv/QsPt2WmpnHswt3YTJC28gg2tQLpG1kEFazieW7E/ku9iSb4k+7tTcyyJuezerQvVko0SF+1K3lTYif10UDlBybg+SMXFbuPcU/f4jD22Lk57/2rnAAMXtDPK8tj3MLDm9sFEz3pqFEBfsQFexLVG1fwgKsJeYpCQESjAghqpnMPDspGXnUq+VT6pRTWk4+J85mk5yRR3J6LqfS8ziVnktSWi4n03JJTMvhXLbeQNFiMlCvlr5o1q/tQ75DceJsNgnncjh5LheHUzGwTQTPDGxB4zr+ZW5nnt3B6j9SWLIjgVVxydjsTtpH1WLRo90qvFdRUlouS3eeZO3+FH6PP1NikTovk5GIIG8Cfcw4neBUCqUg3+kkNSOvWHJzSSuryutslo3lu5P4LjaB3+NLLsTnZTLqab6CDSoL85ZSM/M4nZnH6Swb57Lz8beaqeVrKbh5UcvH4qpTU3ir5eNFLV8LwX5e1PL1ws/LhEMpnE6wO504nAqT0YC3xYTZaHAFZ0op8uxOsm0OsvLsWC1GQvyspf57FF72rratHK5FEowIIWqkbJudzFw7If6lX4zsDie5dudl55+k5+azJf4MnRoGE+RbsfotF8qxOdh85Azr96ew9ehZTp7LISUzr0wjPd4WI+GB3nRrEsrU21qXOWm1LE6ey2HZrkT2JWVw4mwOx89mk5iWe1nbF1wOowGsZhMmo4Fsm73YlKPBAMG+Os8nyNfiShrOyM0nI9eOAnwtJnytJvy8zPh4mfCzmvG3mgu+mkrsv2ybnaw8B5l5djLz7OTZHe6vi4EgHwthgVY9nRhgpZavF/kOJzaHE5vdSb7DiQGd52UxGfEyGzEZITffSY7NQU6+gxybA4MBvC0mfL1M+FhMeFtMOJXC7lTYHQqH04nJaCTIx0Kgj1l/9ba4fu8Lf2cKH+NwKvIdOqjLtjn0/yt5dlcQ17dlWLkC87K4osHIjBkzeP3110lMTKRNmzZMnz6dnj17lnr+2rVrmTx5Mnv27CEyMpJnnnmGcePGlfn1JBgRQtRkNruTU+m5JKblkpVnx2g0YDSA0WDAaDAQ6u9FWKA3gd7mKv20n+/Q7SrcoDIlM4/k9DyAgpESL0L8rdTysZBlc3A228a5bBtns/JJy8knM68oOEjPzedctr6dzbaRbXNc4tVLZjUbsTmcZZ6mE0X+O/IGhraPrNTnLOv1u9wfC+bPn8+kSZOYMWMG3bt354MPPmDQoEHs3buXBg0aFDs/Pj6ewYMH8/DDDzN37lw2btzIY489Rp06dRgxYkR5X14IIWocL7NR52gEV96Km8pgMRkLkoIrv125+Q6ybQ7MJgNmowGT0YDJYMDu1FMyeXYHefn6U76vlwlfqxkfix4psTucnMm2kZphIzUzj7QcPU0U6FO0hYHRYHCNCBR+zbLZycqzk5mnfy6cLlMURTa+Xmb8vEz4e1v06InFxPnhn1JwNtvmmkJMycjjbLbNNQLiZTZiNRlRgM3hJN+uR0wcToXVbMLHy4SvxYS3RU9V5hT0Q26+g9x8J0ZjUX+YjQbyHU7Sc+yk5egALz03H6dSGDBQGJcaDQa3fjQbjXhbjPhbzfr9WM34WU1XdKn5pZR7ZKRLly506NCBmTNnuo61atWKYcOGMW3atGLnT5kyhSVLlhAXF+c6Nm7cOHbs2MFvv/1WpteUkREhhBCi+inr9btcpQ9tNhsxMTH079/f7Xj//v359ddfS3zMb7/9Vuz8AQMGsHXrVvLz80t8TF5eHunp6W43IYQQQlybyhWMpKam4nA4CA933500PDycpKSkEh+TlJRU4vl2u53U1NQSHzNt2jSCgoJct6ioqPI0UwghhBDVSIU2hbgwQUopddGkqZLOL+l4oeeee460tDTX7fjx4xVpphBCCCGqgXIlsIaGhmIymYqNgiQnJxcb/SgUERFR4vlms5mQkJASH2O1WrFay74vhBBCCCGqr3KNjHh5edGxY0dWrlzpdnzlypV069atxMd07dq12PkrVqygU6dOWCyVsy5fCCGEENVXuadpJk+ezEcffcScOXOIi4vjySef5NixY666Ic899xyjR492nT9u3DiOHj3K5MmTiYuLY86cOcyePZunnnqq8t6FEEIIIaqtctcZufvuuzl9+jSvvPIKiYmJtG3blmXLltGwod6KPDExkWPHjrnOj46OZtmyZTz55JO89957REZG8u6770qNESGEEEIAUg5eCCGEEFfIFakzIoQQQghR2SQYEUIIIYRHSTAihBBCCI+SYEQIIYQQHiXBiBBCCCE8SoIRIYQQQnhUueuMeELh6mPZvVcIIYSoPgqv25eqIlItgpGMjAwA2b1XCCGEqIYyMjIICgoq9f5qUfTM6XRy8uRJAgICLro7cHmlp6cTFRXF8ePHpZjaFSZ9XbWkv6uO9HXVkb6uOpXV10opMjIyiIyMxGgsPTOkWoyMGI1G6tevf8WePzAwUH6xq4j0ddWS/q460tdVR/q66lRGX19sRKSQJLAKIYQQwqMkGBFCCCGER5mmTp061dON8CSTyUTv3r0xm6vFjFW1Jn1dtaS/q470ddWRvq46VdnX1SKBVQghhBDXLpmmEUIIIYRHSTAihBBCCI+SYEQIIYQQHiXBiBBCCCE8qkYHIzNmzCA6Ohpvb286duzI+vXrPd2kam/atGl07tyZgIAAwsLCGDZsGPv27XM7RynF1KlTiYyMxMfHh969e7Nnzx4PtfjaMG3aNAwGA5MmTXIdk36uXAkJCdx3332EhITg6+vL9ddfT0xMjOt+6e/KYbfbeeGFF4iOjsbHx4fGjRvzyiuv4HQ6XedIX1fMunXrGDp0KJGRkRgMBhYvXux2f1n6NS8vjwkTJhAaGoqfnx+33XYbJ06cuPzGqRpq3rx5ymKxqFmzZqm9e/eqiRMnKj8/P3X06FFPN61aGzBggPr444/V7t27VWxsrBoyZIhq0KCByszMdJ3z2muvqYCAALVw4UK1a9cudffdd6u6deuq9PR0D7a8+tq8ebNq1KiRateunZo4caLruPRz5Tlz5oxq2LChGjt2rPr9999VfHy8WrVqlTp48KDrHOnvyvHPf/5ThYSEqKVLl6r4+Hi1YMEC5e/vr6ZPn+46R/q6YpYtW6aef/55tXDhQgWob7/91u3+svTruHHjVL169dTKlSvVtm3bVJ8+fVT79u2V3W6/rLbV2GDkxhtvVOPGjXM71rJlS/Xss896qEXXpuTkZAWotWvXKqWUcjqdKiIiQr322muuc3Jzc1VQUJB6//33PdXMaisjI0M1a9ZMrVy5UvXq1csVjEg/V64pU6aoHj16lHq/9HflGTJkiHrwwQfdjg0fPlzdd999Sinp68pyYTBSln49d+6cslgsat68ea5zEhISlNFoVD/++ONltadGTtPYbDZiYmLo37+/2/H+/fvz66+/eqhV16a0tDQAgoODAYiPjycpKcmt761WK7169ZK+r4DHH3+cIUOGcPPNN7sdl36uXEuWLKFTp07ceeedhIWFccMNNzBr1izX/dLfladHjx78/PPP7N+/H4AdO3awYcMGBg8eDEhfXyll6deYmBjy8/PdzomMjKRt27aX3fc1soRdamoqDoeD8PBwt+Ph4eEkJSV5qFXXHqUUkydPpkePHrRt2xbA1b8l9f3Ro0ervI3V2bx589i2bRtbtmwpdp/0c+U6fPgwM2fOZPLkyfztb39j8+bNPPHEE1itVkaPHi39XYmmTJlCWloaLVu2xGQy4XA4ePXVVxk5ciQgv9tXSln6NSkpCS8vL2rXrl3snMu9dtbIYKSQwWBw+1kpVeyYqLjx48ezc+dONmzYUOw+6fvLc/z4cSZOnMiKFSvw9vYu9Tzp58rhdDrp1KkT//rXvwC44YYb2LNnDzNnzmT06NGu86S/L9/8+fOZO3cuX375JW3atCE2NpZJkyYRGRnJmDFjXOdJX18ZFenXyuj7GjlNExoaislkKhbJJScnF4sKRcVMmDCBJUuWsHr1aurXr+86HhERASB9f5liYmJITk6mY8eOmM1mzGYza9eu5d1338VsNrv6Uvq5ctStW5fWrVu7HWvVqhXHjh0D5Pe6Mj399NM8++yz3HPPPVx33XXcf//9PPnkk0ybNg2Qvr5SytKvERER2Gw2zp49W+o5FVUjgxEvLy86duzIypUr3Y6vXLmSbt26eahV1walFOPHj2fRokX88ssvREdHu90fHR1NRESEW9/bbDbWrl0rfV8O/fr1Y9euXcTGxrpunTp1YtSoUcTGxtK4cWPp50rUvXv3YkvU9+/fT8OGDQH5va5M2dnZGI3ulyaTyeRa2it9fWWUpV87duyIxWJxOycxMZHdu3dfft9fVvprNVa4tHf27Nlq7969atKkScrPz08dOXLE002r1h599FEVFBSk1qxZoxITE1237Oxs1zmvvfaaCgoKUosWLVK7du1SI0eOlGV5leD81TRKST9Xps2bNyuz2axeffVVdeDAAfXFF18oX19fNXfuXNc50t+VY8yYMapevXqupb2LFi1SoaGh6plnnnGdI31dMRkZGWr79u1q+/btClBvvfWW2r59u6ukRVn6ddy4cap+/fpq1apVatu2bapv376ytPdyvffee6phw4bKy8tLdejQwbX8VFQcUOLt448/dp3jdDrVSy+9pCIiIpTValU33XST2rVrl+cafY24MBiRfq5c33//vWrbtq2yWq2qZcuW6sMPP3S7X/q7cqSnp6uJEyeqBg0aKG9vb9W4cWP1/PPPq7y8PNc50tcVs3r16hL/Po8ZM0YpVbZ+zcnJUePHj1fBwcHKx8dH3XrrrerYsWOX3TaDUkpd3tiKEEIIIUTF1cicESGEEEJcPSQYEUIIIYRHSTAihBBCCI+SYEQIIYQQHiXBiBBCCCE8SoIRIYQQQniUBCNCCCGE8CgJRoQQ1ZLBYGDx4sWeboYQohJIMCKEKLexY8diMBiK3QYOHOjppgkhqiGzpxsghKieBg4cyMcff+x2zGq1eqg1QojqTEZGhBAVYrVaiYiIcLvVrl0b0FMoM2fOZNCgQfj4+BAdHc2CBQvcHr9r1y769u2Lj48PISEhPPLII2RmZrqdM2fOHNq0aYPVaqVu3bqMHz/e7f7U1FRuv/12fH19adasGUuWLLmyb1oIcUVIMCKEuCJefPFFRowYwY4dO7jvvvsYOXIkcXFxgN4mfuDAgdSuXZstW7awYMECVq1a5RZszJw5k8cff5xHHnmEXbt2sWTJEpo2ber2Gi+//DJ33XUXO3fuZPDgwYwaNYozZ85U6fsUQlSCy95qTwhR44wZM0aZTCbl5+fndnvllVeUUnr35nHjxrk9pkuXLurRRx9VSin14Ycfqtq1a6vMzEzX/T/88IMyGo0qKSlJKaVUZGSkev7550ttA6BeeOEF18+ZmZnKYDCo5cuXV9r7FEJUDckZEUJUSJ8+fZg5c6bbseDgYNf3Xbt2dbuva9euxMbGAhAXF0f79u3x8/Nz3d+9e3ecTif79u3DYDBw8uRJ+vXrd9E2tGvXzvW9n58fAQEBJCcnV/g9CSE8Q4IRIUSF+Pn5FZs2uRSDwQCAUsr1fUnn+Pj4lOn5LBZLscc6nc5ytUkI4XmSMyKEuCI2bdpU7OeWLVsC0Lp1a2JjY8nKynLdv3HjRoxGI82bNycgIIBGjRrx888/V2mbhRCeISMjQogKycvLIykpye2Y2WwmNDQUgAULFtCpUyd69OjBF198webNm5k9ezYAo0aN4qWXXmLMmDFMnTqVlJQUJkyYwP333094eDgAU6dOZdy4cYSFhTFo0CAyMjLYuHEjEyZMqNo3KoS44iQYEUJUyI8//kjdunXdjrVo0YI//vgD0Ctd5s2bx2OPPUZERARffPEFrVu3BsDX15effvqJiRMn0rlzZ3x9fRkxYgRvvfWW67nGjBlDbm4ub7/9Nk899RShoaHccccdVfcGhRBVxqCUUp5uhBDi2mIwGPj2228ZNmyYp5sihKgGJGdECCGEEB4lwYgQQgghPEpyRoQQlU5mf4UQ5SEjI0IIIYTwKAlGhBBCCOFREowIIYQQwqMkGBFCCCGER0kwIoQQQgiPkmBECCGEEB4lwYgQQgghPEqCESGEEEJ4lAQjQgghhPCo/wctGlUdIaYoAAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5.5, min_value+.125, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 4.0970458765155495%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ubun\n",
      "= ['UW', 'B', 'UW', 'N']\n",
      "< UW B UW N ['UW', 'B', 'UW', 'N']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0f1c1edac0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x666.667 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
