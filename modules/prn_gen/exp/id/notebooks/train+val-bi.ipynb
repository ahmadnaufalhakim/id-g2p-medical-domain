{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn_gen/exp/id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL =\"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"32\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models_fallback\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  if split_name == \"train+val\" :\n",
    "    print(f\"Merging train and val entries ..\")\n",
    "    with open(os.path.join(DATA_DIR, f\"train.csv\"), encoding=\"utf-8\") as f_train_csv, \\\n",
    "         open(os.path.join(DATA_DIR, f\"val.csv\"), encoding=\"utf-8\") as f_val_csv :\n",
    "      next(f_train_csv, None)\n",
    "      next(f_val_csv, None)\n",
    "      train_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_train_csv]\n",
    "      val_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_val_csv]\n",
    "      pairs = train_pairs + val_pairs\n",
    "      graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "      phonemes_list = [extract_arpabet_phonemes(pair[-1]) for pair in pairs]\n",
    "      g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "    return g2p_dataset, pairs\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[-1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging train and val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train+val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[274, 122, 295, 167, 436, 291, 83, 203, 310, 458, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f82e2835a00> ([14, 27, 310, 458, 1], [23, 3, 1])\n",
      "([14, 27, 310, 458, 1], [23, 3, 1])\n",
      "([14, 27, 310, 458, 1], [23, 3, 1])\n",
      "train grp 477 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'d\", 7: \"'h\", 8: \"'i\", 9: \"'j\", 10: \"'n\", 11: \"'o\", 12: \"'r\", 13: \"'u\", 14: '--', 15: '-a', 16: '-b', 17: '-c', 18: '-d', 19: '-e', 20: '-g', 21: '-h', 22: '-i', 23: '-j', 24: '-k', 25: '-l', 26: '-m', 27: '-n', 28: '-p', 29: '-q', 30: '-r', 31: '-s', 32: '-t', 33: '-u', 34: '-w', 35: \"a'\", 36: 'a-', 37: 'aa', 38: 'ab', 39: 'ac', 40: 'ad', 41: 'ae', 42: 'af', 43: 'ag', 44: 'ah', 45: 'ai', 46: 'aj', 47: 'ak', 48: 'al', 49: 'am', 50: 'an', 51: 'ao', 52: 'ap', 53: 'aq', 54: 'ar', 55: 'as', 56: 'at', 57: 'au', 58: 'av', 59: 'aw', 60: 'ay', 61: 'az', 62: 'ba', 63: 'bb', 64: 'bd', 65: 'be', 66: 'bh', 67: 'bi', 68: 'bj', 69: 'bk', 70: 'bl', 71: 'bm', 72: 'bn', 73: 'bo', 74: 'br', 75: 'bs', 76: 'bt', 77: 'bu', 78: 'bv', 79: 'by', 80: 'ca', 81: 'ce', 82: 'ch', 83: 'ci', 84: 'ck', 85: 'cm', 86: 'co', 87: 'cr', 88: 'cu', 89: \"d'\", 90: 'da', 91: 'db', 92: 'de', 93: 'dh', 94: 'di', 95: 'dj', 96: 'dk', 97: 'dm', 98: 'dn', 99: 'do', 100: 'dr', 101: 'ds', 102: 'du', 103: 'dv', 104: 'dw', 105: 'dy', 106: 'dz', 107: \"e'\", 108: 'e-', 109: 'ea', 110: 'eb', 111: 'ec', 112: 'ed', 113: 'ee', 114: 'ef', 115: 'eg', 116: 'eh', 117: 'ei', 118: 'ej', 119: 'ek', 120: 'el', 121: 'em', 122: 'en', 123: 'eo', 124: 'ep', 125: 'er', 126: 'es', 127: 'et', 128: 'eu', 129: 'ev', 130: 'ew', 131: 'ey', 132: 'ez', 133: 'fa', 134: 'fb', 135: 'fd', 136: 'fe', 137: 'fi', 138: 'fk', 139: 'fl', 140: 'fn', 141: 'fo', 142: 'fr', 143: 'fs', 144: 'ft', 145: 'fu', 146: 'fw', 147: 'g-', 148: 'ga', 149: 'gb', 150: 'gc', 151: 'gd', 152: 'ge', 153: 'gf', 154: 'gg', 155: 'gh', 156: 'gi', 157: 'gj', 158: 'gk', 159: 'gl', 160: 'gm', 161: 'gn', 162: 'go', 163: 'gp', 164: 'gr', 165: 'gs', 166: 'gt', 167: 'gu', 168: 'h-', 169: 'ha', 170: 'hb', 171: 'hd', 172: 'he', 173: 'hf', 174: 'hg', 175: 'hi', 176: 'hj', 177: 'hk', 178: 'hl', 179: 'hm', 180: 'hn', 181: 'ho', 182: 'hp', 183: 'hr', 184: 'hs', 185: 'ht', 186: 'hu', 187: 'hw', 188: 'hy', 189: 'i-', 190: 'ia', 191: 'ib', 192: 'ic', 193: 'id', 194: 'ie', 195: 'if', 196: 'ig', 197: 'ih', 198: 'ii', 199: 'ij', 200: 'ik', 201: 'il', 202: 'im', 203: 'in', 204: 'io', 205: 'ip', 206: 'ir', 207: 'is', 208: 'it', 209: 'iu', 210: 'iv', 211: 'iw', 212: 'iy', 213: 'iz', 214: 'ja', 215: 'je', 216: 'ji', 217: 'jl', 218: 'jn', 219: 'jo', 220: 'jr', 221: 'jt', 222: 'ju', 223: 'jw', 224: 'k-', 225: 'ka', 226: 'kb', 227: 'kc', 228: 'kd', 229: 'ke', 230: 'kh', 231: 'ki', 232: 'kj', 233: 'kk', 234: 'kl', 235: 'km', 236: 'kn', 237: 'ko', 238: 'kp', 239: 'kr', 240: 'ks', 241: 'kt', 242: 'ku', 243: 'kw', 244: 'ky', 245: 'kz', 246: 'l-', 247: 'la', 248: 'lb', 249: 'ld', 250: 'le', 251: 'lf', 252: 'lg', 253: 'lh', 254: 'li', 255: 'lj', 256: 'lk', 257: 'll', 258: 'lm', 259: 'ln', 260: 'lo', 261: 'lp', 262: 'lr', 263: 'ls', 264: 'lt', 265: 'lu', 266: 'lv', 267: 'lw', 268: 'ly', 269: \"m'\", 270: 'ma', 271: 'mb', 272: 'mc', 273: 'md', 274: 'me', 275: 'mf', 276: 'mi', 277: 'mk', 278: 'ml', 279: 'mm', 280: 'mn', 281: 'mo', 282: 'mp', 283: 'mr', 284: 'ms', 285: 'mt', 286: 'mu', 287: 'mz', 288: 'n-', 289: 'na', 290: 'nb', 291: 'nc', 292: 'nd', 293: 'ne', 294: 'nf', 295: 'ng', 296: 'nh', 297: 'ni', 298: 'nj', 299: 'nk', 300: 'nl', 301: 'nm', 302: 'nn', 303: 'no', 304: 'np', 305: 'ns', 306: 'nt', 307: 'nu', 308: 'nv', 309: 'nw', 310: 'ny', 311: 'nz', 312: \"o'\", 313: 'o-', 314: 'oa', 315: 'ob', 316: 'oc', 317: 'od', 318: 'oe', 319: 'of', 320: 'og', 321: 'oh', 322: 'oi', 323: 'oj', 324: 'ok', 325: 'ol', 326: 'om', 327: 'on', 328: 'oo', 329: 'op', 330: 'or', 331: 'os', 332: 'ot', 333: 'ov', 334: 'ow', 335: 'oy', 336: 'oz', 337: 'pa', 338: 'pc', 339: 'pe', 340: 'pi', 341: 'pj', 342: 'pk', 343: 'pl', 344: 'pm', 345: 'pn', 346: 'po', 347: 'pr', 348: 'ps', 349: 'pt', 350: 'pu', 351: 'py', 352: 'qi', 353: 'qu', 354: \"r'\", 355: 'r-', 356: 'ra', 357: 'rb', 358: 'rc', 359: 'rd', 360: 're', 361: 'rf', 362: 'rg', 363: 'rh', 364: 'ri', 365: 'rj', 366: 'rk', 367: 'rl', 368: 'rm', 369: 'rn', 370: 'ro', 371: 'rp', 372: 'rr', 373: 'rs', 374: 'rt', 375: 'ru', 376: 'rv', 377: 'rw', 378: 'ry', 379: 'rz', 380: 's-', 381: 'sa', 382: 'sb', 383: 'sc', 384: 'sd', 385: 'se', 386: 'sf', 387: 'sh', 388: 'si', 389: 'sj', 390: 'sk', 391: 'sl', 392: 'sm', 393: 'sn', 394: 'so', 395: 'sp', 396: 'sr', 397: 'ss', 398: 'st', 399: 'su', 400: 'sw', 401: 'sy', 402: 't-', 403: 'ta', 404: 'tb', 405: 'te', 406: 'tf', 407: 'tg', 408: 'th', 409: 'ti', 410: 'tk', 411: 'tl', 412: 'tm', 413: 'tn', 414: 'to', 415: 'tp', 416: 'tr', 417: 'ts', 418: 'tt', 419: 'tu', 420: 'tw', 421: \"u'\", 422: 'u-', 423: 'ua', 424: 'ub', 425: 'uc', 426: 'ud', 427: 'ue', 428: 'uf', 429: 'ug', 430: 'uh', 431: 'ui', 432: 'uj', 433: 'uk', 434: 'ul', 435: 'um', 436: 'un', 437: 'uo', 438: 'up', 439: 'ur', 440: 'us', 441: 'ut', 442: 'uv', 443: 'uw', 444: 'uy', 445: 'uz', 446: 'va', 447: 've', 448: 'vg', 449: 'vi', 450: 'vo', 451: 'vu', 452: 'wa', 453: 'we', 454: 'wi', 455: 'wo', 456: 'wt', 457: 'wu', 458: 'ya', 459: 'ye', 460: 'yg', 461: 'yh', 462: 'yi', 463: 'yk', 464: 'yo', 465: 'yr', 466: 'yt', 467: 'yu', 468: 'za', 469: 'zb', 470: 'ze', 471: 'zh', 472: 'zi', 473: 'zm', 474: 'zo', 475: 'zu', 476: 'zz'}\n",
      "test grp 477 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'d\", 7: \"'h\", 8: \"'i\", 9: \"'j\", 10: \"'n\", 11: \"'o\", 12: \"'r\", 13: \"'u\", 14: '--', 15: '-a', 16: '-b', 17: '-c', 18: '-d', 19: '-e', 20: '-g', 21: '-h', 22: '-i', 23: '-j', 24: '-k', 25: '-l', 26: '-m', 27: '-n', 28: '-p', 29: '-q', 30: '-r', 31: '-s', 32: '-t', 33: '-u', 34: '-w', 35: \"a'\", 36: 'a-', 37: 'aa', 38: 'ab', 39: 'ac', 40: 'ad', 41: 'ae', 42: 'af', 43: 'ag', 44: 'ah', 45: 'ai', 46: 'aj', 47: 'ak', 48: 'al', 49: 'am', 50: 'an', 51: 'ao', 52: 'ap', 53: 'aq', 54: 'ar', 55: 'as', 56: 'at', 57: 'au', 58: 'av', 59: 'aw', 60: 'ay', 61: 'az', 62: 'ba', 63: 'bb', 64: 'bd', 65: 'be', 66: 'bh', 67: 'bi', 68: 'bj', 69: 'bk', 70: 'bl', 71: 'bm', 72: 'bn', 73: 'bo', 74: 'br', 75: 'bs', 76: 'bt', 77: 'bu', 78: 'bv', 79: 'by', 80: 'ca', 81: 'ce', 82: 'ch', 83: 'ci', 84: 'ck', 85: 'cm', 86: 'co', 87: 'cr', 88: 'cu', 89: \"d'\", 90: 'da', 91: 'db', 92: 'de', 93: 'dh', 94: 'di', 95: 'dj', 96: 'dk', 97: 'dm', 98: 'dn', 99: 'do', 100: 'dr', 101: 'ds', 102: 'du', 103: 'dv', 104: 'dw', 105: 'dy', 106: 'dz', 107: \"e'\", 108: 'e-', 109: 'ea', 110: 'eb', 111: 'ec', 112: 'ed', 113: 'ee', 114: 'ef', 115: 'eg', 116: 'eh', 117: 'ei', 118: 'ej', 119: 'ek', 120: 'el', 121: 'em', 122: 'en', 123: 'eo', 124: 'ep', 125: 'er', 126: 'es', 127: 'et', 128: 'eu', 129: 'ev', 130: 'ew', 131: 'ey', 132: 'ez', 133: 'fa', 134: 'fb', 135: 'fd', 136: 'fe', 137: 'fi', 138: 'fk', 139: 'fl', 140: 'fn', 141: 'fo', 142: 'fr', 143: 'fs', 144: 'ft', 145: 'fu', 146: 'fw', 147: 'g-', 148: 'ga', 149: 'gb', 150: 'gc', 151: 'gd', 152: 'ge', 153: 'gf', 154: 'gg', 155: 'gh', 156: 'gi', 157: 'gj', 158: 'gk', 159: 'gl', 160: 'gm', 161: 'gn', 162: 'go', 163: 'gp', 164: 'gr', 165: 'gs', 166: 'gt', 167: 'gu', 168: 'h-', 169: 'ha', 170: 'hb', 171: 'hd', 172: 'he', 173: 'hf', 174: 'hg', 175: 'hi', 176: 'hj', 177: 'hk', 178: 'hl', 179: 'hm', 180: 'hn', 181: 'ho', 182: 'hp', 183: 'hr', 184: 'hs', 185: 'ht', 186: 'hu', 187: 'hw', 188: 'hy', 189: 'i-', 190: 'ia', 191: 'ib', 192: 'ic', 193: 'id', 194: 'ie', 195: 'if', 196: 'ig', 197: 'ih', 198: 'ii', 199: 'ij', 200: 'ik', 201: 'il', 202: 'im', 203: 'in', 204: 'io', 205: 'ip', 206: 'ir', 207: 'is', 208: 'it', 209: 'iu', 210: 'iv', 211: 'iw', 212: 'iy', 213: 'iz', 214: 'ja', 215: 'je', 216: 'ji', 217: 'jl', 218: 'jn', 219: 'jo', 220: 'jr', 221: 'jt', 222: 'ju', 223: 'jw', 224: 'k-', 225: 'ka', 226: 'kb', 227: 'kc', 228: 'kd', 229: 'ke', 230: 'kh', 231: 'ki', 232: 'kj', 233: 'kk', 234: 'kl', 235: 'km', 236: 'kn', 237: 'ko', 238: 'kp', 239: 'kr', 240: 'ks', 241: 'kt', 242: 'ku', 243: 'kw', 244: 'ky', 245: 'kz', 246: 'l-', 247: 'la', 248: 'lb', 249: 'ld', 250: 'le', 251: 'lf', 252: 'lg', 253: 'lh', 254: 'li', 255: 'lj', 256: 'lk', 257: 'll', 258: 'lm', 259: 'ln', 260: 'lo', 261: 'lp', 262: 'lr', 263: 'ls', 264: 'lt', 265: 'lu', 266: 'lv', 267: 'lw', 268: 'ly', 269: \"m'\", 270: 'ma', 271: 'mb', 272: 'mc', 273: 'md', 274: 'me', 275: 'mf', 276: 'mi', 277: 'mk', 278: 'ml', 279: 'mm', 280: 'mn', 281: 'mo', 282: 'mp', 283: 'mr', 284: 'ms', 285: 'mt', 286: 'mu', 287: 'mz', 288: 'n-', 289: 'na', 290: 'nb', 291: 'nc', 292: 'nd', 293: 'ne', 294: 'nf', 295: 'ng', 296: 'nh', 297: 'ni', 298: 'nj', 299: 'nk', 300: 'nl', 301: 'nm', 302: 'nn', 303: 'no', 304: 'np', 305: 'ns', 306: 'nt', 307: 'nu', 308: 'nv', 309: 'nw', 310: 'ny', 311: 'nz', 312: \"o'\", 313: 'o-', 314: 'oa', 315: 'ob', 316: 'oc', 317: 'od', 318: 'oe', 319: 'of', 320: 'og', 321: 'oh', 322: 'oi', 323: 'oj', 324: 'ok', 325: 'ol', 326: 'om', 327: 'on', 328: 'oo', 329: 'op', 330: 'or', 331: 'os', 332: 'ot', 333: 'ov', 334: 'ow', 335: 'oy', 336: 'oz', 337: 'pa', 338: 'pc', 339: 'pe', 340: 'pi', 341: 'pj', 342: 'pk', 343: 'pl', 344: 'pm', 345: 'pn', 346: 'po', 347: 'pr', 348: 'ps', 349: 'pt', 350: 'pu', 351: 'py', 352: 'qi', 353: 'qu', 354: \"r'\", 355: 'r-', 356: 'ra', 357: 'rb', 358: 'rc', 359: 'rd', 360: 're', 361: 'rf', 362: 'rg', 363: 'rh', 364: 'ri', 365: 'rj', 366: 'rk', 367: 'rl', 368: 'rm', 369: 'rn', 370: 'ro', 371: 'rp', 372: 'rr', 373: 'rs', 374: 'rt', 375: 'ru', 376: 'rv', 377: 'rw', 378: 'ry', 379: 'rz', 380: 's-', 381: 'sa', 382: 'sb', 383: 'sc', 384: 'sd', 385: 'se', 386: 'sf', 387: 'sh', 388: 'si', 389: 'sj', 390: 'sk', 391: 'sl', 392: 'sm', 393: 'sn', 394: 'so', 395: 'sp', 396: 'sr', 397: 'ss', 398: 'st', 399: 'su', 400: 'sw', 401: 'sy', 402: 't-', 403: 'ta', 404: 'tb', 405: 'te', 406: 'tf', 407: 'tg', 408: 'th', 409: 'ti', 410: 'tk', 411: 'tl', 412: 'tm', 413: 'tn', 414: 'to', 415: 'tp', 416: 'tr', 417: 'ts', 418: 'tt', 419: 'tu', 420: 'tw', 421: \"u'\", 422: 'u-', 423: 'ua', 424: 'ub', 425: 'uc', 426: 'ud', 427: 'ue', 428: 'uf', 429: 'ug', 430: 'uh', 431: 'ui', 432: 'uj', 433: 'uk', 434: 'ul', 435: 'um', 436: 'un', 437: 'uo', 438: 'up', 439: 'ur', 440: 'us', 441: 'ut', 442: 'uv', 443: 'uw', 444: 'uy', 445: 'uz', 446: 'va', 447: 've', 448: 'vg', 449: 'vi', 450: 'vo', 451: 'vu', 452: 'wa', 453: 'we', 454: 'wi', 455: 'wo', 456: 'wt', 457: 'wu', 458: 'ya', 459: 'ye', 460: 'yg', 461: 'yh', 462: 'yi', 463: 'yk', 464: 'yo', 465: 'yr', 466: 'yt', 467: 'yu', 468: 'za', 469: 'zb', 470: 'ze', 471: 'zh', 472: 'zi', 473: 'zm', 474: 'zo', 475: 'zu', 476: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "473 {'--': 14, '-n': 27, 'ny': 310, 'ya': 458, '-a': 15, 'an': 50, 'nd': 292, 'da': 90, '-b': 16, 'be': 65, 'el': 120, 'la': 247, 'as': 55, '-c': 17, 'co': 86, 'om': 326, 'mp': 282, 'pe': 339, 'en': 122, 'ng': 295, '-d': 18, 'de': 92, 'ap': 52, 'pa': 337, '-k': 24, 'ka': 225, 'ku': 242, '-l': 25, 'le': 250, 'gg': 154, 'ga': 148, '-m': 26, 'ma': 270, 'ah': 44, 'ha': 169, 'si': 388, 'is': 207, 'sw': 400, 'wa': 452, '-s': 31, 'se': 385, 'ep': 124, 'pi': 340, 'it': 208, '-t': 32, 'ta': 403, 'ak': 47, '-w': 34, 'at': 56, 'ti': 409, \"a'\": 35, \"'d\": 6, 'du': 102, \"'h\": 7, 'ad': 40, 'ab': 38, 'ba': 62, 'di': 94, 'ia': 190, 'ik': 200, 'ai': 45, 'im': 202, 'na': 289, 'gn': 161, 'ar': 54, 'ra': 356, 'to': 414, 'oa': 314, 'au': 57, 'bd': 64, 'do': 99, 'me': 274, 'mi': 276, 'in': 203, 'al': 48, 'er': 125, 'et': 127, 'bi': 67, 'id': 193, 'il': 201, 'tu': 419, 'ur': 439, 'ri': 364, 'ie': 194, 'bj': 68, 'ja': 214, 'bl': 70, 'ut': 441, 'bn': 72, 'no': 303, 'or': 330, 'rm': 368, 'li': 254, 'nu': 307, 'us': 440, 'bo': 73, 'oi': 322, 'ol': 325, 'on': 327, 'ne': 293, 'em': 121, 'br': 74, 're': 360, 'ea': 109, 'ks': 240, 'ek': 119, 'ev': 129, 'vi': 449, 'ko': 237, 'os': 331, 'bs': 75, 'ns': 305, 'nt': 306, 'te': 405, 'ei': 117, 'sm': 392, 'so': 394, 'lu': 265, 'rp': 371, 'ps': 348, 'st': 398, 'tr': 416, 'su': 399, 'rd': 359, 'bt': 76, 'bu': 77, 'ua': 423, 'uh': 430, 'uk': 433, 'ul': 434, 'lh': 253, 'ay': 60, 'ac': 39, 'ca': 80, 'am': 49, 'ce': 81, 'ci': 83, 'cu': 88, 'um': 435, 'un': 436, 'gk': 158, 'aa': 37, 'ag': 43, 'gi': 156, 'io': 204, 'iu': 209, 'pt': 349, 'if': 195, 'pu': 350, 'eg': 115, 'eh': 116, 'uz': 445, 'ze': 470, 'ib': 191, 'sa': 381, 'ic': 192, 'ig': 196, 'gu': 167, 'ry': 378, 'kk': 233, 'kn': 236, 'od': 317, 'dr': 100, 'kt': 241, 'rg': 362, 'ip': 205, 'gh': 155, 'po': 346, 'ir': 206, 'aj': 46, 'ok': 324, 'oh': 321, 'iw': 211, 'gs': 165, 'rn': 369, 'wi': 454, 'dj': 95, 'je': 215, 'iv': 210, 'va': 446, 'dm': 97, 'ni': 297, 'op': 329, 'es': 126, 'dv': 103, 've': 447, 'rb': 357, 'rt': 374, 'vo': 450, 'ae': 41, 'ro': 370, 'ob': 315, 'og': 320, 'gr': 164, 'lo': 260, 'mo': 281, 'ov': 333, 'sk': 390, 'af': 42, 'fd': 135, 'ru': 375, 'fe': 136, 'fi': 137, 'fr': 142, 'fw': 146, 'aw': 59, 'ge': 152, 'ih': 197, 'gl': 159, 'hk': 177, 'hl': 178, 'lb': 248, 'lk': 256, 'ki': 231, 'ub': 424, 'hm': 179, 'ho': 181, 'hw': 187, 'lf': 251, 'nm': 301, 'mu': 286, 'ji': 216, 'jn': 218, 'jo': 219, 'oj': 323, 'ju': 222, 'ud': 426, 'uj': 432, 'k-': 224, 'ln': 259, 'kh': 230, 'hb': 170, 'hi': 175, 'ht': 185, 'kl': 234, 'km': 235, 'kr': 239, 'ed': 112, 'ui': 431, 'lt': 264, 'up': 438, 'nk': 299, 'kw': 243, 'l-': 246, '-q': 29, 'qu': 353, \"r'\": 354, \"'a\": 4, 'mn': 280, 'go': 162, 'fa': 133, 'fu': 145, 'lg': 252, 'tm': 412, 'md': 273, 'll': 257, 'fb': 134, 'lj': 255, 'sy': 401, 'ke': 229, 'lm': 258, 'rh': 363, 'hu': 186, 'of': 319, 'fo': 141, 'rf': 361, 'lp': 261, 'uv': 442, 'lv': 266, 'eo': 123, 'lw': 267, 'nn': 302, 'hn': 180, 'tl': 411, 'mb': 271, 'by': 79, 'eb': 110, 'mf': 275, 'mk': 277, 'pl': 343, 'pr': 347, 'mr': 283, 'ms': 284, 'mt': 285, 'rk': 366, 'nb': 290, 'nc': 291, 'nf': 294, 'ot': 332, 'tn': 413, 'gp': 163, 'nj': 298, 'jl': 217, 'rr': 372, 'rs': 373, 'i-': 189, 'ug': 429, 'yi': 462, 'th': 408, 'he': 172, 'pk': 342, 'aq': 53, 'qi': 352, 'rc': 358, 'rj': 365, 'ef': 114, 'rw': 377, 'rz': 379, 'za': 468, 'sn': 393, 'sb': 382, 'sf': 386, 'sh': 387, 'sl': 391, 'sp': 395, 'sr': 396, 'av': 58, 'vg': 448, 'we': 453, 'wu': 457, 'ye': 459, 'yo': 464, 'yu': 467, 'az': 61, 'ij': 199, 'zh': 471, 'zi': 472, 'hr': 183, 'kd': 228, 'kp': 238, 'ls': 263, 'gb': 149, 'nz': 311, 'rl': 367, 'zo': 474, 'ec': 111, \"o'\": 312, \"'o\": 11, 'uw': 443, 'r-': 355, 'a-': 36, '-g': 20, 'o-': 313, 'gt': 166, 's-': 380, 'oy': 335, 'uc': 425, 'kc': 227, 'ft': 144, 'gd': 151, 'ue': 427, 'dw': 104, 'fl': 139, 'ew': 130, 'wo': 455, 'ej': 118, 'iz': 213, 'jr': 220, 'tk': 410, 'hf': 173, 'ml': 278, 'oc': 316, 'uy': 444, 'ey': 131, 'tb': 404, 'nv': 308, 'fk': 138, 'fs': 143, 'hy': 188, 'g-': 147, '-j': 23, 'rv': 376, 'ez': 132, 'bh': 66, \"d'\": 89, 'ly': 268, 'ss': 397, 'gm': 160, 'oz': 336, 'nw': 309, 'uf': 428, 'nl': 300, 'ld': 249, 'h-': 168, 'mc': 272, 'gc': 150, 'ao': 51, 'pc': 338, 'pj': 341, \"e'\": 107, 'ee': 113, 'oo': 328, 'ch': 82, 'cm': 85, 'ow': 334, 'cr': 87, \"'i\": 8, 'fn': 140, 'hs': 184, 'ii': 198, 'ts': 417, 'dk': 96, 'bk': 69, 'gj': 157, 'sj': 389, 'iy': 212, 'tp': 415, 'mz': 287, 'tt': 418, '-e': 19, 'pn': 345, 'u-': 422, '-h': 21, 'jt': 221, 'nh': 296, 'np': 304, 'yt': 466, '-r': 30, 'yk': 463, 'ky': 244, '-p': 28, 'eu': 128, 'zu': 475, 'oe': 318, 'uo': 437, 'ds': 101, \"'n\": 10, 'yg': 460, 'yh': 461, 'yr': 465, 'zb': 469, 'zm': 473, 't-': 402, 'kj': 232, '-i': 22, 'py': 351, 'hg': 174, 'mm': 279, \"u'\": 421, \"'j\": 9, 'dz': 106, 'hd': 171, 'bm': 71, 'sc': 383, 'tf': 406, 'e-': 108, 'hj': 176, 'n-': 288, 'lr': 262, 'dh': 93, 'ck': 84, 'tg': 407, \"'b\": 5, 'tw': 420, 'bb': 63, 'bv': 78, 'hp': 182, 'wt': 456, \"'r\": 12, \"'u\": 13, 'db': 91, 'jw': 223, 'kb': 226, 'kz': 245, '-u': 33, \"m'\": 269, 'vu': 451, 'dy': 105, 'dn': 98, 'sd': 384, 'pm': 344, 'zz': 476, 'gf': 153}\n",
      "473 {'--': 14, '-n': 27, 'ny': 310, 'ya': 458, '-a': 15, 'an': 50, 'nd': 292, 'da': 90, '-b': 16, 'be': 65, 'el': 120, 'la': 247, 'as': 55, '-c': 17, 'co': 86, 'om': 326, 'mp': 282, 'pe': 339, 'en': 122, 'ng': 295, '-d': 18, 'de': 92, 'ap': 52, 'pa': 337, '-k': 24, 'ka': 225, 'ku': 242, '-l': 25, 'le': 250, 'gg': 154, 'ga': 148, '-m': 26, 'ma': 270, 'ah': 44, 'ha': 169, 'si': 388, 'is': 207, 'sw': 400, 'wa': 452, '-s': 31, 'se': 385, 'ep': 124, 'pi': 340, 'it': 208, '-t': 32, 'ta': 403, 'ak': 47, '-w': 34, 'at': 56, 'ti': 409, \"a'\": 35, \"'d\": 6, 'du': 102, \"'h\": 7, 'ad': 40, 'ab': 38, 'ba': 62, 'di': 94, 'ia': 190, 'ik': 200, 'ai': 45, 'im': 202, 'na': 289, 'gn': 161, 'ar': 54, 'ra': 356, 'to': 414, 'oa': 314, 'au': 57, 'bd': 64, 'do': 99, 'me': 274, 'mi': 276, 'in': 203, 'al': 48, 'er': 125, 'et': 127, 'bi': 67, 'id': 193, 'il': 201, 'tu': 419, 'ur': 439, 'ri': 364, 'ie': 194, 'bj': 68, 'ja': 214, 'bl': 70, 'ut': 441, 'bn': 72, 'no': 303, 'or': 330, 'rm': 368, 'li': 254, 'nu': 307, 'us': 440, 'bo': 73, 'oi': 322, 'ol': 325, 'on': 327, 'ne': 293, 'em': 121, 'br': 74, 're': 360, 'ea': 109, 'ks': 240, 'ek': 119, 'ev': 129, 'vi': 449, 'ko': 237, 'os': 331, 'bs': 75, 'ns': 305, 'nt': 306, 'te': 405, 'ei': 117, 'sm': 392, 'so': 394, 'lu': 265, 'rp': 371, 'ps': 348, 'st': 398, 'tr': 416, 'su': 399, 'rd': 359, 'bt': 76, 'bu': 77, 'ua': 423, 'uh': 430, 'uk': 433, 'ul': 434, 'lh': 253, 'ay': 60, 'ac': 39, 'ca': 80, 'am': 49, 'ce': 81, 'ci': 83, 'cu': 88, 'um': 435, 'un': 436, 'gk': 158, 'aa': 37, 'ag': 43, 'gi': 156, 'io': 204, 'iu': 209, 'pt': 349, 'if': 195, 'pu': 350, 'eg': 115, 'eh': 116, 'uz': 445, 'ze': 470, 'ib': 191, 'sa': 381, 'ic': 192, 'ig': 196, 'gu': 167, 'ry': 378, 'kk': 233, 'kn': 236, 'od': 317, 'dr': 100, 'kt': 241, 'rg': 362, 'ip': 205, 'gh': 155, 'po': 346, 'ir': 206, 'aj': 46, 'ok': 324, 'oh': 321, 'iw': 211, 'gs': 165, 'rn': 369, 'wi': 454, 'dj': 95, 'je': 215, 'iv': 210, 'va': 446, 'dm': 97, 'ni': 297, 'op': 329, 'es': 126, 'dv': 103, 've': 447, 'rb': 357, 'rt': 374, 'vo': 450, 'ae': 41, 'ro': 370, 'ob': 315, 'og': 320, 'gr': 164, 'lo': 260, 'mo': 281, 'ov': 333, 'sk': 390, 'af': 42, 'fd': 135, 'ru': 375, 'fe': 136, 'fi': 137, 'fr': 142, 'fw': 146, 'aw': 59, 'ge': 152, 'ih': 197, 'gl': 159, 'hk': 177, 'hl': 178, 'lb': 248, 'lk': 256, 'ki': 231, 'ub': 424, 'hm': 179, 'ho': 181, 'hw': 187, 'lf': 251, 'nm': 301, 'mu': 286, 'ji': 216, 'jn': 218, 'jo': 219, 'oj': 323, 'ju': 222, 'ud': 426, 'uj': 432, 'k-': 224, 'ln': 259, 'kh': 230, 'hb': 170, 'hi': 175, 'ht': 185, 'kl': 234, 'km': 235, 'kr': 239, 'ed': 112, 'ui': 431, 'lt': 264, 'up': 438, 'nk': 299, 'kw': 243, 'l-': 246, '-q': 29, 'qu': 353, \"r'\": 354, \"'a\": 4, 'mn': 280, 'go': 162, 'fa': 133, 'fu': 145, 'lg': 252, 'tm': 412, 'md': 273, 'll': 257, 'fb': 134, 'lj': 255, 'sy': 401, 'ke': 229, 'lm': 258, 'rh': 363, 'hu': 186, 'of': 319, 'fo': 141, 'rf': 361, 'lp': 261, 'uv': 442, 'lv': 266, 'eo': 123, 'lw': 267, 'nn': 302, 'hn': 180, 'tl': 411, 'mb': 271, 'by': 79, 'eb': 110, 'mf': 275, 'mk': 277, 'pl': 343, 'pr': 347, 'mr': 283, 'ms': 284, 'mt': 285, 'rk': 366, 'nb': 290, 'nc': 291, 'nf': 294, 'ot': 332, 'tn': 413, 'gp': 163, 'nj': 298, 'jl': 217, 'rr': 372, 'rs': 373, 'i-': 189, 'ug': 429, 'yi': 462, 'th': 408, 'he': 172, 'pk': 342, 'aq': 53, 'qi': 352, 'rc': 358, 'rj': 365, 'ef': 114, 'rw': 377, 'rz': 379, 'za': 468, 'sn': 393, 'sb': 382, 'sf': 386, 'sh': 387, 'sl': 391, 'sp': 395, 'sr': 396, 'av': 58, 'vg': 448, 'we': 453, 'wu': 457, 'ye': 459, 'yo': 464, 'yu': 467, 'az': 61, 'ij': 199, 'zh': 471, 'zi': 472, 'hr': 183, 'kd': 228, 'kp': 238, 'ls': 263, 'gb': 149, 'nz': 311, 'rl': 367, 'zo': 474, 'ec': 111, \"o'\": 312, \"'o\": 11, 'uw': 443, 'r-': 355, 'a-': 36, '-g': 20, 'o-': 313, 'gt': 166, 's-': 380, 'oy': 335, 'uc': 425, 'kc': 227, 'ft': 144, 'gd': 151, 'ue': 427, 'dw': 104, 'fl': 139, 'ew': 130, 'wo': 455, 'ej': 118, 'iz': 213, 'jr': 220, 'tk': 410, 'hf': 173, 'ml': 278, 'oc': 316, 'uy': 444, 'ey': 131, 'tb': 404, 'nv': 308, 'fk': 138, 'fs': 143, 'hy': 188, 'g-': 147, '-j': 23, 'rv': 376, 'ez': 132, 'bh': 66, \"d'\": 89, 'ly': 268, 'ss': 397, 'gm': 160, 'oz': 336, 'nw': 309, 'uf': 428, 'nl': 300, 'ld': 249, 'h-': 168, 'mc': 272, 'gc': 150, 'ao': 51, 'pc': 338, 'pj': 341, \"e'\": 107, 'ee': 113, 'oo': 328, 'ch': 82, 'cm': 85, 'ow': 334, 'cr': 87, \"'i\": 8, 'fn': 140, 'hs': 184, 'ii': 198, 'ts': 417, 'dk': 96, 'bk': 69, 'gj': 157, 'sj': 389, 'iy': 212, 'tp': 415, 'mz': 287, 'tt': 418, '-e': 19, 'pn': 345, 'u-': 422, '-h': 21, 'jt': 221, 'nh': 296, 'np': 304, 'yt': 466, '-r': 30, 'yk': 463, 'ky': 244, '-p': 28, 'eu': 128, 'zu': 475, 'oe': 318, 'uo': 437, 'ds': 101, \"'n\": 10, 'yg': 460, 'yh': 461, 'yr': 465, 'zb': 469, 'zm': 473, 't-': 402, 'kj': 232, '-i': 22, 'py': 351, 'hg': 174, 'mm': 279, \"u'\": 421, \"'j\": 9, 'dz': 106, 'hd': 171, 'bm': 71, 'sc': 383, 'tf': 406, 'e-': 108, 'hj': 176, 'n-': 288, 'lr': 262, 'dh': 93, 'ck': 84, 'tg': 407, \"'b\": 5, 'tw': 420, 'bb': 63, 'bv': 78, 'hp': 182, 'wt': 456, \"'r\": 12, \"'u\": 13, 'db': 91, 'jw': 223, 'kb': 226, 'kz': 245, '-u': 33, \"m'\": 269, 'vu': 451, 'dy': 105, 'dn': 98, 'sd': 384, 'pm': 344, 'zz': 476, 'gf': 153}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 32\n",
      "n_layers: 1\n",
      "Encoder has a total number of 39936 parameters\n",
      "Decoder has a total number of 17124 parameters\n",
      "Total number of all parameters is 57060\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 1 finished in 0m 8.4s (- 13m 51.67s) (1 1.0%). train avg loss: 2.1972\n",
      "Training for epoch 2 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 2 finished in 0m 16.8s (- 13m 43.01s) (2 2.0%). train avg loss: 1.6645\n",
      "Training for epoch 3 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 3 finished in 0m 25.41s (- 13m 41.46s) (3 3.0%). train avg loss: 1.3577\n",
      "Training for epoch 4 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 4 finished in 0m 33.52s (- 13m 24.43s) (4 4.0%). train avg loss: 1.0829\n",
      "Training for epoch 5 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 5 finished in 0m 41.58s (- 13m 10.03s) (5 5.0%). train avg loss: 0.8505\n",
      "Training for epoch 6 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 6 finished in 0m 49.75s (- 12m 59.35s) (6 6.0%). train avg loss: 0.6735\n",
      "Training for epoch 7 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 7 finished in 0m 58.02s (- 12m 50.81s) (7 7.0%). train avg loss: 0.5192\n",
      "Training for epoch 8 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 8 finished in 1m 6.2s (- 12m 41.27s) (8 8.0%). train avg loss: 0.4346\n",
      "Training for epoch 9 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 9 finished in 1m 14.1s (- 12m 29.26s) (9 9.0%). train avg loss: 0.3806\n",
      "Training for epoch 10 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 10 finished in 1m 22.21s (- 12m 19.85s) (10 10.0%). train avg loss: 0.3197\n",
      "Training for epoch 11 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 11 finished in 1m 30.4s (- 12m 11.4s) (11 11.0%). train avg loss: 0.2555\n",
      "Training for epoch 12 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 12 finished in 1m 38.53s (- 12m 2.52s) (12 12.0%). train avg loss: 0.2579\n",
      "Training for epoch 13 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 13 finished in 1m 46.45s (- 11m 52.4s) (13 13.0%). train avg loss: 0.2241\n",
      "Training for epoch 14 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 14 finished in 1m 54.5s (- 11m 43.39s) (14 14.0%). train avg loss: 0.2036\n",
      "Training for epoch 15 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 15 finished in 2m 2.35s (- 11m 33.33s) (15 15.0%). train avg loss: 0.1742\n",
      "Training for epoch 16 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 16 finished in 2m 10.25s (- 11m 23.79s) (16 16.0%). train avg loss: 0.1504\n",
      "Training for epoch 17 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 17 finished in 2m 18.32s (- 11m 15.34s) (17 17.0%). train avg loss: 0.1647\n",
      "Training for epoch 18 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 18 finished in 2m 26.19s (- 11m 6.0s) (18 18.0%). train avg loss: 0.1502\n",
      "Training for epoch 19 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 19 finished in 2m 33.9s (- 10m 56.11s) (19 19.0%). train avg loss: 0.1209\n",
      "Training for epoch 20 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 20 finished in 2m 42.11s (- 10m 48.42s) (20 20.0%). train avg loss: 0.1067\n",
      "Training for epoch 21 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 21 finished in 2m 50.51s (- 10m 41.46s) (21 21.0%). train avg loss: 0.1069\n",
      "Training for epoch 22 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 22 finished in 2m 58.28s (- 10m 32.09s) (22 22.0%). train avg loss: 0.0994\n",
      "Training for epoch 23 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 23 finished in 3m 6.25s (- 10m 23.54s) (23 23.0%). train avg loss: 0.0949\n",
      "Training for epoch 24 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 24 finished in 3m 14.26s (- 10m 15.14s) (24 24.0%). train avg loss: 0.1043\n",
      "Training for epoch 25 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 25 finished in 3m 22.38s (- 10m 7.14s) (25 25.0%). train avg loss: 0.0877\n",
      "Training for epoch 26 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 26 finished in 3m 30.49s (- 9m 59.08s) (26 26.0%). train avg loss: 0.071\n",
      "Training for epoch 27 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 27 finished in 3m 38.46s (- 9m 50.66s) (27 27.0%). train avg loss: 0.0783\n",
      "Training for epoch 28 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 28 finished in 3m 46.46s (- 9m 42.33s) (28 28.0%). train avg loss: 0.0881\n",
      "Training for epoch 29 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 29 finished in 3m 54.21s (- 9m 33.41s) (29 29.0%). train avg loss: 0.1032\n",
      "Training for epoch 30 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 30 finished in 4m 2.44s (- 9m 25.7s) (30 30.0%). train avg loss: 0.1013\n",
      "Training for epoch 31 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 31 finished in 4m 10.56s (- 9m 17.7s) (31 31.0%). train avg loss: 0.098\n",
      "Training for epoch 32 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 32 finished in 4m 18.17s (- 9m 8.62s) (32 32.0%). train avg loss: 0.0644\n",
      "Training for epoch 33 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 33 finished in 4m 26.17s (- 9m 0.4s) (33 33.0%). train avg loss: 0.071\n",
      "Training for epoch 34 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 34 finished in 4m 34.32s (- 8m 52.51s) (34 34.0%). train avg loss: 0.0731\n",
      "Training for epoch 35 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 35 finished in 4m 43.13s (- 8m 45.82s) (35 35.0%). train avg loss: 0.0969\n",
      "Training for epoch 36 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 36 finished in 4m 51.16s (- 8m 37.63s) (36 36.0%). train avg loss: 0.0668\n",
      "Training for epoch 37 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 37 finished in 4m 59.13s (- 8m 29.34s) (37 37.0%). train avg loss: 0.0521\n",
      "Training for epoch 38 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 38 finished in 5m 6.92s (- 8m 20.76s) (38 38.0%). train avg loss: 0.0567\n",
      "Training for epoch 39 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 39 finished in 5m 14.9s (- 8m 12.53s) (39 39.0%). train avg loss: 0.053\n",
      "Training for epoch 40 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 40 finished in 5m 23.17s (- 8m 4.75s) (40 40.0%). train avg loss: 0.0836\n",
      "Training for epoch 41 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 41 finished in 5m 30.88s (- 7m 56.14s) (41 41.0%). train avg loss: 0.0569\n",
      "Training for epoch 42 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 42 finished in 5m 38.82s (- 7m 47.89s) (42 42.0%). train avg loss: 0.0419\n",
      "Training for epoch 43 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 43 finished in 5m 46.69s (- 7m 39.56s) (43 43.0%). train avg loss: 0.0441\n",
      "Training for epoch 44 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 44 finished in 5m 54.77s (- 7m 31.52s) (44 44.0%). train avg loss: 0.0518\n",
      "Training for epoch 45 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 45 finished in 6m 2.94s (- 7m 23.59s) (45 45.0%). train avg loss: 0.0653\n",
      "Training for epoch 46 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 46 finished in 6m 10.8s (- 7m 15.29s) (46 46.0%). train avg loss: 0.0473\n",
      "Training for epoch 47 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 47 finished in 6m 18.68s (- 7m 7.03s) (47 47.0%). train avg loss: 0.0435\n",
      "Training for epoch 48 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 48 finished in 6m 26.51s (- 6m 58.72s) (48 48.0%). train avg loss: 0.0844\n",
      "Training for epoch 49 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 49 finished in 6m 34.61s (- 6m 50.72s) (49 49.0%). train avg loss: 0.0846\n",
      "Training for epoch 50 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 50 finished in 6m 42.75s (- 6m 42.75s) (50 50.0%). train avg loss: 0.0489\n",
      "Training for epoch 51 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 51 finished in 6m 50.55s (- 6m 34.45s) (51 51.0%). train avg loss: 0.054\n",
      "Training for epoch 52 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 52 finished in 6m 58.49s (- 6m 26.3s) (52 52.0%). train avg loss: 0.0474\n",
      "Training for epoch 53 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 53 finished in 7m 6.29s (- 6m 18.03s) (53 53.0%). train avg loss: 0.0391\n",
      "Training for epoch 54 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 54 finished in 7m 14.26s (- 6m 9.93s) (54 54.0%). train avg loss: 0.0335\n",
      "Training for epoch 55 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 55 finished in 7m 22.24s (- 6m 1.84s) (55 55.0%). train avg loss: 0.0367\n",
      "Training for epoch 56 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 56 finished in 7m 30.26s (- 5m 53.77s) (56 56.0%). train avg loss: 0.08\n",
      "Training for epoch 57 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 57 finished in 7m 37.91s (- 5m 45.44s) (57 57.0%). train avg loss: 0.1034\n",
      "Training for epoch 58 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 58 finished in 7m 45.67s (- 5m 37.21s) (58 58.0%). train avg loss: 0.0559\n",
      "Training for epoch 59 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 59 finished in 7m 53.66s (- 5m 29.15s) (59 59.0%). train avg loss: 0.0396\n",
      "Training for epoch 60 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 60 finished in 8m 1.56s (- 5m 21.04s) (60 60.0%). train avg loss: 0.0411\n",
      "Training for epoch 61 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 61 finished in 8m 9.51s (- 5m 12.97s) (61 61.0%). train avg loss: 0.0339\n",
      "Training for epoch 62 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 62 finished in 8m 17.49s (- 5m 4.91s) (62 62.0%). train avg loss: 0.0424\n",
      "Training for epoch 63 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 63 finished in 8m 25.52s (- 4m 56.89s) (63 63.0%). train avg loss: 0.055\n",
      "Training for epoch 64 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 64 finished in 8m 33.66s (- 4m 48.94s) (64 64.0%). train avg loss: 0.0505\n",
      "Training for epoch 65 has started (lr=0.001). Found 396 batch(es).\n",
      "Epoch 65 finished in 8m 41.64s (- 4m 40.88s) (65 65.0%). train avg loss: 0.0364\n",
      "Training for epoch 66 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 66 finished in 8m 49.39s (- 4m 32.71s) (66 66.0%). train avg loss: 0.0364\n",
      "Training for epoch 67 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 67 finished in 8m 57.12s (- 4m 24.55s) (67 67.0%). train avg loss: 0.0343\n",
      "Training for epoch 68 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 68 finished in 9m 5.35s (- 4m 16.64s) (68 68.0%). train avg loss: 0.0297\n",
      "Training for epoch 69 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 69 finished in 9m 13.75s (- 4m 8.79s) (69 69.0%). train avg loss: 0.0304\n",
      "Training for epoch 70 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 70 finished in 9m 21.66s (- 4m 0.71s) (70 70.0%). train avg loss: 0.0313\n",
      "Training for epoch 71 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 71 finished in 9m 29.58s (- 3m 52.65s) (71 71.0%). train avg loss: 0.0298\n",
      "Training for epoch 72 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 72 finished in 9m 37.52s (- 3m 44.59s) (72 72.0%). train avg loss: 0.0345\n",
      "Training for epoch 73 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 73 finished in 9m 45.68s (- 3m 36.62s) (73 73.0%). train avg loss: 0.0389\n",
      "Training for epoch 74 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 74 finished in 9m 53.72s (- 3m 28.6s) (74 74.0%). train avg loss: 0.0361\n",
      "Training for epoch 75 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 75 finished in 10m 1.81s (- 3m 20.6s) (75 75.0%). train avg loss: 0.0317\n",
      "Training for epoch 76 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 76 finished in 10m 9.55s (- 3m 12.49s) (76 76.0%). train avg loss: 0.0278\n",
      "Training for epoch 77 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 77 finished in 10m 17.42s (- 3m 4.43s) (77 77.0%). train avg loss: 0.0298\n",
      "Training for epoch 78 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 78 finished in 10m 25.76s (- 2m 56.5s) (78 78.0%). train avg loss: 0.0276\n",
      "Training for epoch 79 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 79 finished in 10m 33.81s (- 2m 48.48s) (79 79.0%). train avg loss: 0.0291\n",
      "Training for epoch 80 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 80 finished in 10m 41.61s (- 2m 40.4s) (80 80.0%). train avg loss: 0.034\n",
      "Training for epoch 81 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 81 finished in 10m 49.73s (- 2m 32.41s) (81 81.0%). train avg loss: 0.0279\n",
      "Training for epoch 82 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 82 finished in 10m 57.95s (- 2m 24.43s) (82 82.0%). train avg loss: 0.028\n",
      "Training for epoch 83 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 83 finished in 11m 6.07s (- 2m 16.42s) (83 83.0%). train avg loss: 0.026\n",
      "Training for epoch 84 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 84 finished in 11m 14.12s (- 2m 8.4s) (84 84.0%). train avg loss: 0.025\n",
      "Training for epoch 85 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 85 finished in 11m 22.05s (- 2m 0.36s) (85 85.0%). train avg loss: 0.0302\n",
      "Training for epoch 86 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 86 finished in 11m 29.66s (- 1m 52.27s) (86 86.0%). train avg loss: 0.0301\n",
      "Training for epoch 87 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 87 finished in 11m 38.01s (- 1m 44.3s) (87 87.0%). train avg loss: 0.0266\n",
      "Training for epoch 88 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 88 finished in 11m 46.15s (- 1m 36.29s) (88 88.0%). train avg loss: 0.0294\n",
      "Training for epoch 89 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 89 finished in 11m 54.04s (- 1m 28.25s) (89 89.0%). train avg loss: 0.0319\n",
      "Training for epoch 90 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 90 finished in 12m 2.04s (- 1m 20.23s) (90 90.0%). train avg loss: 0.0255\n",
      "Training for epoch 91 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 91 finished in 12m 9.63s (- 1m 12.16s) (91 91.0%). train avg loss: 0.0257\n",
      "Training for epoch 92 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 92 finished in 12m 17.31s (- 1m 4.11s) (92 92.0%). train avg loss: 0.0263\n",
      "Training for epoch 93 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 93 finished in 12m 25.11s (- 0m 56.08s) (93 93.0%). train avg loss: 0.0251\n",
      "Training for epoch 94 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 94 finished in 12m 32.94s (- 0m 48.06s) (94 94.0%). train avg loss: 0.0225\n",
      "Training for epoch 95 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 95 finished in 12m 40.4s (- 0m 40.02s) (95 95.0%). train avg loss: 0.0242\n",
      "Training for epoch 96 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 96 finished in 12m 48.29s (- 0m 32.01s) (96 96.0%). train avg loss: 0.0257\n",
      "Training for epoch 97 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 97 finished in 12m 56.38s (- 0m 24.01s) (97 97.0%). train avg loss: 0.0271\n",
      "Training for epoch 98 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 98 finished in 13m 3.75s (- 0m 15.99s) (98 98.0%). train avg loss: 0.0238\n",
      "Training for epoch 99 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 99 finished in 13m 11.64s (- 0m 8.0s) (99 99.0%). train avg loss: 0.026\n",
      "Training for epoch 100 has started (lr=0.0005). Found 396 batch(es).\n",
      "Epoch 100 finished in 13m 19.28s (- 0m 0.0s) (100 100.0%). train avg loss: 0.0283\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "best_train_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on train loss\n",
    "  encoder_scheduler.step(avg_train_loss)\n",
    "  decoder_scheduler.step(avg_train_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  # Save the model if the train loss is better than the previous iterations' train loss\n",
    "  if avg_train_loss < best_train_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_train_loss = avg_train_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"train\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-6, min_value+.1, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 5.845909593781311%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in test_pairs :\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(test_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> menyerampang\n",
      "= ['M', 'AX', 'NY', 'AX', 'R', 'AA', 'M', 'P', 'AA', 'NG']\n",
      "< M AX NY AX R AA M P AA NG ['M', 'AX', 'NY', 'AX', 'R', 'AA', 'M', 'P', 'AA', 'NG']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f82e3309640>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
