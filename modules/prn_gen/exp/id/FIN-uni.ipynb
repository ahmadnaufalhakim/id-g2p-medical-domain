{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/exp/id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"50\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[-1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 29, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7fd6145c72b0> ([5, 5, 19, 29, 6, 1], [23, 3, 1])\n",
      "([5, 5, 19, 29, 6, 1], [23, 3, 1])\n",
      "([5, 5, 19, 29, 6, 1], [23, 3, 1])\n",
      "train grp 31 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'y', 30: 'z'}\n",
      "valid grp 31 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'y', 30: 'z'}\n",
      "test grp 31 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'y', 30: 'z'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "27 {'-': 5, 'n': 19, 'y': 29, 'a': 6, 'd': 9, 'b': 7, 'e': 10, 'l': 17, 's': 24, 'c': 8, 'o': 20, 'm': 18, 'p': 21, 'g': 12, 'k': 16, 'u': 26, 'h': 13, 'i': 14, 'w': 28, 't': 25, \"'\": 4, 'r': 23, 'j': 15, 'v': 27, 'f': 11, 'z': 30, 'q': 22}\n",
      "27 {'-': 5, 'n': 19, 'y': 29, 'a': 6, 'd': 9, 'b': 7, 'e': 10, 'l': 17, 's': 24, 'c': 8, 'o': 20, 'm': 18, 'p': 21, 'g': 12, 'k': 16, 'u': 26, 'h': 13, 'i': 14, 'w': 28, 't': 25, \"'\": 4, 'r': 23, 'j': 15, 'v': 27, 'f': 11, 'z': 30, 'q': 22}\n",
      "27 {'-': 5, 'n': 19, 'y': 29, 'a': 6, 'd': 9, 'b': 7, 'e': 10, 'l': 17, 's': 24, 'c': 8, 'o': 20, 'm': 18, 'p': 21, 'g': 12, 'k': 16, 'u': 26, 'h': 13, 'i': 14, 'w': 28, 't': 25, \"'\": 4, 'r': 23, 'j': 15, 'v': 27, 'f': 11, 'z': 30, 'q': 22}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "      )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 50\n",
      "n_layers: 1\n",
      "Encoder has a total number of 19384 parameters\n",
      "Decoder has a total number of 30840 parameters\n",
      "Total number of all parameters is 50224\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 1 finished in 0m 10.67s (- 17m 36.39s) (1 1.0%). train avg loss: 2.0037, val avg loss: 1.9689\n",
      "Training for epoch 2 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 2 finished in 0m 20.28s (- 16m 33.91s) (2 2.0%). train avg loss: 1.2119, val avg loss: 1.2656\n",
      "Training for epoch 3 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 3 finished in 0m 28.24s (- 15m 13.07s) (3 3.0%). train avg loss: 0.6192, val avg loss: 0.8814\n",
      "Training for epoch 4 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 4 finished in 0m 36.71s (- 14m 41.12s) (4 4.0%). train avg loss: 0.3752, val avg loss: 0.7199\n",
      "Training for epoch 5 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 5 finished in 0m 45.96s (- 14m 33.2s) (5 5.0%). train avg loss: 0.2505, val avg loss: 0.5715\n",
      "Training for epoch 6 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 6 finished in 0m 55.04s (- 14m 22.24s) (6 6.0%). train avg loss: 0.1789, val avg loss: 0.4579\n",
      "Training for epoch 7 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 7 finished in 1m 3.73s (- 14m 6.65s) (7 7.0%). train avg loss: 0.1855, val avg loss: 0.4267\n",
      "Training for epoch 8 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 8 finished in 1m 11.79s (- 13m 45.63s) (8 8.0%). train avg loss: 0.1321, val avg loss: 0.4699\n",
      "Training for epoch 9 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 9 finished in 1m 19.89s (- 13m 27.75s) (9 9.0%). train avg loss: 0.1416, val avg loss: 0.506\n",
      "Training for epoch 10 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 10 finished in 1m 29.63s (- 13m 26.68s) (10 10.0%). train avg loss: 0.1262, val avg loss: 0.4033\n",
      "Training for epoch 11 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 11 finished in 1m 37.67s (- 13m 10.26s) (11 11.0%). train avg loss: 0.0919, val avg loss: 0.3878\n",
      "Training for epoch 12 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 12 finished in 1m 46.93s (- 13m 4.13s) (12 12.0%). train avg loss: 0.0888, val avg loss: 0.473\n",
      "Training for epoch 13 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 13 finished in 1m 54.45s (- 12m 45.95s) (13 13.0%). train avg loss: 0.0882, val avg loss: 0.3591\n",
      "Training for epoch 14 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 14 finished in 2m 2.75s (- 12m 34.03s) (14 14.0%). train avg loss: 0.0865, val avg loss: 0.4171\n",
      "Training for epoch 15 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 15 finished in 2m 11.46s (- 12m 24.96s) (15 15.0%). train avg loss: 0.089, val avg loss: 0.3759\n",
      "Training for epoch 16 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 16 finished in 2m 19.35s (- 12m 11.58s) (16 16.0%). train avg loss: 0.0789, val avg loss: 0.4738\n",
      "Training for epoch 17 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 17 finished in 2m 26.84s (- 11m 56.93s) (17 17.0%). train avg loss: 0.0778, val avg loss: 0.3584\n",
      "Training for epoch 18 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 18 finished in 2m 34.05s (- 11m 41.8s) (18 18.0%). train avg loss: 0.0693, val avg loss: 0.3382\n",
      "Training for epoch 19 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 19 finished in 2m 42.29s (- 11m 31.85s) (19 19.0%). train avg loss: 0.0713, val avg loss: 0.2973\n",
      "Training for epoch 20 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 20 finished in 2m 51.88s (- 11m 27.53s) (20 20.0%). train avg loss: 0.0511, val avg loss: 0.3038\n",
      "Training for epoch 21 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 21 finished in 2m 59.42s (- 11m 14.94s) (21 21.0%). train avg loss: 0.0576, val avg loss: 0.3891\n",
      "Training for epoch 22 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 22 finished in 3m 7.53s (- 11m 4.87s) (22 22.0%). train avg loss: 0.0611, val avg loss: 0.2945\n",
      "Training for epoch 23 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 23 finished in 3m 15.27s (- 10m 53.73s) (23 23.0%). train avg loss: 0.0759, val avg loss: 0.3093\n",
      "Training for epoch 24 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 24 finished in 3m 23.56s (- 10m 44.62s) (24 24.0%). train avg loss: 0.0637, val avg loss: 0.2873\n",
      "Training for epoch 25 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 25 finished in 3m 32.26s (- 10m 36.79s) (25 25.0%). train avg loss: 0.0751, val avg loss: 0.2982\n",
      "Training for epoch 26 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 26 finished in 3m 40.12s (- 10m 26.49s) (26 26.0%). train avg loss: 0.0511, val avg loss: 0.2733\n",
      "Training for epoch 27 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 27 finished in 3m 48.31s (- 10m 17.29s) (27 27.0%). train avg loss: 0.0547, val avg loss: 0.2566\n",
      "Training for epoch 28 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 28 finished in 3m 56.0s (- 10m 6.85s) (28 28.0%). train avg loss: 0.0809, val avg loss: 0.3175\n",
      "Training for epoch 29 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 29 finished in 4m 5.28s (- 10m 0.51s) (29 29.0%). train avg loss: 0.0758, val avg loss: 0.2673\n",
      "Training for epoch 30 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 30 finished in 4m 14.06s (- 9m 52.8s) (30 30.0%). train avg loss: 0.0671, val avg loss: 0.2185\n",
      "Training for epoch 31 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 31 finished in 4m 22.02s (- 9m 43.21s) (31 31.0%). train avg loss: 0.0497, val avg loss: 0.2596\n",
      "Training for epoch 32 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 32 finished in 4m 30.67s (- 9m 35.17s) (32 32.0%). train avg loss: 0.046, val avg loss: 0.213\n",
      "Training for epoch 33 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 33 finished in 4m 39.15s (- 9m 26.76s) (33 33.0%). train avg loss: 0.0413, val avg loss: 0.2448\n",
      "Training for epoch 34 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 34 finished in 4m 48.32s (- 9m 19.68s) (34 34.0%). train avg loss: 0.0408, val avg loss: 0.305\n",
      "Training for epoch 35 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 35 finished in 4m 57.1s (- 9m 11.75s) (35 35.0%). train avg loss: 0.05, val avg loss: 0.3089\n",
      "Training for epoch 36 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 36 finished in 5m 5.08s (- 9m 2.37s) (36 36.0%). train avg loss: 0.0481, val avg loss: 0.2666\n",
      "Training for epoch 37 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 37 finished in 5m 14.18s (- 8m 54.96s) (37 37.0%). train avg loss: 0.0485, val avg loss: 0.2682\n",
      "Training for epoch 38 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 38 finished in 5m 23.32s (- 8m 47.53s) (38 38.0%). train avg loss: 0.0456, val avg loss: 0.2302\n",
      "Training for epoch 39 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 39 finished in 5m 32.49s (- 8m 40.05s) (39 39.0%). train avg loss: 0.042, val avg loss: 0.2576\n",
      "Training for epoch 40 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 40 finished in 5m 39.6s (- 8m 29.4s) (40 40.0%). train avg loss: 0.0608, val avg loss: 0.2501\n",
      "Training for epoch 41 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 41 finished in 5m 47.59s (- 8m 20.2s) (41 41.0%). train avg loss: 0.0755, val avg loss: 0.3546\n",
      "Training for epoch 42 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 42 finished in 5m 56.15s (- 8m 11.82s) (42 42.0%). train avg loss: 0.0582, val avg loss: 0.3435\n",
      "Training for epoch 43 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 43 finished in 6m 5.28s (- 8m 4.21s) (43 43.0%). train avg loss: 0.0365, val avg loss: 0.2916\n",
      "Training for epoch 44 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 44 finished in 6m 14.19s (- 7m 56.24s) (44 44.0%). train avg loss: 0.0359, val avg loss: 0.2669\n",
      "Training for epoch 45 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 45 finished in 6m 22.48s (- 7m 47.47s) (45 45.0%). train avg loss: 0.0372, val avg loss: 0.2678\n",
      "Training for epoch 46 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 46 finished in 6m 29.65s (- 7m 37.42s) (46 46.0%). train avg loss: 0.0331, val avg loss: 0.2321\n",
      "Training for epoch 47 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 47 finished in 6m 36.57s (- 7m 27.2s) (47 47.0%). train avg loss: 0.0303, val avg loss: 0.2209\n",
      "Training for epoch 48 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 48 finished in 6m 46.43s (- 7m 20.3s) (48 48.0%). train avg loss: 0.0304, val avg loss: 0.2203\n",
      "Training for epoch 49 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 49 finished in 6m 55.59s (- 7m 12.56s) (49 49.0%). train avg loss: 0.0313, val avg loss: 0.2156\n",
      "Training for epoch 50 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 50 finished in 7m 3.73s (- 7m 3.73s) (50 50.0%). train avg loss: 0.0288, val avg loss: 0.2017\n",
      "Training for epoch 51 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 51 finished in 7m 11.83s (- 6m 54.9s) (51 51.0%). train avg loss: 0.0307, val avg loss: 0.2628\n",
      "Training for epoch 52 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 52 finished in 7m 20.91s (- 6m 46.99s) (52 52.0%). train avg loss: 0.0313, val avg loss: 0.2372\n",
      "Training for epoch 53 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 53 finished in 7m 30.43s (- 6m 39.44s) (53 53.0%). train avg loss: 0.0275, val avg loss: 0.2139\n",
      "Training for epoch 54 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 54 finished in 7m 38.79s (- 6m 30.83s) (54 54.0%). train avg loss: 0.028, val avg loss: 0.2237\n",
      "Training for epoch 55 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 55 finished in 7m 46.37s (- 6m 21.57s) (55 55.0%). train avg loss: 0.0312, val avg loss: 0.2228\n",
      "Training for epoch 56 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 56 finished in 7m 54.93s (- 6m 13.16s) (56 56.0%). train avg loss: 0.0261, val avg loss: 0.2134\n",
      "Training for epoch 57 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 57 finished in 8m 4.21s (- 6m 5.28s) (57 57.0%). train avg loss: 0.0349, val avg loss: 0.2449\n",
      "Training for epoch 58 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 58 finished in 8m 14.11s (- 5m 57.81s) (58 58.0%). train avg loss: 0.033, val avg loss: 0.2496\n",
      "Training for epoch 59 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 59 finished in 8m 21.47s (- 5m 48.48s) (59 59.0%). train avg loss: 0.031, val avg loss: 0.2398\n",
      "Training for epoch 60 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 60 finished in 8m 30.84s (- 5m 40.56s) (60 60.0%). train avg loss: 0.0298, val avg loss: 0.2402\n",
      "Training for epoch 61 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 61 finished in 8m 40.31s (- 5m 32.66s) (61 61.0%). train avg loss: 0.0263, val avg loss: 0.2377\n",
      "Training for epoch 62 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 62 finished in 8m 50.37s (- 5m 25.07s) (62 62.0%). train avg loss: 0.0274, val avg loss: 0.2208\n",
      "Training for epoch 63 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 63 finished in 8m 58.79s (- 5m 16.43s) (63 63.0%). train avg loss: 0.0247, val avg loss: 0.225\n",
      "Training for epoch 64 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 64 finished in 9m 6.72s (- 5m 7.53s) (64 64.0%). train avg loss: 0.0245, val avg loss: 0.2166\n",
      "Training for epoch 65 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 65 finished in 9m 15.44s (- 4m 59.08s) (65 65.0%). train avg loss: 0.0229, val avg loss: 0.2246\n",
      "Training for epoch 66 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 66 finished in 9m 24.89s (- 4m 51.0s) (66 66.0%). train avg loss: 0.0353, val avg loss: 0.2243\n",
      "Training for epoch 67 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 67 finished in 9m 33.81s (- 4m 42.62s) (67 67.0%). train avg loss: 0.0259, val avg loss: 0.1985\n",
      "Training for epoch 68 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 68 finished in 9m 40.85s (- 4m 33.34s) (68 68.0%). train avg loss: 0.0226, val avg loss: 0.2185\n",
      "Training for epoch 69 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 69 finished in 9m 48.71s (- 4m 24.49s) (69 69.0%). train avg loss: 0.0207, val avg loss: 0.2197\n",
      "Training for epoch 70 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 70 finished in 9m 56.41s (- 4m 15.6s) (70 70.0%). train avg loss: 0.0219, val avg loss: 0.2107\n",
      "Training for epoch 71 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 71 finished in 10m 5.12s (- 4m 7.16s) (71 71.0%). train avg loss: 0.021, val avg loss: 0.2283\n",
      "Training for epoch 72 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 72 finished in 10m 14.14s (- 3m 58.83s) (72 72.0%). train avg loss: 0.0236, val avg loss: 0.1914\n",
      "Training for epoch 73 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 73 finished in 10m 22.53s (- 3m 50.25s) (73 73.0%). train avg loss: 0.0218, val avg loss: 0.1962\n",
      "Training for epoch 74 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 74 finished in 10m 30.8s (- 3m 41.63s) (74 74.0%). train avg loss: 0.022, val avg loss: 0.2048\n",
      "Training for epoch 75 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 75 finished in 10m 38.19s (- 3m 32.73s) (75 75.0%). train avg loss: 0.0213, val avg loss: 0.2085\n",
      "Training for epoch 76 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 76 finished in 10m 47.97s (- 3m 24.62s) (76 76.0%). train avg loss: 0.0254, val avg loss: 0.2162\n",
      "Training for epoch 77 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 77 finished in 10m 56.13s (- 3m 15.99s) (77 77.0%). train avg loss: 0.0212, val avg loss: 0.2129\n",
      "Training for epoch 78 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 78 finished in 11m 3.71s (- 3m 7.2s) (78 78.0%). train avg loss: 0.022, val avg loss: 0.2209\n",
      "Training for epoch 79 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 79 finished in 11m 11.07s (- 2m 58.39s) (79 79.0%). train avg loss: 0.0242, val avg loss: 0.2286\n",
      "Training for epoch 80 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 80 finished in 11m 19.55s (- 2m 49.89s) (80 80.0%). train avg loss: 0.0209, val avg loss: 0.2106\n",
      "Training for epoch 81 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 81 finished in 11m 29.51s (- 2m 41.74s) (81 81.0%). train avg loss: 0.0221, val avg loss: 0.1959\n",
      "Training for epoch 82 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 82 finished in 11m 38.24s (- 2m 33.27s) (82 82.0%). train avg loss: 0.0198, val avg loss: 0.2254\n",
      "Training for epoch 83 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 83 finished in 11m 45.87s (- 2m 24.58s) (83 83.0%). train avg loss: 0.0238, val avg loss: 0.2165\n",
      "Training for epoch 84 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 84 finished in 11m 52.88s (- 2m 15.79s) (84 84.0%). train avg loss: 0.0197, val avg loss: 0.2183\n",
      "Training for epoch 85 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 85 finished in 12m 0.71s (- 2m 7.18s) (85 85.0%). train avg loss: 0.0194, val avg loss: 0.2271\n",
      "Training for epoch 86 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 86 finished in 12m 10.55s (- 1m 58.93s) (86 86.0%). train avg loss: 0.0203, val avg loss: 0.2197\n",
      "Training for epoch 87 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 87 finished in 12m 18.73s (- 1m 50.38s) (87 87.0%). train avg loss: 0.0201, val avg loss: 0.2353\n",
      "Training for epoch 88 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 88 finished in 12m 27.29s (- 1m 41.9s) (88 88.0%). train avg loss: 0.0212, val avg loss: 0.2468\n",
      "Training for epoch 89 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 89 finished in 12m 36.96s (- 1m 33.56s) (89 89.0%). train avg loss: 0.0192, val avg loss: 0.2148\n",
      "Training for epoch 90 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 90 finished in 12m 48.43s (- 1m 25.38s) (90 90.0%). train avg loss: 0.0185, val avg loss: 0.2167\n",
      "Training for epoch 91 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 91 finished in 12m 57.59s (- 1m 16.9s) (91 91.0%). train avg loss: 0.0177, val avg loss: 0.2211\n",
      "Training for epoch 92 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 92 finished in 13m 5.45s (- 1m 8.3s) (92 92.0%). train avg loss: 0.0196, val avg loss: 0.2293\n",
      "Training for epoch 93 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 93 finished in 13m 12.89s (- 0m 59.68s) (93 93.0%). train avg loss: 0.0207, val avg loss: 0.24\n",
      "Training for epoch 94 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 94 finished in 13m 24.26s (- 0m 51.34s) (94 94.0%). train avg loss: 0.0193, val avg loss: 0.2198\n",
      "Training for epoch 95 has started (lr=6.25e-05). Found 357 batch(es).\n",
      "Epoch 95 finished in 13m 33.79s (- 0m 42.83s) (95 95.0%). train avg loss: 0.02, val avg loss: 0.2363\n",
      "Training for epoch 96 has started (lr=6.25e-05). Found 357 batch(es).\n",
      "Epoch 96 finished in 13m 42.57s (- 0m 34.27s) (96 96.0%). train avg loss: 0.0187, val avg loss: 0.2232\n",
      "Training for epoch 97 has started (lr=6.25e-05). Found 357 batch(es).\n",
      "Epoch 97 finished in 13m 51.28s (- 0m 25.71s) (97 97.0%). train avg loss: 0.0199, val avg loss: 0.2294\n",
      "Training for epoch 98 has started (lr=6.25e-05). Found 357 batch(es).\n",
      "Epoch 98 finished in 14m 1.17s (- 0m 17.17s) (98 98.0%). train avg loss: 0.0175, val avg loss: 0.2454\n",
      "Training for epoch 99 has started (lr=6.25e-05). Found 357 batch(es).\n",
      "Epoch 99 finished in 14m 10.22s (- 0m 8.59s) (99 99.0%). train avg loss: 0.0189, val avg loss: 0.2272\n",
      "Training for epoch 100 has started (lr=6.25e-05). Found 357 batch(es).\n",
      "Epoch 100 finished in 14m 17.64s (- 0m 0.0s) (100 100.0%). train avg loss: 0.0182, val avg loss: 0.233\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5.25, min_value+.1, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 3.9279320595993936%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> mempelajari\n",
      "= ['M', 'AX', 'M', 'P', 'AX', 'L', 'AA', 'JH', 'AA', 'R', 'IY']\n",
      "< M AX M P AX L AA JH AA R IY ['M', 'AX', 'M', 'P', 'AX', 'L', 'AA', 'JH', 'AA', 'R', 'IY']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd49c115ee0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 440x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
