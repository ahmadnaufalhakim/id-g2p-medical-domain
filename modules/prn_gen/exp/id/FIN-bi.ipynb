{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1739957705963,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "29775173-7761-4953-d853-502b8b825ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/exp/id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4880,
     "status": "ok",
     "timestamp": 1739957711340,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "33e9e6e9-f2b0-4d04-e665-6f2600a2c57e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8274,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7a08073c-d249-49ab-ddaf-f827de5d8d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719612,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"32\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "704ba764-a750-40fc-d5c9-0a6d289c3ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    return graphemes, phonemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.')).lower()\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1739957719613,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[-1]) for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1757,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "1a7e253b-ee59-419e-f7d6-0e469cac96fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957721365,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq), ...]\n",
    "  graphemes, phonemes = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  word = word.lower()\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1).to(DEVICE)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1739957721366,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "92da8620-4d32-4614-db38-6fdfcd7e04fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[273, 122, 294, 166, 434, 290, 83, 202, 309, 456, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1522,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "56a821f8-be28-4de8-8e2c-f5e7b8d2bc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f5125377a00> ([14, 27, 309, 456, 1], [23, 3, 1])\n",
      "([14, 27, 309, 456, 1], [23, 3, 1])\n",
      "([14, 27, 309, 456, 1], [23, 3, 1])\n",
      "train grp 474 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'d\", 7: \"'h\", 8: \"'i\", 9: \"'j\", 10: \"'n\", 11: \"'o\", 12: \"'r\", 13: \"'u\", 14: '--', 15: '-a', 16: '-b', 17: '-c', 18: '-d', 19: '-e', 20: '-g', 21: '-h', 22: '-i', 23: '-j', 24: '-k', 25: '-l', 26: '-m', 27: '-n', 28: '-p', 29: '-q', 30: '-r', 31: '-s', 32: '-t', 33: '-u', 34: '-w', 35: \"a'\", 36: 'a-', 37: 'aa', 38: 'ab', 39: 'ac', 40: 'ad', 41: 'ae', 42: 'af', 43: 'ag', 44: 'ah', 45: 'ai', 46: 'aj', 47: 'ak', 48: 'al', 49: 'am', 50: 'an', 51: 'ao', 52: 'ap', 53: 'aq', 54: 'ar', 55: 'as', 56: 'at', 57: 'au', 58: 'av', 59: 'aw', 60: 'ay', 61: 'az', 62: 'ba', 63: 'bb', 64: 'bd', 65: 'be', 66: 'bh', 67: 'bi', 68: 'bj', 69: 'bk', 70: 'bl', 71: 'bm', 72: 'bn', 73: 'bo', 74: 'br', 75: 'bs', 76: 'bt', 77: 'bu', 78: 'bv', 79: 'by', 80: 'ca', 81: 'ce', 82: 'ch', 83: 'ci', 84: 'ck', 85: 'cm', 86: 'co', 87: 'cr', 88: 'cu', 89: \"d'\", 90: 'da', 91: 'db', 92: 'de', 93: 'dh', 94: 'di', 95: 'dj', 96: 'dk', 97: 'dm', 98: 'dn', 99: 'do', 100: 'dr', 101: 'ds', 102: 'du', 103: 'dv', 104: 'dw', 105: 'dy', 106: 'dz', 107: \"e'\", 108: 'e-', 109: 'ea', 110: 'eb', 111: 'ec', 112: 'ed', 113: 'ee', 114: 'ef', 115: 'eg', 116: 'eh', 117: 'ei', 118: 'ej', 119: 'ek', 120: 'el', 121: 'em', 122: 'en', 123: 'eo', 124: 'ep', 125: 'er', 126: 'es', 127: 'et', 128: 'eu', 129: 'ev', 130: 'ew', 131: 'ey', 132: 'ez', 133: 'fa', 134: 'fb', 135: 'fd', 136: 'fe', 137: 'fi', 138: 'fk', 139: 'fl', 140: 'fn', 141: 'fo', 142: 'fr', 143: 'fs', 144: 'ft', 145: 'fu', 146: 'fw', 147: 'g-', 148: 'ga', 149: 'gb', 150: 'gc', 151: 'gd', 152: 'ge', 153: 'gg', 154: 'gh', 155: 'gi', 156: 'gj', 157: 'gk', 158: 'gl', 159: 'gm', 160: 'gn', 161: 'go', 162: 'gp', 163: 'gr', 164: 'gs', 165: 'gt', 166: 'gu', 167: 'h-', 168: 'ha', 169: 'hb', 170: 'hd', 171: 'he', 172: 'hf', 173: 'hg', 174: 'hi', 175: 'hj', 176: 'hk', 177: 'hl', 178: 'hm', 179: 'hn', 180: 'ho', 181: 'hp', 182: 'hr', 183: 'hs', 184: 'ht', 185: 'hu', 186: 'hw', 187: 'hy', 188: 'i-', 189: 'ia', 190: 'ib', 191: 'ic', 192: 'id', 193: 'ie', 194: 'if', 195: 'ig', 196: 'ih', 197: 'ii', 198: 'ij', 199: 'ik', 200: 'il', 201: 'im', 202: 'in', 203: 'io', 204: 'ip', 205: 'ir', 206: 'is', 207: 'it', 208: 'iu', 209: 'iv', 210: 'iw', 211: 'iy', 212: 'iz', 213: 'ja', 214: 'je', 215: 'ji', 216: 'jl', 217: 'jn', 218: 'jo', 219: 'jr', 220: 'jt', 221: 'ju', 222: 'jw', 223: 'k-', 224: 'ka', 225: 'kb', 226: 'kc', 227: 'kd', 228: 'ke', 229: 'kh', 230: 'ki', 231: 'kj', 232: 'kk', 233: 'kl', 234: 'km', 235: 'kn', 236: 'ko', 237: 'kp', 238: 'kr', 239: 'ks', 240: 'kt', 241: 'ku', 242: 'kw', 243: 'ky', 244: 'kz', 245: 'l-', 246: 'la', 247: 'lb', 248: 'ld', 249: 'le', 250: 'lf', 251: 'lg', 252: 'lh', 253: 'li', 254: 'lj', 255: 'lk', 256: 'll', 257: 'lm', 258: 'ln', 259: 'lo', 260: 'lp', 261: 'lr', 262: 'ls', 263: 'lt', 264: 'lu', 265: 'lv', 266: 'lw', 267: 'ly', 268: \"m'\", 269: 'ma', 270: 'mb', 271: 'mc', 272: 'md', 273: 'me', 274: 'mf', 275: 'mi', 276: 'mk', 277: 'ml', 278: 'mm', 279: 'mn', 280: 'mo', 281: 'mp', 282: 'mr', 283: 'ms', 284: 'mt', 285: 'mu', 286: 'mz', 287: 'n-', 288: 'na', 289: 'nb', 290: 'nc', 291: 'nd', 292: 'ne', 293: 'nf', 294: 'ng', 295: 'nh', 296: 'ni', 297: 'nj', 298: 'nk', 299: 'nl', 300: 'nm', 301: 'nn', 302: 'no', 303: 'np', 304: 'ns', 305: 'nt', 306: 'nu', 307: 'nv', 308: 'nw', 309: 'ny', 310: 'nz', 311: \"o'\", 312: 'o-', 313: 'oa', 314: 'ob', 315: 'oc', 316: 'od', 317: 'oe', 318: 'of', 319: 'og', 320: 'oh', 321: 'oi', 322: 'oj', 323: 'ok', 324: 'ol', 325: 'om', 326: 'on', 327: 'oo', 328: 'op', 329: 'or', 330: 'os', 331: 'ot', 332: 'ov', 333: 'ow', 334: 'oy', 335: 'oz', 336: 'pa', 337: 'pc', 338: 'pe', 339: 'pi', 340: 'pj', 341: 'pk', 342: 'pl', 343: 'pn', 344: 'po', 345: 'pr', 346: 'ps', 347: 'pt', 348: 'pu', 349: 'py', 350: 'qi', 351: 'qu', 352: \"r'\", 353: 'r-', 354: 'ra', 355: 'rb', 356: 'rc', 357: 'rd', 358: 're', 359: 'rf', 360: 'rg', 361: 'rh', 362: 'ri', 363: 'rj', 364: 'rk', 365: 'rl', 366: 'rm', 367: 'rn', 368: 'ro', 369: 'rp', 370: 'rr', 371: 'rs', 372: 'rt', 373: 'ru', 374: 'rv', 375: 'rw', 376: 'ry', 377: 'rz', 378: 's-', 379: 'sa', 380: 'sb', 381: 'sc', 382: 'sd', 383: 'se', 384: 'sf', 385: 'sh', 386: 'si', 387: 'sj', 388: 'sk', 389: 'sl', 390: 'sm', 391: 'sn', 392: 'so', 393: 'sp', 394: 'sr', 395: 'ss', 396: 'st', 397: 'su', 398: 'sw', 399: 'sy', 400: 't-', 401: 'ta', 402: 'tb', 403: 'te', 404: 'tf', 405: 'tg', 406: 'th', 407: 'ti', 408: 'tk', 409: 'tl', 410: 'tm', 411: 'tn', 412: 'to', 413: 'tp', 414: 'tr', 415: 'ts', 416: 'tt', 417: 'tu', 418: 'tw', 419: \"u'\", 420: 'u-', 421: 'ua', 422: 'ub', 423: 'uc', 424: 'ud', 425: 'ue', 426: 'uf', 427: 'ug', 428: 'uh', 429: 'ui', 430: 'uj', 431: 'uk', 432: 'ul', 433: 'um', 434: 'un', 435: 'uo', 436: 'up', 437: 'ur', 438: 'us', 439: 'ut', 440: 'uv', 441: 'uw', 442: 'uy', 443: 'uz', 444: 'va', 445: 've', 446: 'vg', 447: 'vi', 448: 'vo', 449: 'vu', 450: 'wa', 451: 'we', 452: 'wi', 453: 'wo', 454: 'wt', 455: 'wu', 456: 'ya', 457: 'ye', 458: 'yg', 459: 'yh', 460: 'yi', 461: 'yk', 462: 'yo', 463: 'yr', 464: 'yt', 465: 'yu', 466: 'za', 467: 'zb', 468: 'ze', 469: 'zh', 470: 'zi', 471: 'zm', 472: 'zo', 473: 'zu'}\n",
      "valid grp 474 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'d\", 7: \"'h\", 8: \"'i\", 9: \"'j\", 10: \"'n\", 11: \"'o\", 12: \"'r\", 13: \"'u\", 14: '--', 15: '-a', 16: '-b', 17: '-c', 18: '-d', 19: '-e', 20: '-g', 21: '-h', 22: '-i', 23: '-j', 24: '-k', 25: '-l', 26: '-m', 27: '-n', 28: '-p', 29: '-q', 30: '-r', 31: '-s', 32: '-t', 33: '-u', 34: '-w', 35: \"a'\", 36: 'a-', 37: 'aa', 38: 'ab', 39: 'ac', 40: 'ad', 41: 'ae', 42: 'af', 43: 'ag', 44: 'ah', 45: 'ai', 46: 'aj', 47: 'ak', 48: 'al', 49: 'am', 50: 'an', 51: 'ao', 52: 'ap', 53: 'aq', 54: 'ar', 55: 'as', 56: 'at', 57: 'au', 58: 'av', 59: 'aw', 60: 'ay', 61: 'az', 62: 'ba', 63: 'bb', 64: 'bd', 65: 'be', 66: 'bh', 67: 'bi', 68: 'bj', 69: 'bk', 70: 'bl', 71: 'bm', 72: 'bn', 73: 'bo', 74: 'br', 75: 'bs', 76: 'bt', 77: 'bu', 78: 'bv', 79: 'by', 80: 'ca', 81: 'ce', 82: 'ch', 83: 'ci', 84: 'ck', 85: 'cm', 86: 'co', 87: 'cr', 88: 'cu', 89: \"d'\", 90: 'da', 91: 'db', 92: 'de', 93: 'dh', 94: 'di', 95: 'dj', 96: 'dk', 97: 'dm', 98: 'dn', 99: 'do', 100: 'dr', 101: 'ds', 102: 'du', 103: 'dv', 104: 'dw', 105: 'dy', 106: 'dz', 107: \"e'\", 108: 'e-', 109: 'ea', 110: 'eb', 111: 'ec', 112: 'ed', 113: 'ee', 114: 'ef', 115: 'eg', 116: 'eh', 117: 'ei', 118: 'ej', 119: 'ek', 120: 'el', 121: 'em', 122: 'en', 123: 'eo', 124: 'ep', 125: 'er', 126: 'es', 127: 'et', 128: 'eu', 129: 'ev', 130: 'ew', 131: 'ey', 132: 'ez', 133: 'fa', 134: 'fb', 135: 'fd', 136: 'fe', 137: 'fi', 138: 'fk', 139: 'fl', 140: 'fn', 141: 'fo', 142: 'fr', 143: 'fs', 144: 'ft', 145: 'fu', 146: 'fw', 147: 'g-', 148: 'ga', 149: 'gb', 150: 'gc', 151: 'gd', 152: 'ge', 153: 'gg', 154: 'gh', 155: 'gi', 156: 'gj', 157: 'gk', 158: 'gl', 159: 'gm', 160: 'gn', 161: 'go', 162: 'gp', 163: 'gr', 164: 'gs', 165: 'gt', 166: 'gu', 167: 'h-', 168: 'ha', 169: 'hb', 170: 'hd', 171: 'he', 172: 'hf', 173: 'hg', 174: 'hi', 175: 'hj', 176: 'hk', 177: 'hl', 178: 'hm', 179: 'hn', 180: 'ho', 181: 'hp', 182: 'hr', 183: 'hs', 184: 'ht', 185: 'hu', 186: 'hw', 187: 'hy', 188: 'i-', 189: 'ia', 190: 'ib', 191: 'ic', 192: 'id', 193: 'ie', 194: 'if', 195: 'ig', 196: 'ih', 197: 'ii', 198: 'ij', 199: 'ik', 200: 'il', 201: 'im', 202: 'in', 203: 'io', 204: 'ip', 205: 'ir', 206: 'is', 207: 'it', 208: 'iu', 209: 'iv', 210: 'iw', 211: 'iy', 212: 'iz', 213: 'ja', 214: 'je', 215: 'ji', 216: 'jl', 217: 'jn', 218: 'jo', 219: 'jr', 220: 'jt', 221: 'ju', 222: 'jw', 223: 'k-', 224: 'ka', 225: 'kb', 226: 'kc', 227: 'kd', 228: 'ke', 229: 'kh', 230: 'ki', 231: 'kj', 232: 'kk', 233: 'kl', 234: 'km', 235: 'kn', 236: 'ko', 237: 'kp', 238: 'kr', 239: 'ks', 240: 'kt', 241: 'ku', 242: 'kw', 243: 'ky', 244: 'kz', 245: 'l-', 246: 'la', 247: 'lb', 248: 'ld', 249: 'le', 250: 'lf', 251: 'lg', 252: 'lh', 253: 'li', 254: 'lj', 255: 'lk', 256: 'll', 257: 'lm', 258: 'ln', 259: 'lo', 260: 'lp', 261: 'lr', 262: 'ls', 263: 'lt', 264: 'lu', 265: 'lv', 266: 'lw', 267: 'ly', 268: \"m'\", 269: 'ma', 270: 'mb', 271: 'mc', 272: 'md', 273: 'me', 274: 'mf', 275: 'mi', 276: 'mk', 277: 'ml', 278: 'mm', 279: 'mn', 280: 'mo', 281: 'mp', 282: 'mr', 283: 'ms', 284: 'mt', 285: 'mu', 286: 'mz', 287: 'n-', 288: 'na', 289: 'nb', 290: 'nc', 291: 'nd', 292: 'ne', 293: 'nf', 294: 'ng', 295: 'nh', 296: 'ni', 297: 'nj', 298: 'nk', 299: 'nl', 300: 'nm', 301: 'nn', 302: 'no', 303: 'np', 304: 'ns', 305: 'nt', 306: 'nu', 307: 'nv', 308: 'nw', 309: 'ny', 310: 'nz', 311: \"o'\", 312: 'o-', 313: 'oa', 314: 'ob', 315: 'oc', 316: 'od', 317: 'oe', 318: 'of', 319: 'og', 320: 'oh', 321: 'oi', 322: 'oj', 323: 'ok', 324: 'ol', 325: 'om', 326: 'on', 327: 'oo', 328: 'op', 329: 'or', 330: 'os', 331: 'ot', 332: 'ov', 333: 'ow', 334: 'oy', 335: 'oz', 336: 'pa', 337: 'pc', 338: 'pe', 339: 'pi', 340: 'pj', 341: 'pk', 342: 'pl', 343: 'pn', 344: 'po', 345: 'pr', 346: 'ps', 347: 'pt', 348: 'pu', 349: 'py', 350: 'qi', 351: 'qu', 352: \"r'\", 353: 'r-', 354: 'ra', 355: 'rb', 356: 'rc', 357: 'rd', 358: 're', 359: 'rf', 360: 'rg', 361: 'rh', 362: 'ri', 363: 'rj', 364: 'rk', 365: 'rl', 366: 'rm', 367: 'rn', 368: 'ro', 369: 'rp', 370: 'rr', 371: 'rs', 372: 'rt', 373: 'ru', 374: 'rv', 375: 'rw', 376: 'ry', 377: 'rz', 378: 's-', 379: 'sa', 380: 'sb', 381: 'sc', 382: 'sd', 383: 'se', 384: 'sf', 385: 'sh', 386: 'si', 387: 'sj', 388: 'sk', 389: 'sl', 390: 'sm', 391: 'sn', 392: 'so', 393: 'sp', 394: 'sr', 395: 'ss', 396: 'st', 397: 'su', 398: 'sw', 399: 'sy', 400: 't-', 401: 'ta', 402: 'tb', 403: 'te', 404: 'tf', 405: 'tg', 406: 'th', 407: 'ti', 408: 'tk', 409: 'tl', 410: 'tm', 411: 'tn', 412: 'to', 413: 'tp', 414: 'tr', 415: 'ts', 416: 'tt', 417: 'tu', 418: 'tw', 419: \"u'\", 420: 'u-', 421: 'ua', 422: 'ub', 423: 'uc', 424: 'ud', 425: 'ue', 426: 'uf', 427: 'ug', 428: 'uh', 429: 'ui', 430: 'uj', 431: 'uk', 432: 'ul', 433: 'um', 434: 'un', 435: 'uo', 436: 'up', 437: 'ur', 438: 'us', 439: 'ut', 440: 'uv', 441: 'uw', 442: 'uy', 443: 'uz', 444: 'va', 445: 've', 446: 'vg', 447: 'vi', 448: 'vo', 449: 'vu', 450: 'wa', 451: 'we', 452: 'wi', 453: 'wo', 454: 'wt', 455: 'wu', 456: 'ya', 457: 'ye', 458: 'yg', 459: 'yh', 460: 'yi', 461: 'yk', 462: 'yo', 463: 'yr', 464: 'yt', 465: 'yu', 466: 'za', 467: 'zb', 468: 'ze', 469: 'zh', 470: 'zi', 471: 'zm', 472: 'zo', 473: 'zu'}\n",
      "test grp 474 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'d\", 7: \"'h\", 8: \"'i\", 9: \"'j\", 10: \"'n\", 11: \"'o\", 12: \"'r\", 13: \"'u\", 14: '--', 15: '-a', 16: '-b', 17: '-c', 18: '-d', 19: '-e', 20: '-g', 21: '-h', 22: '-i', 23: '-j', 24: '-k', 25: '-l', 26: '-m', 27: '-n', 28: '-p', 29: '-q', 30: '-r', 31: '-s', 32: '-t', 33: '-u', 34: '-w', 35: \"a'\", 36: 'a-', 37: 'aa', 38: 'ab', 39: 'ac', 40: 'ad', 41: 'ae', 42: 'af', 43: 'ag', 44: 'ah', 45: 'ai', 46: 'aj', 47: 'ak', 48: 'al', 49: 'am', 50: 'an', 51: 'ao', 52: 'ap', 53: 'aq', 54: 'ar', 55: 'as', 56: 'at', 57: 'au', 58: 'av', 59: 'aw', 60: 'ay', 61: 'az', 62: 'ba', 63: 'bb', 64: 'bd', 65: 'be', 66: 'bh', 67: 'bi', 68: 'bj', 69: 'bk', 70: 'bl', 71: 'bm', 72: 'bn', 73: 'bo', 74: 'br', 75: 'bs', 76: 'bt', 77: 'bu', 78: 'bv', 79: 'by', 80: 'ca', 81: 'ce', 82: 'ch', 83: 'ci', 84: 'ck', 85: 'cm', 86: 'co', 87: 'cr', 88: 'cu', 89: \"d'\", 90: 'da', 91: 'db', 92: 'de', 93: 'dh', 94: 'di', 95: 'dj', 96: 'dk', 97: 'dm', 98: 'dn', 99: 'do', 100: 'dr', 101: 'ds', 102: 'du', 103: 'dv', 104: 'dw', 105: 'dy', 106: 'dz', 107: \"e'\", 108: 'e-', 109: 'ea', 110: 'eb', 111: 'ec', 112: 'ed', 113: 'ee', 114: 'ef', 115: 'eg', 116: 'eh', 117: 'ei', 118: 'ej', 119: 'ek', 120: 'el', 121: 'em', 122: 'en', 123: 'eo', 124: 'ep', 125: 'er', 126: 'es', 127: 'et', 128: 'eu', 129: 'ev', 130: 'ew', 131: 'ey', 132: 'ez', 133: 'fa', 134: 'fb', 135: 'fd', 136: 'fe', 137: 'fi', 138: 'fk', 139: 'fl', 140: 'fn', 141: 'fo', 142: 'fr', 143: 'fs', 144: 'ft', 145: 'fu', 146: 'fw', 147: 'g-', 148: 'ga', 149: 'gb', 150: 'gc', 151: 'gd', 152: 'ge', 153: 'gg', 154: 'gh', 155: 'gi', 156: 'gj', 157: 'gk', 158: 'gl', 159: 'gm', 160: 'gn', 161: 'go', 162: 'gp', 163: 'gr', 164: 'gs', 165: 'gt', 166: 'gu', 167: 'h-', 168: 'ha', 169: 'hb', 170: 'hd', 171: 'he', 172: 'hf', 173: 'hg', 174: 'hi', 175: 'hj', 176: 'hk', 177: 'hl', 178: 'hm', 179: 'hn', 180: 'ho', 181: 'hp', 182: 'hr', 183: 'hs', 184: 'ht', 185: 'hu', 186: 'hw', 187: 'hy', 188: 'i-', 189: 'ia', 190: 'ib', 191: 'ic', 192: 'id', 193: 'ie', 194: 'if', 195: 'ig', 196: 'ih', 197: 'ii', 198: 'ij', 199: 'ik', 200: 'il', 201: 'im', 202: 'in', 203: 'io', 204: 'ip', 205: 'ir', 206: 'is', 207: 'it', 208: 'iu', 209: 'iv', 210: 'iw', 211: 'iy', 212: 'iz', 213: 'ja', 214: 'je', 215: 'ji', 216: 'jl', 217: 'jn', 218: 'jo', 219: 'jr', 220: 'jt', 221: 'ju', 222: 'jw', 223: 'k-', 224: 'ka', 225: 'kb', 226: 'kc', 227: 'kd', 228: 'ke', 229: 'kh', 230: 'ki', 231: 'kj', 232: 'kk', 233: 'kl', 234: 'km', 235: 'kn', 236: 'ko', 237: 'kp', 238: 'kr', 239: 'ks', 240: 'kt', 241: 'ku', 242: 'kw', 243: 'ky', 244: 'kz', 245: 'l-', 246: 'la', 247: 'lb', 248: 'ld', 249: 'le', 250: 'lf', 251: 'lg', 252: 'lh', 253: 'li', 254: 'lj', 255: 'lk', 256: 'll', 257: 'lm', 258: 'ln', 259: 'lo', 260: 'lp', 261: 'lr', 262: 'ls', 263: 'lt', 264: 'lu', 265: 'lv', 266: 'lw', 267: 'ly', 268: \"m'\", 269: 'ma', 270: 'mb', 271: 'mc', 272: 'md', 273: 'me', 274: 'mf', 275: 'mi', 276: 'mk', 277: 'ml', 278: 'mm', 279: 'mn', 280: 'mo', 281: 'mp', 282: 'mr', 283: 'ms', 284: 'mt', 285: 'mu', 286: 'mz', 287: 'n-', 288: 'na', 289: 'nb', 290: 'nc', 291: 'nd', 292: 'ne', 293: 'nf', 294: 'ng', 295: 'nh', 296: 'ni', 297: 'nj', 298: 'nk', 299: 'nl', 300: 'nm', 301: 'nn', 302: 'no', 303: 'np', 304: 'ns', 305: 'nt', 306: 'nu', 307: 'nv', 308: 'nw', 309: 'ny', 310: 'nz', 311: \"o'\", 312: 'o-', 313: 'oa', 314: 'ob', 315: 'oc', 316: 'od', 317: 'oe', 318: 'of', 319: 'og', 320: 'oh', 321: 'oi', 322: 'oj', 323: 'ok', 324: 'ol', 325: 'om', 326: 'on', 327: 'oo', 328: 'op', 329: 'or', 330: 'os', 331: 'ot', 332: 'ov', 333: 'ow', 334: 'oy', 335: 'oz', 336: 'pa', 337: 'pc', 338: 'pe', 339: 'pi', 340: 'pj', 341: 'pk', 342: 'pl', 343: 'pn', 344: 'po', 345: 'pr', 346: 'ps', 347: 'pt', 348: 'pu', 349: 'py', 350: 'qi', 351: 'qu', 352: \"r'\", 353: 'r-', 354: 'ra', 355: 'rb', 356: 'rc', 357: 'rd', 358: 're', 359: 'rf', 360: 'rg', 361: 'rh', 362: 'ri', 363: 'rj', 364: 'rk', 365: 'rl', 366: 'rm', 367: 'rn', 368: 'ro', 369: 'rp', 370: 'rr', 371: 'rs', 372: 'rt', 373: 'ru', 374: 'rv', 375: 'rw', 376: 'ry', 377: 'rz', 378: 's-', 379: 'sa', 380: 'sb', 381: 'sc', 382: 'sd', 383: 'se', 384: 'sf', 385: 'sh', 386: 'si', 387: 'sj', 388: 'sk', 389: 'sl', 390: 'sm', 391: 'sn', 392: 'so', 393: 'sp', 394: 'sr', 395: 'ss', 396: 'st', 397: 'su', 398: 'sw', 399: 'sy', 400: 't-', 401: 'ta', 402: 'tb', 403: 'te', 404: 'tf', 405: 'tg', 406: 'th', 407: 'ti', 408: 'tk', 409: 'tl', 410: 'tm', 411: 'tn', 412: 'to', 413: 'tp', 414: 'tr', 415: 'ts', 416: 'tt', 417: 'tu', 418: 'tw', 419: \"u'\", 420: 'u-', 421: 'ua', 422: 'ub', 423: 'uc', 424: 'ud', 425: 'ue', 426: 'uf', 427: 'ug', 428: 'uh', 429: 'ui', 430: 'uj', 431: 'uk', 432: 'ul', 433: 'um', 434: 'un', 435: 'uo', 436: 'up', 437: 'ur', 438: 'us', 439: 'ut', 440: 'uv', 441: 'uw', 442: 'uy', 443: 'uz', 444: 'va', 445: 've', 446: 'vg', 447: 'vi', 448: 'vo', 449: 'vu', 450: 'wa', 451: 'we', 452: 'wi', 453: 'wo', 454: 'wt', 455: 'wu', 456: 'ya', 457: 'ye', 458: 'yg', 459: 'yh', 460: 'yi', 461: 'yk', 462: 'yo', 463: 'yr', 464: 'yt', 465: 'yu', 466: 'za', 467: 'zb', 468: 'ze', 469: 'zh', 470: 'zi', 471: 'zm', 472: 'zo', 473: 'zu'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "470 {'--': 14, '-n': 27, 'ny': 309, 'ya': 456, '-a': 15, 'an': 50, 'nd': 291, 'da': 90, '-b': 16, 'be': 65, 'el': 120, 'la': 246, 'as': 55, '-c': 17, 'co': 86, 'om': 325, 'mp': 281, 'pe': 338, 'en': 122, 'ng': 294, '-d': 18, 'de': 92, 'ap': 52, 'pa': 336, '-k': 24, 'ka': 224, 'ku': 241, '-l': 25, 'le': 249, 'gg': 153, 'ga': 148, '-m': 26, 'ma': 269, 'ah': 44, 'ha': 168, 'si': 386, 'is': 206, 'sw': 398, 'wa': 450, '-s': 31, 'se': 383, 'ep': 124, 'pi': 339, 'it': 207, '-t': 32, 'ta': 401, 'ak': 47, '-w': 34, 'at': 56, 'ti': 407, \"a'\": 35, \"'d\": 6, 'du': 102, \"'h\": 7, 'ad': 40, 'ab': 38, 'ba': 62, 'di': 94, 'ia': 189, 'ik': 199, 'ai': 45, 'im': 201, 'na': 288, 'gn': 160, 'ar': 54, 'ra': 354, 'to': 412, 'oa': 313, 'au': 57, 'bd': 64, 'do': 99, 'me': 273, 'mi': 275, 'in': 202, 'al': 48, 'er': 125, 'et': 127, 'bi': 67, 'id': 192, 'il': 200, 'tu': 417, 'ur': 437, 'ri': 362, 'ie': 193, 'bj': 68, 'ja': 213, 'bl': 70, 'ut': 439, 'bn': 72, 'no': 302, 'or': 329, 'rm': 366, 'li': 253, 'nu': 306, 'us': 438, 'bo': 73, 'oi': 321, 'ol': 324, 'on': 326, 'ne': 292, 'em': 121, 'br': 74, 're': 358, 'ea': 109, 'ks': 239, 'ek': 119, 'ev': 129, 'vi': 447, 'ko': 236, 'os': 330, 'bs': 75, 'ns': 304, 'nt': 305, 'te': 403, 'ei': 117, 'sm': 390, 'so': 392, 'lu': 264, 'rp': 369, 'ps': 346, 'st': 396, 'tr': 414, 'su': 397, 'rd': 357, 'bt': 76, 'bu': 77, 'ua': 421, 'uh': 428, 'uk': 431, 'ul': 432, 'lh': 252, 'ay': 60, 'ac': 39, 'ca': 80, 'am': 49, 'ce': 81, 'ci': 83, 'cu': 88, 'um': 433, 'un': 434, 'gk': 157, 'aa': 37, 'ag': 43, 'gi': 155, 'io': 203, 'iu': 208, 'pt': 347, 'if': 194, 'pu': 348, 'eg': 115, 'eh': 116, 'uz': 443, 'ze': 468, 'ib': 190, 'sa': 379, 'ic': 191, 'ig': 195, 'gu': 166, 'ry': 376, 'kk': 232, 'kn': 235, 'od': 316, 'dr': 100, 'kt': 240, 'rg': 360, 'ip': 204, 'gh': 154, 'po': 344, 'ir': 205, 'aj': 46, 'ok': 323, 'oh': 320, 'iw': 210, 'gs': 164, 'rn': 367, 'wi': 452, 'dj': 95, 'je': 214, 'iv': 209, 'va': 444, 'dm': 97, 'ni': 296, 'op': 328, 'es': 126, 'dv': 103, 've': 445, 'rb': 355, 'rt': 372, 'vo': 448, 'ae': 41, 'ro': 368, 'ob': 314, 'og': 319, 'gr': 163, 'lo': 259, 'mo': 280, 'ov': 332, 'sk': 388, 'af': 42, 'fd': 135, 'ru': 373, 'fe': 136, 'fi': 137, 'fr': 142, 'fw': 146, 'aw': 59, 'ge': 152, 'ih': 196, 'gl': 158, 'hk': 176, 'hl': 177, 'lb': 247, 'lk': 255, 'ki': 230, 'ub': 422, 'hm': 178, 'ho': 180, 'hw': 186, 'lf': 250, 'nm': 300, 'mu': 285, 'ji': 215, 'jn': 217, 'jo': 218, 'oj': 322, 'ju': 221, 'ud': 424, 'uj': 430, 'k-': 223, 'ln': 258, 'kh': 229, 'hb': 169, 'hi': 174, 'ht': 184, 'kl': 233, 'km': 234, 'kr': 238, 'ed': 112, 'ui': 429, 'lt': 263, 'up': 436, 'nk': 298, 'kw': 242, 'l-': 245, '-q': 29, 'qu': 351, \"r'\": 352, \"'a\": 4, 'mn': 279, 'go': 161, 'fa': 133, 'fu': 145, 'lg': 251, 'tm': 410, 'md': 272, 'll': 256, 'fb': 134, 'lj': 254, 'sy': 399, 'ke': 228, 'lm': 257, 'rh': 361, 'hu': 185, 'of': 318, 'fo': 141, 'rf': 359, 'lp': 260, 'uv': 440, 'lv': 265, 'eo': 123, 'lw': 266, 'nn': 301, 'hn': 179, 'tl': 409, 'mb': 270, 'by': 79, 'eb': 110, 'mf': 274, 'mk': 276, 'pl': 342, 'pr': 345, 'mr': 282, 'ms': 283, 'mt': 284, 'rk': 364, 'nb': 289, 'nc': 290, 'nf': 293, 'ot': 331, 'tn': 411, 'gp': 162, 'nj': 297, 'jl': 216, 'rr': 370, 'rs': 371, 'i-': 188, 'ug': 427, 'yi': 460, 'th': 406, 'he': 171, 'pk': 341, 'aq': 53, 'qi': 350, 'rc': 356, 'rj': 363, 'ef': 114, 'rw': 375, 'rz': 377, 'za': 466, 'sn': 391, 'sb': 380, 'sf': 384, 'sh': 385, 'sl': 389, 'sp': 393, 'sr': 394, 'av': 58, 'vg': 446, 'we': 451, 'wu': 455, 'ye': 457, 'yo': 462, 'yu': 465, 'az': 61, 'ij': 198, 'zh': 469, 'zi': 470, 'hr': 182, 'kd': 227, 'kp': 237, 'ls': 262, 'gb': 149, 'nz': 310, 'rl': 365, 'zo': 472, 'ec': 111, \"o'\": 311, \"'o\": 11, 'uw': 441, 'r-': 353, 'a-': 36, '-g': 20, 'o-': 312, 'gt': 165, 's-': 378, 'oy': 334, 'uc': 423, 'kc': 226, 'ft': 144, 'gd': 151, 'ue': 425, 'dw': 104, 'fl': 139, 'ew': 130, 'wo': 453, 'ej': 118, 'iz': 212, 'jr': 219, 'tk': 408, 'hf': 172, 'ml': 277, 'oc': 315, 'uy': 442, 'ey': 131, 'tb': 402, 'nv': 307, 'fk': 138, 'fs': 143, 'hy': 187, 'g-': 147, '-j': 23, 'rv': 374, 'ez': 132, 'bh': 66, \"d'\": 89, 'ly': 267, 'ss': 395, 'gm': 159, 'oz': 335, 'nw': 308, 'uf': 426, 'nl': 299, 'ld': 248, 'h-': 167, 'mc': 271, 'gc': 150, 'ao': 51, 'pc': 337, 'pj': 340, \"e'\": 107, 'ee': 113, 'oo': 327, 'ch': 82, 'cm': 85, 'ow': 333, 'cr': 87, \"'i\": 8, 'fn': 140, 'hs': 183, 'ii': 197, 'ts': 415, 'dk': 96, 'bk': 69, 'gj': 156, 'sj': 387, 'iy': 211, 'tp': 413, 'mz': 286, 'tt': 416, '-e': 19, 'pn': 343, 'u-': 420, '-h': 21, 'jt': 220, 'nh': 295, 'np': 303, 'yt': 464, '-r': 30, 'yk': 461, 'ky': 243, '-p': 28, 'eu': 128, 'zu': 473, 'oe': 317, 'uo': 435, 'ds': 101, \"'n\": 10, 'yg': 458, 'yh': 459, 'yr': 463, 'zb': 467, 'zm': 471, 't-': 400, 'kj': 231, '-i': 22, 'py': 349, 'hg': 173, 'mm': 278, \"u'\": 419, \"'j\": 9, 'dz': 106, 'hd': 170, 'bm': 71, 'sc': 381, 'tf': 404, 'e-': 108, 'hj': 175, 'n-': 287, 'lr': 261, 'dh': 93, 'ck': 84, 'tg': 405, \"'b\": 5, 'tw': 418, 'bb': 63, 'bv': 78, 'hp': 181, 'wt': 454, \"'r\": 12, \"'u\": 13, 'db': 91, 'jw': 222, 'kb': 225, 'kz': 244, '-u': 33, \"m'\": 268, 'vu': 449, 'dy': 105, 'dn': 98, 'sd': 382}\n",
      "470 {'--': 14, '-n': 27, 'ny': 309, 'ya': 456, '-a': 15, 'an': 50, 'nd': 291, 'da': 90, '-b': 16, 'be': 65, 'el': 120, 'la': 246, 'as': 55, '-c': 17, 'co': 86, 'om': 325, 'mp': 281, 'pe': 338, 'en': 122, 'ng': 294, '-d': 18, 'de': 92, 'ap': 52, 'pa': 336, '-k': 24, 'ka': 224, 'ku': 241, '-l': 25, 'le': 249, 'gg': 153, 'ga': 148, '-m': 26, 'ma': 269, 'ah': 44, 'ha': 168, 'si': 386, 'is': 206, 'sw': 398, 'wa': 450, '-s': 31, 'se': 383, 'ep': 124, 'pi': 339, 'it': 207, '-t': 32, 'ta': 401, 'ak': 47, '-w': 34, 'at': 56, 'ti': 407, \"a'\": 35, \"'d\": 6, 'du': 102, \"'h\": 7, 'ad': 40, 'ab': 38, 'ba': 62, 'di': 94, 'ia': 189, 'ik': 199, 'ai': 45, 'im': 201, 'na': 288, 'gn': 160, 'ar': 54, 'ra': 354, 'to': 412, 'oa': 313, 'au': 57, 'bd': 64, 'do': 99, 'me': 273, 'mi': 275, 'in': 202, 'al': 48, 'er': 125, 'et': 127, 'bi': 67, 'id': 192, 'il': 200, 'tu': 417, 'ur': 437, 'ri': 362, 'ie': 193, 'bj': 68, 'ja': 213, 'bl': 70, 'ut': 439, 'bn': 72, 'no': 302, 'or': 329, 'rm': 366, 'li': 253, 'nu': 306, 'us': 438, 'bo': 73, 'oi': 321, 'ol': 324, 'on': 326, 'ne': 292, 'em': 121, 'br': 74, 're': 358, 'ea': 109, 'ks': 239, 'ek': 119, 'ev': 129, 'vi': 447, 'ko': 236, 'os': 330, 'bs': 75, 'ns': 304, 'nt': 305, 'te': 403, 'ei': 117, 'sm': 390, 'so': 392, 'lu': 264, 'rp': 369, 'ps': 346, 'st': 396, 'tr': 414, 'su': 397, 'rd': 357, 'bt': 76, 'bu': 77, 'ua': 421, 'uh': 428, 'uk': 431, 'ul': 432, 'lh': 252, 'ay': 60, 'ac': 39, 'ca': 80, 'am': 49, 'ce': 81, 'ci': 83, 'cu': 88, 'um': 433, 'un': 434, 'gk': 157, 'aa': 37, 'ag': 43, 'gi': 155, 'io': 203, 'iu': 208, 'pt': 347, 'if': 194, 'pu': 348, 'eg': 115, 'eh': 116, 'uz': 443, 'ze': 468, 'ib': 190, 'sa': 379, 'ic': 191, 'ig': 195, 'gu': 166, 'ry': 376, 'kk': 232, 'kn': 235, 'od': 316, 'dr': 100, 'kt': 240, 'rg': 360, 'ip': 204, 'gh': 154, 'po': 344, 'ir': 205, 'aj': 46, 'ok': 323, 'oh': 320, 'iw': 210, 'gs': 164, 'rn': 367, 'wi': 452, 'dj': 95, 'je': 214, 'iv': 209, 'va': 444, 'dm': 97, 'ni': 296, 'op': 328, 'es': 126, 'dv': 103, 've': 445, 'rb': 355, 'rt': 372, 'vo': 448, 'ae': 41, 'ro': 368, 'ob': 314, 'og': 319, 'gr': 163, 'lo': 259, 'mo': 280, 'ov': 332, 'sk': 388, 'af': 42, 'fd': 135, 'ru': 373, 'fe': 136, 'fi': 137, 'fr': 142, 'fw': 146, 'aw': 59, 'ge': 152, 'ih': 196, 'gl': 158, 'hk': 176, 'hl': 177, 'lb': 247, 'lk': 255, 'ki': 230, 'ub': 422, 'hm': 178, 'ho': 180, 'hw': 186, 'lf': 250, 'nm': 300, 'mu': 285, 'ji': 215, 'jn': 217, 'jo': 218, 'oj': 322, 'ju': 221, 'ud': 424, 'uj': 430, 'k-': 223, 'ln': 258, 'kh': 229, 'hb': 169, 'hi': 174, 'ht': 184, 'kl': 233, 'km': 234, 'kr': 238, 'ed': 112, 'ui': 429, 'lt': 263, 'up': 436, 'nk': 298, 'kw': 242, 'l-': 245, '-q': 29, 'qu': 351, \"r'\": 352, \"'a\": 4, 'mn': 279, 'go': 161, 'fa': 133, 'fu': 145, 'lg': 251, 'tm': 410, 'md': 272, 'll': 256, 'fb': 134, 'lj': 254, 'sy': 399, 'ke': 228, 'lm': 257, 'rh': 361, 'hu': 185, 'of': 318, 'fo': 141, 'rf': 359, 'lp': 260, 'uv': 440, 'lv': 265, 'eo': 123, 'lw': 266, 'nn': 301, 'hn': 179, 'tl': 409, 'mb': 270, 'by': 79, 'eb': 110, 'mf': 274, 'mk': 276, 'pl': 342, 'pr': 345, 'mr': 282, 'ms': 283, 'mt': 284, 'rk': 364, 'nb': 289, 'nc': 290, 'nf': 293, 'ot': 331, 'tn': 411, 'gp': 162, 'nj': 297, 'jl': 216, 'rr': 370, 'rs': 371, 'i-': 188, 'ug': 427, 'yi': 460, 'th': 406, 'he': 171, 'pk': 341, 'aq': 53, 'qi': 350, 'rc': 356, 'rj': 363, 'ef': 114, 'rw': 375, 'rz': 377, 'za': 466, 'sn': 391, 'sb': 380, 'sf': 384, 'sh': 385, 'sl': 389, 'sp': 393, 'sr': 394, 'av': 58, 'vg': 446, 'we': 451, 'wu': 455, 'ye': 457, 'yo': 462, 'yu': 465, 'az': 61, 'ij': 198, 'zh': 469, 'zi': 470, 'hr': 182, 'kd': 227, 'kp': 237, 'ls': 262, 'gb': 149, 'nz': 310, 'rl': 365, 'zo': 472, 'ec': 111, \"o'\": 311, \"'o\": 11, 'uw': 441, 'r-': 353, 'a-': 36, '-g': 20, 'o-': 312, 'gt': 165, 's-': 378, 'oy': 334, 'uc': 423, 'kc': 226, 'ft': 144, 'gd': 151, 'ue': 425, 'dw': 104, 'fl': 139, 'ew': 130, 'wo': 453, 'ej': 118, 'iz': 212, 'jr': 219, 'tk': 408, 'hf': 172, 'ml': 277, 'oc': 315, 'uy': 442, 'ey': 131, 'tb': 402, 'nv': 307, 'fk': 138, 'fs': 143, 'hy': 187, 'g-': 147, '-j': 23, 'rv': 374, 'ez': 132, 'bh': 66, \"d'\": 89, 'ly': 267, 'ss': 395, 'gm': 159, 'oz': 335, 'nw': 308, 'uf': 426, 'nl': 299, 'ld': 248, 'h-': 167, 'mc': 271, 'gc': 150, 'ao': 51, 'pc': 337, 'pj': 340, \"e'\": 107, 'ee': 113, 'oo': 327, 'ch': 82, 'cm': 85, 'ow': 333, 'cr': 87, \"'i\": 8, 'fn': 140, 'hs': 183, 'ii': 197, 'ts': 415, 'dk': 96, 'bk': 69, 'gj': 156, 'sj': 387, 'iy': 211, 'tp': 413, 'mz': 286, 'tt': 416, '-e': 19, 'pn': 343, 'u-': 420, '-h': 21, 'jt': 220, 'nh': 295, 'np': 303, 'yt': 464, '-r': 30, 'yk': 461, 'ky': 243, '-p': 28, 'eu': 128, 'zu': 473, 'oe': 317, 'uo': 435, 'ds': 101, \"'n\": 10, 'yg': 458, 'yh': 459, 'yr': 463, 'zb': 467, 'zm': 471, 't-': 400, 'kj': 231, '-i': 22, 'py': 349, 'hg': 173, 'mm': 278, \"u'\": 419, \"'j\": 9, 'dz': 106, 'hd': 170, 'bm': 71, 'sc': 381, 'tf': 404, 'e-': 108, 'hj': 175, 'n-': 287, 'lr': 261, 'dh': 93, 'ck': 84, 'tg': 405, \"'b\": 5, 'tw': 418, 'bb': 63, 'bv': 78, 'hp': 181, 'wt': 454, \"'r\": 12, \"'u\": 13, 'db': 91, 'jw': 222, 'kb': 225, 'kz': 244, '-u': 33, \"m'\": 268, 'vu': 449, 'dy': 105, 'dn': 98, 'sd': 382}\n",
      "470 {'--': 14, '-n': 27, 'ny': 309, 'ya': 456, '-a': 15, 'an': 50, 'nd': 291, 'da': 90, '-b': 16, 'be': 65, 'el': 120, 'la': 246, 'as': 55, '-c': 17, 'co': 86, 'om': 325, 'mp': 281, 'pe': 338, 'en': 122, 'ng': 294, '-d': 18, 'de': 92, 'ap': 52, 'pa': 336, '-k': 24, 'ka': 224, 'ku': 241, '-l': 25, 'le': 249, 'gg': 153, 'ga': 148, '-m': 26, 'ma': 269, 'ah': 44, 'ha': 168, 'si': 386, 'is': 206, 'sw': 398, 'wa': 450, '-s': 31, 'se': 383, 'ep': 124, 'pi': 339, 'it': 207, '-t': 32, 'ta': 401, 'ak': 47, '-w': 34, 'at': 56, 'ti': 407, \"a'\": 35, \"'d\": 6, 'du': 102, \"'h\": 7, 'ad': 40, 'ab': 38, 'ba': 62, 'di': 94, 'ia': 189, 'ik': 199, 'ai': 45, 'im': 201, 'na': 288, 'gn': 160, 'ar': 54, 'ra': 354, 'to': 412, 'oa': 313, 'au': 57, 'bd': 64, 'do': 99, 'me': 273, 'mi': 275, 'in': 202, 'al': 48, 'er': 125, 'et': 127, 'bi': 67, 'id': 192, 'il': 200, 'tu': 417, 'ur': 437, 'ri': 362, 'ie': 193, 'bj': 68, 'ja': 213, 'bl': 70, 'ut': 439, 'bn': 72, 'no': 302, 'or': 329, 'rm': 366, 'li': 253, 'nu': 306, 'us': 438, 'bo': 73, 'oi': 321, 'ol': 324, 'on': 326, 'ne': 292, 'em': 121, 'br': 74, 're': 358, 'ea': 109, 'ks': 239, 'ek': 119, 'ev': 129, 'vi': 447, 'ko': 236, 'os': 330, 'bs': 75, 'ns': 304, 'nt': 305, 'te': 403, 'ei': 117, 'sm': 390, 'so': 392, 'lu': 264, 'rp': 369, 'ps': 346, 'st': 396, 'tr': 414, 'su': 397, 'rd': 357, 'bt': 76, 'bu': 77, 'ua': 421, 'uh': 428, 'uk': 431, 'ul': 432, 'lh': 252, 'ay': 60, 'ac': 39, 'ca': 80, 'am': 49, 'ce': 81, 'ci': 83, 'cu': 88, 'um': 433, 'un': 434, 'gk': 157, 'aa': 37, 'ag': 43, 'gi': 155, 'io': 203, 'iu': 208, 'pt': 347, 'if': 194, 'pu': 348, 'eg': 115, 'eh': 116, 'uz': 443, 'ze': 468, 'ib': 190, 'sa': 379, 'ic': 191, 'ig': 195, 'gu': 166, 'ry': 376, 'kk': 232, 'kn': 235, 'od': 316, 'dr': 100, 'kt': 240, 'rg': 360, 'ip': 204, 'gh': 154, 'po': 344, 'ir': 205, 'aj': 46, 'ok': 323, 'oh': 320, 'iw': 210, 'gs': 164, 'rn': 367, 'wi': 452, 'dj': 95, 'je': 214, 'iv': 209, 'va': 444, 'dm': 97, 'ni': 296, 'op': 328, 'es': 126, 'dv': 103, 've': 445, 'rb': 355, 'rt': 372, 'vo': 448, 'ae': 41, 'ro': 368, 'ob': 314, 'og': 319, 'gr': 163, 'lo': 259, 'mo': 280, 'ov': 332, 'sk': 388, 'af': 42, 'fd': 135, 'ru': 373, 'fe': 136, 'fi': 137, 'fr': 142, 'fw': 146, 'aw': 59, 'ge': 152, 'ih': 196, 'gl': 158, 'hk': 176, 'hl': 177, 'lb': 247, 'lk': 255, 'ki': 230, 'ub': 422, 'hm': 178, 'ho': 180, 'hw': 186, 'lf': 250, 'nm': 300, 'mu': 285, 'ji': 215, 'jn': 217, 'jo': 218, 'oj': 322, 'ju': 221, 'ud': 424, 'uj': 430, 'k-': 223, 'ln': 258, 'kh': 229, 'hb': 169, 'hi': 174, 'ht': 184, 'kl': 233, 'km': 234, 'kr': 238, 'ed': 112, 'ui': 429, 'lt': 263, 'up': 436, 'nk': 298, 'kw': 242, 'l-': 245, '-q': 29, 'qu': 351, \"r'\": 352, \"'a\": 4, 'mn': 279, 'go': 161, 'fa': 133, 'fu': 145, 'lg': 251, 'tm': 410, 'md': 272, 'll': 256, 'fb': 134, 'lj': 254, 'sy': 399, 'ke': 228, 'lm': 257, 'rh': 361, 'hu': 185, 'of': 318, 'fo': 141, 'rf': 359, 'lp': 260, 'uv': 440, 'lv': 265, 'eo': 123, 'lw': 266, 'nn': 301, 'hn': 179, 'tl': 409, 'mb': 270, 'by': 79, 'eb': 110, 'mf': 274, 'mk': 276, 'pl': 342, 'pr': 345, 'mr': 282, 'ms': 283, 'mt': 284, 'rk': 364, 'nb': 289, 'nc': 290, 'nf': 293, 'ot': 331, 'tn': 411, 'gp': 162, 'nj': 297, 'jl': 216, 'rr': 370, 'rs': 371, 'i-': 188, 'ug': 427, 'yi': 460, 'th': 406, 'he': 171, 'pk': 341, 'aq': 53, 'qi': 350, 'rc': 356, 'rj': 363, 'ef': 114, 'rw': 375, 'rz': 377, 'za': 466, 'sn': 391, 'sb': 380, 'sf': 384, 'sh': 385, 'sl': 389, 'sp': 393, 'sr': 394, 'av': 58, 'vg': 446, 'we': 451, 'wu': 455, 'ye': 457, 'yo': 462, 'yu': 465, 'az': 61, 'ij': 198, 'zh': 469, 'zi': 470, 'hr': 182, 'kd': 227, 'kp': 237, 'ls': 262, 'gb': 149, 'nz': 310, 'rl': 365, 'zo': 472, 'ec': 111, \"o'\": 311, \"'o\": 11, 'uw': 441, 'r-': 353, 'a-': 36, '-g': 20, 'o-': 312, 'gt': 165, 's-': 378, 'oy': 334, 'uc': 423, 'kc': 226, 'ft': 144, 'gd': 151, 'ue': 425, 'dw': 104, 'fl': 139, 'ew': 130, 'wo': 453, 'ej': 118, 'iz': 212, 'jr': 219, 'tk': 408, 'hf': 172, 'ml': 277, 'oc': 315, 'uy': 442, 'ey': 131, 'tb': 402, 'nv': 307, 'fk': 138, 'fs': 143, 'hy': 187, 'g-': 147, '-j': 23, 'rv': 374, 'ez': 132, 'bh': 66, \"d'\": 89, 'ly': 267, 'ss': 395, 'gm': 159, 'oz': 335, 'nw': 308, 'uf': 426, 'nl': 299, 'ld': 248, 'h-': 167, 'mc': 271, 'gc': 150, 'ao': 51, 'pc': 337, 'pj': 340, \"e'\": 107, 'ee': 113, 'oo': 327, 'ch': 82, 'cm': 85, 'ow': 333, 'cr': 87, \"'i\": 8, 'fn': 140, 'hs': 183, 'ii': 197, 'ts': 415, 'dk': 96, 'bk': 69, 'gj': 156, 'sj': 387, 'iy': 211, 'tp': 413, 'mz': 286, 'tt': 416, '-e': 19, 'pn': 343, 'u-': 420, '-h': 21, 'jt': 220, 'nh': 295, 'np': 303, 'yt': 464, '-r': 30, 'yk': 461, 'ky': 243, '-p': 28, 'eu': 128, 'zu': 473, 'oe': 317, 'uo': 435, 'ds': 101, \"'n\": 10, 'yg': 458, 'yh': 459, 'yr': 463, 'zb': 467, 'zm': 471, 't-': 400, 'kj': 231, '-i': 22, 'py': 349, 'hg': 173, 'mm': 278, \"u'\": 419, \"'j\": 9, 'dz': 106, 'hd': 170, 'bm': 71, 'sc': 381, 'tf': 404, 'e-': 108, 'hj': 175, 'n-': 287, 'lr': 261, 'dh': 93, 'ck': 84, 'tg': 405, \"'b\": 5, 'tw': 418, 'bb': 63, 'bv': 78, 'hp': 181, 'wt': 454, \"'r\": 12, \"'u\": 13, 'db': 91, 'jw': 222, 'kb': 225, 'kz': 244, '-u': 33, \"m'\": 268, 'vu': 449, 'dy': 105, 'dn': 98, 'sd': 382}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n",
      "33 {'NY': 23, 'AA': 3, 'N': 21, 'D': 10, 'B': 8, 'AX': 6, 'L': 19, 'S': 28, 'CH': 9, 'AO': 4, 'M': 20, 'P': 25, 'EH': 11, 'NG': 22, 'K': 18, 'UW': 31, 'G': 14, 'HH': 15, 'IY': 16, 'W': 33, 'T': 30, 'Q': 26, 'AY': 7, 'R': 27, 'AW': 5, 'JH': 17, 'OY': 24, 'V': 32, 'Y': 34, 'F': 13, 'Z': 35, 'SH': 29, 'EY': 12}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).to(DEVICE)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size).to(DEVICE)\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size).to(DEVICE)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size)).to(DEVICE)\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim).to(DEVICE)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False).to(DEVICE)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size).to(DEVICE)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size).to(DEVICE)\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "9a095505-f179-48d0-c305-f4e69125f170"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]]).to(DEVICE)\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size).to(DEVICE) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n",
    "\n",
    "del encoder_test\n",
    "del decoder_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1739957722884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    decoder_input = decoder_input.to(DEVICE)\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Backpropagate loss\n",
    "  loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  input_batch = input_batch.to(DEVICE)\n",
    "  target_batch = target_batch.to(DEVICE)\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]).to(DEVICE)\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "        decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "      )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1739957723364,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "68700f7a-b173-4800-d808-8d922cd64e2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 32\n",
      "n_layers: 1\n",
      "Encoder has a total number of 39744 parameters\n",
      "Decoder has a total number of 17124 parameters\n",
      "Total number of all parameters is 56868\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "encoder = encoder.to(DEVICE)\n",
    "decoder = decoder.to(DEVICE)\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2952362,
     "status": "ok",
     "timestamp": 1739960675722,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "e9e1eaf7-3a18-4620-b007-cbad6be38496"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 1 finished in 0m 8.34s (- 13m 45.92s) (1 1.0%). train avg loss: 2.2216, val avg loss: 2.2519\n",
      "Training for epoch 2 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 2 finished in 0m 15.29s (- 12m 29.27s) (2 2.0%). train avg loss: 1.692, val avg loss: 1.9691\n",
      "Training for epoch 3 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 3 finished in 0m 23.24s (- 12m 31.34s) (3 3.0%). train avg loss: 1.4117, val avg loss: 1.7543\n",
      "Training for epoch 4 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 4 finished in 0m 30.54s (- 12m 12.85s) (4 4.0%). train avg loss: 1.2041, val avg loss: 1.6542\n",
      "Training for epoch 5 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 5 finished in 0m 37.66s (- 11m 55.45s) (5 5.0%). train avg loss: 1.0078, val avg loss: 1.4283\n",
      "Training for epoch 6 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 6 finished in 0m 45.3s (- 11m 49.7s) (6 6.0%). train avg loss: 0.8327, val avg loss: 1.3262\n",
      "Training for epoch 7 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 7 finished in 0m 52.5s (- 11m 37.49s) (7 7.0%). train avg loss: 0.7078, val avg loss: 1.2456\n",
      "Training for epoch 8 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 8 finished in 0m 59.93s (- 11m 29.18s) (8 8.0%). train avg loss: 0.5981, val avg loss: 1.095\n",
      "Training for epoch 9 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 9 finished in 1m 7.65s (- 11m 23.97s) (9 9.0%). train avg loss: 0.5251, val avg loss: 1.042\n",
      "Training for epoch 10 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 10 finished in 1m 14.83s (- 11m 13.45s) (10 10.0%). train avg loss: 0.4639, val avg loss: 0.9026\n",
      "Training for epoch 11 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 11 finished in 1m 22.07s (- 11m 4.02s) (11 11.0%). train avg loss: 0.4216, val avg loss: 0.8381\n",
      "Training for epoch 12 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 12 finished in 1m 29.18s (- 10m 53.99s) (12 12.0%). train avg loss: 0.3755, val avg loss: 0.834\n",
      "Training for epoch 13 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 13 finished in 1m 36.31s (- 10m 44.52s) (13 13.0%). train avg loss: 0.3316, val avg loss: 0.7533\n",
      "Training for epoch 14 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 14 finished in 1m 44.01s (- 10m 38.93s) (14 14.0%). train avg loss: 0.302, val avg loss: 0.7616\n",
      "Training for epoch 15 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 15 finished in 1m 51.25s (- 10m 30.42s) (15 15.0%). train avg loss: 0.2925, val avg loss: 0.7426\n",
      "Training for epoch 16 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 16 finished in 1m 58.07s (- 10m 19.89s) (16 16.0%). train avg loss: 0.2387, val avg loss: 0.7281\n",
      "Training for epoch 17 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 17 finished in 2m 5.3s (- 10m 11.76s) (17 17.0%). train avg loss: 0.2194, val avg loss: 0.6415\n",
      "Training for epoch 18 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 18 finished in 2m 12.77s (- 10m 4.85s) (18 18.0%). train avg loss: 0.216, val avg loss: 0.6478\n",
      "Training for epoch 19 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 19 finished in 2m 19.95s (- 9m 56.61s) (19 19.0%). train avg loss: 0.2071, val avg loss: 0.7168\n",
      "Training for epoch 20 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 20 finished in 2m 27.37s (- 9m 49.48s) (20 20.0%). train avg loss: 0.1812, val avg loss: 0.6682\n",
      "Training for epoch 21 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 21 finished in 2m 34.74s (- 9m 42.1s) (21 21.0%). train avg loss: 0.1766, val avg loss: 0.5793\n",
      "Training for epoch 22 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 22 finished in 2m 42.14s (- 9m 34.85s) (22 22.0%). train avg loss: 0.1836, val avg loss: 0.5398\n",
      "Training for epoch 23 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 23 finished in 2m 49.12s (- 9m 26.17s) (23 23.0%). train avg loss: 0.1539, val avg loss: 0.4363\n",
      "Training for epoch 24 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 24 finished in 2m 56.11s (- 9m 17.69s) (24 24.0%). train avg loss: 0.1552, val avg loss: 0.5758\n",
      "Training for epoch 25 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 25 finished in 3m 3.8s (- 9m 11.39s) (25 25.0%). train avg loss: 0.1407, val avg loss: 0.6814\n",
      "Training for epoch 26 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 26 finished in 3m 11.38s (- 9m 4.69s) (26 26.0%). train avg loss: 0.1288, val avg loss: 0.4905\n",
      "Training for epoch 27 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 27 finished in 3m 18.83s (- 8m 57.58s) (27 27.0%). train avg loss: 0.109, val avg loss: 0.3845\n",
      "Training for epoch 28 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 28 finished in 3m 25.98s (- 8m 49.66s) (28 28.0%). train avg loss: 0.1175, val avg loss: 0.4869\n",
      "Training for epoch 29 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 29 finished in 3m 33.22s (- 8m 42.02s) (29 29.0%). train avg loss: 0.1105, val avg loss: 0.5002\n",
      "Training for epoch 30 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 30 finished in 3m 40.8s (- 8m 35.21s) (30 30.0%). train avg loss: 0.1178, val avg loss: 0.3755\n",
      "Training for epoch 31 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 31 finished in 3m 48.34s (- 8m 28.24s) (31 31.0%). train avg loss: 0.0971, val avg loss: 0.4709\n",
      "Training for epoch 32 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 32 finished in 3m 55.77s (- 8m 21.02s) (32 32.0%). train avg loss: 0.1179, val avg loss: 0.4822\n",
      "Training for epoch 33 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 33 finished in 4m 3.04s (- 8m 13.45s) (33 33.0%). train avg loss: 0.0993, val avg loss: 0.3913\n",
      "Training for epoch 34 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 34 finished in 4m 9.87s (- 8m 5.04s) (34 34.0%). train avg loss: 0.0691, val avg loss: 0.3159\n",
      "Training for epoch 35 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 35 finished in 4m 16.9s (- 7m 57.1s) (35 35.0%). train avg loss: 0.076, val avg loss: 0.33\n",
      "Training for epoch 36 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 36 finished in 4m 24.84s (- 7m 50.82s) (36 36.0%). train avg loss: 0.1013, val avg loss: 0.4058\n",
      "Training for epoch 37 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 37 finished in 4m 32.38s (- 7m 43.78s) (37 37.0%). train avg loss: 0.1007, val avg loss: 0.3822\n",
      "Training for epoch 38 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 38 finished in 4m 39.55s (- 7m 36.11s) (38 38.0%). train avg loss: 0.0898, val avg loss: 0.2978\n",
      "Training for epoch 39 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 39 finished in 4m 46.63s (- 7m 28.32s) (39 39.0%). train avg loss: 0.0869, val avg loss: 0.4055\n",
      "Training for epoch 40 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 40 finished in 4m 53.7s (- 7m 20.55s) (40 40.0%). train avg loss: 0.0712, val avg loss: 0.342\n",
      "Training for epoch 41 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 41 finished in 5m 1.24s (- 7m 13.49s) (41 41.0%). train avg loss: 0.0749, val avg loss: 0.3743\n",
      "Training for epoch 42 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 42 finished in 5m 9.01s (- 7m 6.73s) (42 42.0%). train avg loss: 0.0664, val avg loss: 0.2994\n",
      "Training for epoch 43 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 43 finished in 5m 16.23s (- 6m 59.19s) (43 43.0%). train avg loss: 0.0535, val avg loss: 0.2682\n",
      "Training for epoch 44 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 44 finished in 5m 23.37s (- 6m 51.57s) (44 44.0%). train avg loss: 0.0545, val avg loss: 0.2838\n",
      "Training for epoch 45 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 45 finished in 5m 30.44s (- 6m 43.87s) (45 45.0%). train avg loss: 0.0661, val avg loss: 0.3276\n",
      "Training for epoch 46 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 46 finished in 5m 37.69s (- 6m 36.42s) (46 46.0%). train avg loss: 0.074, val avg loss: 0.3363\n",
      "Training for epoch 47 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 47 finished in 5m 45.54s (- 6m 29.65s) (47 47.0%). train avg loss: 0.0788, val avg loss: 0.3537\n",
      "Training for epoch 48 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 48 finished in 5m 52.82s (- 6m 22.22s) (48 48.0%). train avg loss: 0.059, val avg loss: 0.2855\n",
      "Training for epoch 49 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 49 finished in 5m 59.59s (- 6m 14.27s) (49 49.0%). train avg loss: 0.0467, val avg loss: 0.2226\n",
      "Training for epoch 50 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 50 finished in 6m 6.68s (- 6m 6.68s) (50 50.0%). train avg loss: 0.0484, val avg loss: 0.2582\n",
      "Training for epoch 51 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 51 finished in 6m 14.31s (- 5m 59.63s) (51 51.0%). train avg loss: 0.0457, val avg loss: 0.2644\n",
      "Training for epoch 52 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 52 finished in 6m 22.08s (- 5m 52.69s) (52 52.0%). train avg loss: 0.054, val avg loss: 0.2823\n",
      "Training for epoch 53 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 53 finished in 6m 29.4s (- 5m 45.31s) (53 53.0%). train avg loss: 0.0643, val avg loss: 0.2888\n",
      "Training for epoch 54 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 54 finished in 6m 36.77s (- 5m 37.99s) (54 54.0%). train avg loss: 0.0498, val avg loss: 0.2581\n",
      "Training for epoch 55 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 55 finished in 6m 44.08s (- 5m 30.61s) (55 55.0%). train avg loss: 0.0437, val avg loss: 0.2718\n",
      "Training for epoch 56 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 56 finished in 6m 51.0s (- 5m 22.93s) (56 56.0%). train avg loss: 0.0399, val avg loss: 0.262\n",
      "Training for epoch 57 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 57 finished in 6m 58.06s (- 5m 15.38s) (57 57.0%). train avg loss: 0.098, val avg loss: 0.4127\n",
      "Training for epoch 58 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 58 finished in 7m 5.72s (- 5m 8.28s) (58 58.0%). train avg loss: 0.0714, val avg loss: 0.3191\n",
      "Training for epoch 59 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 59 finished in 7m 13.01s (- 5m 0.9s) (59 59.0%). train avg loss: 0.0529, val avg loss: 0.2486\n",
      "Training for epoch 60 has started (lr=0.001). Found 357 batch(es).\n",
      "Epoch 60 finished in 7m 19.92s (- 4m 53.28s) (60 60.0%). train avg loss: 0.0565, val avg loss: 0.2542\n",
      "Training for epoch 61 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 61 finished in 7m 26.81s (- 4m 45.67s) (61 61.0%). train avg loss: 0.043, val avg loss: 0.2182\n",
      "Training for epoch 62 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 62 finished in 7m 33.98s (- 4m 38.24s) (62 62.0%). train avg loss: 0.037, val avg loss: 0.2336\n",
      "Training for epoch 63 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 63 finished in 7m 41.53s (- 4m 31.06s) (63 63.0%). train avg loss: 0.0371, val avg loss: 0.2289\n",
      "Training for epoch 64 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 64 finished in 7m 49.19s (- 4m 23.92s) (64 64.0%). train avg loss: 0.0392, val avg loss: 0.2567\n",
      "Training for epoch 65 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 65 finished in 7m 56.68s (- 4m 16.67s) (65 65.0%). train avg loss: 0.0452, val avg loss: 0.2088\n",
      "Training for epoch 66 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 66 finished in 8m 3.92s (- 4m 9.29s) (66 66.0%). train avg loss: 0.036, val avg loss: 0.2035\n",
      "Training for epoch 67 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 67 finished in 8m 10.92s (- 4m 1.8s) (67 67.0%). train avg loss: 0.0332, val avg loss: 0.233\n",
      "Training for epoch 68 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 68 finished in 8m 18.22s (- 3m 54.46s) (68 68.0%). train avg loss: 0.0346, val avg loss: 0.2068\n",
      "Training for epoch 69 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 69 finished in 8m 25.84s (- 3m 47.26s) (69 69.0%). train avg loss: 0.0311, val avg loss: 0.2282\n",
      "Training for epoch 70 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 70 finished in 8m 32.99s (- 3m 39.85s) (70 70.0%). train avg loss: 0.0336, val avg loss: 0.1968\n",
      "Training for epoch 71 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 71 finished in 8m 39.93s (- 3m 32.36s) (71 71.0%). train avg loss: 0.0332, val avg loss: 0.2573\n",
      "Training for epoch 72 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 72 finished in 8m 46.9s (- 3m 24.91s) (72 72.0%). train avg loss: 0.0323, val avg loss: 0.2286\n",
      "Training for epoch 73 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 73 finished in 8m 54.24s (- 3m 17.59s) (73 73.0%). train avg loss: 0.0393, val avg loss: 0.2742\n",
      "Training for epoch 74 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 74 finished in 9m 1.73s (- 3m 10.34s) (74 74.0%). train avg loss: 0.0316, val avg loss: 0.2602\n",
      "Training for epoch 75 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 75 finished in 9m 8.99s (- 3m 3.0s) (75 75.0%). train avg loss: 0.0312, val avg loss: 0.2295\n",
      "Training for epoch 76 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 76 finished in 9m 16.15s (- 2m 55.63s) (76 76.0%). train avg loss: 0.0305, val avg loss: 0.246\n",
      "Training for epoch 77 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 77 finished in 9m 23.34s (- 2m 48.27s) (77 77.0%). train avg loss: 0.0378, val avg loss: 0.2371\n",
      "Training for epoch 78 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 78 finished in 9m 30.41s (- 2m 40.88s) (78 78.0%). train avg loss: 0.034, val avg loss: 0.2716\n",
      "Training for epoch 79 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 79 finished in 9m 37.99s (- 2m 33.64s) (79 79.0%). train avg loss: 0.0417, val avg loss: 0.263\n",
      "Training for epoch 80 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 80 finished in 9m 45.51s (- 2m 26.38s) (80 80.0%). train avg loss: 0.0395, val avg loss: 0.2408\n",
      "Training for epoch 81 has started (lr=0.0005). Found 357 batch(es).\n",
      "Epoch 81 finished in 9m 52.46s (- 2m 18.97s) (81 81.0%). train avg loss: 0.0335, val avg loss: 0.2866\n",
      "Training for epoch 82 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 82 finished in 9m 59.46s (- 2m 11.59s) (82 82.0%). train avg loss: 0.0363, val avg loss: 0.2399\n",
      "Training for epoch 83 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 83 finished in 10m 6.87s (- 2m 4.3s) (83 83.0%). train avg loss: 0.029, val avg loss: 0.2626\n",
      "Training for epoch 84 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 84 finished in 10m 13.82s (- 1m 56.92s) (84 84.0%). train avg loss: 0.0317, val avg loss: 0.2608\n",
      "Training for epoch 85 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 85 finished in 10m 21.1s (- 1m 49.61s) (85 85.0%). train avg loss: 0.029, val avg loss: 0.2388\n",
      "Training for epoch 86 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 86 finished in 10m 28.38s (- 1m 42.29s) (86 86.0%). train avg loss: 0.028, val avg loss: 0.2444\n",
      "Training for epoch 87 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 87 finished in 10m 36.17s (- 1m 35.06s) (87 87.0%). train avg loss: 0.029, val avg loss: 0.237\n",
      "Training for epoch 88 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 88 finished in 10m 44.2s (- 1m 27.85s) (88 88.0%). train avg loss: 0.0263, val avg loss: 0.2401\n",
      "Training for epoch 89 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 89 finished in 10m 51.37s (- 1m 20.51s) (89 89.0%). train avg loss: 0.0278, val avg loss: 0.2502\n",
      "Training for epoch 90 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 90 finished in 10m 58.58s (- 1m 13.18s) (90 90.0%). train avg loss: 0.0255, val avg loss: 0.2535\n",
      "Training for epoch 91 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 91 finished in 11m 6.08s (- 1m 5.88s) (91 91.0%). train avg loss: 0.0277, val avg loss: 0.2548\n",
      "Training for epoch 92 has started (lr=0.00025). Found 357 batch(es).\n",
      "Epoch 92 finished in 11m 14.6s (- 0m 58.66s) (92 92.0%). train avg loss: 0.027, val avg loss: 0.2688\n",
      "Training for epoch 93 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 93 finished in 11m 22.95s (- 0m 51.4s) (93 93.0%). train avg loss: 0.0245, val avg loss: 0.2649\n",
      "Training for epoch 94 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 94 finished in 11m 30.14s (- 0m 44.05s) (94 94.0%). train avg loss: 0.025, val avg loss: 0.2588\n",
      "Training for epoch 95 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 95 finished in 11m 37.76s (- 0m 36.72s) (95 95.0%). train avg loss: 0.0236, val avg loss: 0.2655\n",
      "Training for epoch 96 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 96 finished in 11m 44.62s (- 0m 29.36s) (96 96.0%). train avg loss: 0.0244, val avg loss: 0.2602\n",
      "Training for epoch 97 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 97 finished in 11m 52.6s (- 0m 22.04s) (97 97.0%). train avg loss: 0.0241, val avg loss: 0.2887\n",
      "Training for epoch 98 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 98 finished in 12m 1.33s (- 0m 14.72s) (98 98.0%). train avg loss: 0.0238, val avg loss: 0.2681\n",
      "Training for epoch 99 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 99 finished in 12m 8.37s (- 0m 7.36s) (99 99.0%). train avg loss: 0.0246, val avg loss: 0.2689\n",
      "Training for epoch 100 has started (lr=0.000125). Found 357 batch(es).\n",
      "Epoch 100 finished in 12m 15.57s (- 0m 0.0s) (100 100.0%). train avg loss: 0.0243, val avg loss: 0.3188\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get loss\n",
    "    unweighted_train_loss = train_batch(grps, phns, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "-498emHUaNzb",
    "outputId": "bab4a42d-c2d8-4a89-c7a9-eb0ace0bc12c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5.25, min_value+.125, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1739960676044,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]).to(DEVICE) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size).to(DEVICE)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]]).to(DEVICE)\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67072,
     "status": "ok",
     "timestamp": 1739960743114,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "MlSPdqo3QDyr",
    "outputId": "364c407d-3bb7-4fd9-ac12-19a8480c9076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 5.989973304275948%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, _, _, arpabet_phoneme_sequence = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1739961996036,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "HSHGOjSmc3Vi",
    "outputId": "7c2a7917-9217-4397-8be2-0c96496d6b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> tutur\n",
      "= ['T', 'UW', 'T', 'UW', 'R']\n",
      "< T UW T UW R ['T', 'UW', 'T', 'UW', 'R']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5080224040>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLJmB0V/sNcUHuHtZcdQwt",
   "collapsed_sections": [
    "8mDO6QlJZpUZ",
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
