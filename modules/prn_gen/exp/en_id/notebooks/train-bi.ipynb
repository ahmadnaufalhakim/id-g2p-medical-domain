{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn_gen/exp/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"256\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"128\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6141991030673498\n",
      "ID_WEIGHT: 2.6891590501596188\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[375, 170, 404, 233, 584, 400, 111, 281, 422, 671, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7fc0c84e3790> ([6, 103, 71, 589, 520, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 103, 71, 589, 520, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 103, 71, 589, 520, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 722 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-f', 31: '-g', 32: '-h', 33: '-i', 34: '-j', 35: '-k', 36: '-l', 37: '-m', 38: '-n', 39: '-o', 40: '-p', 41: '-q', 42: '-r', 43: '-s', 44: '-t', 45: '-u', 46: '-v', 47: '-w', 48: '-y', 49: \"a'\", 50: 'a-', 51: 'aa', 52: 'ab', 53: 'ac', 54: 'ad', 55: 'ae', 56: 'af', 57: 'ag', 58: 'ah', 59: 'ai', 60: 'aj', 61: 'ak', 62: 'al', 63: 'am', 64: 'an', 65: 'ao', 66: 'ap', 67: 'aq', 68: 'ar', 69: 'as', 70: 'at', 71: 'au', 72: 'av', 73: 'aw', 74: 'ax', 75: 'ay', 76: 'az', 77: \"b'\", 78: 'ba', 79: 'bb', 80: 'bc', 81: 'bd', 82: 'be', 83: 'bf', 84: 'bg', 85: 'bh', 86: 'bi', 87: 'bj', 88: 'bk', 89: 'bl', 90: 'bm', 91: 'bn', 92: 'bo', 93: 'bp', 94: 'br', 95: 'bs', 96: 'bt', 97: 'bu', 98: 'bv', 99: 'bw', 100: 'by', 101: 'bz', 102: \"c'\", 103: 'ca', 104: 'cb', 105: 'cc', 106: 'cd', 107: 'ce', 108: 'cf', 109: 'cg', 110: 'ch', 111: 'ci', 112: 'cj', 113: 'ck', 114: 'cl', 115: 'cm', 116: 'cn', 117: 'co', 118: 'cp', 119: 'cq', 120: 'cr', 121: 'cs', 122: 'ct', 123: 'cu', 124: 'cv', 125: 'cw', 126: 'cy', 127: 'cz', 128: \"d'\", 129: 'd-', 130: 'da', 131: 'db', 132: 'dc', 133: 'dd', 134: 'de', 135: 'df', 136: 'dg', 137: 'dh', 138: 'di', 139: 'dj', 140: 'dk', 141: 'dl', 142: 'dm', 143: 'dn', 144: 'do', 145: 'dp', 146: 'dq', 147: 'dr', 148: 'ds', 149: 'dt', 150: 'du', 151: 'dv', 152: 'dw', 153: 'dy', 154: 'dz', 155: \"e'\", 156: 'e-', 157: 'ea', 158: 'eb', 159: 'ec', 160: 'ed', 161: 'ee', 162: 'ef', 163: 'eg', 164: 'eh', 165: 'ei', 166: 'ej', 167: 'ek', 168: 'el', 169: 'em', 170: 'en', 171: 'eo', 172: 'ep', 173: 'eq', 174: 'er', 175: 'es', 176: 'et', 177: 'eu', 178: 'ev', 179: 'ew', 180: 'ex', 181: 'ey', 182: 'ez', 183: \"f'\", 184: 'f-', 185: 'fa', 186: 'fb', 187: 'fc', 188: 'fd', 189: 'fe', 190: 'ff', 191: 'fg', 192: 'fh', 193: 'fi', 194: 'fj', 195: 'fk', 196: 'fl', 197: 'fm', 198: 'fn', 199: 'fo', 200: 'fp', 201: 'fq', 202: 'fr', 203: 'fs', 204: 'ft', 205: 'fu', 206: 'fv', 207: 'fw', 208: 'fx', 209: 'fy', 210: 'fz', 211: \"g'\", 212: 'g-', 213: 'ga', 214: 'gb', 215: 'gc', 216: 'gd', 217: 'ge', 218: 'gf', 219: 'gg', 220: 'gh', 221: 'gi', 222: 'gj', 223: 'gk', 224: 'gl', 225: 'gm', 226: 'gn', 227: 'go', 228: 'gp', 229: 'gq', 230: 'gr', 231: 'gs', 232: 'gt', 233: 'gu', 234: 'gv', 235: 'gw', 236: 'gx', 237: 'gy', 238: 'gz', 239: \"h'\", 240: 'h-', 241: 'ha', 242: 'hb', 243: 'hc', 244: 'hd', 245: 'he', 246: 'hf', 247: 'hg', 248: 'hh', 249: 'hi', 250: 'hj', 251: 'hk', 252: 'hl', 253: 'hm', 254: 'hn', 255: 'ho', 256: 'hp', 257: 'hq', 258: 'hr', 259: 'hs', 260: 'ht', 261: 'hu', 262: 'hv', 263: 'hw', 264: 'hy', 265: 'i', 266: \"i'\", 267: 'i-', 268: 'ia', 269: 'ib', 270: 'ic', 271: 'id', 272: 'ie', 273: 'if', 274: 'ig', 275: 'ih', 276: 'ii', 277: 'ij', 278: 'ik', 279: 'il', 280: 'im', 281: 'in', 282: 'io', 283: 'ip', 284: 'iq', 285: 'ir', 286: 'is', 287: 'it', 288: 'iu', 289: 'iv', 290: 'iw', 291: 'ix', 292: 'iy', 293: 'iz', 294: \"j'\", 295: 'ja', 296: 'jc', 297: 'jd', 298: 'je', 299: 'jf', 300: 'jh', 301: 'ji', 302: 'jj', 303: 'jk', 304: 'jl', 305: 'jm', 306: 'jn', 307: 'jo', 308: 'jr', 309: 'js', 310: 'jt', 311: 'ju', 312: 'jv', 313: 'jw', 314: 'jy', 315: \"k'\", 316: 'k-', 317: 'ka', 318: 'kb', 319: 'kc', 320: 'kd', 321: 'ke', 322: 'kf', 323: 'kg', 324: 'kh', 325: 'ki', 326: 'kj', 327: 'kk', 328: 'kl', 329: 'km', 330: 'kn', 331: 'ko', 332: 'kp', 333: 'kr', 334: 'ks', 335: 'kt', 336: 'ku', 337: 'kv', 338: 'kw', 339: 'ky', 340: 'kz', 341: \"l'\", 342: 'l-', 343: 'la', 344: 'lb', 345: 'lc', 346: 'ld', 347: 'le', 348: 'lf', 349: 'lg', 350: 'lh', 351: 'li', 352: 'lj', 353: 'lk', 354: 'll', 355: 'lm', 356: 'ln', 357: 'lo', 358: 'lp', 359: 'lq', 360: 'lr', 361: 'ls', 362: 'lt', 363: 'lu', 364: 'lv', 365: 'lw', 366: 'lx', 367: 'ly', 368: 'lz', 369: \"m'\", 370: 'm-', 371: 'ma', 372: 'mb', 373: 'mc', 374: 'md', 375: 'me', 376: 'mf', 377: 'mg', 378: 'mh', 379: 'mi', 380: 'mj', 381: 'mk', 382: 'ml', 383: 'mm', 384: 'mn', 385: 'mo', 386: 'mp', 387: 'mq', 388: 'mr', 389: 'ms', 390: 'mt', 391: 'mu', 392: 'mv', 393: 'mw', 394: 'my', 395: 'mz', 396: \"n'\", 397: 'n-', 398: 'na', 399: 'nb', 400: 'nc', 401: 'nd', 402: 'ne', 403: 'nf', 404: 'ng', 405: 'nh', 406: 'ni', 407: 'nj', 408: 'nk', 409: 'nl', 410: 'nm', 411: 'nn', 412: 'no', 413: 'np', 414: 'nq', 415: 'nr', 416: 'ns', 417: 'nt', 418: 'nu', 419: 'nv', 420: 'nw', 421: 'nx', 422: 'ny', 423: 'nz', 424: 'o', 425: \"o'\", 426: 'o-', 427: 'oa', 428: 'ob', 429: 'oc', 430: 'od', 431: 'oe', 432: 'of', 433: 'og', 434: 'oh', 435: 'oi', 436: 'oj', 437: 'ok', 438: 'ol', 439: 'om', 440: 'on', 441: 'oo', 442: 'op', 443: 'oq', 444: 'or', 445: 'os', 446: 'ot', 447: 'ou', 448: 'ov', 449: 'ow', 450: 'ox', 451: 'oy', 452: 'oz', 453: \"p'\", 454: 'p-', 455: 'pa', 456: 'pb', 457: 'pc', 458: 'pd', 459: 'pe', 460: 'pf', 461: 'pg', 462: 'ph', 463: 'pi', 464: 'pj', 465: 'pk', 466: 'pl', 467: 'pm', 468: 'pn', 469: 'po', 470: 'pp', 471: 'pr', 472: 'ps', 473: 'pt', 474: 'pu', 475: 'pw', 476: 'py', 477: 'pz', 478: \"q'\", 479: 'qa', 480: 'qb', 481: 'qg', 482: 'qi', 483: 'qo', 484: 'qu', 485: 'qv', 486: \"r'\", 487: 'r-', 488: 'ra', 489: 'rb', 490: 'rc', 491: 'rd', 492: 're', 493: 'rf', 494: 'rg', 495: 'rh', 496: 'ri', 497: 'rj', 498: 'rk', 499: 'rl', 500: 'rm', 501: 'rn', 502: 'ro', 503: 'rp', 504: 'rq', 505: 'rr', 506: 'rs', 507: 'rt', 508: 'ru', 509: 'rv', 510: 'rw', 511: 'rx', 512: 'ry', 513: 'rz', 514: \"s'\", 515: 's-', 516: 'sa', 517: 'sb', 518: 'sc', 519: 'sd', 520: 'se', 521: 'sf', 522: 'sg', 523: 'sh', 524: 'si', 525: 'sj', 526: 'sk', 527: 'sl', 528: 'sm', 529: 'sn', 530: 'so', 531: 'sp', 532: 'sq', 533: 'sr', 534: 'ss', 535: 'st', 536: 'su', 537: 'sv', 538: 'sw', 539: 'sx', 540: 'sy', 541: 'sz', 542: \"t'\", 543: 't-', 544: 'ta', 545: 'tb', 546: 'tc', 547: 'td', 548: 'te', 549: 'tf', 550: 'tg', 551: 'th', 552: 'ti', 553: 'tj', 554: 'tk', 555: 'tl', 556: 'tm', 557: 'tn', 558: 'to', 559: 'tp', 560: 'tr', 561: 'ts', 562: 'tt', 563: 'tu', 564: 'tv', 565: 'tw', 566: 'tx', 567: 'ty', 568: 'tz', 569: \"u'\", 570: 'u-', 571: 'ua', 572: 'ub', 573: 'uc', 574: 'ud', 575: 'ue', 576: 'uf', 577: 'ug', 578: 'uh', 579: 'ui', 580: 'uj', 581: 'uk', 582: 'ul', 583: 'um', 584: 'un', 585: 'uo', 586: 'up', 587: 'uq', 588: 'ur', 589: 'us', 590: 'ut', 591: 'uu', 592: 'uv', 593: 'uw', 594: 'ux', 595: 'uy', 596: 'uz', 597: \"v'\", 598: 'va', 599: 'vc', 600: 'vd', 601: 've', 602: 'vg', 603: 'vh', 604: 'vi', 605: 'vj', 606: 'vk', 607: 'vl', 608: 'vn', 609: 'vo', 610: 'vr', 611: 'vs', 612: 'vt', 613: 'vu', 614: 'vv', 615: 'vy', 616: 'vz', 617: \"w'\", 618: 'w-', 619: 'wa', 620: 'wb', 621: 'wc', 622: 'wd', 623: 'we', 624: 'wf', 625: 'wg', 626: 'wh', 627: 'wi', 628: 'wk', 629: 'wl', 630: 'wm', 631: 'wn', 632: 'wo', 633: 'wp', 634: 'wq', 635: 'wr', 636: 'ws', 637: 'wt', 638: 'wu', 639: 'wv', 640: 'ww', 641: 'wy', 642: 'wz', 643: \"x'\", 644: 'x-', 645: 'xa', 646: 'xb', 647: 'xc', 648: 'xd', 649: 'xe', 650: 'xf', 651: 'xg', 652: 'xh', 653: 'xi', 654: 'xl', 655: 'xm', 656: 'xn', 657: 'xo', 658: 'xp', 659: 'xq', 660: 'xr', 661: 'xs', 662: 'xt', 663: 'xu', 664: 'xv', 665: 'xw', 666: 'xx', 667: 'xy', 668: 'xz', 669: \"y'\", 670: 'y-', 671: 'ya', 672: 'yb', 673: 'yc', 674: 'yd', 675: 'ye', 676: 'yf', 677: 'yg', 678: 'yh', 679: 'yi', 680: 'yj', 681: 'yk', 682: 'yl', 683: 'ym', 684: 'yn', 685: 'yo', 686: 'yp', 687: 'yq', 688: 'yr', 689: 'ys', 690: 'yt', 691: 'yu', 692: 'yv', 693: 'yw', 694: 'yx', 695: 'yy', 696: 'yz', 697: \"z'\", 698: 'za', 699: 'zb', 700: 'zc', 701: 'zd', 702: 'ze', 703: 'zf', 704: 'zg', 705: 'zh', 706: 'zi', 707: 'zk', 708: 'zl', 709: 'zm', 710: 'zn', 711: 'zo', 712: 'zp', 713: 'zq', 714: 'zr', 715: 'zs', 716: 'zt', 717: 'zu', 718: 'zv', 719: 'zw', 720: 'zy', 721: 'zz'}\n",
      "valid grp 722 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-f', 31: '-g', 32: '-h', 33: '-i', 34: '-j', 35: '-k', 36: '-l', 37: '-m', 38: '-n', 39: '-o', 40: '-p', 41: '-q', 42: '-r', 43: '-s', 44: '-t', 45: '-u', 46: '-v', 47: '-w', 48: '-y', 49: \"a'\", 50: 'a-', 51: 'aa', 52: 'ab', 53: 'ac', 54: 'ad', 55: 'ae', 56: 'af', 57: 'ag', 58: 'ah', 59: 'ai', 60: 'aj', 61: 'ak', 62: 'al', 63: 'am', 64: 'an', 65: 'ao', 66: 'ap', 67: 'aq', 68: 'ar', 69: 'as', 70: 'at', 71: 'au', 72: 'av', 73: 'aw', 74: 'ax', 75: 'ay', 76: 'az', 77: \"b'\", 78: 'ba', 79: 'bb', 80: 'bc', 81: 'bd', 82: 'be', 83: 'bf', 84: 'bg', 85: 'bh', 86: 'bi', 87: 'bj', 88: 'bk', 89: 'bl', 90: 'bm', 91: 'bn', 92: 'bo', 93: 'bp', 94: 'br', 95: 'bs', 96: 'bt', 97: 'bu', 98: 'bv', 99: 'bw', 100: 'by', 101: 'bz', 102: \"c'\", 103: 'ca', 104: 'cb', 105: 'cc', 106: 'cd', 107: 'ce', 108: 'cf', 109: 'cg', 110: 'ch', 111: 'ci', 112: 'cj', 113: 'ck', 114: 'cl', 115: 'cm', 116: 'cn', 117: 'co', 118: 'cp', 119: 'cq', 120: 'cr', 121: 'cs', 122: 'ct', 123: 'cu', 124: 'cv', 125: 'cw', 126: 'cy', 127: 'cz', 128: \"d'\", 129: 'd-', 130: 'da', 131: 'db', 132: 'dc', 133: 'dd', 134: 'de', 135: 'df', 136: 'dg', 137: 'dh', 138: 'di', 139: 'dj', 140: 'dk', 141: 'dl', 142: 'dm', 143: 'dn', 144: 'do', 145: 'dp', 146: 'dq', 147: 'dr', 148: 'ds', 149: 'dt', 150: 'du', 151: 'dv', 152: 'dw', 153: 'dy', 154: 'dz', 155: \"e'\", 156: 'e-', 157: 'ea', 158: 'eb', 159: 'ec', 160: 'ed', 161: 'ee', 162: 'ef', 163: 'eg', 164: 'eh', 165: 'ei', 166: 'ej', 167: 'ek', 168: 'el', 169: 'em', 170: 'en', 171: 'eo', 172: 'ep', 173: 'eq', 174: 'er', 175: 'es', 176: 'et', 177: 'eu', 178: 'ev', 179: 'ew', 180: 'ex', 181: 'ey', 182: 'ez', 183: \"f'\", 184: 'f-', 185: 'fa', 186: 'fb', 187: 'fc', 188: 'fd', 189: 'fe', 190: 'ff', 191: 'fg', 192: 'fh', 193: 'fi', 194: 'fj', 195: 'fk', 196: 'fl', 197: 'fm', 198: 'fn', 199: 'fo', 200: 'fp', 201: 'fq', 202: 'fr', 203: 'fs', 204: 'ft', 205: 'fu', 206: 'fv', 207: 'fw', 208: 'fx', 209: 'fy', 210: 'fz', 211: \"g'\", 212: 'g-', 213: 'ga', 214: 'gb', 215: 'gc', 216: 'gd', 217: 'ge', 218: 'gf', 219: 'gg', 220: 'gh', 221: 'gi', 222: 'gj', 223: 'gk', 224: 'gl', 225: 'gm', 226: 'gn', 227: 'go', 228: 'gp', 229: 'gq', 230: 'gr', 231: 'gs', 232: 'gt', 233: 'gu', 234: 'gv', 235: 'gw', 236: 'gx', 237: 'gy', 238: 'gz', 239: \"h'\", 240: 'h-', 241: 'ha', 242: 'hb', 243: 'hc', 244: 'hd', 245: 'he', 246: 'hf', 247: 'hg', 248: 'hh', 249: 'hi', 250: 'hj', 251: 'hk', 252: 'hl', 253: 'hm', 254: 'hn', 255: 'ho', 256: 'hp', 257: 'hq', 258: 'hr', 259: 'hs', 260: 'ht', 261: 'hu', 262: 'hv', 263: 'hw', 264: 'hy', 265: 'i', 266: \"i'\", 267: 'i-', 268: 'ia', 269: 'ib', 270: 'ic', 271: 'id', 272: 'ie', 273: 'if', 274: 'ig', 275: 'ih', 276: 'ii', 277: 'ij', 278: 'ik', 279: 'il', 280: 'im', 281: 'in', 282: 'io', 283: 'ip', 284: 'iq', 285: 'ir', 286: 'is', 287: 'it', 288: 'iu', 289: 'iv', 290: 'iw', 291: 'ix', 292: 'iy', 293: 'iz', 294: \"j'\", 295: 'ja', 296: 'jc', 297: 'jd', 298: 'je', 299: 'jf', 300: 'jh', 301: 'ji', 302: 'jj', 303: 'jk', 304: 'jl', 305: 'jm', 306: 'jn', 307: 'jo', 308: 'jr', 309: 'js', 310: 'jt', 311: 'ju', 312: 'jv', 313: 'jw', 314: 'jy', 315: \"k'\", 316: 'k-', 317: 'ka', 318: 'kb', 319: 'kc', 320: 'kd', 321: 'ke', 322: 'kf', 323: 'kg', 324: 'kh', 325: 'ki', 326: 'kj', 327: 'kk', 328: 'kl', 329: 'km', 330: 'kn', 331: 'ko', 332: 'kp', 333: 'kr', 334: 'ks', 335: 'kt', 336: 'ku', 337: 'kv', 338: 'kw', 339: 'ky', 340: 'kz', 341: \"l'\", 342: 'l-', 343: 'la', 344: 'lb', 345: 'lc', 346: 'ld', 347: 'le', 348: 'lf', 349: 'lg', 350: 'lh', 351: 'li', 352: 'lj', 353: 'lk', 354: 'll', 355: 'lm', 356: 'ln', 357: 'lo', 358: 'lp', 359: 'lq', 360: 'lr', 361: 'ls', 362: 'lt', 363: 'lu', 364: 'lv', 365: 'lw', 366: 'lx', 367: 'ly', 368: 'lz', 369: \"m'\", 370: 'm-', 371: 'ma', 372: 'mb', 373: 'mc', 374: 'md', 375: 'me', 376: 'mf', 377: 'mg', 378: 'mh', 379: 'mi', 380: 'mj', 381: 'mk', 382: 'ml', 383: 'mm', 384: 'mn', 385: 'mo', 386: 'mp', 387: 'mq', 388: 'mr', 389: 'ms', 390: 'mt', 391: 'mu', 392: 'mv', 393: 'mw', 394: 'my', 395: 'mz', 396: \"n'\", 397: 'n-', 398: 'na', 399: 'nb', 400: 'nc', 401: 'nd', 402: 'ne', 403: 'nf', 404: 'ng', 405: 'nh', 406: 'ni', 407: 'nj', 408: 'nk', 409: 'nl', 410: 'nm', 411: 'nn', 412: 'no', 413: 'np', 414: 'nq', 415: 'nr', 416: 'ns', 417: 'nt', 418: 'nu', 419: 'nv', 420: 'nw', 421: 'nx', 422: 'ny', 423: 'nz', 424: 'o', 425: \"o'\", 426: 'o-', 427: 'oa', 428: 'ob', 429: 'oc', 430: 'od', 431: 'oe', 432: 'of', 433: 'og', 434: 'oh', 435: 'oi', 436: 'oj', 437: 'ok', 438: 'ol', 439: 'om', 440: 'on', 441: 'oo', 442: 'op', 443: 'oq', 444: 'or', 445: 'os', 446: 'ot', 447: 'ou', 448: 'ov', 449: 'ow', 450: 'ox', 451: 'oy', 452: 'oz', 453: \"p'\", 454: 'p-', 455: 'pa', 456: 'pb', 457: 'pc', 458: 'pd', 459: 'pe', 460: 'pf', 461: 'pg', 462: 'ph', 463: 'pi', 464: 'pj', 465: 'pk', 466: 'pl', 467: 'pm', 468: 'pn', 469: 'po', 470: 'pp', 471: 'pr', 472: 'ps', 473: 'pt', 474: 'pu', 475: 'pw', 476: 'py', 477: 'pz', 478: \"q'\", 479: 'qa', 480: 'qb', 481: 'qg', 482: 'qi', 483: 'qo', 484: 'qu', 485: 'qv', 486: \"r'\", 487: 'r-', 488: 'ra', 489: 'rb', 490: 'rc', 491: 'rd', 492: 're', 493: 'rf', 494: 'rg', 495: 'rh', 496: 'ri', 497: 'rj', 498: 'rk', 499: 'rl', 500: 'rm', 501: 'rn', 502: 'ro', 503: 'rp', 504: 'rq', 505: 'rr', 506: 'rs', 507: 'rt', 508: 'ru', 509: 'rv', 510: 'rw', 511: 'rx', 512: 'ry', 513: 'rz', 514: \"s'\", 515: 's-', 516: 'sa', 517: 'sb', 518: 'sc', 519: 'sd', 520: 'se', 521: 'sf', 522: 'sg', 523: 'sh', 524: 'si', 525: 'sj', 526: 'sk', 527: 'sl', 528: 'sm', 529: 'sn', 530: 'so', 531: 'sp', 532: 'sq', 533: 'sr', 534: 'ss', 535: 'st', 536: 'su', 537: 'sv', 538: 'sw', 539: 'sx', 540: 'sy', 541: 'sz', 542: \"t'\", 543: 't-', 544: 'ta', 545: 'tb', 546: 'tc', 547: 'td', 548: 'te', 549: 'tf', 550: 'tg', 551: 'th', 552: 'ti', 553: 'tj', 554: 'tk', 555: 'tl', 556: 'tm', 557: 'tn', 558: 'to', 559: 'tp', 560: 'tr', 561: 'ts', 562: 'tt', 563: 'tu', 564: 'tv', 565: 'tw', 566: 'tx', 567: 'ty', 568: 'tz', 569: \"u'\", 570: 'u-', 571: 'ua', 572: 'ub', 573: 'uc', 574: 'ud', 575: 'ue', 576: 'uf', 577: 'ug', 578: 'uh', 579: 'ui', 580: 'uj', 581: 'uk', 582: 'ul', 583: 'um', 584: 'un', 585: 'uo', 586: 'up', 587: 'uq', 588: 'ur', 589: 'us', 590: 'ut', 591: 'uu', 592: 'uv', 593: 'uw', 594: 'ux', 595: 'uy', 596: 'uz', 597: \"v'\", 598: 'va', 599: 'vc', 600: 'vd', 601: 've', 602: 'vg', 603: 'vh', 604: 'vi', 605: 'vj', 606: 'vk', 607: 'vl', 608: 'vn', 609: 'vo', 610: 'vr', 611: 'vs', 612: 'vt', 613: 'vu', 614: 'vv', 615: 'vy', 616: 'vz', 617: \"w'\", 618: 'w-', 619: 'wa', 620: 'wb', 621: 'wc', 622: 'wd', 623: 'we', 624: 'wf', 625: 'wg', 626: 'wh', 627: 'wi', 628: 'wk', 629: 'wl', 630: 'wm', 631: 'wn', 632: 'wo', 633: 'wp', 634: 'wq', 635: 'wr', 636: 'ws', 637: 'wt', 638: 'wu', 639: 'wv', 640: 'ww', 641: 'wy', 642: 'wz', 643: \"x'\", 644: 'x-', 645: 'xa', 646: 'xb', 647: 'xc', 648: 'xd', 649: 'xe', 650: 'xf', 651: 'xg', 652: 'xh', 653: 'xi', 654: 'xl', 655: 'xm', 656: 'xn', 657: 'xo', 658: 'xp', 659: 'xq', 660: 'xr', 661: 'xs', 662: 'xt', 663: 'xu', 664: 'xv', 665: 'xw', 666: 'xx', 667: 'xy', 668: 'xz', 669: \"y'\", 670: 'y-', 671: 'ya', 672: 'yb', 673: 'yc', 674: 'yd', 675: 'ye', 676: 'yf', 677: 'yg', 678: 'yh', 679: 'yi', 680: 'yj', 681: 'yk', 682: 'yl', 683: 'ym', 684: 'yn', 685: 'yo', 686: 'yp', 687: 'yq', 688: 'yr', 689: 'ys', 690: 'yt', 691: 'yu', 692: 'yv', 693: 'yw', 694: 'yx', 695: 'yy', 696: 'yz', 697: \"z'\", 698: 'za', 699: 'zb', 700: 'zc', 701: 'zd', 702: 'ze', 703: 'zf', 704: 'zg', 705: 'zh', 706: 'zi', 707: 'zk', 708: 'zl', 709: 'zm', 710: 'zn', 711: 'zo', 712: 'zp', 713: 'zq', 714: 'zr', 715: 'zs', 716: 'zt', 717: 'zu', 718: 'zv', 719: 'zw', 720: 'zy', 721: 'zz'}\n",
      "test grp 722 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-f', 31: '-g', 32: '-h', 33: '-i', 34: '-j', 35: '-k', 36: '-l', 37: '-m', 38: '-n', 39: '-o', 40: '-p', 41: '-q', 42: '-r', 43: '-s', 44: '-t', 45: '-u', 46: '-v', 47: '-w', 48: '-y', 49: \"a'\", 50: 'a-', 51: 'aa', 52: 'ab', 53: 'ac', 54: 'ad', 55: 'ae', 56: 'af', 57: 'ag', 58: 'ah', 59: 'ai', 60: 'aj', 61: 'ak', 62: 'al', 63: 'am', 64: 'an', 65: 'ao', 66: 'ap', 67: 'aq', 68: 'ar', 69: 'as', 70: 'at', 71: 'au', 72: 'av', 73: 'aw', 74: 'ax', 75: 'ay', 76: 'az', 77: \"b'\", 78: 'ba', 79: 'bb', 80: 'bc', 81: 'bd', 82: 'be', 83: 'bf', 84: 'bg', 85: 'bh', 86: 'bi', 87: 'bj', 88: 'bk', 89: 'bl', 90: 'bm', 91: 'bn', 92: 'bo', 93: 'bp', 94: 'br', 95: 'bs', 96: 'bt', 97: 'bu', 98: 'bv', 99: 'bw', 100: 'by', 101: 'bz', 102: \"c'\", 103: 'ca', 104: 'cb', 105: 'cc', 106: 'cd', 107: 'ce', 108: 'cf', 109: 'cg', 110: 'ch', 111: 'ci', 112: 'cj', 113: 'ck', 114: 'cl', 115: 'cm', 116: 'cn', 117: 'co', 118: 'cp', 119: 'cq', 120: 'cr', 121: 'cs', 122: 'ct', 123: 'cu', 124: 'cv', 125: 'cw', 126: 'cy', 127: 'cz', 128: \"d'\", 129: 'd-', 130: 'da', 131: 'db', 132: 'dc', 133: 'dd', 134: 'de', 135: 'df', 136: 'dg', 137: 'dh', 138: 'di', 139: 'dj', 140: 'dk', 141: 'dl', 142: 'dm', 143: 'dn', 144: 'do', 145: 'dp', 146: 'dq', 147: 'dr', 148: 'ds', 149: 'dt', 150: 'du', 151: 'dv', 152: 'dw', 153: 'dy', 154: 'dz', 155: \"e'\", 156: 'e-', 157: 'ea', 158: 'eb', 159: 'ec', 160: 'ed', 161: 'ee', 162: 'ef', 163: 'eg', 164: 'eh', 165: 'ei', 166: 'ej', 167: 'ek', 168: 'el', 169: 'em', 170: 'en', 171: 'eo', 172: 'ep', 173: 'eq', 174: 'er', 175: 'es', 176: 'et', 177: 'eu', 178: 'ev', 179: 'ew', 180: 'ex', 181: 'ey', 182: 'ez', 183: \"f'\", 184: 'f-', 185: 'fa', 186: 'fb', 187: 'fc', 188: 'fd', 189: 'fe', 190: 'ff', 191: 'fg', 192: 'fh', 193: 'fi', 194: 'fj', 195: 'fk', 196: 'fl', 197: 'fm', 198: 'fn', 199: 'fo', 200: 'fp', 201: 'fq', 202: 'fr', 203: 'fs', 204: 'ft', 205: 'fu', 206: 'fv', 207: 'fw', 208: 'fx', 209: 'fy', 210: 'fz', 211: \"g'\", 212: 'g-', 213: 'ga', 214: 'gb', 215: 'gc', 216: 'gd', 217: 'ge', 218: 'gf', 219: 'gg', 220: 'gh', 221: 'gi', 222: 'gj', 223: 'gk', 224: 'gl', 225: 'gm', 226: 'gn', 227: 'go', 228: 'gp', 229: 'gq', 230: 'gr', 231: 'gs', 232: 'gt', 233: 'gu', 234: 'gv', 235: 'gw', 236: 'gx', 237: 'gy', 238: 'gz', 239: \"h'\", 240: 'h-', 241: 'ha', 242: 'hb', 243: 'hc', 244: 'hd', 245: 'he', 246: 'hf', 247: 'hg', 248: 'hh', 249: 'hi', 250: 'hj', 251: 'hk', 252: 'hl', 253: 'hm', 254: 'hn', 255: 'ho', 256: 'hp', 257: 'hq', 258: 'hr', 259: 'hs', 260: 'ht', 261: 'hu', 262: 'hv', 263: 'hw', 264: 'hy', 265: 'i', 266: \"i'\", 267: 'i-', 268: 'ia', 269: 'ib', 270: 'ic', 271: 'id', 272: 'ie', 273: 'if', 274: 'ig', 275: 'ih', 276: 'ii', 277: 'ij', 278: 'ik', 279: 'il', 280: 'im', 281: 'in', 282: 'io', 283: 'ip', 284: 'iq', 285: 'ir', 286: 'is', 287: 'it', 288: 'iu', 289: 'iv', 290: 'iw', 291: 'ix', 292: 'iy', 293: 'iz', 294: \"j'\", 295: 'ja', 296: 'jc', 297: 'jd', 298: 'je', 299: 'jf', 300: 'jh', 301: 'ji', 302: 'jj', 303: 'jk', 304: 'jl', 305: 'jm', 306: 'jn', 307: 'jo', 308: 'jr', 309: 'js', 310: 'jt', 311: 'ju', 312: 'jv', 313: 'jw', 314: 'jy', 315: \"k'\", 316: 'k-', 317: 'ka', 318: 'kb', 319: 'kc', 320: 'kd', 321: 'ke', 322: 'kf', 323: 'kg', 324: 'kh', 325: 'ki', 326: 'kj', 327: 'kk', 328: 'kl', 329: 'km', 330: 'kn', 331: 'ko', 332: 'kp', 333: 'kr', 334: 'ks', 335: 'kt', 336: 'ku', 337: 'kv', 338: 'kw', 339: 'ky', 340: 'kz', 341: \"l'\", 342: 'l-', 343: 'la', 344: 'lb', 345: 'lc', 346: 'ld', 347: 'le', 348: 'lf', 349: 'lg', 350: 'lh', 351: 'li', 352: 'lj', 353: 'lk', 354: 'll', 355: 'lm', 356: 'ln', 357: 'lo', 358: 'lp', 359: 'lq', 360: 'lr', 361: 'ls', 362: 'lt', 363: 'lu', 364: 'lv', 365: 'lw', 366: 'lx', 367: 'ly', 368: 'lz', 369: \"m'\", 370: 'm-', 371: 'ma', 372: 'mb', 373: 'mc', 374: 'md', 375: 'me', 376: 'mf', 377: 'mg', 378: 'mh', 379: 'mi', 380: 'mj', 381: 'mk', 382: 'ml', 383: 'mm', 384: 'mn', 385: 'mo', 386: 'mp', 387: 'mq', 388: 'mr', 389: 'ms', 390: 'mt', 391: 'mu', 392: 'mv', 393: 'mw', 394: 'my', 395: 'mz', 396: \"n'\", 397: 'n-', 398: 'na', 399: 'nb', 400: 'nc', 401: 'nd', 402: 'ne', 403: 'nf', 404: 'ng', 405: 'nh', 406: 'ni', 407: 'nj', 408: 'nk', 409: 'nl', 410: 'nm', 411: 'nn', 412: 'no', 413: 'np', 414: 'nq', 415: 'nr', 416: 'ns', 417: 'nt', 418: 'nu', 419: 'nv', 420: 'nw', 421: 'nx', 422: 'ny', 423: 'nz', 424: 'o', 425: \"o'\", 426: 'o-', 427: 'oa', 428: 'ob', 429: 'oc', 430: 'od', 431: 'oe', 432: 'of', 433: 'og', 434: 'oh', 435: 'oi', 436: 'oj', 437: 'ok', 438: 'ol', 439: 'om', 440: 'on', 441: 'oo', 442: 'op', 443: 'oq', 444: 'or', 445: 'os', 446: 'ot', 447: 'ou', 448: 'ov', 449: 'ow', 450: 'ox', 451: 'oy', 452: 'oz', 453: \"p'\", 454: 'p-', 455: 'pa', 456: 'pb', 457: 'pc', 458: 'pd', 459: 'pe', 460: 'pf', 461: 'pg', 462: 'ph', 463: 'pi', 464: 'pj', 465: 'pk', 466: 'pl', 467: 'pm', 468: 'pn', 469: 'po', 470: 'pp', 471: 'pr', 472: 'ps', 473: 'pt', 474: 'pu', 475: 'pw', 476: 'py', 477: 'pz', 478: \"q'\", 479: 'qa', 480: 'qb', 481: 'qg', 482: 'qi', 483: 'qo', 484: 'qu', 485: 'qv', 486: \"r'\", 487: 'r-', 488: 'ra', 489: 'rb', 490: 'rc', 491: 'rd', 492: 're', 493: 'rf', 494: 'rg', 495: 'rh', 496: 'ri', 497: 'rj', 498: 'rk', 499: 'rl', 500: 'rm', 501: 'rn', 502: 'ro', 503: 'rp', 504: 'rq', 505: 'rr', 506: 'rs', 507: 'rt', 508: 'ru', 509: 'rv', 510: 'rw', 511: 'rx', 512: 'ry', 513: 'rz', 514: \"s'\", 515: 's-', 516: 'sa', 517: 'sb', 518: 'sc', 519: 'sd', 520: 'se', 521: 'sf', 522: 'sg', 523: 'sh', 524: 'si', 525: 'sj', 526: 'sk', 527: 'sl', 528: 'sm', 529: 'sn', 530: 'so', 531: 'sp', 532: 'sq', 533: 'sr', 534: 'ss', 535: 'st', 536: 'su', 537: 'sv', 538: 'sw', 539: 'sx', 540: 'sy', 541: 'sz', 542: \"t'\", 543: 't-', 544: 'ta', 545: 'tb', 546: 'tc', 547: 'td', 548: 'te', 549: 'tf', 550: 'tg', 551: 'th', 552: 'ti', 553: 'tj', 554: 'tk', 555: 'tl', 556: 'tm', 557: 'tn', 558: 'to', 559: 'tp', 560: 'tr', 561: 'ts', 562: 'tt', 563: 'tu', 564: 'tv', 565: 'tw', 566: 'tx', 567: 'ty', 568: 'tz', 569: \"u'\", 570: 'u-', 571: 'ua', 572: 'ub', 573: 'uc', 574: 'ud', 575: 'ue', 576: 'uf', 577: 'ug', 578: 'uh', 579: 'ui', 580: 'uj', 581: 'uk', 582: 'ul', 583: 'um', 584: 'un', 585: 'uo', 586: 'up', 587: 'uq', 588: 'ur', 589: 'us', 590: 'ut', 591: 'uu', 592: 'uv', 593: 'uw', 594: 'ux', 595: 'uy', 596: 'uz', 597: \"v'\", 598: 'va', 599: 'vc', 600: 'vd', 601: 've', 602: 'vg', 603: 'vh', 604: 'vi', 605: 'vj', 606: 'vk', 607: 'vl', 608: 'vn', 609: 'vo', 610: 'vr', 611: 'vs', 612: 'vt', 613: 'vu', 614: 'vv', 615: 'vy', 616: 'vz', 617: \"w'\", 618: 'w-', 619: 'wa', 620: 'wb', 621: 'wc', 622: 'wd', 623: 'we', 624: 'wf', 625: 'wg', 626: 'wh', 627: 'wi', 628: 'wk', 629: 'wl', 630: 'wm', 631: 'wn', 632: 'wo', 633: 'wp', 634: 'wq', 635: 'wr', 636: 'ws', 637: 'wt', 638: 'wu', 639: 'wv', 640: 'ww', 641: 'wy', 642: 'wz', 643: \"x'\", 644: 'x-', 645: 'xa', 646: 'xb', 647: 'xc', 648: 'xd', 649: 'xe', 650: 'xf', 651: 'xg', 652: 'xh', 653: 'xi', 654: 'xl', 655: 'xm', 656: 'xn', 657: 'xo', 658: 'xp', 659: 'xq', 660: 'xr', 661: 'xs', 662: 'xt', 663: 'xu', 664: 'xv', 665: 'xw', 666: 'xx', 667: 'xy', 668: 'xz', 669: \"y'\", 670: 'y-', 671: 'ya', 672: 'yb', 673: 'yc', 674: 'yd', 675: 'ye', 676: 'yf', 677: 'yg', 678: 'yh', 679: 'yi', 680: 'yj', 681: 'yk', 682: 'yl', 683: 'ym', 684: 'yn', 685: 'yo', 686: 'yp', 687: 'yq', 688: 'yr', 689: 'ys', 690: 'yt', 691: 'yu', 692: 'yv', 693: 'yw', 694: 'yx', 695: 'yy', 696: 'yz', 697: \"z'\", 698: 'za', 699: 'zb', 700: 'zc', 701: 'zd', 702: 'ze', 703: 'zf', 704: 'zg', 705: 'zh', 706: 'zi', 707: 'zk', 708: 'zl', 709: 'zm', 710: 'zn', 711: 'zo', 712: 'zp', 713: 'zq', 714: 'zr', 715: 'zs', 716: 'zt', 717: 'zu', 718: 'zv', 719: 'zw', 720: 'zy', 721: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "718 {\"'c\": 6, 'ca': 103, 'au': 71, 'us': 589, 'se': 520, 'co': 117, 'ou': 447, 'ur': 588, 'rs': 506, \"'e\": 8, 'em': 169, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 484, 'uo': 585, 'ot': 446, 'te': 548, \"'t\": 21, 'ti': 552, 'is': 286, '--': 24, '-n': 38, 'ny': 422, 'ya': 671, '-b': 26, 'ba': 78, 'be': 82, 'el': 168, 'la': 343, 'as': 69, '-c': 27, 'om': 439, 'mp': 386, 'pe': 459, 'en': 170, 'ng': 404, '-k': 35, 'ka': 317, 'an': 64, '-l': 36, 'ah': 58, 'le': 347, 'gg': 219, 'ga': 213, '-m': 37, 'ma': 371, 'ha': 241, 'si': 524, 'sw': 538, 'wa': 619, '-s': 43, 'ed': 160, 'da': 130, 'ar': 68, 'ra': 488, 'ep': 172, 'pi': 463, 'it': 287, '-t': 44, 'ta': 544, 'ak': 61, '-w': 47, 'at': 70, \"a'\": 49, \"'b\": 5, \"'d\": 7, 'du': 150, \"'h\": 10, 'ad': 54, 'aa': 51, 'ab': 52, 'er': 174, 'rg': 494, 'ac': 53, 'ch': 110, 'he': 245, 'ke': 321, 'al': 62, 'ls': 361, 'et': 176, 'th': 551, 'am': 63, 'mo': 385, 'od': 430, 'dt': 149, 'nc': 400, 'or': 444, 'rd': 491, 'de': 134, 'dv': 151, 'va': 598, 'rk': 498, 'ro': 502, 'on': 440, \"n'\": 396, \"'s\": 20, 'ns': 416, 'so': 530, 'rt': 507, \"b'\": 77, \"'a\": 4, 'a-': 50, '-a': 25, 'ck': 113, 'cu': 123, 'di': 138, 'ia': 268, 'ik': 278, 'h-': 240, 'ai': 59, 'im': 280, 'na': 398, 'ir': 285, 'kt': 335, 'in': 281, 'ku': 336, 'lk': 353, 'ki': 325, 'lo': 357, 'ne': 402, 'os': 445, 'nd': 401, 'do': 144, 'ni': 406, 'nm': 410, 'me': 375, 'nt': 417, 'ts': 561, 'gn': 226, 'to': 558, 'r-': 487, 'rc': 490, 're': 492, 'sc': 518, 'sh': 523, 'es': 175, 'bb': 79, 'ie': 272, 'll': 354, 'nh': 405, 'tt': 562, 'ev': 178, 'vi': 604, 'il': 279, 'ey': 181, \"y'\": 669, 'bi': 86, 'bo': 92, \"t'\": 542, 'ud': 574, 'br': 94, 'io': 282, 'bs': 95, 'by': 100, 'bc': 80, 'ek': 167, 'bd': 81, 'ic': 270, 'dn': 143, 'no': 412, 'ol': 438, 'mi': 379, 'uc': 573, 'ct': 122, 'ee': 161, 'uk': 581, 'ks': 334, 'ul': 582, 'az': 76, 'zi': 706, 'iz': 293, 'ln': 356, 'ow': 449, 'dr': 147, 'cr': 120, 'mb': 372, 'rl': 499, 'rm': 500, 'rr': 505, 'ce': 107, 'yt': 690, 'bh': 85, 'ho': 255, 'id': 271, 'ig': 274, 'li': 351, 'ty': 567, 'gt': 232, 'og': 433, 'ge': 217, 'sf': 521, 'fe': 189, 'sa': 516, 'ib': 269, 'tu': 563, 'ri': 496, 'tz': 568, 'bj': 87, 'ja': 295, 'je': 298, 'ec': 159, 'bk': 88, 'kh': 324, 'bl': 89, 'ut': 590, 'ze': 702, 'e-': 156, 'ps': 472, 'st': 535, 'oo': 441, 'lu': 363, 'ly': 367, 'bn': 91, 'nu': 418, \"o'\": 425, 'oa': 427, 'oi': 435, 'hi': 249, 'sm': 528, 'gi': 221, 'rn': 501, 'if': 273, 'fa': 185, 'ci': 111, 'iv': 289, 've': 601, 'uh': 578, 'un': 584, 'ds': 148, 'ov': 448, \"e'\": 155, 'bp': 93, 'pl': 466, 'lp': 358, 'ms': 389, 'mc': 373, 'cz': 127, 'zy': 720, 'yk': 681, 'wi': 627, 'ea': 157, 'eg': 163, 'go': 227, 'dg': 136, 'ko': 331, 'ru': 508, 'up': 586, 'pt': 473, 'tl': 555, 'tn': 557, 'ss': 534, 'yn': 684, 'uz': 596, 'zz': 721, 'zo': 711, 'ei': 165, 'lv': 364, 'rb': 489, 'rp': 503, 'tr': 560, 'su': 536, 'dl': 141, 'bt': 96, 'bu': 97, 'ua': 571, 'hm': 253, 'dz': 154, 'lh': 350, 'ay': 75, 'ys': 689, 'cs': 121, 'my': 394, 'po': 469, 'ap': 66, 'pu': 474, 'lc': 345, 'aw': 73, 'cc': 105, 'ry': 512, 't-': 543, '-p': 40, 'pr': 471, 'cl': 114, 'mm': 383, 'pa': 455, 'yi': 679, \"r'\": 486, 'gl': 224, 'cy': 126, \"s'\": 514, 'ue': 575, 'ui': 579, 'um': 583, 'mu': 391, 'op': 442, 'ph': 462, 'yl': 682, 'eb': 158, 'nb': 399, 'ht': 260, 'hy': 264, 'fi': 193, 'fy': 209, 'kl': 328, 'km': 329, 'kn': 330, 'wl': 629, 'gm': 225, 'kr': 333, 'oy': 451, 'yd': 674, \"d'\": 128, 'cm': 115, 'cn': 116, 'oc': 429, 'of': 432, 'ff': 190, 'cq': 119, 'av': 72, 'ag': 57, 'ob': 428, 'ym': 683, 'tm': 556, 'uf': 576, 'gk': 223, 'sy': 540, 'yc': 673, 'iu': 288, \"m'\": 369, 'za': 698, 'sk': 526, 'wn': 631, 'ex': 180, 'dc': 132, 'dd': 133, 'eo': 171, 'dw': 152, 'we': 623, 'dy': 153, 'eh': 164, 'lb': 344, \"l'\": 341, 'lm': 355, 'nn': 411, 'sb': 517, 'eq': 173, 'rh': 495, 'ld': 346, 'lt': 362, 'dh': 137, 'eu': 177, 'gu': 233, 'kk': 327, 'hu': 261, 'lf': 348, 'ip': 283, 'aj': 60, 'ok': 437, 'oh': 434, 'iw': 290, 'gs': 231, 'dj': 139, 'jo': 307, 'ju': 311, 'dk': 140, 'dm': 142, \"f'\": 183, 'fo': 199, 'hs': 259, '-f': 30, 'ae': 55, 'sd': 519, \"g'\": 211, 'vo': 609, 'gr': 230, 'ft': 204, 'oq': 443, \"h'\": 239, 'sp': 531, 'hl': 252, \"p'\": 453, 'af': 56, 'ye': 675, 'fd': 188, 'ix': 291, 'xe': 649, 'fl': 196, 'fr': 202, 'ax': 74, 'fg': 191, 'gh': 220, \"i'\": 266, 'fh': 192, 'ox': 450, 'xi': 653, 'fm': 197, 'fs': 203, 'ef': 162, 'ug': 577, 'rw': 510, 'fw': 207, 'mn': 384, 'gy': 237, 'gc': 215, 'gf': 218, 'ih': 275, 'ew': 179, 'o-': 426, '-g': 31, 'xc': 647, 'fu': 205, 'pp': 470, 'hh': 248, 'hk': 251, 'lg': 349, 'lq': 359, 'ub': 572, 'hn': 254, 'hr': 258, 'hw': 263, 'ml': 382, 'sl': 527, 'wo': 632, \"c'\": 102, 'rf': 493, 'nk': 408, 'tc': 546, \"x'\": 643, 'ji': 301, 'jn': 306, 'uj': 580, 'k-': 316, 'kb': 318, 'kc': 319, 'iy': 292, 'kw': 338, 'ky': 339, 'kz': 340, 'l-': 342, '-q': 41, 'tv': 564, 'uq': 587, 'rq': 504, 'ej': 166, 'xa': 645, 'xo': 657, 'xy': 667, 'oj': 436, 'nq': 414, 'nz': 423, 'f-': 184, 'ij': 277, 'iq': 284, '-i': 33, 'n-': 397, '-o': 39, 'nw': 420, 'yw': 693, 'lr': 360, 'uv': 592, 'lw': 365, 'ez': 182, 'mq': 387, 'fn': 198, \"k'\": 315, 'tf': 549, 'zh': 705, 'wy': 641, 'lz': 368, 'np': 413, 'xt': 662, 'md': 374, 'zc': 700, 'zq': 713, 'mf': 376, 'mg': 377, 'mh': 378, 'yv': 692, 'mk': 381, 'oe': 431, 'pc': 457, 'mr': 388, 'mt': 390, 'mv': 392, 'mw': 393, 'yo': 685, 'yx': 694, 'yz': 696, 'sq': 532, 'nv': 419, 'oz': 452, 'ws': 636, 'cd': 106, 'nf': 403, 'gp': 228, 'nj': 407, 'jl': 304, 'nr': 415, 'sg': 522, 'i-': 267, 'bm': 90, 'tk': 554, \"u'\": 569, 'tw': 565, 'nx': 421, 'yb': 672, 'yh': 678, 'yp': 686, 'wh': 626, 'ao': 65, 'pf': 460, 'pg': 461, 'pk': 465, 'aq': 67, 'qa': 479, 'qi': 482, 'rv': 509, 'ux': 594, 'hb': 242, 'hd': 244, 'zm': 709, 'rj': 497, 'wr': 635, 'd-': 129, \"w'\": 617, 'zt': 716, 'zu': 717, 'rx': 511, 'rz': 513, 'ii': 276, 'xu': 663, 'hc': 243, 'hf': 246, \"v'\": 597, 'sn': 529, 'pn': 468, 'sr': 533, 'yr': 688, 'yu': 691, 'uy': 595, 'vd': 600, 'vg': 602, 'vn': 608, 'vr': 610, 'vt': 612, 'wb': 620, 'wf': 624, 'wk': 628, 'wt': 637, 'wu': 638, 'xf': 650, 'xl': 654, 'xs': 661, 'yg': 677, 'yy': 695, 'zb': 699, \"'r\": 19, 'kd': 320, 'kf': 322, 'kg': 323, 'kp': 332, 'y-': 670, 'tj': 553, 'gb': 214, 'gd': 216, 'gw': 235, \"'i\": 11, 'sz': 541, 'kv': 337, 'xq': 659, 'fk': 195, 'sv': 537, 'vs': 611, 'wm': 630, 'tb': 545, 'td': 547, 'hg': 247, 'uw': 593, 'wd': 622, 'zl': 708, 'cv': 124, 'db': 131, 'df': 135, 'dp': 145, 'vu': 613, \"'o\": 17, 'zr': 714, 'nl': 409, 'jy': 314, \"z'\": 697, 's-': 515, 'gq': 229, 'jr': 308, 'hp': 256, 'vy': 615, 'zd': 701, 'zn': 710, 'lj': 352, 'fb': 186, 'xb': 646, 'kj': 326, '-r': 42, '-v': 46, 'bz': 101, 'tg': 550, 'sj': 525, 'gj': 222, 'wc': 621, 'xw': 665, 'xx': 666, 'yf': 676, 'jd': 297, 'zk': 707, 'tp': 559, 'fc': 187, '-d': 28, 'uu': 591, '-u': 45, 'py': 476, 'zw': 719, 'pb': 456, 'pj': 464, 'pw': 475, \"q'\": 478, \"'v\": 23, 'jk': 303, 'pd': 458, 'pm': 467, 'gx': 236, 'zp': 712, 'bw': 99, 'g-': 212, '-e': 29, 'wg': 625, 'zf': 703, 'vl': 607, 'cw': 125, 'hv': 262, 'vc': 599, 'zs': 715, 'mj': 380, 'xh': 652, '-j': 34, 'vv': 614, 'xv': 664, 'mz': 395, 'bf': 83, 'hq': 257, 'dq': 146, 'lx': 366, '-h': 32, 'vj': 605, 'x-': 644, 'xn': 656, 'xp': 658, 'jv': 312, 'zg': 704, '-y': 48, 'fj': 194, 'jt': 310, 'w-': 618, 'xg': 651, 'xm': 655, 'tx': 566, 'gz': 238, 'gv': 234, 'jj': 302, 'wv': 639, \"'l\": 14, 'hj': 250, 'fp': 200, 'js': 309, 'vh': 603, 'wz': 642, 'u-': 570, 'i': 265, 'qb': 480, 'qg': 481, 'zv': 718, 'jf': 299, 'jh': 300, 'jc': 296, 'wp': 633, 'bv': 98, 'pz': 477, 'fq': 201, 'cb': 104, 'cf': 108, 'cg': 109, 'cp': 118, \"j'\": 294, 'xz': 668, 'cj': 112, 'fx': 208, \"'j\": 12, 'fz': 210, 'qv': 485, 'wq': 634, 'ww': 640, 'xr': 660, 'xd': 648, 'o': 424, \"'g\": 9, \"'k\": 13, 'vk': 606, 'qo': 483, 'vz': 616, 'jm': 305, 'yj': 680, 'p-': 454, 'fv': 206, 'bg': 84, \"'u\": 22, 'jw': 313, 'sx': 539, 'm-': 370, 'yq': 687}\n",
      "718 {\"'c\": 6, 'ca': 103, 'au': 71, 'us': 589, 'se': 520, 'co': 117, 'ou': 447, 'ur': 588, 'rs': 506, \"'e\": 8, 'em': 169, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 484, 'uo': 585, 'ot': 446, 'te': 548, \"'t\": 21, 'ti': 552, 'is': 286, '--': 24, '-n': 38, 'ny': 422, 'ya': 671, '-b': 26, 'ba': 78, 'be': 82, 'el': 168, 'la': 343, 'as': 69, '-c': 27, 'om': 439, 'mp': 386, 'pe': 459, 'en': 170, 'ng': 404, '-k': 35, 'ka': 317, 'an': 64, '-l': 36, 'ah': 58, 'le': 347, 'gg': 219, 'ga': 213, '-m': 37, 'ma': 371, 'ha': 241, 'si': 524, 'sw': 538, 'wa': 619, '-s': 43, 'ed': 160, 'da': 130, 'ar': 68, 'ra': 488, 'ep': 172, 'pi': 463, 'it': 287, '-t': 44, 'ta': 544, 'ak': 61, '-w': 47, 'at': 70, \"a'\": 49, \"'b\": 5, \"'d\": 7, 'du': 150, \"'h\": 10, 'ad': 54, 'aa': 51, 'ab': 52, 'er': 174, 'rg': 494, 'ac': 53, 'ch': 110, 'he': 245, 'ke': 321, 'al': 62, 'ls': 361, 'et': 176, 'th': 551, 'am': 63, 'mo': 385, 'od': 430, 'dt': 149, 'nc': 400, 'or': 444, 'rd': 491, 'de': 134, 'dv': 151, 'va': 598, 'rk': 498, 'ro': 502, 'on': 440, \"n'\": 396, \"'s\": 20, 'ns': 416, 'so': 530, 'rt': 507, \"b'\": 77, \"'a\": 4, 'a-': 50, '-a': 25, 'ck': 113, 'cu': 123, 'di': 138, 'ia': 268, 'ik': 278, 'h-': 240, 'ai': 59, 'im': 280, 'na': 398, 'ir': 285, 'kt': 335, 'in': 281, 'ku': 336, 'lk': 353, 'ki': 325, 'lo': 357, 'ne': 402, 'os': 445, 'nd': 401, 'do': 144, 'ni': 406, 'nm': 410, 'me': 375, 'nt': 417, 'ts': 561, 'gn': 226, 'to': 558, 'r-': 487, 'rc': 490, 're': 492, 'sc': 518, 'sh': 523, 'es': 175, 'bb': 79, 'ie': 272, 'll': 354, 'nh': 405, 'tt': 562, 'ev': 178, 'vi': 604, 'il': 279, 'ey': 181, \"y'\": 669, 'bi': 86, 'bo': 92, \"t'\": 542, 'ud': 574, 'br': 94, 'io': 282, 'bs': 95, 'by': 100, 'bc': 80, 'ek': 167, 'bd': 81, 'ic': 270, 'dn': 143, 'no': 412, 'ol': 438, 'mi': 379, 'uc': 573, 'ct': 122, 'ee': 161, 'uk': 581, 'ks': 334, 'ul': 582, 'az': 76, 'zi': 706, 'iz': 293, 'ln': 356, 'ow': 449, 'dr': 147, 'cr': 120, 'mb': 372, 'rl': 499, 'rm': 500, 'rr': 505, 'ce': 107, 'yt': 690, 'bh': 85, 'ho': 255, 'id': 271, 'ig': 274, 'li': 351, 'ty': 567, 'gt': 232, 'og': 433, 'ge': 217, 'sf': 521, 'fe': 189, 'sa': 516, 'ib': 269, 'tu': 563, 'ri': 496, 'tz': 568, 'bj': 87, 'ja': 295, 'je': 298, 'ec': 159, 'bk': 88, 'kh': 324, 'bl': 89, 'ut': 590, 'ze': 702, 'e-': 156, 'ps': 472, 'st': 535, 'oo': 441, 'lu': 363, 'ly': 367, 'bn': 91, 'nu': 418, \"o'\": 425, 'oa': 427, 'oi': 435, 'hi': 249, 'sm': 528, 'gi': 221, 'rn': 501, 'if': 273, 'fa': 185, 'ci': 111, 'iv': 289, 've': 601, 'uh': 578, 'un': 584, 'ds': 148, 'ov': 448, \"e'\": 155, 'bp': 93, 'pl': 466, 'lp': 358, 'ms': 389, 'mc': 373, 'cz': 127, 'zy': 720, 'yk': 681, 'wi': 627, 'ea': 157, 'eg': 163, 'go': 227, 'dg': 136, 'ko': 331, 'ru': 508, 'up': 586, 'pt': 473, 'tl': 555, 'tn': 557, 'ss': 534, 'yn': 684, 'uz': 596, 'zz': 721, 'zo': 711, 'ei': 165, 'lv': 364, 'rb': 489, 'rp': 503, 'tr': 560, 'su': 536, 'dl': 141, 'bt': 96, 'bu': 97, 'ua': 571, 'hm': 253, 'dz': 154, 'lh': 350, 'ay': 75, 'ys': 689, 'cs': 121, 'my': 394, 'po': 469, 'ap': 66, 'pu': 474, 'lc': 345, 'aw': 73, 'cc': 105, 'ry': 512, 't-': 543, '-p': 40, 'pr': 471, 'cl': 114, 'mm': 383, 'pa': 455, 'yi': 679, \"r'\": 486, 'gl': 224, 'cy': 126, \"s'\": 514, 'ue': 575, 'ui': 579, 'um': 583, 'mu': 391, 'op': 442, 'ph': 462, 'yl': 682, 'eb': 158, 'nb': 399, 'ht': 260, 'hy': 264, 'fi': 193, 'fy': 209, 'kl': 328, 'km': 329, 'kn': 330, 'wl': 629, 'gm': 225, 'kr': 333, 'oy': 451, 'yd': 674, \"d'\": 128, 'cm': 115, 'cn': 116, 'oc': 429, 'of': 432, 'ff': 190, 'cq': 119, 'av': 72, 'ag': 57, 'ob': 428, 'ym': 683, 'tm': 556, 'uf': 576, 'gk': 223, 'sy': 540, 'yc': 673, 'iu': 288, \"m'\": 369, 'za': 698, 'sk': 526, 'wn': 631, 'ex': 180, 'dc': 132, 'dd': 133, 'eo': 171, 'dw': 152, 'we': 623, 'dy': 153, 'eh': 164, 'lb': 344, \"l'\": 341, 'lm': 355, 'nn': 411, 'sb': 517, 'eq': 173, 'rh': 495, 'ld': 346, 'lt': 362, 'dh': 137, 'eu': 177, 'gu': 233, 'kk': 327, 'hu': 261, 'lf': 348, 'ip': 283, 'aj': 60, 'ok': 437, 'oh': 434, 'iw': 290, 'gs': 231, 'dj': 139, 'jo': 307, 'ju': 311, 'dk': 140, 'dm': 142, \"f'\": 183, 'fo': 199, 'hs': 259, '-f': 30, 'ae': 55, 'sd': 519, \"g'\": 211, 'vo': 609, 'gr': 230, 'ft': 204, 'oq': 443, \"h'\": 239, 'sp': 531, 'hl': 252, \"p'\": 453, 'af': 56, 'ye': 675, 'fd': 188, 'ix': 291, 'xe': 649, 'fl': 196, 'fr': 202, 'ax': 74, 'fg': 191, 'gh': 220, \"i'\": 266, 'fh': 192, 'ox': 450, 'xi': 653, 'fm': 197, 'fs': 203, 'ef': 162, 'ug': 577, 'rw': 510, 'fw': 207, 'mn': 384, 'gy': 237, 'gc': 215, 'gf': 218, 'ih': 275, 'ew': 179, 'o-': 426, '-g': 31, 'xc': 647, 'fu': 205, 'pp': 470, 'hh': 248, 'hk': 251, 'lg': 349, 'lq': 359, 'ub': 572, 'hn': 254, 'hr': 258, 'hw': 263, 'ml': 382, 'sl': 527, 'wo': 632, \"c'\": 102, 'rf': 493, 'nk': 408, 'tc': 546, \"x'\": 643, 'ji': 301, 'jn': 306, 'uj': 580, 'k-': 316, 'kb': 318, 'kc': 319, 'iy': 292, 'kw': 338, 'ky': 339, 'kz': 340, 'l-': 342, '-q': 41, 'tv': 564, 'uq': 587, 'rq': 504, 'ej': 166, 'xa': 645, 'xo': 657, 'xy': 667, 'oj': 436, 'nq': 414, 'nz': 423, 'f-': 184, 'ij': 277, 'iq': 284, '-i': 33, 'n-': 397, '-o': 39, 'nw': 420, 'yw': 693, 'lr': 360, 'uv': 592, 'lw': 365, 'ez': 182, 'mq': 387, 'fn': 198, \"k'\": 315, 'tf': 549, 'zh': 705, 'wy': 641, 'lz': 368, 'np': 413, 'xt': 662, 'md': 374, 'zc': 700, 'zq': 713, 'mf': 376, 'mg': 377, 'mh': 378, 'yv': 692, 'mk': 381, 'oe': 431, 'pc': 457, 'mr': 388, 'mt': 390, 'mv': 392, 'mw': 393, 'yo': 685, 'yx': 694, 'yz': 696, 'sq': 532, 'nv': 419, 'oz': 452, 'ws': 636, 'cd': 106, 'nf': 403, 'gp': 228, 'nj': 407, 'jl': 304, 'nr': 415, 'sg': 522, 'i-': 267, 'bm': 90, 'tk': 554, \"u'\": 569, 'tw': 565, 'nx': 421, 'yb': 672, 'yh': 678, 'yp': 686, 'wh': 626, 'ao': 65, 'pf': 460, 'pg': 461, 'pk': 465, 'aq': 67, 'qa': 479, 'qi': 482, 'rv': 509, 'ux': 594, 'hb': 242, 'hd': 244, 'zm': 709, 'rj': 497, 'wr': 635, 'd-': 129, \"w'\": 617, 'zt': 716, 'zu': 717, 'rx': 511, 'rz': 513, 'ii': 276, 'xu': 663, 'hc': 243, 'hf': 246, \"v'\": 597, 'sn': 529, 'pn': 468, 'sr': 533, 'yr': 688, 'yu': 691, 'uy': 595, 'vd': 600, 'vg': 602, 'vn': 608, 'vr': 610, 'vt': 612, 'wb': 620, 'wf': 624, 'wk': 628, 'wt': 637, 'wu': 638, 'xf': 650, 'xl': 654, 'xs': 661, 'yg': 677, 'yy': 695, 'zb': 699, \"'r\": 19, 'kd': 320, 'kf': 322, 'kg': 323, 'kp': 332, 'y-': 670, 'tj': 553, 'gb': 214, 'gd': 216, 'gw': 235, \"'i\": 11, 'sz': 541, 'kv': 337, 'xq': 659, 'fk': 195, 'sv': 537, 'vs': 611, 'wm': 630, 'tb': 545, 'td': 547, 'hg': 247, 'uw': 593, 'wd': 622, 'zl': 708, 'cv': 124, 'db': 131, 'df': 135, 'dp': 145, 'vu': 613, \"'o\": 17, 'zr': 714, 'nl': 409, 'jy': 314, \"z'\": 697, 's-': 515, 'gq': 229, 'jr': 308, 'hp': 256, 'vy': 615, 'zd': 701, 'zn': 710, 'lj': 352, 'fb': 186, 'xb': 646, 'kj': 326, '-r': 42, '-v': 46, 'bz': 101, 'tg': 550, 'sj': 525, 'gj': 222, 'wc': 621, 'xw': 665, 'xx': 666, 'yf': 676, 'jd': 297, 'zk': 707, 'tp': 559, 'fc': 187, '-d': 28, 'uu': 591, '-u': 45, 'py': 476, 'zw': 719, 'pb': 456, 'pj': 464, 'pw': 475, \"q'\": 478, \"'v\": 23, 'jk': 303, 'pd': 458, 'pm': 467, 'gx': 236, 'zp': 712, 'bw': 99, 'g-': 212, '-e': 29, 'wg': 625, 'zf': 703, 'vl': 607, 'cw': 125, 'hv': 262, 'vc': 599, 'zs': 715, 'mj': 380, 'xh': 652, '-j': 34, 'vv': 614, 'xv': 664, 'mz': 395, 'bf': 83, 'hq': 257, 'dq': 146, 'lx': 366, '-h': 32, 'vj': 605, 'x-': 644, 'xn': 656, 'xp': 658, 'jv': 312, 'zg': 704, '-y': 48, 'fj': 194, 'jt': 310, 'w-': 618, 'xg': 651, 'xm': 655, 'tx': 566, 'gz': 238, 'gv': 234, 'jj': 302, 'wv': 639, \"'l\": 14, 'hj': 250, 'fp': 200, 'js': 309, 'vh': 603, 'wz': 642, 'u-': 570, 'i': 265, 'qb': 480, 'qg': 481, 'zv': 718, 'jf': 299, 'jh': 300, 'jc': 296, 'wp': 633, 'bv': 98, 'pz': 477, 'fq': 201, 'cb': 104, 'cf': 108, 'cg': 109, 'cp': 118, \"j'\": 294, 'xz': 668, 'cj': 112, 'fx': 208, \"'j\": 12, 'fz': 210, 'qv': 485, 'wq': 634, 'ww': 640, 'xr': 660, 'xd': 648, 'o': 424, \"'g\": 9, \"'k\": 13, 'vk': 606, 'qo': 483, 'vz': 616, 'jm': 305, 'yj': 680, 'p-': 454, 'fv': 206, 'bg': 84, \"'u\": 22, 'jw': 313, 'sx': 539, 'm-': 370, 'yq': 687}\n",
      "718 {\"'c\": 6, 'ca': 103, 'au': 71, 'us': 589, 'se': 520, 'co': 117, 'ou': 447, 'ur': 588, 'rs': 506, \"'e\": 8, 'em': 169, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 484, 'uo': 585, 'ot': 446, 'te': 548, \"'t\": 21, 'ti': 552, 'is': 286, '--': 24, '-n': 38, 'ny': 422, 'ya': 671, '-b': 26, 'ba': 78, 'be': 82, 'el': 168, 'la': 343, 'as': 69, '-c': 27, 'om': 439, 'mp': 386, 'pe': 459, 'en': 170, 'ng': 404, '-k': 35, 'ka': 317, 'an': 64, '-l': 36, 'ah': 58, 'le': 347, 'gg': 219, 'ga': 213, '-m': 37, 'ma': 371, 'ha': 241, 'si': 524, 'sw': 538, 'wa': 619, '-s': 43, 'ed': 160, 'da': 130, 'ar': 68, 'ra': 488, 'ep': 172, 'pi': 463, 'it': 287, '-t': 44, 'ta': 544, 'ak': 61, '-w': 47, 'at': 70, \"a'\": 49, \"'b\": 5, \"'d\": 7, 'du': 150, \"'h\": 10, 'ad': 54, 'aa': 51, 'ab': 52, 'er': 174, 'rg': 494, 'ac': 53, 'ch': 110, 'he': 245, 'ke': 321, 'al': 62, 'ls': 361, 'et': 176, 'th': 551, 'am': 63, 'mo': 385, 'od': 430, 'dt': 149, 'nc': 400, 'or': 444, 'rd': 491, 'de': 134, 'dv': 151, 'va': 598, 'rk': 498, 'ro': 502, 'on': 440, \"n'\": 396, \"'s\": 20, 'ns': 416, 'so': 530, 'rt': 507, \"b'\": 77, \"'a\": 4, 'a-': 50, '-a': 25, 'ck': 113, 'cu': 123, 'di': 138, 'ia': 268, 'ik': 278, 'h-': 240, 'ai': 59, 'im': 280, 'na': 398, 'ir': 285, 'kt': 335, 'in': 281, 'ku': 336, 'lk': 353, 'ki': 325, 'lo': 357, 'ne': 402, 'os': 445, 'nd': 401, 'do': 144, 'ni': 406, 'nm': 410, 'me': 375, 'nt': 417, 'ts': 561, 'gn': 226, 'to': 558, 'r-': 487, 'rc': 490, 're': 492, 'sc': 518, 'sh': 523, 'es': 175, 'bb': 79, 'ie': 272, 'll': 354, 'nh': 405, 'tt': 562, 'ev': 178, 'vi': 604, 'il': 279, 'ey': 181, \"y'\": 669, 'bi': 86, 'bo': 92, \"t'\": 542, 'ud': 574, 'br': 94, 'io': 282, 'bs': 95, 'by': 100, 'bc': 80, 'ek': 167, 'bd': 81, 'ic': 270, 'dn': 143, 'no': 412, 'ol': 438, 'mi': 379, 'uc': 573, 'ct': 122, 'ee': 161, 'uk': 581, 'ks': 334, 'ul': 582, 'az': 76, 'zi': 706, 'iz': 293, 'ln': 356, 'ow': 449, 'dr': 147, 'cr': 120, 'mb': 372, 'rl': 499, 'rm': 500, 'rr': 505, 'ce': 107, 'yt': 690, 'bh': 85, 'ho': 255, 'id': 271, 'ig': 274, 'li': 351, 'ty': 567, 'gt': 232, 'og': 433, 'ge': 217, 'sf': 521, 'fe': 189, 'sa': 516, 'ib': 269, 'tu': 563, 'ri': 496, 'tz': 568, 'bj': 87, 'ja': 295, 'je': 298, 'ec': 159, 'bk': 88, 'kh': 324, 'bl': 89, 'ut': 590, 'ze': 702, 'e-': 156, 'ps': 472, 'st': 535, 'oo': 441, 'lu': 363, 'ly': 367, 'bn': 91, 'nu': 418, \"o'\": 425, 'oa': 427, 'oi': 435, 'hi': 249, 'sm': 528, 'gi': 221, 'rn': 501, 'if': 273, 'fa': 185, 'ci': 111, 'iv': 289, 've': 601, 'uh': 578, 'un': 584, 'ds': 148, 'ov': 448, \"e'\": 155, 'bp': 93, 'pl': 466, 'lp': 358, 'ms': 389, 'mc': 373, 'cz': 127, 'zy': 720, 'yk': 681, 'wi': 627, 'ea': 157, 'eg': 163, 'go': 227, 'dg': 136, 'ko': 331, 'ru': 508, 'up': 586, 'pt': 473, 'tl': 555, 'tn': 557, 'ss': 534, 'yn': 684, 'uz': 596, 'zz': 721, 'zo': 711, 'ei': 165, 'lv': 364, 'rb': 489, 'rp': 503, 'tr': 560, 'su': 536, 'dl': 141, 'bt': 96, 'bu': 97, 'ua': 571, 'hm': 253, 'dz': 154, 'lh': 350, 'ay': 75, 'ys': 689, 'cs': 121, 'my': 394, 'po': 469, 'ap': 66, 'pu': 474, 'lc': 345, 'aw': 73, 'cc': 105, 'ry': 512, 't-': 543, '-p': 40, 'pr': 471, 'cl': 114, 'mm': 383, 'pa': 455, 'yi': 679, \"r'\": 486, 'gl': 224, 'cy': 126, \"s'\": 514, 'ue': 575, 'ui': 579, 'um': 583, 'mu': 391, 'op': 442, 'ph': 462, 'yl': 682, 'eb': 158, 'nb': 399, 'ht': 260, 'hy': 264, 'fi': 193, 'fy': 209, 'kl': 328, 'km': 329, 'kn': 330, 'wl': 629, 'gm': 225, 'kr': 333, 'oy': 451, 'yd': 674, \"d'\": 128, 'cm': 115, 'cn': 116, 'oc': 429, 'of': 432, 'ff': 190, 'cq': 119, 'av': 72, 'ag': 57, 'ob': 428, 'ym': 683, 'tm': 556, 'uf': 576, 'gk': 223, 'sy': 540, 'yc': 673, 'iu': 288, \"m'\": 369, 'za': 698, 'sk': 526, 'wn': 631, 'ex': 180, 'dc': 132, 'dd': 133, 'eo': 171, 'dw': 152, 'we': 623, 'dy': 153, 'eh': 164, 'lb': 344, \"l'\": 341, 'lm': 355, 'nn': 411, 'sb': 517, 'eq': 173, 'rh': 495, 'ld': 346, 'lt': 362, 'dh': 137, 'eu': 177, 'gu': 233, 'kk': 327, 'hu': 261, 'lf': 348, 'ip': 283, 'aj': 60, 'ok': 437, 'oh': 434, 'iw': 290, 'gs': 231, 'dj': 139, 'jo': 307, 'ju': 311, 'dk': 140, 'dm': 142, \"f'\": 183, 'fo': 199, 'hs': 259, '-f': 30, 'ae': 55, 'sd': 519, \"g'\": 211, 'vo': 609, 'gr': 230, 'ft': 204, 'oq': 443, \"h'\": 239, 'sp': 531, 'hl': 252, \"p'\": 453, 'af': 56, 'ye': 675, 'fd': 188, 'ix': 291, 'xe': 649, 'fl': 196, 'fr': 202, 'ax': 74, 'fg': 191, 'gh': 220, \"i'\": 266, 'fh': 192, 'ox': 450, 'xi': 653, 'fm': 197, 'fs': 203, 'ef': 162, 'ug': 577, 'rw': 510, 'fw': 207, 'mn': 384, 'gy': 237, 'gc': 215, 'gf': 218, 'ih': 275, 'ew': 179, 'o-': 426, '-g': 31, 'xc': 647, 'fu': 205, 'pp': 470, 'hh': 248, 'hk': 251, 'lg': 349, 'lq': 359, 'ub': 572, 'hn': 254, 'hr': 258, 'hw': 263, 'ml': 382, 'sl': 527, 'wo': 632, \"c'\": 102, 'rf': 493, 'nk': 408, 'tc': 546, \"x'\": 643, 'ji': 301, 'jn': 306, 'uj': 580, 'k-': 316, 'kb': 318, 'kc': 319, 'iy': 292, 'kw': 338, 'ky': 339, 'kz': 340, 'l-': 342, '-q': 41, 'tv': 564, 'uq': 587, 'rq': 504, 'ej': 166, 'xa': 645, 'xo': 657, 'xy': 667, 'oj': 436, 'nq': 414, 'nz': 423, 'f-': 184, 'ij': 277, 'iq': 284, '-i': 33, 'n-': 397, '-o': 39, 'nw': 420, 'yw': 693, 'lr': 360, 'uv': 592, 'lw': 365, 'ez': 182, 'mq': 387, 'fn': 198, \"k'\": 315, 'tf': 549, 'zh': 705, 'wy': 641, 'lz': 368, 'np': 413, 'xt': 662, 'md': 374, 'zc': 700, 'zq': 713, 'mf': 376, 'mg': 377, 'mh': 378, 'yv': 692, 'mk': 381, 'oe': 431, 'pc': 457, 'mr': 388, 'mt': 390, 'mv': 392, 'mw': 393, 'yo': 685, 'yx': 694, 'yz': 696, 'sq': 532, 'nv': 419, 'oz': 452, 'ws': 636, 'cd': 106, 'nf': 403, 'gp': 228, 'nj': 407, 'jl': 304, 'nr': 415, 'sg': 522, 'i-': 267, 'bm': 90, 'tk': 554, \"u'\": 569, 'tw': 565, 'nx': 421, 'yb': 672, 'yh': 678, 'yp': 686, 'wh': 626, 'ao': 65, 'pf': 460, 'pg': 461, 'pk': 465, 'aq': 67, 'qa': 479, 'qi': 482, 'rv': 509, 'ux': 594, 'hb': 242, 'hd': 244, 'zm': 709, 'rj': 497, 'wr': 635, 'd-': 129, \"w'\": 617, 'zt': 716, 'zu': 717, 'rx': 511, 'rz': 513, 'ii': 276, 'xu': 663, 'hc': 243, 'hf': 246, \"v'\": 597, 'sn': 529, 'pn': 468, 'sr': 533, 'yr': 688, 'yu': 691, 'uy': 595, 'vd': 600, 'vg': 602, 'vn': 608, 'vr': 610, 'vt': 612, 'wb': 620, 'wf': 624, 'wk': 628, 'wt': 637, 'wu': 638, 'xf': 650, 'xl': 654, 'xs': 661, 'yg': 677, 'yy': 695, 'zb': 699, \"'r\": 19, 'kd': 320, 'kf': 322, 'kg': 323, 'kp': 332, 'y-': 670, 'tj': 553, 'gb': 214, 'gd': 216, 'gw': 235, \"'i\": 11, 'sz': 541, 'kv': 337, 'xq': 659, 'fk': 195, 'sv': 537, 'vs': 611, 'wm': 630, 'tb': 545, 'td': 547, 'hg': 247, 'uw': 593, 'wd': 622, 'zl': 708, 'cv': 124, 'db': 131, 'df': 135, 'dp': 145, 'vu': 613, \"'o\": 17, 'zr': 714, 'nl': 409, 'jy': 314, \"z'\": 697, 's-': 515, 'gq': 229, 'jr': 308, 'hp': 256, 'vy': 615, 'zd': 701, 'zn': 710, 'lj': 352, 'fb': 186, 'xb': 646, 'kj': 326, '-r': 42, '-v': 46, 'bz': 101, 'tg': 550, 'sj': 525, 'gj': 222, 'wc': 621, 'xw': 665, 'xx': 666, 'yf': 676, 'jd': 297, 'zk': 707, 'tp': 559, 'fc': 187, '-d': 28, 'uu': 591, '-u': 45, 'py': 476, 'zw': 719, 'pb': 456, 'pj': 464, 'pw': 475, \"q'\": 478, \"'v\": 23, 'jk': 303, 'pd': 458, 'pm': 467, 'gx': 236, 'zp': 712, 'bw': 99, 'g-': 212, '-e': 29, 'wg': 625, 'zf': 703, 'vl': 607, 'cw': 125, 'hv': 262, 'vc': 599, 'zs': 715, 'mj': 380, 'xh': 652, '-j': 34, 'vv': 614, 'xv': 664, 'mz': 395, 'bf': 83, 'hq': 257, 'dq': 146, 'lx': 366, '-h': 32, 'vj': 605, 'x-': 644, 'xn': 656, 'xp': 658, 'jv': 312, 'zg': 704, '-y': 48, 'fj': 194, 'jt': 310, 'w-': 618, 'xg': 651, 'xm': 655, 'tx': 566, 'gz': 238, 'gv': 234, 'jj': 302, 'wv': 639, \"'l\": 14, 'hj': 250, 'fp': 200, 'js': 309, 'vh': 603, 'wz': 642, 'u-': 570, 'i': 265, 'qb': 480, 'qg': 481, 'zv': 718, 'jf': 299, 'jh': 300, 'jc': 296, 'wp': 633, 'bv': 98, 'pz': 477, 'fq': 201, 'cb': 104, 'cf': 108, 'cg': 109, 'cp': 118, \"j'\": 294, 'xz': 668, 'cj': 112, 'fx': 208, \"'j\": 12, 'fz': 210, 'qv': 485, 'wq': 634, 'ww': 640, 'xr': 660, 'xd': 648, 'o': 424, \"'g\": 9, \"'k\": 13, 'vk': 606, 'qo': 483, 'vz': 616, 'jm': 305, 'yj': 680, 'p-': 454, 'fv': 206, 'bg': 84, \"'u\": 22, 'jw': 313, 'sx': 539, 'm-': 370, 'yq': 687}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'B': 8, 'UW': 31, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'HH': 15, 'G': 14, 'D': 10, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'B': 8, 'UW': 31, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'HH': 15, 'G': 14, 'D': 10, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'B': 8, 'UW': 31, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'HH': 15, 'G': 14, 'D': 10, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 256\n",
      "hidden_size: 128\n",
      "n_layers: 1\n",
      "Encoder has a total number of 333056 parameters\n",
      "Decoder has a total number of 215844 parameters\n",
      "Total number of all parameters is 548900\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 1 finished in 0m 56.48s (- 93m 11.68s) (1 1.0%). train avg loss: 1.1729, val avg loss: 1.1578\n",
      "Training for epoch 2 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 2 finished in 1m 50.94s (- 90m 36.07s) (2 2.0%). train avg loss: 0.5621, val avg loss: 0.9499\n",
      "Training for epoch 3 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 3 finished in 2m 45.39s (- 89m 7.74s) (3 3.0%). train avg loss: 0.5046, val avg loss: 0.9556\n",
      "Training for epoch 4 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 4 finished in 3m 26.35s (- 82m 32.31s) (4 4.0%). train avg loss: 0.477, val avg loss: 0.9457\n",
      "Training for epoch 5 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 5 finished in 4m 21.21s (- 82m 43.05s) (5 5.0%). train avg loss: 0.4157, val avg loss: 0.9381\n",
      "Training for epoch 6 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 6 finished in 5m 10.17s (- 80m 59.4s) (6 6.0%). train avg loss: 0.411, val avg loss: 0.8624\n",
      "Training for epoch 7 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 7 finished in 5m 59.7s (- 79m 38.84s) (7 7.0%). train avg loss: 0.3868, val avg loss: 0.8517\n",
      "Training for epoch 8 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 8 finished in 6m 45.3s (- 77m 40.97s) (8 8.0%). train avg loss: 0.3881, val avg loss: 0.8242\n",
      "Training for epoch 9 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 9 finished in 7m 33.65s (- 76m 26.9s) (9 9.0%). train avg loss: 0.3603, val avg loss: 0.8463\n",
      "Training for epoch 10 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 10 finished in 8m 22.43s (- 75m 21.83s) (10 10.0%). train avg loss: 0.3421, val avg loss: 0.8235\n",
      "Training for epoch 11 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 11 finished in 9m 13.39s (- 74m 37.39s) (11 11.0%). train avg loss: 0.3375, val avg loss: 0.8236\n",
      "Training for epoch 12 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 12 finished in 10m 4.18s (- 73m 50.69s) (12 12.0%). train avg loss: 0.3567, val avg loss: 0.8426\n",
      "Training for epoch 13 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 13 finished in 10m 52.1s (- 72m 44.06s) (13 13.0%). train avg loss: 0.3388, val avg loss: 0.8281\n",
      "Training for epoch 14 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 14 finished in 11m 41.16s (- 71m 47.11s) (14 14.0%). train avg loss: 0.3258, val avg loss: 0.7978\n",
      "Training for epoch 15 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 15 finished in 12m 28.86s (- 70m 43.57s) (15 15.0%). train avg loss: 0.3062, val avg loss: 0.7627\n",
      "Training for epoch 16 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 16 finished in 13m 14.65s (- 69m 31.92s) (16 16.0%). train avg loss: 0.2996, val avg loss: 0.8731\n",
      "Training for epoch 17 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 17 finished in 14m 0.75s (- 68m 24.85s) (17 17.0%). train avg loss: 0.3146, val avg loss: 0.734\n",
      "Training for epoch 18 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 18 finished in 14m 48.88s (- 67m 29.34s) (18 18.0%). train avg loss: 0.3029, val avg loss: 0.7763\n",
      "Training for epoch 19 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 19 finished in 15m 32.39s (- 66m 14.91s) (19 19.0%). train avg loss: 0.3045, val avg loss: 0.7546\n",
      "Training for epoch 20 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 20 finished in 16m 19.39s (- 65m 17.57s) (20 20.0%). train avg loss: 0.2932, val avg loss: 0.7847\n",
      "Training for epoch 21 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 21 finished in 17m 4.43s (- 64m 13.81s) (21 21.0%). train avg loss: 0.2808, val avg loss: 0.7456\n",
      "Training for epoch 22 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 22 finished in 17m 49.95s (- 63m 13.46s) (22 22.0%). train avg loss: 0.2996, val avg loss: 0.7411\n",
      "Training for epoch 23 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 23 finished in 18m 36.65s (- 62m 18.34s) (23 23.0%). train avg loss: 0.2885, val avg loss: 0.7151\n",
      "Training for epoch 24 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 24 finished in 19m 26.2s (- 61m 32.97s) (24 24.0%). train avg loss: 0.2919, val avg loss: 0.7754\n",
      "Training for epoch 25 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 25 finished in 20m 13.23s (- 60m 39.68s) (25 25.0%). train avg loss: 0.27, val avg loss: 0.7766\n",
      "Training for epoch 26 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 26 finished in 20m 58.12s (- 59m 40.8s) (26 26.0%). train avg loss: 0.2772, val avg loss: 0.7089\n",
      "Training for epoch 27 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 27 finished in 21m 38.51s (- 58m 30.79s) (27 27.0%). train avg loss: 0.2769, val avg loss: 0.7257\n",
      "Training for epoch 28 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 28 finished in 22m 20.68s (- 57m 27.46s) (28 28.0%). train avg loss: 0.2971, val avg loss: 0.7748\n",
      "Training for epoch 29 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 29 finished in 23m 2.82s (- 56m 25.52s) (29 29.0%). train avg loss: 0.2751, val avg loss: 0.7012\n",
      "Training for epoch 30 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 30 finished in 23m 49.04s (- 55m 34.43s) (30 30.0%). train avg loss: 0.271, val avg loss: 0.6981\n",
      "Training for epoch 31 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 31 finished in 24m 33.65s (- 54m 40.06s) (31 31.0%). train avg loss: 0.269, val avg loss: 0.7298\n",
      "Training for epoch 32 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 32 finished in 25m 18.27s (- 53m 46.33s) (32 32.0%). train avg loss: 0.2877, val avg loss: 0.7025\n",
      "Training for epoch 33 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 33 finished in 26m 3.65s (- 52m 54.67s) (33 33.0%). train avg loss: 0.2462, val avg loss: 0.6379\n",
      "Training for epoch 34 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 34 finished in 26m 47.89s (- 52m 1.21s) (34 34.0%). train avg loss: 0.2515, val avg loss: 0.6701\n",
      "Training for epoch 35 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 35 finished in 27m 33.98s (- 51m 11.68s) (35 35.0%). train avg loss: 0.261, val avg loss: 0.6789\n",
      "Training for epoch 36 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 36 finished in 28m 19.09s (- 50m 20.61s) (36 36.0%). train avg loss: 0.2588, val avg loss: 0.6662\n",
      "Training for epoch 37 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 37 finished in 28m 59.7s (- 49m 22.19s) (37 37.0%). train avg loss: 0.2437, val avg loss: 0.6984\n",
      "Training for epoch 38 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 38 finished in 29m 45.79s (- 48m 33.66s) (38 38.0%). train avg loss: 0.2663, val avg loss: 0.804\n",
      "Training for epoch 39 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 39 finished in 30m 32.08s (- 47m 45.56s) (39 39.0%). train avg loss: 0.2573, val avg loss: 0.7233\n",
      "Training for epoch 40 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 40 finished in 31m 16.1s (- 46m 54.14s) (40 40.0%). train avg loss: 0.2497, val avg loss: 0.6852\n",
      "Training for epoch 41 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 41 finished in 32m 1.72s (- 46m 5.4s) (41 41.0%). train avg loss: 0.2442, val avg loss: 0.693\n",
      "Training for epoch 42 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 42 finished in 32m 47.76s (- 45m 17.38s) (42 42.0%). train avg loss: 0.2373, val avg loss: 0.6923\n",
      "Training for epoch 43 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 43 finished in 33m 33.61s (- 44m 29.21s) (43 43.0%). train avg loss: 0.2594, val avg loss: 0.7481\n",
      "Training for epoch 44 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 44 finished in 34m 18.0s (- 43m 39.27s) (44 44.0%). train avg loss: 0.2515, val avg loss: 0.6929\n",
      "Training for epoch 45 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 45 finished in 35m 7.42s (- 42m 55.73s) (45 45.0%). train avg loss: 0.2144, val avg loss: 0.6418\n",
      "Training for epoch 46 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 46 finished in 35m 54.77s (- 42m 9.51s) (46 46.0%). train avg loss: 0.2012, val avg loss: 0.6058\n",
      "Training for epoch 47 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 47 finished in 36m 37.86s (- 41m 18.44s) (47 47.0%). train avg loss: 0.1919, val avg loss: 0.5959\n",
      "Training for epoch 48 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 48 finished in 37m 23.3s (- 40m 30.24s) (48 48.0%). train avg loss: 0.1843, val avg loss: 0.5712\n",
      "Training for epoch 49 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 49 finished in 38m 10.98s (- 39m 44.49s) (49 49.0%). train avg loss: 0.1886, val avg loss: 0.5938\n",
      "Training for epoch 50 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 50 finished in 38m 58.75s (- 38m 58.75s) (50 50.0%). train avg loss: 0.1781, val avg loss: 0.5605\n",
      "Training for epoch 51 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 51 finished in 39m 45.19s (- 38m 11.65s) (51 51.0%). train avg loss: 0.1722, val avg loss: 0.5732\n",
      "Training for epoch 52 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 52 finished in 40m 30.44s (- 37m 23.48s) (52 52.0%). train avg loss: 0.1691, val avg loss: 0.5734\n",
      "Training for epoch 53 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 53 finished in 41m 16.63s (- 36m 36.26s) (53 53.0%). train avg loss: 0.1696, val avg loss: 0.5779\n",
      "Training for epoch 54 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 54 finished in 42m 5.06s (- 35m 50.98s) (54 54.0%). train avg loss: 0.1731, val avg loss: 0.5595\n",
      "Training for epoch 55 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 55 finished in 42m 48.42s (- 35m 1.43s) (55 55.0%). train avg loss: 0.1719, val avg loss: 0.566\n",
      "Training for epoch 56 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 56 finished in 43m 29.2s (- 34m 10.09s) (56 56.0%). train avg loss: 0.169, val avg loss: 0.5539\n",
      "Training for epoch 57 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 57 finished in 44m 12.93s (- 33m 21.34s) (57 57.0%). train avg loss: 0.1582, val avg loss: 0.53\n",
      "Training for epoch 58 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 58 finished in 44m 56.61s (- 32m 32.71s) (58 58.0%). train avg loss: 0.1605, val avg loss: 0.5652\n",
      "Training for epoch 59 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 59 finished in 45m 39.17s (- 31m 43.49s) (59 59.0%). train avg loss: 0.1537, val avg loss: 0.5719\n",
      "Training for epoch 60 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 60 finished in 46m 21.25s (- 30m 54.17s) (60 60.0%). train avg loss: 0.1517, val avg loss: 0.5789\n",
      "Training for epoch 61 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 61 finished in 47m 2.07s (- 30m 4.28s) (61 61.0%). train avg loss: 0.1527, val avg loss: 0.5574\n",
      "Training for epoch 62 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 62 finished in 47m 43.57s (- 29m 15.09s) (62 62.0%). train avg loss: 0.1472, val avg loss: 0.5685\n",
      "Training for epoch 63 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 63 finished in 48m 24.72s (- 28m 25.94s) (63 63.0%). train avg loss: 0.1464, val avg loss: 0.555\n",
      "Training for epoch 64 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 64 finished in 49m 6.32s (- 27m 37.31s) (64 64.0%). train avg loss: 0.1566, val avg loss: 0.5659\n",
      "Training for epoch 65 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 65 finished in 49m 46.66s (- 26m 48.2s) (65 65.0%). train avg loss: 0.1557, val avg loss: 0.5594\n",
      "Training for epoch 66 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 66 finished in 50m 27.28s (- 25m 59.51s) (66 66.0%). train avg loss: 0.144, val avg loss: 0.589\n",
      "Training for epoch 67 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 67 finished in 51m 8.06s (- 25m 11.14s) (67 67.0%). train avg loss: 0.1419, val avg loss: 0.5556\n",
      "Training for epoch 68 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 68 finished in 51m 49.5s (- 24m 23.29s) (68 68.0%). train avg loss: 0.1469, val avg loss: 0.5544\n",
      "Training for epoch 69 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 69 finished in 52m 29.96s (- 23m 35.2s) (69 69.0%). train avg loss: 0.1326, val avg loss: 0.5391\n",
      "Training for epoch 70 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 70 finished in 53m 11.67s (- 22m 47.86s) (70 70.0%). train avg loss: 0.1255, val avg loss: 0.5176\n",
      "Training for epoch 71 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 71 finished in 53m 52.85s (- 22m 0.46s) (71 71.0%). train avg loss: 0.1252, val avg loss: 0.5352\n",
      "Training for epoch 72 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 72 finished in 54m 36.14s (- 21m 14.06s) (72 72.0%). train avg loss: 0.1266, val avg loss: 0.5263\n",
      "Training for epoch 73 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 73 finished in 55m 19.91s (- 20m 27.91s) (73 73.0%). train avg loss: 0.1232, val avg loss: 0.5305\n",
      "Training for epoch 74 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 74 finished in 56m 5.02s (- 19m 42.3s) (74 74.0%). train avg loss: 0.1205, val avg loss: 0.551\n",
      "Training for epoch 75 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 75 finished in 56m 52.3s (- 18m 57.43s) (75 75.0%). train avg loss: 0.1175, val avg loss: 0.5457\n",
      "Training for epoch 76 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 76 finished in 57m 41.42s (- 18m 13.08s) (76 76.0%). train avg loss: 0.1208, val avg loss: 0.5411\n",
      "Training for epoch 77 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 77 finished in 58m 25.48s (- 17m 27.09s) (77 77.0%). train avg loss: 0.1179, val avg loss: 0.5718\n",
      "Training for epoch 78 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 78 finished in 59m 10.97s (- 16m 41.55s) (78 78.0%). train avg loss: 0.1169, val avg loss: 0.5394\n",
      "Training for epoch 79 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 79 finished in 59m 56.08s (- 15m 55.92s) (79 79.0%). train avg loss: 0.1149, val avg loss: 0.5292\n",
      "Training for epoch 80 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 80 finished in 60m 39.48s (- 15m 9.87s) (80 80.0%). train avg loss: 0.1153, val avg loss: 0.5585\n",
      "Training for epoch 81 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 81 finished in 61m 20.17s (- 14m 23.25s) (81 81.0%). train avg loss: 0.1137, val avg loss: 0.5311\n",
      "Training for epoch 82 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 82 finished in 62m 1.26s (- 13m 36.86s) (82 82.0%). train avg loss: 0.1062, val avg loss: 0.5372\n",
      "Training for epoch 83 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 83 finished in 62m 43.9s (- 12m 50.92s) (83 83.0%). train avg loss: 0.1046, val avg loss: 0.5489\n",
      "Training for epoch 84 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 84 finished in 63m 25.81s (- 12m 4.92s) (84 84.0%). train avg loss: 0.1042, val avg loss: 0.5452\n",
      "Training for epoch 85 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 85 finished in 64m 8.76s (- 11m 19.19s) (85 85.0%). train avg loss: 0.1032, val avg loss: 0.5414\n",
      "Training for epoch 86 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 86 finished in 64m 51.02s (- 10m 33.42s) (86 86.0%). train avg loss: 0.1007, val avg loss: 0.5498\n",
      "Training for epoch 87 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 87 finished in 65m 34.97s (- 9m 47.98s) (87 87.0%). train avg loss: 0.101, val avg loss: 0.5636\n",
      "Training for epoch 88 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 88 finished in 66m 19.35s (- 9m 2.64s) (88 88.0%). train avg loss: 0.1023, val avg loss: 0.536\n",
      "Training for epoch 89 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 89 finished in 67m 5.18s (- 8m 17.49s) (89 89.0%). train avg loss: 0.101, val avg loss: 0.5439\n",
      "Training for epoch 90 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 90 finished in 67m 49.16s (- 7m 32.13s) (90 90.0%). train avg loss: 0.0991, val avg loss: 0.5691\n",
      "Training for epoch 91 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 91 finished in 68m 32.33s (- 6m 46.71s) (91 91.0%). train avg loss: 0.0999, val avg loss: 0.5535\n",
      "Training for epoch 92 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 92 finished in 69m 16.0s (- 6m 1.39s) (92 92.0%). train avg loss: 0.0994, val avg loss: 0.5521\n",
      "Training for epoch 93 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 93 finished in 69m 57.74s (- 5m 15.96s) (93 93.0%). train avg loss: 0.0957, val avg loss: 0.5334\n",
      "Training for epoch 94 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 94 finished in 70m 39.94s (- 4m 30.63s) (94 94.0%). train avg loss: 0.0949, val avg loss: 0.551\n",
      "Training for epoch 95 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 95 finished in 71m 22.86s (- 3m 45.41s) (95 95.0%). train avg loss: 0.0947, val avg loss: 0.5387\n",
      "Training for epoch 96 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 96 finished in 72m 6.3s (- 3m 0.26s) (96 96.0%). train avg loss: 0.0941, val avg loss: 0.5461\n",
      "Training for epoch 97 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 97 finished in 72m 51.56s (- 2m 15.2s) (97 97.0%). train avg loss: 0.093, val avg loss: 0.5532\n",
      "Training for epoch 98 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 98 finished in 73m 34.11s (- 1m 30.08s) (98 98.0%). train avg loss: 0.0937, val avg loss: 0.5495\n",
      "Training for epoch 99 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 99 finished in 74m 19.14s (- 0m 45.04s) (99 99.0%). train avg loss: 0.0931, val avg loss: 0.5602\n",
      "Training for epoch 100 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 100 finished in 75m 4.26s (- 0m 0.0s) (100 100.0%). train avg loss: 0.093, val avg loss: 0.5485\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"train-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"train-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"train-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"train-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5.5, min_value-.1, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 11.800518588614862%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word, len(word)+1)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ruam\n",
      "= ['R', 'UW', 'AA', 'M']\n",
      "< R UW AA M ['R', 'UW', 'AA', 'M']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbf40b6bb80>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x666.667 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
