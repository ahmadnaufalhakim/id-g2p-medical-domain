{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn_gen/exp/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"256\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"128\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models_fallback\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  if split_name == \"train+val\" :\n",
    "    print(f\"Merging train and val entries ..\")\n",
    "    with open(os.path.join(DATA_DIR, f\"train.csv\"), encoding=\"utf-8\") as f_train_csv, \\\n",
    "         open(os.path.join(DATA_DIR, f\"val.csv\"), encoding=\"utf-8\") as f_val_csv :\n",
    "      next(f_train_csv, None)\n",
    "      next(f_val_csv, None)\n",
    "      train_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_train_csv]\n",
    "      val_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_val_csv]\n",
    "      pairs = train_pairs + val_pairs\n",
    "      graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "      phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "      lang_list = [pair[2] for pair in pairs]\n",
    "      g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "    return g2p_dataset, pairs\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging train and val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6138403889183434\n",
      "ID_WEIGHT: 2.6960571496230807\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train+val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[375, 170, 404, 233, 585, 400, 111, 281, 422, 673, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7fe4887c6cd0> ([6, 103, 71, 590, 520, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 103, 71, 590, 520, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 103, 71, 590, 520, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 724 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-f', 31: '-g', 32: '-h', 33: '-i', 34: '-j', 35: '-k', 36: '-l', 37: '-m', 38: '-n', 39: '-o', 40: '-p', 41: '-q', 42: '-r', 43: '-s', 44: '-t', 45: '-u', 46: '-v', 47: '-w', 48: '-y', 49: \"a'\", 50: 'a-', 51: 'aa', 52: 'ab', 53: 'ac', 54: 'ad', 55: 'ae', 56: 'af', 57: 'ag', 58: 'ah', 59: 'ai', 60: 'aj', 61: 'ak', 62: 'al', 63: 'am', 64: 'an', 65: 'ao', 66: 'ap', 67: 'aq', 68: 'ar', 69: 'as', 70: 'at', 71: 'au', 72: 'av', 73: 'aw', 74: 'ax', 75: 'ay', 76: 'az', 77: \"b'\", 78: 'ba', 79: 'bb', 80: 'bc', 81: 'bd', 82: 'be', 83: 'bf', 84: 'bg', 85: 'bh', 86: 'bi', 87: 'bj', 88: 'bk', 89: 'bl', 90: 'bm', 91: 'bn', 92: 'bo', 93: 'bp', 94: 'br', 95: 'bs', 96: 'bt', 97: 'bu', 98: 'bv', 99: 'bw', 100: 'by', 101: 'bz', 102: \"c'\", 103: 'ca', 104: 'cb', 105: 'cc', 106: 'cd', 107: 'ce', 108: 'cf', 109: 'cg', 110: 'ch', 111: 'ci', 112: 'cj', 113: 'ck', 114: 'cl', 115: 'cm', 116: 'cn', 117: 'co', 118: 'cp', 119: 'cq', 120: 'cr', 121: 'cs', 122: 'ct', 123: 'cu', 124: 'cv', 125: 'cw', 126: 'cy', 127: 'cz', 128: \"d'\", 129: 'd-', 130: 'da', 131: 'db', 132: 'dc', 133: 'dd', 134: 'de', 135: 'df', 136: 'dg', 137: 'dh', 138: 'di', 139: 'dj', 140: 'dk', 141: 'dl', 142: 'dm', 143: 'dn', 144: 'do', 145: 'dp', 146: 'dq', 147: 'dr', 148: 'ds', 149: 'dt', 150: 'du', 151: 'dv', 152: 'dw', 153: 'dy', 154: 'dz', 155: \"e'\", 156: 'e-', 157: 'ea', 158: 'eb', 159: 'ec', 160: 'ed', 161: 'ee', 162: 'ef', 163: 'eg', 164: 'eh', 165: 'ei', 166: 'ej', 167: 'ek', 168: 'el', 169: 'em', 170: 'en', 171: 'eo', 172: 'ep', 173: 'eq', 174: 'er', 175: 'es', 176: 'et', 177: 'eu', 178: 'ev', 179: 'ew', 180: 'ex', 181: 'ey', 182: 'ez', 183: \"f'\", 184: 'f-', 185: 'fa', 186: 'fb', 187: 'fc', 188: 'fd', 189: 'fe', 190: 'ff', 191: 'fg', 192: 'fh', 193: 'fi', 194: 'fj', 195: 'fk', 196: 'fl', 197: 'fm', 198: 'fn', 199: 'fo', 200: 'fp', 201: 'fq', 202: 'fr', 203: 'fs', 204: 'ft', 205: 'fu', 206: 'fv', 207: 'fw', 208: 'fx', 209: 'fy', 210: 'fz', 211: \"g'\", 212: 'g-', 213: 'ga', 214: 'gb', 215: 'gc', 216: 'gd', 217: 'ge', 218: 'gf', 219: 'gg', 220: 'gh', 221: 'gi', 222: 'gj', 223: 'gk', 224: 'gl', 225: 'gm', 226: 'gn', 227: 'go', 228: 'gp', 229: 'gq', 230: 'gr', 231: 'gs', 232: 'gt', 233: 'gu', 234: 'gv', 235: 'gw', 236: 'gx', 237: 'gy', 238: 'gz', 239: \"h'\", 240: 'h-', 241: 'ha', 242: 'hb', 243: 'hc', 244: 'hd', 245: 'he', 246: 'hf', 247: 'hg', 248: 'hh', 249: 'hi', 250: 'hj', 251: 'hk', 252: 'hl', 253: 'hm', 254: 'hn', 255: 'ho', 256: 'hp', 257: 'hq', 258: 'hr', 259: 'hs', 260: 'ht', 261: 'hu', 262: 'hv', 263: 'hw', 264: 'hy', 265: 'i', 266: \"i'\", 267: 'i-', 268: 'ia', 269: 'ib', 270: 'ic', 271: 'id', 272: 'ie', 273: 'if', 274: 'ig', 275: 'ih', 276: 'ii', 277: 'ij', 278: 'ik', 279: 'il', 280: 'im', 281: 'in', 282: 'io', 283: 'ip', 284: 'iq', 285: 'ir', 286: 'is', 287: 'it', 288: 'iu', 289: 'iv', 290: 'iw', 291: 'ix', 292: 'iy', 293: 'iz', 294: \"j'\", 295: 'ja', 296: 'jc', 297: 'jd', 298: 'je', 299: 'jf', 300: 'jh', 301: 'ji', 302: 'jj', 303: 'jk', 304: 'jl', 305: 'jm', 306: 'jn', 307: 'jo', 308: 'jr', 309: 'js', 310: 'jt', 311: 'ju', 312: 'jv', 313: 'jw', 314: 'jy', 315: \"k'\", 316: 'k-', 317: 'ka', 318: 'kb', 319: 'kc', 320: 'kd', 321: 'ke', 322: 'kf', 323: 'kg', 324: 'kh', 325: 'ki', 326: 'kj', 327: 'kk', 328: 'kl', 329: 'km', 330: 'kn', 331: 'ko', 332: 'kp', 333: 'kr', 334: 'ks', 335: 'kt', 336: 'ku', 337: 'kv', 338: 'kw', 339: 'ky', 340: 'kz', 341: \"l'\", 342: 'l-', 343: 'la', 344: 'lb', 345: 'lc', 346: 'ld', 347: 'le', 348: 'lf', 349: 'lg', 350: 'lh', 351: 'li', 352: 'lj', 353: 'lk', 354: 'll', 355: 'lm', 356: 'ln', 357: 'lo', 358: 'lp', 359: 'lq', 360: 'lr', 361: 'ls', 362: 'lt', 363: 'lu', 364: 'lv', 365: 'lw', 366: 'lx', 367: 'ly', 368: 'lz', 369: \"m'\", 370: 'm-', 371: 'ma', 372: 'mb', 373: 'mc', 374: 'md', 375: 'me', 376: 'mf', 377: 'mg', 378: 'mh', 379: 'mi', 380: 'mj', 381: 'mk', 382: 'ml', 383: 'mm', 384: 'mn', 385: 'mo', 386: 'mp', 387: 'mq', 388: 'mr', 389: 'ms', 390: 'mt', 391: 'mu', 392: 'mv', 393: 'mw', 394: 'my', 395: 'mz', 396: \"n'\", 397: 'n-', 398: 'na', 399: 'nb', 400: 'nc', 401: 'nd', 402: 'ne', 403: 'nf', 404: 'ng', 405: 'nh', 406: 'ni', 407: 'nj', 408: 'nk', 409: 'nl', 410: 'nm', 411: 'nn', 412: 'no', 413: 'np', 414: 'nq', 415: 'nr', 416: 'ns', 417: 'nt', 418: 'nu', 419: 'nv', 420: 'nw', 421: 'nx', 422: 'ny', 423: 'nz', 424: 'o', 425: \"o'\", 426: 'o-', 427: 'oa', 428: 'ob', 429: 'oc', 430: 'od', 431: 'oe', 432: 'of', 433: 'og', 434: 'oh', 435: 'oi', 436: 'oj', 437: 'ok', 438: 'ol', 439: 'om', 440: 'on', 441: 'oo', 442: 'op', 443: 'oq', 444: 'or', 445: 'os', 446: 'ot', 447: 'ou', 448: 'ov', 449: 'ow', 450: 'ox', 451: 'oy', 452: 'oz', 453: \"p'\", 454: 'p-', 455: 'pa', 456: 'pb', 457: 'pc', 458: 'pd', 459: 'pe', 460: 'pf', 461: 'pg', 462: 'ph', 463: 'pi', 464: 'pj', 465: 'pk', 466: 'pl', 467: 'pm', 468: 'pn', 469: 'po', 470: 'pp', 471: 'pr', 472: 'ps', 473: 'pt', 474: 'pu', 475: 'pw', 476: 'py', 477: 'pz', 478: \"q'\", 479: 'qa', 480: 'qb', 481: 'qg', 482: 'qi', 483: 'qo', 484: 'qu', 485: 'qv', 486: \"r'\", 487: 'r-', 488: 'ra', 489: 'rb', 490: 'rc', 491: 'rd', 492: 're', 493: 'rf', 494: 'rg', 495: 'rh', 496: 'ri', 497: 'rj', 498: 'rk', 499: 'rl', 500: 'rm', 501: 'rn', 502: 'ro', 503: 'rp', 504: 'rq', 505: 'rr', 506: 'rs', 507: 'rt', 508: 'ru', 509: 'rv', 510: 'rw', 511: 'rx', 512: 'ry', 513: 'rz', 514: \"s'\", 515: 's-', 516: 'sa', 517: 'sb', 518: 'sc', 519: 'sd', 520: 'se', 521: 'sf', 522: 'sg', 523: 'sh', 524: 'si', 525: 'sj', 526: 'sk', 527: 'sl', 528: 'sm', 529: 'sn', 530: 'so', 531: 'sp', 532: 'sq', 533: 'sr', 534: 'ss', 535: 'st', 536: 'su', 537: 'sv', 538: 'sw', 539: 'sx', 540: 'sy', 541: 'sz', 542: \"t'\", 543: 't-', 544: 'ta', 545: 'tb', 546: 'tc', 547: 'td', 548: 'te', 549: 'tf', 550: 'tg', 551: 'th', 552: 'ti', 553: 'tj', 554: 'tk', 555: 'tl', 556: 'tm', 557: 'tn', 558: 'to', 559: 'tp', 560: 'tq', 561: 'tr', 562: 'ts', 563: 'tt', 564: 'tu', 565: 'tv', 566: 'tw', 567: 'tx', 568: 'ty', 569: 'tz', 570: \"u'\", 571: 'u-', 572: 'ua', 573: 'ub', 574: 'uc', 575: 'ud', 576: 'ue', 577: 'uf', 578: 'ug', 579: 'uh', 580: 'ui', 581: 'uj', 582: 'uk', 583: 'ul', 584: 'um', 585: 'un', 586: 'uo', 587: 'up', 588: 'uq', 589: 'ur', 590: 'us', 591: 'ut', 592: 'uu', 593: 'uv', 594: 'uw', 595: 'ux', 596: 'uy', 597: 'uz', 598: \"v'\", 599: 'va', 600: 'vc', 601: 'vd', 602: 've', 603: 'vg', 604: 'vh', 605: 'vi', 606: 'vj', 607: 'vk', 608: 'vl', 609: 'vm', 610: 'vn', 611: 'vo', 612: 'vr', 613: 'vs', 614: 'vt', 615: 'vu', 616: 'vv', 617: 'vy', 618: 'vz', 619: \"w'\", 620: 'w-', 621: 'wa', 622: 'wb', 623: 'wc', 624: 'wd', 625: 'we', 626: 'wf', 627: 'wg', 628: 'wh', 629: 'wi', 630: 'wk', 631: 'wl', 632: 'wm', 633: 'wn', 634: 'wo', 635: 'wp', 636: 'wq', 637: 'wr', 638: 'ws', 639: 'wt', 640: 'wu', 641: 'wv', 642: 'ww', 643: 'wy', 644: 'wz', 645: \"x'\", 646: 'x-', 647: 'xa', 648: 'xb', 649: 'xc', 650: 'xd', 651: 'xe', 652: 'xf', 653: 'xg', 654: 'xh', 655: 'xi', 656: 'xl', 657: 'xm', 658: 'xn', 659: 'xo', 660: 'xp', 661: 'xq', 662: 'xr', 663: 'xs', 664: 'xt', 665: 'xu', 666: 'xv', 667: 'xw', 668: 'xx', 669: 'xy', 670: 'xz', 671: \"y'\", 672: 'y-', 673: 'ya', 674: 'yb', 675: 'yc', 676: 'yd', 677: 'ye', 678: 'yf', 679: 'yg', 680: 'yh', 681: 'yi', 682: 'yj', 683: 'yk', 684: 'yl', 685: 'ym', 686: 'yn', 687: 'yo', 688: 'yp', 689: 'yq', 690: 'yr', 691: 'ys', 692: 'yt', 693: 'yu', 694: 'yv', 695: 'yw', 696: 'yx', 697: 'yy', 698: 'yz', 699: \"z'\", 700: 'za', 701: 'zb', 702: 'zc', 703: 'zd', 704: 'ze', 705: 'zf', 706: 'zg', 707: 'zh', 708: 'zi', 709: 'zk', 710: 'zl', 711: 'zm', 712: 'zn', 713: 'zo', 714: 'zp', 715: 'zq', 716: 'zr', 717: 'zs', 718: 'zt', 719: 'zu', 720: 'zv', 721: 'zw', 722: 'zy', 723: 'zz'}\n",
      "test grp 724 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-f', 31: '-g', 32: '-h', 33: '-i', 34: '-j', 35: '-k', 36: '-l', 37: '-m', 38: '-n', 39: '-o', 40: '-p', 41: '-q', 42: '-r', 43: '-s', 44: '-t', 45: '-u', 46: '-v', 47: '-w', 48: '-y', 49: \"a'\", 50: 'a-', 51: 'aa', 52: 'ab', 53: 'ac', 54: 'ad', 55: 'ae', 56: 'af', 57: 'ag', 58: 'ah', 59: 'ai', 60: 'aj', 61: 'ak', 62: 'al', 63: 'am', 64: 'an', 65: 'ao', 66: 'ap', 67: 'aq', 68: 'ar', 69: 'as', 70: 'at', 71: 'au', 72: 'av', 73: 'aw', 74: 'ax', 75: 'ay', 76: 'az', 77: \"b'\", 78: 'ba', 79: 'bb', 80: 'bc', 81: 'bd', 82: 'be', 83: 'bf', 84: 'bg', 85: 'bh', 86: 'bi', 87: 'bj', 88: 'bk', 89: 'bl', 90: 'bm', 91: 'bn', 92: 'bo', 93: 'bp', 94: 'br', 95: 'bs', 96: 'bt', 97: 'bu', 98: 'bv', 99: 'bw', 100: 'by', 101: 'bz', 102: \"c'\", 103: 'ca', 104: 'cb', 105: 'cc', 106: 'cd', 107: 'ce', 108: 'cf', 109: 'cg', 110: 'ch', 111: 'ci', 112: 'cj', 113: 'ck', 114: 'cl', 115: 'cm', 116: 'cn', 117: 'co', 118: 'cp', 119: 'cq', 120: 'cr', 121: 'cs', 122: 'ct', 123: 'cu', 124: 'cv', 125: 'cw', 126: 'cy', 127: 'cz', 128: \"d'\", 129: 'd-', 130: 'da', 131: 'db', 132: 'dc', 133: 'dd', 134: 'de', 135: 'df', 136: 'dg', 137: 'dh', 138: 'di', 139: 'dj', 140: 'dk', 141: 'dl', 142: 'dm', 143: 'dn', 144: 'do', 145: 'dp', 146: 'dq', 147: 'dr', 148: 'ds', 149: 'dt', 150: 'du', 151: 'dv', 152: 'dw', 153: 'dy', 154: 'dz', 155: \"e'\", 156: 'e-', 157: 'ea', 158: 'eb', 159: 'ec', 160: 'ed', 161: 'ee', 162: 'ef', 163: 'eg', 164: 'eh', 165: 'ei', 166: 'ej', 167: 'ek', 168: 'el', 169: 'em', 170: 'en', 171: 'eo', 172: 'ep', 173: 'eq', 174: 'er', 175: 'es', 176: 'et', 177: 'eu', 178: 'ev', 179: 'ew', 180: 'ex', 181: 'ey', 182: 'ez', 183: \"f'\", 184: 'f-', 185: 'fa', 186: 'fb', 187: 'fc', 188: 'fd', 189: 'fe', 190: 'ff', 191: 'fg', 192: 'fh', 193: 'fi', 194: 'fj', 195: 'fk', 196: 'fl', 197: 'fm', 198: 'fn', 199: 'fo', 200: 'fp', 201: 'fq', 202: 'fr', 203: 'fs', 204: 'ft', 205: 'fu', 206: 'fv', 207: 'fw', 208: 'fx', 209: 'fy', 210: 'fz', 211: \"g'\", 212: 'g-', 213: 'ga', 214: 'gb', 215: 'gc', 216: 'gd', 217: 'ge', 218: 'gf', 219: 'gg', 220: 'gh', 221: 'gi', 222: 'gj', 223: 'gk', 224: 'gl', 225: 'gm', 226: 'gn', 227: 'go', 228: 'gp', 229: 'gq', 230: 'gr', 231: 'gs', 232: 'gt', 233: 'gu', 234: 'gv', 235: 'gw', 236: 'gx', 237: 'gy', 238: 'gz', 239: \"h'\", 240: 'h-', 241: 'ha', 242: 'hb', 243: 'hc', 244: 'hd', 245: 'he', 246: 'hf', 247: 'hg', 248: 'hh', 249: 'hi', 250: 'hj', 251: 'hk', 252: 'hl', 253: 'hm', 254: 'hn', 255: 'ho', 256: 'hp', 257: 'hq', 258: 'hr', 259: 'hs', 260: 'ht', 261: 'hu', 262: 'hv', 263: 'hw', 264: 'hy', 265: 'i', 266: \"i'\", 267: 'i-', 268: 'ia', 269: 'ib', 270: 'ic', 271: 'id', 272: 'ie', 273: 'if', 274: 'ig', 275: 'ih', 276: 'ii', 277: 'ij', 278: 'ik', 279: 'il', 280: 'im', 281: 'in', 282: 'io', 283: 'ip', 284: 'iq', 285: 'ir', 286: 'is', 287: 'it', 288: 'iu', 289: 'iv', 290: 'iw', 291: 'ix', 292: 'iy', 293: 'iz', 294: \"j'\", 295: 'ja', 296: 'jc', 297: 'jd', 298: 'je', 299: 'jf', 300: 'jh', 301: 'ji', 302: 'jj', 303: 'jk', 304: 'jl', 305: 'jm', 306: 'jn', 307: 'jo', 308: 'jr', 309: 'js', 310: 'jt', 311: 'ju', 312: 'jv', 313: 'jw', 314: 'jy', 315: \"k'\", 316: 'k-', 317: 'ka', 318: 'kb', 319: 'kc', 320: 'kd', 321: 'ke', 322: 'kf', 323: 'kg', 324: 'kh', 325: 'ki', 326: 'kj', 327: 'kk', 328: 'kl', 329: 'km', 330: 'kn', 331: 'ko', 332: 'kp', 333: 'kr', 334: 'ks', 335: 'kt', 336: 'ku', 337: 'kv', 338: 'kw', 339: 'ky', 340: 'kz', 341: \"l'\", 342: 'l-', 343: 'la', 344: 'lb', 345: 'lc', 346: 'ld', 347: 'le', 348: 'lf', 349: 'lg', 350: 'lh', 351: 'li', 352: 'lj', 353: 'lk', 354: 'll', 355: 'lm', 356: 'ln', 357: 'lo', 358: 'lp', 359: 'lq', 360: 'lr', 361: 'ls', 362: 'lt', 363: 'lu', 364: 'lv', 365: 'lw', 366: 'lx', 367: 'ly', 368: 'lz', 369: \"m'\", 370: 'm-', 371: 'ma', 372: 'mb', 373: 'mc', 374: 'md', 375: 'me', 376: 'mf', 377: 'mg', 378: 'mh', 379: 'mi', 380: 'mj', 381: 'mk', 382: 'ml', 383: 'mm', 384: 'mn', 385: 'mo', 386: 'mp', 387: 'mq', 388: 'mr', 389: 'ms', 390: 'mt', 391: 'mu', 392: 'mv', 393: 'mw', 394: 'my', 395: 'mz', 396: \"n'\", 397: 'n-', 398: 'na', 399: 'nb', 400: 'nc', 401: 'nd', 402: 'ne', 403: 'nf', 404: 'ng', 405: 'nh', 406: 'ni', 407: 'nj', 408: 'nk', 409: 'nl', 410: 'nm', 411: 'nn', 412: 'no', 413: 'np', 414: 'nq', 415: 'nr', 416: 'ns', 417: 'nt', 418: 'nu', 419: 'nv', 420: 'nw', 421: 'nx', 422: 'ny', 423: 'nz', 424: 'o', 425: \"o'\", 426: 'o-', 427: 'oa', 428: 'ob', 429: 'oc', 430: 'od', 431: 'oe', 432: 'of', 433: 'og', 434: 'oh', 435: 'oi', 436: 'oj', 437: 'ok', 438: 'ol', 439: 'om', 440: 'on', 441: 'oo', 442: 'op', 443: 'oq', 444: 'or', 445: 'os', 446: 'ot', 447: 'ou', 448: 'ov', 449: 'ow', 450: 'ox', 451: 'oy', 452: 'oz', 453: \"p'\", 454: 'p-', 455: 'pa', 456: 'pb', 457: 'pc', 458: 'pd', 459: 'pe', 460: 'pf', 461: 'pg', 462: 'ph', 463: 'pi', 464: 'pj', 465: 'pk', 466: 'pl', 467: 'pm', 468: 'pn', 469: 'po', 470: 'pp', 471: 'pr', 472: 'ps', 473: 'pt', 474: 'pu', 475: 'pw', 476: 'py', 477: 'pz', 478: \"q'\", 479: 'qa', 480: 'qb', 481: 'qg', 482: 'qi', 483: 'qo', 484: 'qu', 485: 'qv', 486: \"r'\", 487: 'r-', 488: 'ra', 489: 'rb', 490: 'rc', 491: 'rd', 492: 're', 493: 'rf', 494: 'rg', 495: 'rh', 496: 'ri', 497: 'rj', 498: 'rk', 499: 'rl', 500: 'rm', 501: 'rn', 502: 'ro', 503: 'rp', 504: 'rq', 505: 'rr', 506: 'rs', 507: 'rt', 508: 'ru', 509: 'rv', 510: 'rw', 511: 'rx', 512: 'ry', 513: 'rz', 514: \"s'\", 515: 's-', 516: 'sa', 517: 'sb', 518: 'sc', 519: 'sd', 520: 'se', 521: 'sf', 522: 'sg', 523: 'sh', 524: 'si', 525: 'sj', 526: 'sk', 527: 'sl', 528: 'sm', 529: 'sn', 530: 'so', 531: 'sp', 532: 'sq', 533: 'sr', 534: 'ss', 535: 'st', 536: 'su', 537: 'sv', 538: 'sw', 539: 'sx', 540: 'sy', 541: 'sz', 542: \"t'\", 543: 't-', 544: 'ta', 545: 'tb', 546: 'tc', 547: 'td', 548: 'te', 549: 'tf', 550: 'tg', 551: 'th', 552: 'ti', 553: 'tj', 554: 'tk', 555: 'tl', 556: 'tm', 557: 'tn', 558: 'to', 559: 'tp', 560: 'tq', 561: 'tr', 562: 'ts', 563: 'tt', 564: 'tu', 565: 'tv', 566: 'tw', 567: 'tx', 568: 'ty', 569: 'tz', 570: \"u'\", 571: 'u-', 572: 'ua', 573: 'ub', 574: 'uc', 575: 'ud', 576: 'ue', 577: 'uf', 578: 'ug', 579: 'uh', 580: 'ui', 581: 'uj', 582: 'uk', 583: 'ul', 584: 'um', 585: 'un', 586: 'uo', 587: 'up', 588: 'uq', 589: 'ur', 590: 'us', 591: 'ut', 592: 'uu', 593: 'uv', 594: 'uw', 595: 'ux', 596: 'uy', 597: 'uz', 598: \"v'\", 599: 'va', 600: 'vc', 601: 'vd', 602: 've', 603: 'vg', 604: 'vh', 605: 'vi', 606: 'vj', 607: 'vk', 608: 'vl', 609: 'vm', 610: 'vn', 611: 'vo', 612: 'vr', 613: 'vs', 614: 'vt', 615: 'vu', 616: 'vv', 617: 'vy', 618: 'vz', 619: \"w'\", 620: 'w-', 621: 'wa', 622: 'wb', 623: 'wc', 624: 'wd', 625: 'we', 626: 'wf', 627: 'wg', 628: 'wh', 629: 'wi', 630: 'wk', 631: 'wl', 632: 'wm', 633: 'wn', 634: 'wo', 635: 'wp', 636: 'wq', 637: 'wr', 638: 'ws', 639: 'wt', 640: 'wu', 641: 'wv', 642: 'ww', 643: 'wy', 644: 'wz', 645: \"x'\", 646: 'x-', 647: 'xa', 648: 'xb', 649: 'xc', 650: 'xd', 651: 'xe', 652: 'xf', 653: 'xg', 654: 'xh', 655: 'xi', 656: 'xl', 657: 'xm', 658: 'xn', 659: 'xo', 660: 'xp', 661: 'xq', 662: 'xr', 663: 'xs', 664: 'xt', 665: 'xu', 666: 'xv', 667: 'xw', 668: 'xx', 669: 'xy', 670: 'xz', 671: \"y'\", 672: 'y-', 673: 'ya', 674: 'yb', 675: 'yc', 676: 'yd', 677: 'ye', 678: 'yf', 679: 'yg', 680: 'yh', 681: 'yi', 682: 'yj', 683: 'yk', 684: 'yl', 685: 'ym', 686: 'yn', 687: 'yo', 688: 'yp', 689: 'yq', 690: 'yr', 691: 'ys', 692: 'yt', 693: 'yu', 694: 'yv', 695: 'yw', 696: 'yx', 697: 'yy', 698: 'yz', 699: \"z'\", 700: 'za', 701: 'zb', 702: 'zc', 703: 'zd', 704: 'ze', 705: 'zf', 706: 'zg', 707: 'zh', 708: 'zi', 709: 'zk', 710: 'zl', 711: 'zm', 712: 'zn', 713: 'zo', 714: 'zp', 715: 'zq', 716: 'zr', 717: 'zs', 718: 'zt', 719: 'zu', 720: 'zv', 721: 'zw', 722: 'zy', 723: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "720 {\"'c\": 6, 'ca': 103, 'au': 71, 'us': 590, 'se': 520, 'co': 117, 'ou': 447, 'ur': 589, 'rs': 506, \"'e\": 8, 'em': 169, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 484, 'uo': 586, 'ot': 446, 'te': 548, \"'t\": 21, 'ti': 552, 'is': 286, '--': 24, '-n': 38, 'ny': 422, 'ya': 673, '-a': 25, 'an': 64, 'nd': 401, 'da': 130, '-b': 26, 'be': 82, 'el': 168, 'la': 343, 'as': 69, '-c': 27, 'om': 439, 'mp': 386, 'pe': 459, 'en': 170, 'ng': 404, '-d': 28, 'de': 134, 'ap': 66, 'pa': 455, '-k': 35, 'ka': 317, 'ku': 336, '-l': 36, 'le': 347, 'gg': 219, 'ga': 213, '-m': 37, 'ma': 371, 'ah': 58, 'ha': 241, 'si': 524, 'sw': 538, 'wa': 621, '-s': 43, 'ep': 172, 'pi': 463, 'it': 287, '-t': 44, 'ta': 544, 'ak': 61, '-w': 47, 'at': 70, \"a'\": 49, \"'d\": 7, 'du': 150, \"'h\": 10, 'ad': 54, 'aa': 51, 'ab': 52, 'er': 174, 'rg': 494, 'ac': 53, 'ch': 110, 'he': 245, 'ke': 321, 'al': 62, 'ls': 361, 'et': 176, 'th': 551, 'am': 63, 'mo': 385, 'od': 430, 'dt': 149, 'nc': 400, 'or': 444, 'ar': 68, 'rd': 491, 'dv': 151, 'va': 599, 'rk': 498, 'ro': 502, 'on': 440, \"n'\": 396, \"'s\": 20, 'ns': 416, 'so': 530, 'rt': 507, 'ba': 78, 'ck': 113, 'cu': 123, 'di': 138, 'ia': 268, 'ik': 278, 'ai': 59, 'im': 280, 'na': 398, 'ir': 285, 'lk': 353, 'ki': 325, 'in': 281, 'lo': 357, 'ne': 402, 'os': 445, 'do': 144, 'ni': 406, 'nm': 410, 'me': 375, 'nt': 417, 'ts': 562, 'gn': 226, 'to': 558, 'ra': 488, 'rc': 490, 're': 492, 'sc': 518, 'sh': 523, 'ed': 160, 'es': 175, 'oa': 427, 'bb': 79, 'ie': 272, 'll': 354, 'nh': 405, 'tt': 563, 'ev': 178, 'vi': 605, 'il': 279, 'ey': 181, \"y'\": 671, 'bi': 86, 'bo': 92, \"t'\": 542, 'ud': 575, 'br': 94, 'io': 282, 'bs': 95, 'by': 100, 'bc': 80, 'ek': 167, 'bd': 81, 'ic': 270, 'dn': 143, 'no': 412, 'ol': 438, 'mi': 379, 'uc': 574, 'ct': 122, 'ee': 161, 'ul': 583, 'az': 76, 'zi': 708, 'iz': 293, 'ln': 356, 'ow': 449, 'dr': 147, 'cr': 120, 'mb': 372, 'rl': 499, 'rm': 500, 'rr': 505, 'ce': 107, 'yt': 692, 'bh': 85, 'ho': 255, 'id': 271, 'ig': 274, 'li': 351, 'ty': 568, 'gt': 232, 'ib': 269, 'tu': 564, 'ri': 496, 'tz': 569, 'bj': 87, 'ja': 295, 'je': 298, 'ec': 159, 'bk': 88, 'kh': 324, 'bl': 89, 'ut': 591, 'ze': 704, 'e-': 156, 'st': 535, 'oo': 441, 'ly': 367, 'bn': 91, 'nu': 418, \"o'\": 425, 'oi': 435, 'hi': 249, 'sm': 528, 'gi': 221, 'rn': 501, 'if': 273, 'fa': 185, 'ci': 111, 'iv': 289, 've': 602, 'uh': 579, 'un': 585, 'ds': 148, 'ov': 448, \"e'\": 155, 'bp': 93, 'pl': 466, 'lp': 358, 'ms': 389, 'mc': 373, 'cz': 127, 'zy': 722, 'yk': 683, 'wi': 629, 'ea': 157, 'ks': 334, 'eg': 163, 'go': 227, 'dg': 136, 'ge': 217, 'ko': 331, 'og': 433, 'ru': 508, 'up': 587, 'pt': 473, 'tl': 555, 'tn': 557, 'ss': 534, 'yn': 686, 'uz': 597, 'zz': 723, 'zo': 713, 'sa': 516, 'ei': 165, 'lu': 363, 'lv': 364, 'rb': 489, 'rp': 503, 'ps': 472, 'tr': 561, 'su': 536, 'dl': 141, 'bt': 96, 'bu': 97, 'ua': 572, 'hm': 253, 'uk': 582, 'dz': 154, 'lh': 350, 'ay': 75, 'ys': 691, 'cs': 121, 'my': 394, 'po': 469, 'pu': 474, 'lc': 345, 'cc': 105, 'ry': 512, 't-': 543, '-p': 40, 'pr': 471, 'cl': 114, 'mm': 383, 'yi': 681, \"r'\": 486, 'gl': 224, 'cy': 126, \"s'\": 514, 'ue': 576, 'ui': 580, 'um': 584, 'mu': 391, 'op': 442, 'ph': 462, 'yl': 684, 'eb': 158, 'nb': 399, 'ht': 260, 'hy': 264, 'fi': 193, 'fy': 209, 'kl': 328, 'km': 329, 'kn': 330, 'wl': 631, 'gm': 225, 'kr': 333, 'oy': 451, 'yd': 676, \"d'\": 128, 'cm': 115, 'cn': 116, 'oc': 429, 'of': 432, 'ff': 190, 'cq': 119, 'av': 72, 'ag': 57, 'ob': 428, 'ym': 685, 'tm': 556, 'uf': 577, 'gk': 223, 'sy': 540, 'yc': 675, 'iu': 288, \"m'\": 369, 'za': 700, 'sk': 526, 'wn': 633, 'ex': 180, 'dc': 132, 'dd': 133, 'eo': 171, 'dw': 152, 'we': 625, 'dy': 153, 'eh': 164, 'lb': 344, \"l'\": 341, 'lm': 355, 'nn': 411, 'sb': 517, 'eq': 173, 'rh': 495, 'ld': 346, 'lt': 362, 'dh': 137, 'eu': 177, 'gu': 233, 'kk': 327, 'kt': 335, 'lf': 348, 'ip': 283, 'gh': 220, 'aj': 60, 'ok': 437, 'oh': 434, 'iw': 290, 'gs': 231, 'dj': 139, 'jo': 307, 'ju': 311, 'dk': 140, 'dm': 142, \"f'\": 183, 'fo': 199, 'hs': 259, '-f': 30, 'ae': 55, 'sd': 519, \"g'\": 211, 'vo': 611, 'gr': 230, 'ft': 204, 'oq': 443, \"h'\": 239, 'sp': 531, 'hl': 252, \"p'\": 453, 'af': 56, 'ye': 677, 'fd': 188, 'fe': 189, 'ix': 291, 'xe': 651, 'fl': 196, 'fr': 202, 'ax': 74, 'fg': 191, \"i'\": 266, 'fh': 192, 'ox': 450, 'xi': 655, 'fm': 197, 'fs': 203, 'ef': 162, 'ug': 578, 'rw': 510, 'fw': 207, 'aw': 73, 'mn': 384, 'gy': 237, 'gc': 215, 'gf': 218, 'ih': 275, 'ew': 179, 'xc': 649, 'fu': 205, 'pp': 470, 'hh': 248, 'hk': 251, 'lg': 349, 'lq': 359, 'ub': 573, 'hn': 254, 'hr': 258, 'hu': 261, 'hw': 263, 'ml': 382, 'sl': 527, 'wo': 634, \"c'\": 102, 'rf': 493, 'nk': 408, 'tc': 546, \"x'\": 645, 'ji': 301, 'jn': 306, 'oj': 436, 'uj': 581, 'k-': 316, 'kc': 319, 'hb': 242, 'iy': 292, 'kw': 338, 'ky': 339, 'kz': 340, 'l-': 342, '-q': 41, \"'a\": 4, 'tv': 565, 'uq': 588, 'rq': 504, 'ej': 166, 'xa': 647, 'xo': 659, 'xy': 669, 'nq': 414, 'md': 374, 'nz': 423, 'fb': 186, 'ij': 277, 'iq': 284, 'lj': 352, '-i': 33, 'n-': 397, '-o': 39, 'nw': 420, 'yw': 695, 'lr': 360, 'uv': 593, 'lw': 365, 'ez': 182, 'mq': 387, 'fn': 198, \"k'\": 315, 'tf': 549, 'zh': 707, 'wy': 643, 'lz': 368, 'np': 413, 'xt': 664, 'zc': 702, 'zq': 715, 'mf': 376, 'mg': 377, 'mh': 378, 'yv': 694, 'mk': 381, 'oe': 431, 'pc': 457, 'mr': 388, 'mt': 390, 'mv': 392, 'mw': 393, 'yo': 687, 'yx': 696, 'yz': 698, 'sq': 532, 'nv': 419, 'oz': 452, 'ws': 638, 'cd': 106, 'nf': 403, 'gp': 228, 'nj': 407, 'jl': 304, 'nr': 415, 'sg': 522, 'i-': 267, 'bm': 90, 'tk': 554, \"u'\": 570, 'tw': 566, 'nx': 421, 'yb': 674, 'yh': 680, 'yp': 688, 'wh': 628, 'ao': 65, 'pf': 460, 'pg': 461, 'pk': 465, 'aq': 67, 'qa': 479, 'qi': 482, 'rv': 509, 'ux': 595, 'hd': 244, 'zm': 711, 'rj': 497, 'wr': 637, 'd-': 129, \"w'\": 619, 'zt': 718, 'zu': 719, 'rx': 511, 'rz': 513, 'sn': 529, 'ii': 276, 'sf': 521, 'hc': 243, 'hf': 246, \"v'\": 598, 'pn': 468, 'sr': 533, 'yr': 690, 'uy': 596, 'vd': 601, 'vg': 603, 'vn': 610, 'vr': 612, 'vt': 614, 'wb': 622, 'wf': 626, 'wk': 630, 'wt': 639, 'wu': 640, 'xf': 652, 'xl': 656, 'xs': 663, 'yg': 679, 'yu': 693, 'yy': 697, 'zb': 701, \"b'\": 77, \"'r\": 19, 'kb': 318, 'kd': 320, 'kf': 322, 'kg': 323, 'kp': 332, 'y-': 672, 'tj': 553, 'gb': 214, 'gd': 216, 'gw': 235, \"'i\": 11, 'sz': 541, 'kv': 337, 'xq': 661, 'fk': 195, 'sv': 537, 'vs': 613, 'wm': 632, 'tb': 545, 'td': 547, 'hg': 247, 'uw': 594, 'wd': 624, 'zl': 710, 'cv': 124, 'db': 131, 'df': 135, 'dp': 145, 'vu': 615, \"'o\": 17, 'zr': 716, 'nl': 409, 'jy': 314, \"z'\": 699, 'r-': 487, 'a-': 50, '-g': 31, 'o-': 426, 's-': 515, 'gq': 229, 'jr': 308, 'g-': 212, '-j': 34, 'hp': 256, 'vy': 617, 'zd': 703, 'zn': 712, 'xu': 665, 'xb': 648, 'kj': 326, '-r': 42, '-v': 46, 'bz': 101, 'tg': 550, 'sj': 525, 'gj': 222, 'wc': 623, 'xw': 667, 'xx': 668, 'yf': 678, 'jd': 297, 'zk': 709, 'tp': 559, 'fc': 187, 'uu': 592, '-u': 45, 'py': 476, 'h-': 240, 'zw': 721, 'pb': 456, 'pj': 464, 'pw': 475, \"q'\": 478, \"'v\": 23, 'jk': 303, 'pd': 458, 'pm': 467, 'gx': 236, 'zp': 714, 'bw': 99, '-e': 29, 'wg': 627, 'zf': 705, 'vl': 608, 'cw': 125, 'hv': 262, 'vc': 600, 'zs': 717, 'mj': 380, 'xh': 654, 'vv': 616, 'xv': 666, 'mz': 395, 'bf': 83, 'hq': 257, 'dq': 146, 'lx': 366, '-h': 32, 'vj': 606, 'x-': 646, 'xn': 658, 'xp': 660, 'jv': 312, 'zg': 706, '-y': 48, 'fj': 194, 'jt': 310, 'w-': 620, 'xg': 653, 'xm': 657, 'tx': 567, 'gz': 238, 'gv': 234, 'jj': 302, 'f-': 184, 'wv': 641, \"'l\": 14, 'hj': 250, 'fp': 200, 'js': 309, 'vh': 604, 'wz': 644, 'u-': 571, 'i': 265, 'qb': 480, 'qg': 481, 'zv': 720, 'jf': 299, 'jh': 300, 'jc': 296, 'wp': 635, 'bv': 98, 'pz': 477, 'fq': 201, 'cb': 104, 'cf': 108, 'cg': 109, 'cp': 118, \"j'\": 294, 'xz': 670, 'cj': 112, 'fx': 208, \"'j\": 12, 'fz': 210, 'qv': 485, 'wq': 636, 'ww': 642, 'xr': 662, 'xd': 650, 'o': 424, \"'b\": 5, \"'g\": 9, \"'k\": 13, 'vk': 607, 'qo': 483, 'vz': 618, 'jm': 305, 'yj': 682, 'p-': 454, 'fv': 206, 'bg': 84, \"'u\": 22, 'jw': 313, 'sx': 539, 'm-': 370, 'yq': 689, 'vm': 609, 'tq': 560}\n",
      "720 {\"'c\": 6, 'ca': 103, 'au': 71, 'us': 590, 'se': 520, 'co': 117, 'ou': 447, 'ur': 589, 'rs': 506, \"'e\": 8, 'em': 169, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 484, 'uo': 586, 'ot': 446, 'te': 548, \"'t\": 21, 'ti': 552, 'is': 286, '--': 24, '-n': 38, 'ny': 422, 'ya': 673, '-a': 25, 'an': 64, 'nd': 401, 'da': 130, '-b': 26, 'be': 82, 'el': 168, 'la': 343, 'as': 69, '-c': 27, 'om': 439, 'mp': 386, 'pe': 459, 'en': 170, 'ng': 404, '-d': 28, 'de': 134, 'ap': 66, 'pa': 455, '-k': 35, 'ka': 317, 'ku': 336, '-l': 36, 'le': 347, 'gg': 219, 'ga': 213, '-m': 37, 'ma': 371, 'ah': 58, 'ha': 241, 'si': 524, 'sw': 538, 'wa': 621, '-s': 43, 'ep': 172, 'pi': 463, 'it': 287, '-t': 44, 'ta': 544, 'ak': 61, '-w': 47, 'at': 70, \"a'\": 49, \"'d\": 7, 'du': 150, \"'h\": 10, 'ad': 54, 'aa': 51, 'ab': 52, 'er': 174, 'rg': 494, 'ac': 53, 'ch': 110, 'he': 245, 'ke': 321, 'al': 62, 'ls': 361, 'et': 176, 'th': 551, 'am': 63, 'mo': 385, 'od': 430, 'dt': 149, 'nc': 400, 'or': 444, 'ar': 68, 'rd': 491, 'dv': 151, 'va': 599, 'rk': 498, 'ro': 502, 'on': 440, \"n'\": 396, \"'s\": 20, 'ns': 416, 'so': 530, 'rt': 507, 'ba': 78, 'ck': 113, 'cu': 123, 'di': 138, 'ia': 268, 'ik': 278, 'ai': 59, 'im': 280, 'na': 398, 'ir': 285, 'lk': 353, 'ki': 325, 'in': 281, 'lo': 357, 'ne': 402, 'os': 445, 'do': 144, 'ni': 406, 'nm': 410, 'me': 375, 'nt': 417, 'ts': 562, 'gn': 226, 'to': 558, 'ra': 488, 'rc': 490, 're': 492, 'sc': 518, 'sh': 523, 'ed': 160, 'es': 175, 'oa': 427, 'bb': 79, 'ie': 272, 'll': 354, 'nh': 405, 'tt': 563, 'ev': 178, 'vi': 605, 'il': 279, 'ey': 181, \"y'\": 671, 'bi': 86, 'bo': 92, \"t'\": 542, 'ud': 575, 'br': 94, 'io': 282, 'bs': 95, 'by': 100, 'bc': 80, 'ek': 167, 'bd': 81, 'ic': 270, 'dn': 143, 'no': 412, 'ol': 438, 'mi': 379, 'uc': 574, 'ct': 122, 'ee': 161, 'ul': 583, 'az': 76, 'zi': 708, 'iz': 293, 'ln': 356, 'ow': 449, 'dr': 147, 'cr': 120, 'mb': 372, 'rl': 499, 'rm': 500, 'rr': 505, 'ce': 107, 'yt': 692, 'bh': 85, 'ho': 255, 'id': 271, 'ig': 274, 'li': 351, 'ty': 568, 'gt': 232, 'ib': 269, 'tu': 564, 'ri': 496, 'tz': 569, 'bj': 87, 'ja': 295, 'je': 298, 'ec': 159, 'bk': 88, 'kh': 324, 'bl': 89, 'ut': 591, 'ze': 704, 'e-': 156, 'st': 535, 'oo': 441, 'ly': 367, 'bn': 91, 'nu': 418, \"o'\": 425, 'oi': 435, 'hi': 249, 'sm': 528, 'gi': 221, 'rn': 501, 'if': 273, 'fa': 185, 'ci': 111, 'iv': 289, 've': 602, 'uh': 579, 'un': 585, 'ds': 148, 'ov': 448, \"e'\": 155, 'bp': 93, 'pl': 466, 'lp': 358, 'ms': 389, 'mc': 373, 'cz': 127, 'zy': 722, 'yk': 683, 'wi': 629, 'ea': 157, 'ks': 334, 'eg': 163, 'go': 227, 'dg': 136, 'ge': 217, 'ko': 331, 'og': 433, 'ru': 508, 'up': 587, 'pt': 473, 'tl': 555, 'tn': 557, 'ss': 534, 'yn': 686, 'uz': 597, 'zz': 723, 'zo': 713, 'sa': 516, 'ei': 165, 'lu': 363, 'lv': 364, 'rb': 489, 'rp': 503, 'ps': 472, 'tr': 561, 'su': 536, 'dl': 141, 'bt': 96, 'bu': 97, 'ua': 572, 'hm': 253, 'uk': 582, 'dz': 154, 'lh': 350, 'ay': 75, 'ys': 691, 'cs': 121, 'my': 394, 'po': 469, 'pu': 474, 'lc': 345, 'cc': 105, 'ry': 512, 't-': 543, '-p': 40, 'pr': 471, 'cl': 114, 'mm': 383, 'yi': 681, \"r'\": 486, 'gl': 224, 'cy': 126, \"s'\": 514, 'ue': 576, 'ui': 580, 'um': 584, 'mu': 391, 'op': 442, 'ph': 462, 'yl': 684, 'eb': 158, 'nb': 399, 'ht': 260, 'hy': 264, 'fi': 193, 'fy': 209, 'kl': 328, 'km': 329, 'kn': 330, 'wl': 631, 'gm': 225, 'kr': 333, 'oy': 451, 'yd': 676, \"d'\": 128, 'cm': 115, 'cn': 116, 'oc': 429, 'of': 432, 'ff': 190, 'cq': 119, 'av': 72, 'ag': 57, 'ob': 428, 'ym': 685, 'tm': 556, 'uf': 577, 'gk': 223, 'sy': 540, 'yc': 675, 'iu': 288, \"m'\": 369, 'za': 700, 'sk': 526, 'wn': 633, 'ex': 180, 'dc': 132, 'dd': 133, 'eo': 171, 'dw': 152, 'we': 625, 'dy': 153, 'eh': 164, 'lb': 344, \"l'\": 341, 'lm': 355, 'nn': 411, 'sb': 517, 'eq': 173, 'rh': 495, 'ld': 346, 'lt': 362, 'dh': 137, 'eu': 177, 'gu': 233, 'kk': 327, 'kt': 335, 'lf': 348, 'ip': 283, 'gh': 220, 'aj': 60, 'ok': 437, 'oh': 434, 'iw': 290, 'gs': 231, 'dj': 139, 'jo': 307, 'ju': 311, 'dk': 140, 'dm': 142, \"f'\": 183, 'fo': 199, 'hs': 259, '-f': 30, 'ae': 55, 'sd': 519, \"g'\": 211, 'vo': 611, 'gr': 230, 'ft': 204, 'oq': 443, \"h'\": 239, 'sp': 531, 'hl': 252, \"p'\": 453, 'af': 56, 'ye': 677, 'fd': 188, 'fe': 189, 'ix': 291, 'xe': 651, 'fl': 196, 'fr': 202, 'ax': 74, 'fg': 191, \"i'\": 266, 'fh': 192, 'ox': 450, 'xi': 655, 'fm': 197, 'fs': 203, 'ef': 162, 'ug': 578, 'rw': 510, 'fw': 207, 'aw': 73, 'mn': 384, 'gy': 237, 'gc': 215, 'gf': 218, 'ih': 275, 'ew': 179, 'xc': 649, 'fu': 205, 'pp': 470, 'hh': 248, 'hk': 251, 'lg': 349, 'lq': 359, 'ub': 573, 'hn': 254, 'hr': 258, 'hu': 261, 'hw': 263, 'ml': 382, 'sl': 527, 'wo': 634, \"c'\": 102, 'rf': 493, 'nk': 408, 'tc': 546, \"x'\": 645, 'ji': 301, 'jn': 306, 'oj': 436, 'uj': 581, 'k-': 316, 'kc': 319, 'hb': 242, 'iy': 292, 'kw': 338, 'ky': 339, 'kz': 340, 'l-': 342, '-q': 41, \"'a\": 4, 'tv': 565, 'uq': 588, 'rq': 504, 'ej': 166, 'xa': 647, 'xo': 659, 'xy': 669, 'nq': 414, 'md': 374, 'nz': 423, 'fb': 186, 'ij': 277, 'iq': 284, 'lj': 352, '-i': 33, 'n-': 397, '-o': 39, 'nw': 420, 'yw': 695, 'lr': 360, 'uv': 593, 'lw': 365, 'ez': 182, 'mq': 387, 'fn': 198, \"k'\": 315, 'tf': 549, 'zh': 707, 'wy': 643, 'lz': 368, 'np': 413, 'xt': 664, 'zc': 702, 'zq': 715, 'mf': 376, 'mg': 377, 'mh': 378, 'yv': 694, 'mk': 381, 'oe': 431, 'pc': 457, 'mr': 388, 'mt': 390, 'mv': 392, 'mw': 393, 'yo': 687, 'yx': 696, 'yz': 698, 'sq': 532, 'nv': 419, 'oz': 452, 'ws': 638, 'cd': 106, 'nf': 403, 'gp': 228, 'nj': 407, 'jl': 304, 'nr': 415, 'sg': 522, 'i-': 267, 'bm': 90, 'tk': 554, \"u'\": 570, 'tw': 566, 'nx': 421, 'yb': 674, 'yh': 680, 'yp': 688, 'wh': 628, 'ao': 65, 'pf': 460, 'pg': 461, 'pk': 465, 'aq': 67, 'qa': 479, 'qi': 482, 'rv': 509, 'ux': 595, 'hd': 244, 'zm': 711, 'rj': 497, 'wr': 637, 'd-': 129, \"w'\": 619, 'zt': 718, 'zu': 719, 'rx': 511, 'rz': 513, 'sn': 529, 'ii': 276, 'sf': 521, 'hc': 243, 'hf': 246, \"v'\": 598, 'pn': 468, 'sr': 533, 'yr': 690, 'uy': 596, 'vd': 601, 'vg': 603, 'vn': 610, 'vr': 612, 'vt': 614, 'wb': 622, 'wf': 626, 'wk': 630, 'wt': 639, 'wu': 640, 'xf': 652, 'xl': 656, 'xs': 663, 'yg': 679, 'yu': 693, 'yy': 697, 'zb': 701, \"b'\": 77, \"'r\": 19, 'kb': 318, 'kd': 320, 'kf': 322, 'kg': 323, 'kp': 332, 'y-': 672, 'tj': 553, 'gb': 214, 'gd': 216, 'gw': 235, \"'i\": 11, 'sz': 541, 'kv': 337, 'xq': 661, 'fk': 195, 'sv': 537, 'vs': 613, 'wm': 632, 'tb': 545, 'td': 547, 'hg': 247, 'uw': 594, 'wd': 624, 'zl': 710, 'cv': 124, 'db': 131, 'df': 135, 'dp': 145, 'vu': 615, \"'o\": 17, 'zr': 716, 'nl': 409, 'jy': 314, \"z'\": 699, 'r-': 487, 'a-': 50, '-g': 31, 'o-': 426, 's-': 515, 'gq': 229, 'jr': 308, 'g-': 212, '-j': 34, 'hp': 256, 'vy': 617, 'zd': 703, 'zn': 712, 'xu': 665, 'xb': 648, 'kj': 326, '-r': 42, '-v': 46, 'bz': 101, 'tg': 550, 'sj': 525, 'gj': 222, 'wc': 623, 'xw': 667, 'xx': 668, 'yf': 678, 'jd': 297, 'zk': 709, 'tp': 559, 'fc': 187, 'uu': 592, '-u': 45, 'py': 476, 'h-': 240, 'zw': 721, 'pb': 456, 'pj': 464, 'pw': 475, \"q'\": 478, \"'v\": 23, 'jk': 303, 'pd': 458, 'pm': 467, 'gx': 236, 'zp': 714, 'bw': 99, '-e': 29, 'wg': 627, 'zf': 705, 'vl': 608, 'cw': 125, 'hv': 262, 'vc': 600, 'zs': 717, 'mj': 380, 'xh': 654, 'vv': 616, 'xv': 666, 'mz': 395, 'bf': 83, 'hq': 257, 'dq': 146, 'lx': 366, '-h': 32, 'vj': 606, 'x-': 646, 'xn': 658, 'xp': 660, 'jv': 312, 'zg': 706, '-y': 48, 'fj': 194, 'jt': 310, 'w-': 620, 'xg': 653, 'xm': 657, 'tx': 567, 'gz': 238, 'gv': 234, 'jj': 302, 'f-': 184, 'wv': 641, \"'l\": 14, 'hj': 250, 'fp': 200, 'js': 309, 'vh': 604, 'wz': 644, 'u-': 571, 'i': 265, 'qb': 480, 'qg': 481, 'zv': 720, 'jf': 299, 'jh': 300, 'jc': 296, 'wp': 635, 'bv': 98, 'pz': 477, 'fq': 201, 'cb': 104, 'cf': 108, 'cg': 109, 'cp': 118, \"j'\": 294, 'xz': 670, 'cj': 112, 'fx': 208, \"'j\": 12, 'fz': 210, 'qv': 485, 'wq': 636, 'ww': 642, 'xr': 662, 'xd': 650, 'o': 424, \"'b\": 5, \"'g\": 9, \"'k\": 13, 'vk': 607, 'qo': 483, 'vz': 618, 'jm': 305, 'yj': 682, 'p-': 454, 'fv': 206, 'bg': 84, \"'u\": 22, 'jw': 313, 'sx': 539, 'm-': 370, 'yq': 689, 'vm': 609, 'tq': 560}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 256\n",
      "hidden_size: 128\n",
      "n_layers: 1\n",
      "Encoder has a total number of 333568 parameters\n",
      "Decoder has a total number of 215844 parameters\n",
      "Total number of all parameters is 549412\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 1 finished in 0m 47.68s (- 78m 40.62s) (1 1.0%). train avg loss: 1.274\n",
      "Training for epoch 2 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 2 finished in 1m 33.3s (- 76m 11.87s) (2 2.0%). train avg loss: 0.6161\n",
      "Training for epoch 3 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 3 finished in 2m 19.63s (- 75m 14.79s) (3 3.0%). train avg loss: 0.5295\n",
      "Training for epoch 4 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 4 finished in 3m 6.79s (- 74m 42.87s) (4 4.0%). train avg loss: 0.4705\n",
      "Training for epoch 5 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 5 finished in 3m 53.83s (- 74m 2.73s) (5 5.0%). train avg loss: 0.4465\n",
      "Training for epoch 6 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 6 finished in 4m 40.29s (- 73m 11.26s) (6 6.0%). train avg loss: 0.4205\n",
      "Training for epoch 7 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 7 finished in 5m 27.16s (- 72m 26.59s) (7 7.0%). train avg loss: 0.4164\n",
      "Training for epoch 8 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 8 finished in 6m 13.65s (- 71m 37.03s) (8 8.0%). train avg loss: 0.3802\n",
      "Training for epoch 9 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 9 finished in 6m 60.0s (- 70m 46.67s) (9 9.0%). train avg loss: 0.3984\n",
      "Training for epoch 10 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 10 finished in 7m 45.71s (- 69m 51.43s) (10 10.0%). train avg loss: 0.3754\n",
      "Training for epoch 11 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 11 finished in 8m 31.91s (- 69m 1.78s) (11 11.0%). train avg loss: 0.3608\n",
      "Training for epoch 12 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 12 finished in 9m 17.1s (- 68m 5.38s) (12 12.0%). train avg loss: 0.3569\n",
      "Training for epoch 13 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 13 finished in 10m 4.44s (- 67m 25.09s) (13 13.0%). train avg loss: 0.3553\n",
      "Training for epoch 14 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 14 finished in 10m 52.76s (- 66m 49.83s) (14 14.0%). train avg loss: 0.3646\n",
      "Training for epoch 15 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 15 finished in 11m 38.65s (- 65m 59.03s) (15 15.0%). train avg loss: 0.3233\n",
      "Training for epoch 16 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 16 finished in 12m 25.16s (- 65m 12.1s) (16 16.0%). train avg loss: 0.3364\n",
      "Training for epoch 17 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 17 finished in 13m 10.23s (- 64m 18.2s) (17 17.0%). train avg loss: 0.3236\n",
      "Training for epoch 18 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 18 finished in 13m 56.55s (- 63m 30.93s) (18 18.0%). train avg loss: 0.3015\n",
      "Training for epoch 19 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 19 finished in 14m 43.55s (- 62m 46.7s) (19 19.0%). train avg loss: 0.3151\n",
      "Training for epoch 20 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 20 finished in 15m 28.8s (- 61m 55.2s) (20 20.0%). train avg loss: 0.3256\n",
      "Training for epoch 21 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 21 finished in 16m 14.5s (- 61m 5.98s) (21 21.0%). train avg loss: 0.3114\n",
      "Training for epoch 22 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 22 finished in 17m 0.84s (- 60m 19.33s) (22 22.0%). train avg loss: 0.3081\n",
      "Training for epoch 23 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 23 finished in 17m 47.26s (- 59m 33.01s) (23 23.0%). train avg loss: 0.2958\n",
      "Training for epoch 24 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 24 finished in 18m 33.62s (- 58m 46.46s) (24 24.0%). train avg loss: 0.3063\n",
      "Training for epoch 25 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 25 finished in 19m 20.18s (- 58m 0.53s) (25 25.0%). train avg loss: 0.2917\n",
      "Training for epoch 26 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 26 finished in 20m 5.58s (- 57m 11.28s) (26 26.0%). train avg loss: 0.2912\n",
      "Training for epoch 27 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 27 finished in 20m 50.78s (- 56m 21.75s) (27 27.0%). train avg loss: 0.2775\n",
      "Training for epoch 28 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 28 finished in 21m 37.63s (- 55m 36.75s) (28 28.0%). train avg loss: 0.2964\n",
      "Training for epoch 29 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 29 finished in 22m 23.91s (- 54m 50.27s) (29 29.0%). train avg loss: 0.2762\n",
      "Training for epoch 30 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 30 finished in 23m 8.83s (- 54m 0.61s) (30 30.0%). train avg loss: 0.2662\n",
      "Training for epoch 31 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 31 finished in 23m 54.19s (- 53m 12.23s) (31 31.0%). train avg loss: 0.2993\n",
      "Training for epoch 32 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 32 finished in 24m 41.86s (- 52m 28.95s) (32 32.0%). train avg loss: 0.2842\n",
      "Training for epoch 33 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 33 finished in 25m 29.06s (- 51m 44.45s) (33 33.0%). train avg loss: 0.2785\n",
      "Training for epoch 34 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 34 finished in 26m 16.38s (- 51m 0.04s) (34 34.0%). train avg loss: 0.2864\n",
      "Training for epoch 35 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 35 finished in 27m 4.79s (- 50m 17.47s) (35 35.0%). train avg loss: 0.278\n",
      "Training for epoch 36 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 36 finished in 27m 49.22s (- 49m 27.51s) (36 36.0%). train avg loss: 0.2523\n",
      "Training for epoch 37 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 37 finished in 28m 34.84s (- 48m 39.86s) (37 37.0%). train avg loss: 0.2483\n",
      "Training for epoch 38 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 38 finished in 29m 20.27s (- 47m 52.02s) (38 38.0%). train avg loss: 0.2563\n",
      "Training for epoch 39 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 39 finished in 30m 5.99s (- 47m 4.76s) (39 39.0%). train avg loss: 0.2522\n",
      "Training for epoch 40 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 40 finished in 30m 52.43s (- 46m 18.65s) (40 40.0%). train avg loss: 0.2362\n",
      "Training for epoch 41 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 41 finished in 31m 37.7s (- 45m 30.84s) (41 41.0%). train avg loss: 0.2576\n",
      "Training for epoch 42 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 42 finished in 32m 22.24s (- 44m 42.14s) (42 42.0%). train avg loss: 0.247\n",
      "Training for epoch 43 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 43 finished in 33m 8.53s (- 43m 55.96s) (43 43.0%). train avg loss: 0.2693\n",
      "Training for epoch 44 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 44 finished in 33m 53.91s (- 43m 8.61s) (44 44.0%). train avg loss: 0.2379\n",
      "Training for epoch 45 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 45 finished in 34m 40.32s (- 42m 22.61s) (45 45.0%). train avg loss: 0.2292\n",
      "Training for epoch 46 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 46 finished in 35m 25.23s (- 41m 34.84s) (46 46.0%). train avg loss: 0.2321\n",
      "Training for epoch 47 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 47 finished in 36m 9.97s (- 40m 46.98s) (47 47.0%). train avg loss: 0.2315\n",
      "Training for epoch 48 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 48 finished in 36m 54.43s (- 39m 58.97s) (48 48.0%). train avg loss: 0.2189\n",
      "Training for epoch 49 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 49 finished in 37m 38.91s (- 39m 11.11s) (49 49.0%). train avg loss: 0.2264\n",
      "Training for epoch 50 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 50 finished in 38m 24.84s (- 38m 24.84s) (50 50.0%). train avg loss: 0.248\n",
      "Training for epoch 51 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 51 finished in 39m 10.23s (- 37m 38.06s) (51 51.0%). train avg loss: 0.2427\n",
      "Training for epoch 52 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 52 finished in 39m 55.45s (- 36m 51.18s) (52 52.0%). train avg loss: 0.2191\n",
      "Training for epoch 53 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 53 finished in 40m 41.06s (- 36m 4.72s) (53 53.0%). train avg loss: 0.2386\n",
      "Training for epoch 54 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 54 finished in 41m 25.47s (- 35m 17.25s) (54 54.0%). train avg loss: 0.2174\n",
      "Training for epoch 55 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 55 finished in 42m 10.15s (- 34m 30.13s) (55 55.0%). train avg loss: 0.2215\n",
      "Training for epoch 56 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 56 finished in 42m 55.11s (- 33m 43.3s) (56 56.0%). train avg loss: 0.2504\n",
      "Training for epoch 57 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 57 finished in 43m 40.3s (- 32m 56.71s) (57 57.0%). train avg loss: 0.2262\n",
      "Training for epoch 58 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 58 finished in 44m 24.66s (- 32m 9.58s) (58 58.0%). train avg loss: 0.2201\n",
      "Training for epoch 59 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 59 finished in 45m 9.24s (- 31m 22.69s) (59 59.0%). train avg loss: 0.2243\n",
      "Training for epoch 60 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 60 finished in 45m 54.15s (- 30m 36.1s) (60 60.0%). train avg loss: 0.2605\n",
      "Training for epoch 61 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 61 finished in 46m 38.04s (- 29m 48.91s) (61 61.0%). train avg loss: 0.2272\n",
      "Training for epoch 62 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 62 finished in 47m 23.16s (- 29m 2.58s) (62 62.0%). train avg loss: 0.2035\n",
      "Training for epoch 63 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 63 finished in 48m 7.76s (- 28m 15.99s) (63 63.0%). train avg loss: 0.2064\n",
      "Training for epoch 64 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 64 finished in 48m 52.04s (- 27m 29.27s) (64 64.0%). train avg loss: 0.21\n",
      "Training for epoch 65 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 65 finished in 49m 36.61s (- 26m 42.79s) (65 65.0%). train avg loss: 0.2101\n",
      "Training for epoch 66 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 66 finished in 50m 26.83s (- 25m 59.28s) (66 66.0%). train avg loss: 0.2188\n",
      "Training for epoch 67 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 67 finished in 51m 11.76s (- 25m 12.96s) (67 67.0%). train avg loss: 0.2084\n",
      "Training for epoch 68 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 68 finished in 51m 57.92s (- 24m 27.26s) (68 68.0%). train avg loss: 0.231\n",
      "Training for epoch 69 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 69 finished in 52m 42.93s (- 23m 41.03s) (69 69.0%). train avg loss: 0.2053\n",
      "Training for epoch 70 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 70 finished in 53m 27.83s (- 22m 54.79s) (70 70.0%). train avg loss: 0.1871\n",
      "Training for epoch 71 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 71 finished in 54m 12.98s (- 22m 8.68s) (71 71.0%). train avg loss: 0.244\n",
      "Training for epoch 72 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 72 finished in 54m 57.67s (- 21m 22.43s) (72 72.0%). train avg loss: 0.2095\n",
      "Training for epoch 73 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 73 finished in 55m 45.48s (- 20m 37.37s) (73 73.0%). train avg loss: 0.2371\n",
      "Training for epoch 74 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 74 finished in 56m 30.89s (- 19m 51.39s) (74 74.0%). train avg loss: 0.2092\n",
      "Training for epoch 75 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 75 finished in 57m 14.63s (- 19m 4.88s) (75 75.0%). train avg loss: 0.2051\n",
      "Training for epoch 76 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 76 finished in 57m 58.13s (- 18m 18.36s) (76 76.0%). train avg loss: 0.2052\n",
      "Training for epoch 77 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 77 finished in 58m 43.74s (- 17m 32.55s) (77 77.0%). train avg loss: 0.1957\n",
      "Training for epoch 78 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 78 finished in 59m 28.93s (- 16m 46.62s) (78 78.0%). train avg loss: 0.2402\n",
      "Training for epoch 79 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 79 finished in 60m 13.37s (- 16m 0.52s) (79 79.0%). train avg loss: 0.1952\n",
      "Training for epoch 80 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 80 finished in 60m 58.48s (- 15m 14.62s) (80 80.0%). train avg loss: 0.1911\n",
      "Training for epoch 81 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 81 finished in 61m 43.66s (- 14m 28.76s) (81 81.0%). train avg loss: 0.1932\n",
      "Training for epoch 82 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 82 finished in 62m 28.0s (- 13m 42.73s) (82 82.0%). train avg loss: 0.1781\n",
      "Training for epoch 83 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 83 finished in 63m 12.38s (- 12m 56.75s) (83 83.0%). train avg loss: 0.1613\n",
      "Training for epoch 84 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 84 finished in 63m 57.12s (- 12m 10.88s) (84 84.0%). train avg loss: 0.1574\n",
      "Training for epoch 85 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 85 finished in 64m 41.54s (- 11m 24.98s) (85 85.0%). train avg loss: 0.1506\n",
      "Training for epoch 86 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 86 finished in 65m 26.72s (- 10m 39.23s) (86 86.0%). train avg loss: 0.1474\n",
      "Training for epoch 87 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 87 finished in 66m 11.14s (- 9m 53.39s) (87 87.0%). train avg loss: 0.1501\n",
      "Training for epoch 88 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 88 finished in 66m 56.72s (- 9m 7.73s) (88 88.0%). train avg loss: 0.1439\n",
      "Training for epoch 89 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 89 finished in 67m 41.44s (- 8m 21.98s) (89 89.0%). train avg loss: 0.137\n",
      "Training for epoch 90 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 90 finished in 68m 27.86s (- 7m 36.43s) (90 90.0%). train avg loss: 0.1386\n",
      "Training for epoch 91 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 91 finished in 69m 15.17s (- 6m 50.95s) (91 91.0%). train avg loss: 0.1376\n",
      "Training for epoch 92 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 92 finished in 70m 0.04s (- 6m 5.22s) (92 92.0%). train avg loss: 0.1405\n",
      "Training for epoch 93 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 93 finished in 70m 43.77s (- 5m 19.42s) (93 93.0%). train avg loss: 0.1414\n",
      "Training for epoch 94 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 94 finished in 71m 28.43s (- 4m 33.73s) (94 94.0%). train avg loss: 0.1352\n",
      "Training for epoch 95 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 95 finished in 72m 13.19s (- 3m 48.06s) (95 95.0%). train avg loss: 0.135\n",
      "Training for epoch 96 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 96 finished in 72m 57.22s (- 3m 2.38s) (96 96.0%). train avg loss: 0.1354\n",
      "Training for epoch 97 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 97 finished in 73m 42.33s (- 2m 16.77s) (97 97.0%). train avg loss: 0.1445\n",
      "Training for epoch 98 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 98 finished in 74m 30.04s (- 1m 31.23s) (98 98.0%). train avg loss: 0.1351\n",
      "Training for epoch 99 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 99 finished in 75m 15.12s (- 0m 45.61s) (99 99.0%). train avg loss: 0.1287\n",
      "Training for epoch 100 has started (lr=0.0005). Found 2135 batch(es).\n",
      "Epoch 100 finished in 76m 0.72s (- 0m 0.0s) (100 100.0%). train avg loss: 0.13\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "best_train_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on train loss\n",
    "  encoder_scheduler.step(avg_train_loss)\n",
    "  decoder_scheduler.step(avg_train_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "\n",
    "  # Save the model if the train loss is better than the previous iterations' train loss\n",
    "  if avg_train_loss < best_train_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_train_loss = avg_train_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"train\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5, min_value+.075, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 17.908648168217535%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in test_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(test_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> minoru\n",
      "= ['M', 'IY', 'N', 'AO', 'R', 'UW']\n",
      "< M IY N AO R UW ['M', 'IY', 'N', 'AO', 'R', 'UW']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe3af091af0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x560 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
