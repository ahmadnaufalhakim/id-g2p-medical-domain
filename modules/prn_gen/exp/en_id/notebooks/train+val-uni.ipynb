{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn_gen/exp/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"128\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models_fallback\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  if split_name == \"train+val\" :\n",
    "    print(f\"Merging train and val entries ..\")\n",
    "    with open(os.path.join(DATA_DIR, f\"train.csv\"), encoding=\"utf-8\") as f_train_csv, \\\n",
    "         open(os.path.join(DATA_DIR, f\"val.csv\"), encoding=\"utf-8\") as f_val_csv :\n",
    "      next(f_train_csv, None)\n",
    "      next(f_val_csv, None)\n",
    "      train_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_train_csv]\n",
    "      val_pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_val_csv]\n",
    "      pairs = train_pairs + val_pairs\n",
    "      graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "      phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "      lang_list = [pair[2] for pair in pairs]\n",
    "      g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "    return g2p_dataset, pairs\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging train and val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6138403889183434\n",
      "ID_WEIGHT: 2.6960571496230807\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train+val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 30, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f036c6c95e0> ([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "test grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'l': 17, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'w': 28, 'v': 27, 'z': 31, 'j': 15, 'f': 11, 'x': 29}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'l': 17, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'w': 28, 'v': 27, 'z': 31, 'j': 15, 'f': 11, 'x': 29}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 128\n",
      "n_layers: 1\n",
      "Encoder has a total number of 76544 parameters\n",
      "Decoder has a total number of 135204 parameters\n",
      "Total number of all parameters is 211748\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 1 finished in 0m 58.58s (- 96m 39.06s) (1 1.0%). train avg loss: 0.9486\n",
      "Training for epoch 2 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 2 finished in 1m 47.34s (- 87m 39.83s) (2 2.0%). train avg loss: 0.5342\n",
      "Training for epoch 3 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 3 finished in 2m 37.38s (- 84m 48.58s) (3 3.0%). train avg loss: 0.4887\n",
      "Training for epoch 4 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 4 finished in 3m 24.45s (- 81m 46.72s) (4 4.0%). train avg loss: 0.4273\n",
      "Training for epoch 5 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 5 finished in 4m 15.11s (- 80m 47.13s) (5 5.0%). train avg loss: 0.3986\n",
      "Training for epoch 6 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 6 finished in 5m 11.12s (- 81m 14.23s) (6 6.0%). train avg loss: 0.3877\n",
      "Training for epoch 7 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 7 finished in 6m 1.63s (- 80m 4.51s) (7 7.0%). train avg loss: 0.3705\n",
      "Training for epoch 8 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 8 finished in 6m 48.48s (- 78m 17.52s) (8 8.0%). train avg loss: 0.3887\n",
      "Training for epoch 9 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 9 finished in 7m 35.23s (- 76m 42.89s) (9 9.0%). train avg loss: 0.35\n",
      "Training for epoch 10 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 10 finished in 8m 22.42s (- 75m 21.8s) (10 10.0%). train avg loss: 0.3411\n",
      "Training for epoch 11 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 11 finished in 9m 10.06s (- 74m 10.49s) (11 11.0%). train avg loss: 0.3052\n",
      "Training for epoch 12 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 12 finished in 9m 57.17s (- 72m 59.22s) (12 12.0%). train avg loss: 0.3237\n",
      "Training for epoch 13 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 13 finished in 10m 44.28s (- 71m 51.74s) (13 13.0%). train avg loss: 0.3222\n",
      "Training for epoch 14 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 14 finished in 11m 34.41s (- 71m 5.66s) (14 14.0%). train avg loss: 0.298\n",
      "Training for epoch 15 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 15 finished in 12m 21.64s (- 70m 2.63s) (15 15.0%). train avg loss: 0.2954\n",
      "Training for epoch 16 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 16 finished in 13m 9.37s (- 69m 4.17s) (16 16.0%). train avg loss: 0.2968\n",
      "Training for epoch 17 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 17 finished in 13m 56.62s (- 68m 4.66s) (17 17.0%). train avg loss: 0.2744\n",
      "Training for epoch 18 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 18 finished in 14m 44.45s (- 67m 9.16s) (18 18.0%). train avg loss: 0.2717\n",
      "Training for epoch 19 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 19 finished in 15m 31.7s (- 66m 11.97s) (19 19.0%). train avg loss: 0.2687\n",
      "Training for epoch 20 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 20 finished in 16m 19.34s (- 65m 17.35s) (20 20.0%). train avg loss: 0.2597\n",
      "Training for epoch 21 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 21 finished in 17m 7.07s (- 64m 23.76s) (21 21.0%). train avg loss: 0.2931\n",
      "Training for epoch 22 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 22 finished in 17m 52.06s (- 63m 20.96s) (22 22.0%). train avg loss: 0.2561\n",
      "Training for epoch 23 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 23 finished in 18m 36.72s (- 62m 18.57s) (23 23.0%). train avg loss: 0.249\n",
      "Training for epoch 24 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 24 finished in 19m 18.53s (- 61m 8.67s) (24 24.0%). train avg loss: 0.2471\n",
      "Training for epoch 25 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 25 finished in 20m 1.54s (- 60m 4.61s) (25 25.0%). train avg loss: 0.2329\n",
      "Training for epoch 26 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 26 finished in 20m 43.89s (- 59m 0.3s) (26 26.0%). train avg loss: 0.2359\n",
      "Training for epoch 27 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 27 finished in 21m 26.21s (- 57m 57.54s) (27 27.0%). train avg loss: 0.2395\n",
      "Training for epoch 28 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 28 finished in 22m 8.32s (- 56m 55.69s) (28 28.0%). train avg loss: 0.2493\n",
      "Training for epoch 29 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 29 finished in 22m 50.44s (- 55m 55.21s) (29 29.0%). train avg loss: 0.2292\n",
      "Training for epoch 30 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 30 finished in 23m 32.01s (- 54m 54.7s) (30 30.0%). train avg loss: 0.2197\n",
      "Training for epoch 31 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 31 finished in 24m 19.29s (- 54m 8.1s) (31 31.0%). train avg loss: 0.2299\n",
      "Training for epoch 32 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 32 finished in 25m 6.48s (- 53m 21.26s) (32 32.0%). train avg loss: 0.2224\n",
      "Training for epoch 33 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 33 finished in 25m 53.69s (- 52m 34.47s) (33 33.0%). train avg loss: 0.2189\n",
      "Training for epoch 34 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 34 finished in 26m 40.9s (- 51m 47.62s) (34 34.0%). train avg loss: 0.2063\n",
      "Training for epoch 35 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 35 finished in 27m 27.41s (- 50m 59.48s) (35 35.0%). train avg loss: 0.2136\n",
      "Training for epoch 36 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 36 finished in 28m 14.11s (- 50m 11.76s) (36 36.0%). train avg loss: 0.2181\n",
      "Training for epoch 37 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 37 finished in 29m 1.27s (- 49m 24.86s) (37 37.0%). train avg loss: 0.2185\n",
      "Training for epoch 38 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 38 finished in 29m 48.08s (- 48m 37.4s) (38 38.0%). train avg loss: 0.2196\n",
      "Training for epoch 39 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 39 finished in 30m 34.85s (- 47m 49.89s) (39 39.0%). train avg loss: 0.219\n",
      "Training for epoch 40 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 40 finished in 31m 22.13s (- 47m 3.2s) (40 40.0%). train avg loss: 0.2053\n",
      "Training for epoch 41 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 41 finished in 32m 9.16s (- 46m 16.11s) (41 41.0%). train avg loss: 0.2077\n",
      "Training for epoch 42 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 42 finished in 32m 55.67s (- 45m 28.31s) (42 42.0%). train avg loss: 0.1921\n",
      "Training for epoch 43 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 43 finished in 33m 42.93s (- 44m 41.55s) (43 43.0%). train avg loss: 0.2011\n",
      "Training for epoch 44 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 44 finished in 34m 29.96s (- 43m 54.49s) (44 44.0%). train avg loss: 0.2007\n",
      "Training for epoch 45 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 45 finished in 35m 16.83s (- 43m 7.23s) (45 45.0%). train avg loss: 0.2062\n",
      "Training for epoch 46 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 46 finished in 36m 3.42s (- 42m 19.67s) (46 46.0%). train avg loss: 0.1933\n",
      "Training for epoch 47 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 47 finished in 36m 50.52s (- 41m 32.71s) (47 47.0%). train avg loss: 0.185\n",
      "Training for epoch 48 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 48 finished in 37m 37.36s (- 40m 45.47s) (48 48.0%). train avg loss: 0.1848\n",
      "Training for epoch 49 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 49 finished in 38m 24.68s (- 39m 58.74s) (49 49.0%). train avg loss: 0.1961\n",
      "Training for epoch 50 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 50 finished in 39m 11.58s (- 39m 11.58s) (50 50.0%). train avg loss: 0.1912\n",
      "Training for epoch 51 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 51 finished in 39m 58.61s (- 38m 24.55s) (51 51.0%). train avg loss: 0.1857\n",
      "Training for epoch 52 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 52 finished in 40m 45.52s (- 37m 37.41s) (52 52.0%). train avg loss: 0.1899\n",
      "Training for epoch 53 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 53 finished in 41m 32.4s (- 36m 50.24s) (53 53.0%). train avg loss: 0.1805\n",
      "Training for epoch 54 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 54 finished in 42m 19.42s (- 36m 3.21s) (54 54.0%). train avg loss: 0.1979\n",
      "Training for epoch 55 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 55 finished in 43m 6.39s (- 35m 16.14s) (55 55.0%). train avg loss: 0.1915\n",
      "Training for epoch 56 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 56 finished in 43m 53.53s (- 34m 29.2s) (56 56.0%). train avg loss: 0.2103\n",
      "Training for epoch 57 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 57 finished in 44m 40.48s (- 33m 42.12s) (57 57.0%). train avg loss: 0.1863\n",
      "Training for epoch 58 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 58 finished in 45m 27.84s (- 32m 55.33s) (58 58.0%). train avg loss: 0.1722\n",
      "Training for epoch 59 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 59 finished in 46m 14.63s (- 32m 8.14s) (59 59.0%). train avg loss: 0.1834\n",
      "Training for epoch 60 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 60 finished in 47m 1.6s (- 31m 21.07s) (60 60.0%). train avg loss: 0.1834\n",
      "Training for epoch 61 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 61 finished in 47m 48.6s (- 30m 34.02s) (61 61.0%). train avg loss: 0.1887\n",
      "Training for epoch 62 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 62 finished in 48m 35.72s (- 29m 47.05s) (62 62.0%). train avg loss: 0.1791\n",
      "Training for epoch 63 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 63 finished in 49m 22.61s (- 28m 59.95s) (63 63.0%). train avg loss: 0.1743\n",
      "Training for epoch 64 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 64 finished in 50m 18.13s (- 28m 17.7s) (64 64.0%). train avg loss: 0.1718\n",
      "Training for epoch 65 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 65 finished in 51m 5.19s (- 27m 30.49s) (65 65.0%). train avg loss: 0.1793\n",
      "Training for epoch 66 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 66 finished in 51m 48.93s (- 26m 41.57s) (66 66.0%). train avg loss: 0.1762\n",
      "Training for epoch 67 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 67 finished in 52m 35.31s (- 25m 54.11s) (67 67.0%). train avg loss: 0.1634\n",
      "Training for epoch 68 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 68 finished in 53m 22.15s (- 25m 6.89s) (68 68.0%). train avg loss: 0.1841\n",
      "Training for epoch 69 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 69 finished in 54m 6.75s (- 24m 18.68s) (69 69.0%). train avg loss: 0.1772\n",
      "Training for epoch 70 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 70 finished in 54m 50.95s (- 23m 30.41s) (70 70.0%). train avg loss: 0.1685\n",
      "Training for epoch 71 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 71 finished in 55m 38.17s (- 22m 43.48s) (71 71.0%). train avg loss: 0.1778\n",
      "Training for epoch 72 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 72 finished in 56m 24.58s (- 21m 56.23s) (72 72.0%). train avg loss: 0.1784\n",
      "Training for epoch 73 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 73 finished in 57m 8.9s (- 21m 8.22s) (73 73.0%). train avg loss: 0.1711\n",
      "Training for epoch 74 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 74 finished in 57m 51.39s (- 20m 19.68s) (74 74.0%). train avg loss: 0.1618\n",
      "Training for epoch 75 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 75 finished in 58m 34.58s (- 19m 31.53s) (75 75.0%). train avg loss: 0.1824\n",
      "Training for epoch 76 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 76 finished in 59m 16.68s (- 18m 43.16s) (76 76.0%). train avg loss: 0.1657\n",
      "Training for epoch 77 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 77 finished in 59m 58.36s (- 17m 54.84s) (77 77.0%). train avg loss: 0.1705\n",
      "Training for epoch 78 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 78 finished in 60m 40.7s (- 17m 6.86s) (78 78.0%). train avg loss: 0.1637\n",
      "Training for epoch 79 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 79 finished in 61m 25.19s (- 16m 19.61s) (79 79.0%). train avg loss: 0.1649\n",
      "Training for epoch 80 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 80 finished in 62m 8.03s (- 15m 32.01s) (80 80.0%). train avg loss: 0.1726\n",
      "Training for epoch 81 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 81 finished in 62m 50.04s (- 14m 44.33s) (81 81.0%). train avg loss: 0.1587\n",
      "Training for epoch 82 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 82 finished in 63m 33.63s (- 13m 57.14s) (82 82.0%). train avg loss: 0.1734\n",
      "Training for epoch 83 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 83 finished in 64m 16.33s (- 13m 9.85s) (83 83.0%). train avg loss: 0.1682\n",
      "Training for epoch 84 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 84 finished in 65m 0.69s (- 12m 22.99s) (84 84.0%). train avg loss: 0.1704\n",
      "Training for epoch 85 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 85 finished in 65m 45.63s (- 11m 36.29s) (85 85.0%). train avg loss: 0.1681\n",
      "Training for epoch 86 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 86 finished in 66m 30.99s (- 10m 49.7s) (86 86.0%). train avg loss: 0.1617\n",
      "Training for epoch 87 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 87 finished in 67m 16.59s (- 10m 3.17s) (87 87.0%). train avg loss: 0.1694\n",
      "Training for epoch 88 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 88 finished in 68m 1.45s (- 9m 16.56s) (88 88.0%). train avg loss: 0.1652\n",
      "Training for epoch 89 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 89 finished in 68m 44.2s (- 8m 29.73s) (89 89.0%). train avg loss: 0.1674\n",
      "Training for epoch 90 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 90 finished in 69m 31.15s (- 7m 43.46s) (90 90.0%). train avg loss: 0.1594\n",
      "Training for epoch 91 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 91 finished in 70m 16.49s (- 6m 57.02s) (91 91.0%). train avg loss: 0.1675\n",
      "Training for epoch 92 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 92 finished in 71m 1.27s (- 6m 10.55s) (92 92.0%). train avg loss: 0.1575\n",
      "Training for epoch 93 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 93 finished in 71m 48.15s (- 5m 24.27s) (93 93.0%). train avg loss: 0.1658\n",
      "Training for epoch 94 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 94 finished in 72m 32.97s (- 4m 37.85s) (94 94.0%). train avg loss: 0.1537\n",
      "Training for epoch 95 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 95 finished in 73m 17.79s (- 3m 51.46s) (95 95.0%). train avg loss: 0.1536\n",
      "Training for epoch 96 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 96 finished in 74m 2.31s (- 3m 5.1s) (96 96.0%). train avg loss: 0.156\n",
      "Training for epoch 97 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 97 finished in 74m 47.32s (- 2m 18.78s) (97 97.0%). train avg loss: 0.1609\n",
      "Training for epoch 98 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 98 finished in 75m 33.94s (- 1m 32.53s) (98 98.0%). train avg loss: 0.1606\n",
      "Training for epoch 99 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 99 finished in 76m 18.86s (- 0m 46.25s) (99 99.0%). train avg loss: 0.1586\n",
      "Training for epoch 100 has started (lr=0.001). Found 2135 batch(es).\n",
      "Epoch 100 finished in 77m 3.24s (- 0m 0.0s) (100 100.0%). train avg loss: 0.1586\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "best_train_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on train loss\n",
    "  encoder_scheduler.step(avg_train_loss)\n",
    "  decoder_scheduler.step(avg_train_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "\n",
    "  # Save the model if the train loss is better than the previous iterations' train loss\n",
    "  if avg_train_loss < best_train_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_train_loss = avg_train_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAHFCAYAAAAg3/mzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOzdd3RUdf7G8ffMpHdSSAKEJPTeexEVBRERdRUsC6KgYmORtSG768q6skVRUEH8IaIuIhZcG4KwdJAqTXoLoSSEBNJJm5nfH0kGYhJIQjKXMM/rnDnH3Nx753MDMk++1WS32+2IiIiIGMRsdAEiIiLi2hRGRERExFAKIyIiImIohRERERExlMKIiIiIGEphRERERAylMCIiIiKGUhgRERERQymMiIiIiKEURkTkisydOxeTycSWLVuMLkVEaimFERERETGUwoiIiIgYSmFERGpcfHw8v//976lbty6enp60bNmSN954A5vNVuK8mTNn0r59e/z8/PD396dFixa89NJLju9nZ2fz7LPPEhsbi5eXF8HBwXTp0oX58+c7+5FEpBq5GV2AiFzbzpw5Q69evcjLy+Nvf/sbMTExfP/99zz77LMcPnyYGTNmAPDZZ5/xxBNP8PTTT/P6669jNps5dOgQe/bscdxrwoQJfPLJJ7z66qt07NiRrKwsfv31V1JSUox6PBGpBgojIlKjpk6dysmTJ9m4cSPdunUDYODAgVitVt577z3Gjx9Ps2bNWLduHUFBQUyfPt1xbf/+/Uvca926dQwYMIBnnnnGcWzw4MHOeRARqTHqphGRGrV8+XJatWrlCCLFRo0ahd1uZ/ny5QB069aN1NRU7rvvPr755huSk5NL3atbt278+OOPvPjii6xcuZLz58875RlEpGYpjIhIjUpJSSEyMrLU8Xr16jm+DzBixAjmzJnDsWPH+N3vfkfdunXp3r07S5cudVwzffp0XnjhBf773/9yww03EBwczB133MHBgwed8zAiUiMURkSkRoWEhJCQkFDq+KlTpwAIDQ11HHvooYdYv349aWlp/PDDD9jtdm677TaOHTsGgK+vL6+88gr79u0jMTGRmTNnsmHDBoYMGeKchxGRGqEwIiI1qn///uzZs4dffvmlxPGPP/4Yk8nEDTfcUOoaX19fBg0axKRJk8jLy2P37t2lzgkPD2fUqFHcd9997N+/n+zs7Bp7BhGpWRrAKiLVYvny5cTFxZU6/thjj/Hxxx8zePBgJk+eTHR0ND/88AMzZszg8ccfp1mzZgA88sgjeHt707t3byIjI0lMTGTKlCkEBgbStWtXALp3785tt91Gu3btqFOnDnv37uWTTz6hZ8+e+Pj4OPNxRaQamex2u93oIkSk9po7dy4PPfRQud8/evQoZrOZiRMnsmTJEtLT02nUqBFjxoxhwoQJmM2FDbQff/wxc+fOZc+ePZw7d47Q0FD69OnDn/70J9q2bQvAxIkTWbZsGYcPHyY7O5v69eszdOhQJk2aREhIiFOeV0Sqn8KIiIiIGEpjRkRERMRQCiMiIiJiKIURERERMZTCiIiIiBhKYUREREQMpTAiIiIihqoVi57ZbDZOnTqFv78/JpPJ6HJERESkAux2OxkZGdSrV8+xplBZakUYOXXqFFFRUUaXISIiIlVw/PhxGjRoUO73a0UY8ff3BwofJiAgwOBqREREpCLS09OJiopyfI6Xp1aEkeKumYCAAIURERGRWuZyQyw0gFVEREQMpTAiIiIihlIYEREREUPVijEjIiIiNcVqtZKfn290GbWSu7s7Fovliu+jMCIiIi7JbreTmJhIamqq0aXUakFBQURERFzROmAKIyIi4pKKg0jdunXx8fHRopqVZLfbyc7OJikpCYDIyMgq30thREREXI7VanUEkZCQEKPLqbW8vb0BSEpKom7dulXustEAVhERcTnFY0R8fHwMrqT2K/4ZXsm4G4URERFxWeqauXLV8TNUGBERERFDKYyIiIi4qJiYGN566y2jy9AAVhERkdrk+uuvp0OHDtUSIjZv3oyvr281VHVlXDqMpGbnkZFTQKCPOwFe7kaXIyIicsXsdjtWqxU3t8t/xIeFhTmhostz6W6aSV//St9/reDrX04aXYqIiMhljRo1ilWrVjFt2jRMJhMmk4m5c+diMplYsmQJXbp0wdPTkzVr1nD48GGGDh1KeHg4fn5+dO3alWXLlpW432+7aUwmE7Nnz+bOO+/Ex8eHpk2b8u2339b4c7l0GPFwK3z8vAKbwZWIiIjR7HY72XkFhrzsdnuFapw2bRo9e/bkkUceISEhgYSEBKKiogB4/vnnmTJlCnv37qVdu3ZkZmZy6623smzZMrZt28bAgQMZMmQI8fHxl3yPV155hWHDhrFz505uvfVWHnjgAc6ePXvFP99LceluGndL4XSkPKvCiIiIqzufb6XVX5YY8t57Jg/Ex+PyH8mBgYF4eHjg4+NDREQEAPv27QNg8uTJ3HzzzY5zQ0JCaN++vePrV199la+//ppvv/2Wp556qtz3GDVqFPfddx8Ar732Gm+//TabNm3illtuqdKzVYRaRlDLiIiI1H5dunQp8XVWVhbPP/88rVq1IigoCD8/P/bt23fZlpF27do5/tvX1xd/f3/Hku81xaVbRjyKlq1Vy4iIiHi7W9gzeaBh732lfjsr5rnnnmPJkiW8/vrrNGnSBG9vb+6++27y8vIueR9395ITOkwmEzZbzX5OunQYcXcr7KbJV8uIiIjLM5lMFeoqMZqHhwdWq/Wy561Zs4ZRo0Zx5513ApCZmUlcXFwNV1c1Lt1N42kp6qZRy4iIiNQSMTExbNy4kbi4OJKTk8tttWjSpAkLFy5k+/bt7Nixg/vvv7/GWziqyqXDiMaMiIhIbfPss89isVho1aoVYWFh5Y4BefPNN6lTpw69evViyJAhDBw4kE6dOjm52oq5+tujapC7WkZERKSWadasGT///HOJY6NGjSp1XkxMDMuXLy9x7Mknnyzx9W+7bcqaYpyamlq1QitBLSOoZURERMRICiMojIiIiBjJpcNIcTdNvrppREREDOPSYcTTTWNGREREjObSYcTDom4aERFXVtE9YaR81fEzdOkwcmE2jf4yioi4kuJVRrOzsw2upPYr/hn+duXWynDpqb0awCoi4posFgtBQUGOPVd8fHwwmUwGV1W72O12srOzSUpKIigoCIul6kvaK4wAeQWXX1ZXRESuLcW73tb0JnDXuqCgIMfPsqpcOoxcmE2jbhoREVdjMpmIjIykbt265OfnG11OreTu7n5FLSLFXDqMeKqbRkTE5Vkslmr5QJWqc+kBrB6a2isiImI4lw4jjm4atYyIiIgYxqXDSHHLSK5aRkRERAzj2mHkokXPtPCNiIiIMRRGihTYFEZERESM4NphxO3C42tGjYiIiDEURooojIiIiBjDpcOIxWzCXLT6b74GsYqIiBjCpcMIXDSjRi0jIiIihlAYcSwJrzAiIiJiBIURrcIqIiJiKIURi/anERERMZLLhxF3N3XTiIiIGMnlw0hxy4gGsIqIiBhDYcRN3TQiIiJGcvkw4ti516rl4EVERIzg8mFELSMiIiLGcvkw4umY2ms1uBIRERHX5PJhxNFNU6BuGhERESO4fBhxzKbR1F4RERFDKIxozIiIiIihXD6MuGtvGhEREUO5fBhRy4iIiIixXD6MeCqMiIiIGMrlw4i7xQSom0ZERMQoLh9GirtptDeNiIiIMRRGLBYA8tQyIiIiYgiXDyPubkXdNGoZERERMYTLh5HiRc/UMiIiImIMlw8jmk0jIiJiLJcPI1r0TERExFguH0Y0m0ZERMRYCiPqphERETGUy4cRddOIiIgYy+XDiKNlRGFERETEEC4fRjwt6qYRERExksuHEXe34m4au8GViIiIuCaXDyMeahkRERExlMKIxoyIiIgYyuXDiLtaRkRERAxVpTAyY8YMYmNj8fLyonPnzqxZs+aS58+bN4/27dvj4+NDZGQkDz30ECkpKVUquLppOXgRERFjVTqMLFiwgPHjxzNp0iS2bdtG3759GTRoEPHx8WWev3btWkaOHMno0aPZvXs3X3zxBZs3b2bMmDFXXHx18HDTOiMiIiJGqnQYmTp1KqNHj2bMmDG0bNmSt956i6ioKGbOnFnm+Rs2bCAmJoZx48YRGxtLnz59eOyxx9iyZcsVF18dirtpCmx2bDbNqBEREXG2SoWRvLw8tm7dyoABA0ocHzBgAOvXry/zml69enHixAkWLVqE3W7n9OnTfPnllwwePLjqVVej4pYR0CBWERERI1QqjCQnJ2O1WgkPDy9xPDw8nMTExDKv6dWrF/PmzWP48OF4eHgQERFBUFAQb7/9drnvk5ubS3p6eolXTSme2gsKIyIiIkao0gBWk8lU4mu73V7qWLE9e/Ywbtw4/vKXv7B161YWL17M0aNHGTt2bLn3nzJlCoGBgY5XVFRUVcqsEHfLhbo1iFVERMT5KhVGQkNDsVgspVpBkpKSSrWWFJsyZQq9e/fmueeeo127dgwcOJAZM2YwZ84cEhISyrxm4sSJpKWlOV7Hjx+vTJmVYjKZtPCZiIiIgSoVRjw8POjcuTNLly4tcXzp0qX06tWrzGuys7Mxm0u+jcViAQpbVMri6elJQEBAiVdN0owaERER41S6m2bChAnMnj2bOXPmsHfvXp555hni4+Md3S4TJ05k5MiRjvOHDBnCwoULmTlzJkeOHGHdunWMGzeObt26Ua9evep7kitQ3FWjlhERERHnc6vsBcOHDyclJYXJkyeTkJBAmzZtWLRoEdHR0QAkJCSUWHNk1KhRZGRk8M477/DHP/6RoKAgbrzxRv75z39W31NcoeKWkVyFEREREacz2cvrK7mKpKenExgYSFpaWo102fT553JOnDvP10/0omPDOtV+fxEREVdU0c9vl9+bBi7aLE8tIyIiIk6nMMKFtUa0zoiIiIjzKYyg2TQiIiJGUhgBrTMiIiJiIIURNJtGRETESAojXNi5N9961U8sEhERueYojKDZNCIiIkZSGOHiMGI1uBIRERHXozDChQGs6qYRERFxPoURtM6IiIiIkRRG0GwaERERIymMcPFsGoURERERZ1MYQbNpREREjKQwgsKIiIiIkRRGAA+LCVA3jYiIiBEURlDLiIiIiJEURrgwtTdXLSMiIiJOpzACuBe1jOSrZURERMTpFEbQomciIiJGUhjhwpgRDWAVERFxPoURLmoZUTeNiIiI0ymMoNk0IiIiRlIY4aIwol17RUREnE5hhAt70+QVWA2uRERExPUojHBxy4i6aURERJxNYYQLA1jzC9RNIyIi4mwKI6hlRERExEgKI2hqr4iIiJEURlDLiIiIiJEURrh4No0Nu13jRkRERJxJYYQLLSMA+VprRERExKkURgDPEmFEXTUiIiLOpDDChW4a0CBWERERZ1MYASxmExazCdAgVhEREWdTGCnibikKI2oZERERcSqFkSKOtUbUMiIiIuJUCiNFPNwsgFpGREREnE1hpIhHUTeNZtOIiIg4l8JIEccqrGoZERERcSqFkSIKIyIiIsZQGCnirgGsIiIihlAYKaKWEREREWMojBTR1F4RERFjKIwUKW4Z0WwaERER51IYKeJoGVE3jYiIiFMpjBTRmBERERFjKIwUuTCbxm5wJSIiIq5FYaSIWkZERESMoTBSRANYRUREjKEwUkQDWEVERIyhMFLE0U2jlhERERGnUhgpopYRERERYyiMFNHeNCIiIsZQGCmi2TQiIiLGUBgpotk0IiIixlAYKeJhMQFqGREREXE2hZEi6qYRERExhsJIEU3tFRERMYbCSBF3Te0VERExhMJIEQ9N7RURETGEwkgRzaYRERExhsJIEa3AKiIiYgyFkSKaTSMiImIMhZEiF7pp7AZXIiIi4loURooUz6bJVcuIiIiIUymMFLnQTWM1uBIRERHXojBSpHgAq7ppREREnEthpIhWYBURETGGwkiR4pYRq82O1abWEREREWdRGCni7nbhR6GFz0RERJxHYaRIccsIaEaNiIiIMymMFHG3mBz/rYXPREREnEdhpIjJZLpoRo3CiIiIiLMojFxES8KLiIg4X5XCyIwZM4iNjcXLy4vOnTuzZs2aS56fm5vLpEmTiI6OxtPTk8aNGzNnzpwqFVyTNL1XRETE+dwqe8GCBQsYP348M2bMoHfv3syaNYtBgwaxZ88eGjZsWOY1w4YN4/Tp03zwwQc0adKEpKQkCgoKrrj46lY8bkQtIyIiIs5T6TAydepURo8ezZgxYwB46623WLJkCTNnzmTKlCmlzl+8eDGrVq3iyJEjBAcHAxATE3NlVdcQtYyIiIg4X6W6afLy8ti6dSsDBgwocXzAgAGsX7++zGu+/fZbunTpwr/+9S/q169Ps2bNePbZZzl//ny575Obm0t6enqJlzMUD2BVy4iIiIjzVKplJDk5GavVSnh4eInj4eHhJCYmlnnNkSNHWLt2LV5eXnz99dckJyfzxBNPcPbs2XLHjUyZMoVXXnmlMqVVC3fNphEREXG6Kg1gNZlMJb622+2ljhWz2WyYTCbmzZtHt27duPXWW5k6dSpz584tt3Vk4sSJpKWlOV7Hjx+vSpmV5qnZNCIiIk5XqZaR0NBQLBZLqVaQpKSkUq0lxSIjI6lfvz6BgYGOYy1btsRut3PixAmaNm1a6hpPT088PT0rU1q1KB4zopYRERER56lUy4iHhwedO3dm6dKlJY4vXbqUXr16lXlN7969OXXqFJmZmY5jBw4cwGw206BBgyqUXHOKu2m0HLyIiIjzVLqbZsKECcyePZs5c+awd+9ennnmGeLj4xk7dixQ2MUycuRIx/n3338/ISEhPPTQQ+zZs4fVq1fz3HPP8fDDD+Pt7V19T1INtOiZiIiI81V6au/w4cNJSUlh8uTJJCQk0KZNGxYtWkR0dDQACQkJxMfHO8738/Nj6dKlPP3003Tp0oWQkBCGDRvGq6++Wn1PUU0uLAdvN7gSERER12Gy2+1X/Sdveno6gYGBpKWlERAQUGPv8+Snv/DDzgT+OqQVo3rH1tj7iIiIuIKKfn5rb5qLeFq06JmIiIizKYxc5MJsmqu+sUhEROSaoTByEc2mERERcT6FkYtoNo2IiIjzKYxcRIueiYiIOJ/CyEXctVGeiIiI0ymMXER704iIiDifwshFPLRrr4iIiNMpjFzE3VK483CuwoiIiIjTKIxcxMPNAqibRkRExJkURi6i2TQiIiLOpzBykeJuGrWMiIiIOI/CyEU0m0ZERMT5FEYuom4aERER51MYuYj2phEREXE+hZGLFK8zkqeWEREREadRGLmIumlEREScT2HkItqbRkRExPkURi6i2TQiIiLOpzByEXfH3jR2gysRERFxHQojF/FQy4iIiIjTKYxcpI6PBxaziTyrjZOp540uR0RExCUojFzE28NCm3oBAGyJO2twNSIiIq5BYeQ3usQEA7DpqMKIiIiIMyiM/EbXojCyWS0jIiIiTqEw8htdY+oAcOB0JqnZeQZXIyIicu1TGPmNED9PGoX5ArAl7pzB1YiIiFz7FEbK0E1dNSIiIk6jMFIGjRsRERFxHoWRMhSHkV0n08jJtxpcjYiIyLVNYaQMUcHehAd4km+1sy0+1ehyRERErmkKI2UwmUyO1hEtfiYiIlKzFEbKURxGNimMiIiI1CiFkXIUh5Ffjp2jwKqN80RERGqKwkg5mkf44+/lRlaelb0JGUaXIyIics1SGCmHxWyic3Thaqya4isiIlJzFEYuQeuNiIiI1DyFkUvoFnshjNjtdoOrERERuTYpjFxC2/qBeFjMJGfmEZeSbXQ5IiIi1ySFkUvwcrfQPioQgM1H1VUjIiJSExRGLqNzdGFXzfYTWolVRESkJiiMXEbLSH8A9idqeq+IiEhNUBi5jBYRAQAcSMzQIFYREZEaoDByGY3CfHG3mMjILeBk6nmjyxEREbnmKIxchrvFTOMwP0BdNSIiIjVBYaQCmkcUjhvZpzAiIiJS7RRGKqBZuAaxioiI1BSFkQpoEaEwIiIiUlMURiqguJvm8JlM8gpsBlcjIiJybVEYqYD6Qd74e7pRYLNzJDnT6HJERESuKQojFWAymWimrhoREZEaoTBSQZpRIyIiUjMURipIg1hFRERqhsJIBTXX9F4REZEaoTBSQcV71JxMPU96Tr7B1YiIiFw7FEYqKNDHnYgAL6Bw0zwRERGpHgojlaBBrCIiItVPYaQSNIhVRESk+imMVEJzhREREZFqpzBSCRe6adKx2+0GVyMiInJtUBiphCZ1/bCYTaTnFJCYnmN0OSIiItcEhZFK8HSzEBvqC2gQq4iISHVRGKkkjRsRERGpXgojldRCK7GKiIhUK4WRSlLLiIiISPVSGKmk4mXhDyVlUmC1GVyNiIhI7acwUkkN6ngT5ONOntXGz0dSjC5HRESk1lMYqSSz2cSQdvUA+HLrCYOrERERqf0URqrg7s4NAFiyO1E7+IqIiFwhhZEqaNcgkKZ1/cjJt7FoZ4LR5YiIiNRqCiNVYDKZHK0j6qoRERG5MgojVXRnx/qYTbDl2DmOJmcZXY6IiEitpTBSRXUDvOjXLAyAr9Q6IiIiUmVVCiMzZswgNjYWLy8vOnfuzJo1ayp03bp163Bzc6NDhw5Vedurzt2dowD46pcTWG3axVdERKQqKh1GFixYwPjx45k0aRLbtm2jb9++DBo0iPj4+Etel5aWxsiRI+nfv3+Vi73a9G9ZlwAvNxLScvj5sNYcERERqYpKh5GpU6cyevRoxowZQ8uWLXnrrbeIiopi5syZl7zuscce4/7776dnz55VLvZq4+Vu4fYOxWuOHDe4GhERkdqpUmEkLy+PrVu3MmDAgBLHBwwYwPr168u97sMPP+Tw4cO8/PLLVavyKlbcVbNYa46IiIhUSaXCSHJyMlarlfDw8BLHw8PDSUxMLPOagwcP8uKLLzJv3jzc3Nwq9D65ubmkp6eXeF2t2jcIpEnRmiPf7ThldDkiIiK1TpUGsJpMphJf2+32UscArFYr999/P6+88grNmjWr8P2nTJlCYGCg4xUVFVWVMp3CZDIxrEvhmiN//2Evm46eNbgiERGR2qVSYSQ0NBSLxVKqFSQpKalUawlARkYGW7Zs4amnnsLNzQ03NzcmT57Mjh07cHNzY/ny5WW+z8SJE0lLS3O8jh+/usdjjOwZQ9+moWTnWRn14SYFEhERkUqoVBjx8PCgc+fOLF26tMTxpUuX0qtXr1LnBwQEsGvXLrZv3+54jR07lubNm7N9+3a6d+9e5vt4enoSEBBQ4nU183K38H8juyiQiIiIVEGlu2kmTJjA7NmzmTNnDnv37uWZZ54hPj6esWPHAoWtGiNHjiy8udlMmzZtSrzq1q2Ll5cXbdq0wdfXt3qfxkBlBZLNcZcOJMfPZvPN9pPY7VqjREREXFfFRpReZPjw4aSkpDB58mQSEhJo06YNixYtIjo6GoCEhITLrjlyrSoOJGM+2sLaQ8mMmrOJH8b1JSa0dOjKybfywOyNxJ/NxtPNwi1tIgyoWERExHgmey34tTw9PZ3AwEDS0tKu+i4bgPN5VkbO2cjmuHP0aRLKJ6O7lRrg+/b/DvLG0gMADOvSgH/d3d6IUkVERGpMRT+/tTdNDfD2sPDvu9vj4WZm7aFkvv3NlN8T57J5d+Uhx9erDySrq0ZERFyWwkgNiQn1ZdyNTQD42/d7SMu+sCDaa4v2kpNvo0t0HTzdzCSm53DgdKZRpYqIiBhKYaQGPXpdY5rU9SM5M49/LN4HwLpDySzalYjFbOLVO9vQo1EIAKsPnDGyVBEREcMojNQgDzczf7+jDQDzN8Xz8+EUXv52NwAjekTTIiKA65qFAbD6oMKIiIi4JoWRGta9UYhjhdaH5m7iUFImIb4ePHNz4Yq0/ZqFArDx6FnO51kNq1NERMQoCiNOMHFQS4J9PcjJtwHw/C3NCfR2B6BxmB/1g7zJK7Cx4WiKkWWKiIgYQmHECer4evCX21oB0LFhEPd0vrDXjslk4rqi1pFV+9VVIyIirqfSi55J1dzRsT5N6vrRMMQHs7nkmiP9moUxf9NxjRsRERGXpJYRJ2pTP5AAL/dSx3s1CcViNnHkTBbHz2YbUJmIiIhxFEauAgFe7nSMCgI0q0ZERFyPwshVol/xFF+tNyIiIi5GYeQqUbzeyLpDKeRbbQZXIyIi4jwKI1eJtvUDCfb1IDO3gG3xqUaXIyIi4jQKI1cJs9lEnyaFU3zVVSMiIq5EYeQqUjxu5OttJzmWkmVwNSIiIs6hMHIVualVOJGBXpxMPc/Qd9ex/lCy0SWJiIjUOIWRq0igtzv/fbI37aOCSM3OZ8ScTXz8cxx2u93o0kRERGqMwshVJjzAiwWP9uCODvWw2uz85ZvdTPrvrxRoho2IiFyjFEauQl7uFt4c3oEXB7XAZIJPN8azYMtxo8sSERGpEQojVymTycTYfo155qZmACz+NdHgikRERGqGwshV7ta2kQBsPHKWzNwCg6sRERGpfgojV7nGYb7EhPiQZ7WxVvvWiIjINUhh5CpnMpm4sUU4AMv2JhlcjYiISPVTGKkFbmpZF4AV+5Kw2jTNV0REri0KI7VA19hg/L3cSMnKY8cJ7VsjIiLXFjejC5DLc7eY6dcsjO93JvC/vafp1LBOie9/u+MUL361kzo+HkQFe9Mw2IeGwT70aBRCl5hgg6oWERGpGLWM1BL9i7pq/vebcSNp2fm8/M2vZOdZOZl6ng1HzvL5lhO8/tMB7n1/g/a4ERGRq57CSC1xfbO6mE2wLzGDE+eyHcffXHaAc9n5NK3rx1eP9+St4R2YcHMzmoX7UWCz8+G6OOOKFhERqQCFkVqijq8HXaILu1yW7ytsHTlwOoNPNhwD4OUhrekcHcwdHeszrn9T/nxbKwA+33KctPP5xhQtIiJSAQojtUhxV82yvUnY7XYmf7cHq83OgFbh9GkaWuLcPk1CaR7uT3aelc82xRtRroiISIUojNQixWFkw+EU/rv9JGsPJePhZuZPg1uVOtdkMjG6b74ntswAACAASURBVCwAH62PI18b7YmIyFVKYaQWaRzmR3TRaqwvfLkLgEf6xtIwxKfM829vX49QPw9OpeXwo/a2ERGRq5TCSC1iMpnoX7Qaa57VRniAJ09c36Tc873cLYzoEQPA7DVHsNu1YJqIiFx9FEZqmeKuGoCJg1ri63nppWJ+36MhHm5mdp5IY8uxczVdnoiISKUpjNQy3WODuaV1BPd0bsDQDvUue36Inye/61QfgA/WHK3p8kRERCpNK7DWMm4WM++N6Fypax7uHcv8TcdZsieRYylZRIf41lB1IiIilacw4gKahvtzffMwVu4/w6Bpa+jUsA5dY4LpGluHBkE+nEjN5sTZ88SfzeZ0eg53dqpPr8ahl7+xiIhINVAYcRHPD2zBvoQMEtNzWHsombWHkss9d9WBM6x+/ga83C3VWoPNZsdsNlXrPUVEpPbTmBEX0apeAOtfvJHF4/vyt6GtGdK+HhEBXnhYzDQK9eW6ZmE80L0hdf09ScrI5atfTlTr+3+97QSNXlrEj7sSqvW+IiJS+6llxIWYzSZaRATQIiKAET1jALDb7ZhMF1orGof5Mfn7Pby36jDDu0ThZqmevFq8R868jfEMahtZLfcUEZFrg1pGXNzFQQTgvm4NCfH14PjZ83y741S1vMeJc9nsPJEGwMajKWTlFlTLfUVE5NqgMCIleHtYeLhP4TLyM1Yexma78oXSluw+7fjvfKv9kuNVRETE9SiMSCkjekbj7+XGoaRMftpz5cvILylaij7Ixx2AlfuTrvieIiJy7VAYkVICvNwZ1SsGgHdWHLqiZeSTMnLYfOwsAC/e0gKAFfvOaGl6ERFxUBiRMj3UOxZvdwu/nkxn9cGqd6ss3XMaux3aNwjkjo718Xa3kJiew56E9GqsVkREajOFESlTsK8H93dvCMC7y6veOrK4qIvmljaReLlb6N0kBICV+89UT6EiIlLrKYxIuR69rhEeFjOb4s7Sc8py/vDZNuZtPMahpMwKhZPU7Dx+PpwCwC1tIgC4oUXhRn/L92nciIiIFNI6I1Ku8AAvXhzUgik/7iUxPYdvtp/im+2nir7nSd+mYfRtGkrfpmEE+3qUuv5/e5MosNlpEeFPbGjhfjjXNy8MI9viz3EuK486ZVwnIiKuRWFELunhPrHc160h2+LPsfHoWTYeTWFbfCqn03P5cusJvtx6ApMJOkQFMfn2NrRtEOi49seiLpqBrSMcx+oHedM83J/9pzNYffAMQzvUd/oziYjI1UVhRC7L28NCryah9GpSuHleTr6VLXHnWH3wDKsPnGFfYgbb4lO5Z9Z6pg7rwK1tI8nKLWD1wcJxIYPaRpS43w0t6rL/dAYr9iUpjIiIiMaMSOV5uVvo0zSUl25tyeLx17FhYn/6NQsjJ9/GE/N+4Z3lB1mxP4m8AhsxIT40D/cvcf0NzcOAwg35rNWwqJqIiNRuCiNyxSICvfjgwS481DsGgNd/OsDEhbsAGNgmotSS852j6+Dv5ca57Hy2H091drkiInKVURiRauFmMfPykNb8/c42WMwmMnIK958Z1Kb0pnhuFjPXNStsHVmhWTUiIi5PYUSq1QPdo/n44W7U8XGnVWQA7eoHlnnejc0vTPHVaqwiIq7NZK8FnwTp6ekEBgaSlpZGQECA0eVIBeTkW3Ezm3CzlJ13kzNz6fr3ZdjtcFPLcP5+ZxvCA7ycXKWIiNSkin5+q2VEaoSXu6XcIAIQ6ufJX4e0xt1iYtne09w8dRVfbDmuVhIRERekMCKGebBXDN8/3Zf2DQJJzynguS93MurDzSzdc5rjZ7OrNZjsS0zn/1Yf4XyetdruKSIi1UPdNGK4AquN2WuPMnXpAfIKbI7jfp5uNI/w57qmYTx5Q+NLtrRcyuoDZxj7n61k51kZ1SuGv97eurpKFxGRS1A3jdQabhYzY/s15sc/9OWezg1oGRmAu8VEZm4BW4+d481lB3hj6YEq3fvbHacY/dFmsotaROZtPEZ8SnZ1li8iIldILSNyVcq32jhyJosV+5P4x4/7APjgwS70bxle4XvMXXeUV77fg90Ot7WL5Fx2HusOpXBHh3q8dW/HmipdRFxEWloa2dmu88uNj48PgYFlz5AsT0U/v7UcvFyV3C1mmkf40zzCn4TU83z08zEmfL6DH8b1oUEdn0tem5qdx3urjvDeqsMAPNgzmpeHtGZPQjq3vb2Wb3ac4tHrGtOqnoKtiFRNWloa77zzDvn5+UaX4jTu7u489dRTlQ4kFaEwIle9lwa3ZPvxVHacSOPJT7fxxWM98XAr2cOYkpnLT3tO8+Oviaw/lExB0TLzE25uxtM3NsFkMtGmfiC3tYvk+50J/GvJPuY+1M2IxxGRa0B2djb5+fncddddhIWFGV1OjTtz5gwLFy4kOztbYURck6ebhXfu78Rtb69lx/FUXlu0l5eHtGL3qXRWHTjDqv1n2Bp/rsQ+Ny0i/HmsXyPu7NigxL2eHdCcxb8msnL/GTYcSaFHoxBnP46IXEPCwsKIjCy90rRUjsKI1ApRwT5MHdae0R9tYe76OL7feYrkzLwS57SpH8CgNpEMahNBozC/Mu8TE+rLvd2i+M+GeP7x4z6+fqJXqb1zRETEuRRGpNbo3zKcsf0a896qwyRn5uHjYaFX41D6NQ/j+mZhRAVfeixJsXH9m/LV1pNsP57Kkt2J3FLG/jnF9idmkFtgpV2DoOp6DBER+Q2FEalVnhvYnJaR/oT6edIlpg6ebpZK36Ouvxej+8TyzopDvPrDXlrXCywzyHy/8xTjP9tOgc3O73s05KVbW+Ljof9lRESqm9YZkVrFYjYxtEN9ejcJrVIQKfZov0bUD/LmxLnz3DljPbtOpJX4/udbjjNu/jbHQNj/bIhn8PS1bIs/d0X1i4hIaQoj4pICvNz56vFetIjwJzkzl+Hv/8yK/UkAfLQ+jue/3InNDvd1i+Ljh7sREeDF0eQs7n7vZ6YuPUBOvpaVF5HLmzFjBrGxsXh5edG5c2fWrFlT7rkJCQncf//9NG/eHLPZzPjx40udM3fuXEwmU6lXTk6O45yZM2fSrl07AgICCAgIoGfPnvz444+l7rV3715uv/12AgMD8ff3p0ePHsTHx1fPg1eSwoi4rIhAL74Y25M+TULJzrMy5qMtPPXpL7z87W4ARveJ5bU723JdszCWjL+OoR3qYbXZmf6/g3R5dRnjP9vGkt2JNRpMrDY7NttVvy6hiJRhwYIFjB8/nkmTJrFt2zb69u3LoEGDyv3Az83NJSwsjEmTJtG+ffty7xsQEEBCQkKJl5fXhV3PGzRowD/+8Q+2bNnCli1buPHGGxk6dCi7d+92nHP48GH69OlDixYtWLlyJTt27ODPf/5zifs4k1ZgFZeXV2Djxa92snDbScexcf2b8sxNTUvNtPluxymmLNrLqbQLv4X4eli4vUM9Xh7SGi/3inUd5RZYWbHvDIlp58kpsJGTb+V8vpWs3AJOp+dyOj2H0+k5nMnIxU7hPj0BXu74e7lRL8ib529pTosI/b8gYpSEhARmzZrFY489Vu7U3u7du9OpUydmzpzpONayZUvuuOMOpkyZcsn7X3/99XTo0IG33nqrxPG5c+cyfvx4UlNTK1VvcHAw//73vxk9ejQA9957L+7u7nzyyScVur4iz1sWrcAqUkEebmbeGNaeBnW8mbs+jnH9mzKmb6Myzx3Svh6D20ay7XgqP+5K4MdfEzmZep75m46TkVPA9Hs7YjaXP1X4VOp5Pt0Yz/xN8aRk5ZV73m9l5BSQkVMAwL7EDPYlpPPd030I8fOs3MOKiFPk5eWxdetWXnzxxRLHBwwYwPr166/o3pmZmURHR2O1WunQoQN/+9vf6Nix7C0urFYrX3zxBVlZWfTs2RMAm83GDz/8wPPPP8/AgQPZtm0bsbGxTJw4kTvuuOOKaqsqhRERwGQyMWFAc565udll1x0xm010jq5D5+g6TBrckmV7k3j8P1v5fmcCsaG+/HFA81LX7DqRxrsrDrF072nH4mwRAV50ig7Cy92Ct7sFL3cLvh4WwgK8iAjwIjzAk/AALyxmE+nn80nPKSDtfD6vfLubI8lZPD1/Gx8/3K3KuxmLSM1JTk7GarUSHl5yP63w8HASExOrfN8WLVowd+5c2rZtS3p6OtOmTaN3797s2LGDpk2bOs7btWsXPXv2JCcnBz8/P77++mtatWoFQFJSEpmZmfzjH//g1Vdf5Z///CeLFy/mrrvuYsWKFfTr16/K9VWVwojIRSq7AJrJZOLmVuG8dldbnv9yJ28vP0R0iC93dy5c+TWvwMbbyw8yY+VhRwjp0SiYB3vGcFOrcNwrGCRCL2oBmTWiM0PfXcf6wyn8e8l+Jt7aslI1uyKrzY7ZVPk/X5Er9du/c3a7/Yr+Hvbo0YMePXo4vu7duzedOnXi7bffZvr06Y7jzZs3Z/v27aSmpvLVV1/x4IMPsmrVKlq1aoXNZgNg6NChPPPMMwB06NCB9evX89577ymMiNRWw7pEEZecxYyVh5m4cCf1g7wJ8nFnwuc72JuQDsDgdpH8oX9TmoX7X9F7NQ335/V72vPEvF+YtfoIbRsEclu7etXxGNekpIwc7nhnHVHBPnz2aA8FEnGK0NBQLBZLqVaQpKSkUq0lV8JsNtO1a1cOHjxY4riHhwdNmjQBoEuXLmzevJlp06Yxa9YsQkNDcXNzc7SUFGvZsiVr166tttoqo0rtu5WZqrRw4UJuvvlmwsLCHFOMlixZUuWCRa5Wzw5ozuC2keRb7Tz68RZuf2ctexPSqePjzrv3d+Ld+ztdcRApdmvbSMb2awzA81/uZN2hZL7ZfpK/fruboe+spfVfFvPR+rgreg+73c6WuLMkZ+ZWQ8XGeX3Jfk6l5bDx6FlWHThjdDniIjw8POjcuTNLly4tcXzp0qX06tWr2t7Hbrezffv2yw4qtdvt5ObmOmrr2rUr+/fvL3HOgQMHiI6OrrbaKqPSLSPFU5VmzJhB7969mTVrFoMGDWLPnj00bNiw1PmrV6/m5ptv5rXXXiMoKIgPP/yQIUOGsHHjxnIH3IjURmaziTeGtedk6nm2Hy8c6X5Ty3Cm3NWWMP/qH2j67IBm/HoyjbWHknlg9sZS33/1hz10ialD63qV22HTbrfzv71JvLnsALtPpRMZ6MU3T/amboAxU/6uxK8n0/hi6wnH17PXHOX65nUNrEhcyYQJExgxYgRdunShZ8+evP/++8THxzN27FgAJk6cyMmTJ/n4448d12zfvh0oHKR65swZtm/fjoeHh6MV45VXXqFHjx40bdqU9PR0pk+fzvbt23n33Xcd93jppZcYNGgQUVFRZGRk8Nlnn7Fy5UoWL17sOOe5555j+PDhXHfdddxwww0sXryY7777jpUrVzrhJ1NapcPI1KlTGT16NGPGjAHgrbfeYsmSJcycObPMqUq/nZb02muv8c033/Ddd98pjMg1x8vdwuwHu/DWsgN0iQ5maId6NdYt4GYxM/2+jvxu5nqOn82mdb0AOjasQ6foOny7/RTL9p7mmQXb+fapPmVOOU5KzyE9pwAvdzOebha83M1sOXaON5ceYOdFK9ImpOXwyCdbWfBojwpPXb4a2O12XvluN3Y79G4Sws+HU1h7KJk9p9JpVU/ToqXmDR8+nJSUFCZPnkxCQgJt2rRh0aJFjtaHhISEUmuOXPy5uHXrVj799FOio6OJi4sDIDU1lUcffZTExEQCAwPp2LEjq1evplu3bo7rTp8+zYgRI0hISCAwMJB27dqxePFibr75Zsc5d955J++99x5Tpkxh3LhxNG/enK+++oo+ffrU4E+kfJVaZyQvLw8fHx+++OIL7rzzTsfxP/zhD2zfvp1Vq1Zd9h42m42YmBief/55nnrqqTLPyc3NdTQnQeE85aioKK0zIlKGAqsNq91eYnn8lMxcBr61huTMXB7pG8ukwRf6hm02O28uO8Dbyw+Ve09vdwujescwsHUEoz7cRGp2PkPa12P6vR1qzZiL73ee4qlPt+Hlbmb5H6/n74v28sPOBO7qVJ+pwzoYXZ7UclVdd6O2qul1Rio1ZqQ6piq98cYbZGVlMWzYsHLPmTJlCoGBgY5XVFRUZcoUcSluFnOpfXpC/Dz55+/aAjB77VE2HEkBICu3gMfnbXUEkQAvN7zcL/wz4O1u4dHrGrHmhRt44ZYWdIgKYuYDnXEzm/huxymm/6/8AHMp+VabYzZRdTqXlcejH2/hb9/vITX7wrotOflWpizaB8DYfo2pF+TNI0Vrx3y34xSn03PKvJ+IGKNKs2mqOlVp/vz5/PWvf+Wbb76hbt3y+20nTpzIhAkTHF8Xt4yISMX1bxnOvV2j+Gzzcf74+Q4+GNWF8Z9tZ19iBh4WM1PuasvviqYg2+12cgtsWMymUtONezYO4dU72vDiwl28uewAjev6Vmr2zolz2TwweyOp2fmM6BHNg71iqm0Mzd8X7eWnPacBWPjLCZ4b2ILhXaP4v9VHOJl6nnqBXjx2XeFA3w5RQXSLCWZT3Fnmro/jhVtaVEsNInLlKtUyciVTlRYsWMDo0aP5/PPPuemmmy55rqenp2ODn+KXiFTen25rRVSwNydTzzNo2hr2JWYQ6ufJ/Ed7OIIIFP6C4eVuKXfdk3u7NWR0n1gA/vj5DpbvO12h909Kz+H3szdyLCWbtPP5vLPiEL3/uZyJC3dy+EzmFT3bhiMpfFk0OLVRqC/nsvN56etdDH13LTNWHgbghUEt8Pa40Go0pm/hM8zbcIys3IIren8RqT6VCiNVnao0f/58Ro0axaeffsrgwYOrVqmIVJqfpxtTh3XAZAK7HVrXC+Dbp3rTObpOpe/10q0tubFFXXILbDw8dwuvL9l/ya6Xc1l5/P6DjcSlZNOgjjev39Oejg2DyCuwMX/TcW6auoqnPv2FQ0llh5KkjBw+2xTP8bPZpb6XW2Bl0te7ALi/e0N+euY6Xh7SCn9PN349mc75fCudo+twe/uSLTg3tQwnNtSX9JwCvthyvNI/AxGpGZXupqnsVKX58+czcuRIpk2bRo8ePRytKt7e3gQGVm7KoYhUXteYYN6+ryMHT2fyWL9G+HhUba1Di9nEzN934u8/7OXjn4/xzopDbDt+jmn3diyxQixAek4+I+ds4sDpTMIDPPl0TA8ahvjwu0712XLsHLNWHWbZ3iS+35nAol0J3NGhPuP6N6VBHW9WHzzDZ5uO8799SVhtdgK93Xl/RGe6Nwpx3P/9VUc4fCaLUD8PXhjYAjeLmYd6x3Jbu3q8vmQ/246f49U72pTqPjabTTzcJ5Y///dX5qyLY0TPGCyX2EvoUtKy8/lw/VG+3HqCwe0ieWFgi0vuSyQi5av0v0qVnao0a9YsCgoKePLJJ3nyyScdxx988EHmzp175U8gIpdVXSu0erpZmDy0DZ2j6/DiV7tYdyiF26av5ckbm+DrYcHNYsbdbGLOuqPsOplGsK8H88Z0p2GID1DYHdQ1JpiuMcHsOZXOm8sOsHTPaRZuO8k3O04R7OvBmYwLM+mCfT04m5XHiA828e972jG0Q33ikrN4e0XhQNo/39aKQB93x/lh/p788+52l3yGuzs1YOpP+4k/m813O05xR8f6lfoZnMvK44O1R/lofRwZRV09s1Yd4fjZbKYO61Crpj+LXC0qNbXXKBWdGiQiznPgdAZj/7OVI2eyyvy+v5cb8x/pQZv6l24B3XkilalLD7Byf+HqqHV83LmrUwOGd42iYbAPzyzYzo+/FraoPjewORuOpLDmYDJ9moTyyehuVZpqPP1/B5m69AD+Xm58/3QfokN8S51zKCmT6f87SNr5fMcxO7Al7izZeVYAmoX7MbB1BLNWHSHPaqNrTB3+b2QXgnw8Kl1TVaw6cIZnv9jBza3CeeGWFgR6u1/+omqUb7WxPzGD1vUCas2U7+pSPNX1rrvuIiwszOhyatyZM2dYuHBhjU3tVRgRkSrLzC1g+v8OcvB0BgU2u2MKr4+HGxNubkb7qKAK3+vXk2mcycylV+OQElOVbTY7ry3ay+y1Rx3HPNzM/DT+OmJCS4eIisi32rj3/Q1sPXaOVpEBLHyiV4kWjSNnMhk2a0O5S+G3rhfA0zc2YUCrCMxmEz8fTuHRT7aQkVNAk7p+zH2oKw3q+FSptso8w81TVxGXUjimpq6/J5OHtuaWNs5b8+JP/93FfzbEM/6mpoy/qZnT3vdqkJaWxjvvvEN+fv7lT75GuLu789RTT1VqiIXCiIhcUz5aH8cr3+3GZocJNzdjXP+ml7/oEhLSzjN4+lrOZuVxX7coptxV2L0Tn5LNsFk/k5ieQ4sIf0b3iXX81m8CIoO86NkopFRLwP7EDEZ9uImEtBxC/TwY268xw7tG4e91+daKnw+n8OoPe8jJt3JdszBuaF6XbrHBl+zy+WxTPC8u3EWwrwdB3u4cSS5soRrYOpzJQ9sQfonl+5Myclh3KJm07HzScwpIP59PVp6Vfs1CGdg6okKtHIfPZDLgzdVYbfbCdWie7kPLSNf69zktLY3s7NIDrK9VPj4+lR7rqTAiItecjUdS2H0qnRE9o8udhlwZaw6eYeScTdjt8Po97enZOIRh7/3MydTzNK3rx2eP9iDEr+JroiSkneehDzezLzEDKJzNdG/XKB7qE0v9IO9S5+fkW3l9yX4+WHeU3/5L7O1uoU/TUP48uJVjzM3F1934+kpOpeXw59ta8UD3hryz/BDvrTpMgc1OgJcb0+7ryA1l7MOzOe4sj368hXPZZf9GP7htJH+7ow3Bvpfuanrq01/4fmcCFrMJq81OuwaBLHy8F27V8Oci1w6FERGRCpi27CBvLjuAl7uZMH9Pjp89T2yoLwse7VGlzQFz8q38d9tJZq896pi2bDGb6B4bTJeYYLpE16FjwyDiz2bzzILtHDhdeM593RpyXdNQVh04w4r9SZxOL+wiahTqy8InepUYhzJn7VEmf7+HyEAvVjx7vaMFZV9iOs9/uZOdJ9IwmQrH2Dzer7GjpeOb7Sd57oud5FltNAr1pWVkAAHebgR4uZOdZ2X+pngKbHZCi1bw7d+y7PWjfj2Zxm1vF241/9HD3Xjq01/IyCngpVtb8GjRInNXymazk5VXcMmWpRPnsllzMJlbWkdQ5zLhSYyhMCIiUgE2m51Rczez+kDhANqoYG8+f6wnkYGlWzIqe99VB87wf2uOsP5wSonvmU2FM4us5Xzw2+12dp9K57FPtnIy9Tw9G4Xw0cPd8HAzk5VbQL9/ryA5M48pd7Xlvm4ld0vPK7Dx1+928+nGwlmNg9tG8q+72zF7zVHeXHYAgFtaR/Dm8A4lFoQD2HUijQmfb+dgUYga3iWKv97eutR5D324iRX7z3B7+3pMv68jn28+zvNf7cTTzcyS34zlybfa2JuQTouIADzcKtZqsutEGn9YsI2E1Bw+fKgrPS6a1l0s7Xw+g6ev4cS58/h4WBjRI5rRfWOp618yQGbk5HMqNYfoEB/NdDKAwoiISAWdzcrjvvc3kG+z8dFD3YgKrt7Bp4fPZLLhSApb4s6x5dhZjp89DxSO73jtzrbldgXtS0zndzPWk5VnZXiXKP7xu7bMWHmYfy/ZT3SID8sm9Cu3u2rexmP89dvd5FvtBPm4k1rULfPYdY144Zby10TJybfyxk/7mb22sOuofYNA/u/BLo4P+c1xZ7nnvZ+xmE0sm9CP2FBf7HY7Iz7YxNpDyXSPDWb+Iz3IyCng003xfPxzHAlpOTSt68eUu9rSJSa43J+TzWbn/TVHeOOn/eRbCz+aQv08+WFcnxJjYOx2O099uo0fdiXgZjZRULT4nqebmXu7RhET6svOE2nsPJHKkeQs7HZwM5toHuFPuwZBtG8QSJ+moeUOMs7KLeD1n/ZzKvU8d3asz40twiscpMpS0S1TrkUKIyIilWC12TGBUxYuO52eQ0ZOPo3D/C77IbV832nGfLQFmx3G3diED9fHkZFTwLR7OzC0w6XXSNkSd5bH5/3CmYxcLGYTr97RplRLSnl+PpzCE/O2ci47n/pB3swZ1ZVm4X4Mn7WBTXFnua9bQ6bc1dZx/vGz2Qx4czXn8630bRrKlrhznM+3lrrvA90b8nwZ05AT0s4zYcEOfi7a1HFQmwiOJmexLzGDLtF1mP9oD0fwmrfxGJO+/hU3s4kvH+/F2axc3l5+iG3xqWU+i6+Hhay8krW4W0yM6duIp29sUmIhwJ0nUvnDZ9s5mnxhynqIrwe/69yAuzrVJyu3gB3HC4POjhNpuJlN/Pm2VlzXrPT03vScfCZ9/SvrDyXz2l1tGdg6osz68q024pKzaFL38n8fahuFERGRa0TxGJFizcP9+fEPfSsUnBLTcvhg7RH6twwvs7vjUuKSs3h47maOJGfh5+nGg72ieXfFYTzczKx67vpSXVm/rbN4NlK/ZmG88dMBFhQtwR/m78mIHtGkZueTkHaeU2k5HDqdQVaeFR8PC38d0pp7ujQgLiWb299eS0ZuAaP7xPLn21qxLzGdoe+sI7fAxqRbW/LIdYW7Mdvtdn4+nMLc9XFFA2qDaNcgkLYNAgnx9eBUWg47j6ey/UQqm4+e5Zei4BIZ6MWfBrfiljYRvL+6sFWmwGYnMtCLQW0i+W7nqRIL8ZXFZIInr2/C+JuaOgbw/noyjSfm/UJ80XYGZhP86+723H3RnlAAx1KyeGLeL+w+lc6oXjG8PKTVNRVIFEZERK4RdrudP/33V+YVjQN5f0RnBpTzW3Z1S83OY+x/trLhyFnHsTF9YvnTba1KnWu12fnzN7+Slp3PAz0alpoC/fPhFCZ9vcsxDfm32jUIZNq9HYm9aMzJkt2JPPbJVgD+fXc7Zq0+wqGkTK5vHsacB7tWqSXLbrezbG8Sr3y3mxPnCrvM6vp7klQUOm5tG8Frd7YlAi9UNgAADzZJREFUyMeDAquNFfvPsGDzcVbsT6KOjwcdogJp3yCItg0CWbrntOPPpVtsMNPv7ciyvaeZ/P0e8gps1A/ypkNUED/sSgDgL7e14uGiTSeX7E7k2S92kJFzYdPGF25pwePXlzEI2GqFNWsgIQEiI6FvX7CUHANjt9vZfzqD/AI7Hm5mPN3MeLqbCfByx9ezattAXCmFERGRa0i+1cYr3+3GzWx2+m/PeQU2Ji7cxVe/nMDP043Vz99w2am/5cnJt/Lhujj2JqQTEehFZKAXkYHeNKjjTcvIgDL3CvrHj/t4b9Vhx9d1/T358Q99KzXturxaZqw8zHurDpNXYCvRKlPWz7fAasNiNpX63rc7TvHSwl1k5hbg6WYmt8AGwE0t6/LGPR0I8Hbj7z9cWLjv6RubkFtg4/3VRwDoHF2H3o1DmL68cJuD1+/5TQvKwoXwhz/AiRMXjjVoANOmwV13kW+18f3OU7y/+ih7E9LLfNboEB9a1wugVWQAreoFEFA0S8lO4Saadvv/t3f/QVHV/R7A32d3YdldkQtyYUFQoUxS0gzMq1Kidg2legztBwO6Pt3GwYQgp9IJu5KT4cydMae5RaNDzJ0rXbrkj0tWFppZ2lM4wCr+SGskNYEhH0sWDQj28/zB47nPtpgIyx4W3q+ZnWG/57vHz3nPjvuZ81NwW9gIt+dM9RebESIi8hgRwYHTzYgIMnn95madXU4sLa7CX87+FYoClD49HTNvC/XY+s/99Sr2HGvEwrsiXPbK3Ir6S1eR/W734Ra9TsHalDg8fd//3zBPRNSTj//R00kxWLMgDn56HV776BS2fnEWep2CYlsikieEdTciS5bA7UY0igIBULnhP7FePwGNV9oAAAF+OgSb/dHR6UR7pxMdnU50dDl7tQ1vpE91e9J1f7EZISKiIeNSazte3XMSSeP/2e28i8Gi7bcuvF/9o3oIpyf//fU5/Pv/HccIfwP+47HJLrfvdzoFq//Xjt32Bpj99XjziSlI+tdEGBob0NN+MCeApsBQJGUVI2SkGX+eNQ4Z08e4PRvp56sdONnYghMNV3CioQWnmxxo+/vJxYqidK9bAdal3om5cT3fW6av2IwQERENQt83t+KfzH49HhLp6HTi3/7rCL787hL+5fwxlP3PSzdd34Gi9zDz6cUuz3QaLHr7+8379hIREXnR7X9wboa/QYeizASkTLLitk5Hr9Y3J6hrUDYit0Kb02uJiIioRyOMBry9NAGIdgDlhTf/QIT3ntQ8ULhnhIiIaDC6777uq2ZudOWUogDR0d3zfBybESIiosFIr+++fBdwb0iuv9+yxe1+I76IzQgREdFglZYGvP8+MPp3t/6PiuoeT0vTpi4P4zkjREREg1laGvCnP930Dqy+jM0IERHRYKfXA8nJWlcxYHiYhoiIiDTFZoSIiIg0xWaEiIiINMVmhIiIiDTFZoSIiIg0xWaEiIiINMVmhIiIiDTFZoSIiIg0xWaEiIiINOUTd2AVEQBAS0uLxpUQERFRb13/3b7+O34jPtGMOBwOAEB0dLTGlRAREdGtcjgcCAoKuuFyRW7WrgwCTqcTDQ0NCAwMhPL7xyj3Q0tLC6Kjo3HhwgWMHDnSY+sld8zau5i39zBr72HW3uOprEUEDocDkZGR0OlufGaIT+wZ0el0iIqKGrD1jxw5kl9sL2HW3sW8vYdZew+z9h5PZP1He0Su4wmsREREpCk2I0RERKQpfUFBQYHWRWhJr9cjOTkZBoNPHLHyaczau5i39zBr72HW3uPNrH3iBFYiIiIauniYhoiIiDTFZoSIiIg0xWaEiIiINMVmhIiIiDQ1rJuRt956CzExMQgICEBCQgK+/PJLrUvyeYWFhZg2bRoCAwMRFhaGRYsW4fTp0y5zRAQFBQWIjIyEyWRCcnIyTpw4oVHFQ0NhYSEURUFeXp46xpw96+LFi8jMzMSoUaNgNptx9913o7q6Wl3OvD2js7MT69atQ0xMDEwmE2JjY7FhwwY4nU51DrPumy+++AIPP/wwIiMjoSgKdu/e7bK8N7m2t7cjJycHoaGhsFgseOSRR/Djjz/2vzgZpsrKysTPz0+2bdsmJ0+elNzcXLFYLHLu3DmtS/NpDz74oJSUlMjx48fFbrdLamqqjBkzRlpbW9U5mzZtksDAQNmxY4fU1dXJE088IREREdLS0qJh5b6rqqpKxo0bJ5MnT5bc3Fx1nDl7zuXLl2Xs2LGyfPly+eabb6S+vl727dsn33//vTqHeXvGq6++KqNGjZI9e/ZIfX29lJeXy4gRI2TLli3qHGbdNx999JHk5+fLjh07BIDs2rXLZXlvcs3KypLRo0dLZWWl1NTUyJw5c2TKlCnS2dnZr9qGbTNy7733SlZWlstYXFycrF27VqOKhqbm5mYBIAcPHhQREafTKVarVTZt2qTOaWtrk6CgIHn77be1KtNnORwOGT9+vFRWVsrs2bPVZoQ5e9aaNWskKSnphsuZt+ekpqbKU0895TKWlpYmmZmZIsKsPeX3zUhvcv3ll1/Ez89PysrK1DkXL14UnU4ne/fu7Vc9w/IwTUdHB6qrqzF//nyX8fnz5+Orr77SqKqh6cqVKwCAkJAQAEB9fT2amppcsjcajZg9ezaz74NVq1YhNTUVDzzwgMs4c/asiooKJCYm4rHHHkNYWBimTp2Kbdu2qcuZt+ckJSVh//79OHPmDADg6NGjOHToEBYuXAiAWQ+U3uRaXV2N3377zWVOZGQk4uPj+539sLyF3aVLl9DV1YXw8HCX8fDwcDQ1NWlU1dAjIli9ejWSkpIQHx8PAGq+PWV/7tw5r9foy8rKylBTU4MjR464LWPOnnX27FkUFRVh9erVeOmll1BVVYVnn30WRqMRy5YtY94etGbNGly5cgVxcXHQ6/Xo6urCxo0bkZ6eDoDf7YHSm1ybmprg7++P4OBgtzn9/e0cls3IdYqiuLwXEbcx6rvs7GwcO3YMhw4dclvG7PvnwoULyM3NxaeffoqAgIAbzmPOnuF0OpGYmIjXXnsNADB16lScOHECRUVFWLZsmTqPefffe++9h+3bt+Pdd9/FpEmTYLfbkZeXh8jISNhsNnUesx4YfcnVE9kPy8M0oaGh0Ov1bp1cc3OzW1dIfZOTk4OKigocOHAAUVFR6rjVagUAZt9P1dXVaG5uRkJCAgwGAwwGAw4ePIg33ngDBoNBzZI5e0ZERAQmTpzoMnbnnXfi/PnzAPi99qQXXngBa9euxZNPPom77roLS5cuxXPPPYfCwkIAzHqg9CZXq9WKjo4O/Pzzzzec01fDshnx9/dHQkICKisrXcYrKysxc+ZMjaoaGkQE2dnZ2LlzJz777DPExMS4LI+JiYHVanXJvqOjAwcPHmT2t2DevHmoq6uD3W5XX4mJicjIyIDdbkdsbCxz9qBZs2a5XaJ+5swZjB07FgC/15507do16HSuP016vV69tJdZD4ze5JqQkAA/Pz+XOY2NjTh+/Hj/s+/X6a8+7PqlvcXFxXLy5EnJy8sTi8UiP/zwg9al+bSVK1dKUFCQfP7559LY2Ki+rl27ps7ZtGmTBAUFyc6dO6Wurk7S09N5WZ4H/OPVNCLM2ZOqqqrEYDDIxo0b5bvvvpPS0lIxm82yfft2dQ7z9gybzSajR49WL+3duXOnhIaGyosvvqjOYdZ943A4pLa2VmprawWAbN68WWpra9VbWvQm16ysLImKipJ9+/ZJTU2NzJ07l5f29tebb74pY8eOFX9/f7nnnnvUy0+p7wD0+CopKVHnOJ1OWb9+vVitVjEajXL//fdLXV2ddkUPEb9vRpizZ33wwQcSHx8vRqNR4uLiZOvWrS7LmbdntLS0SG5urowZM0YCAgIkNjZW8vPzpb29XZ3DrPvmwIEDPf7/bLPZRKR3uf7666+SnZ0tISEhYjKZ5KGHHpLz58/3uzZFRKR/+1aIiIiI+m5YnjNCREREgwebESIiItIUmxEiIiLSFJsRIiIi0hSbESIiItIUmxEiIiLSFJsRIiIi0hSbESLySYqiYPfu3VqXQUQewGaEiG7Z8uXLoSiK2yslJUXr0ojIBxm0LoCIfFNKSgpKSkpcxoxGo0bVEJEv454RIuoTo9EIq9Xq8goODgbQfQilqKgICxYsgMlkQkxMDMrLy10+X1dXh7lz58JkMmHUqFFYsWIFWltbXea88847mDRpEoxGIyIiIpCdne2y/NKlS3j00UdhNpsxfvx4VFRUDOxGE9GAYDNCRAPi5ZdfxuLFi3H06FFkZmYiPT0dp06dAtD9mPiUlBQEBwfjyJEjKC8vx759+1yajaKiIqxatQorVqxAXV0dKioqcPvtt7v8G6+88goef/xxHDt2DAsXLkRGRgYuX77s1e0kIg/o96P2iGjYsdlsotfrxWKxuLw2bNggIt1Pb87KynL5zPTp02XlypUiIrJ161YJDg6W1tZWdfmHH34oOp1OmpqaREQkMjJS8vPzb1gDAFm3bp36vrW1VRRFkY8//thj20lE3sFzRoioT+bMmYOioiKXsZCQEPXvGTNmuCybMWMG7HY7AODUqVOYMmUKLBaLunzWrFlwOp04ffo0FEVBQ0MD5s2b94c1TJ48Wf3bYrEgMDAQzc3Nfd4mItIGmxEi6hOLxeJ22ORmFEUBAIiI+ndPc0wmU6/W5+fn5/ZZp9N5SzURkfZ4zggRDYivv/7a7X1cXBwAYOLEibDb7bh69aq6/PDhw9DpdLjjjjsQGBiIcePGYf/+/V6tmYi0wT0jRNQn7e3taGpqchkzGAwIDQ0FAJSXlyMxMRFJSUkoLS1FVVUViouLAQAZGRlYv349bDYbCgoK8NNPPyEnJwdLly5FeHg4AKCgoABZWVkICwvDggUL4HA4cPjwYeTk5Hh3Q4lowLEZIaI+2bt3LyIiIlzGJkyYgG+//RZA95UuZWVleOaZZ2C1WlFaWoqJEycCAMxmMz755BPk5uZi2rRpMJvNWLx4MTZv3qyuy2azoa2tDa+//jqef/55hIaGYsmSJd7bQCLyGkVEROsiiGhoURQFu3btwqJFi7QuhYh8AM8ZISIiIk2xGSEiIiJN8ZwRIvI4Hv0lolvBPSNERESkKTYjREREpCk2I0RERKQpNiNERESkKTYjREREpCk2I0RERKQpNiNERESkKTYjREREpCk2I0RERKSpvwGe/eCZ0OiZaAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"train\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5, min_value+.05, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on test set: 16.52829267553528%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in test_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(test_pairs)\n",
    "print(f\"Phone error rate (PER) on test set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> sometimes\n",
      "= ['S', 'AX', 'M', 'T', 'AY', 'M', 'Z']\n",
      "< S AO M T IY M Z ['S', 'AO', 'M', 'T', 'IY', 'M', 'Z']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f01ca35bfa0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 540x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
