{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/exp/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"128\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6138648959009436\n",
      "ID_WEIGHT: 2.6955844953082524\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 30, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7fc90f246820> ([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "valid grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "test grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'l': 17, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'w': 28, 'v': 27, 'z': 31, 'j': 15, 'f': 11, 'x': 29}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'l': 17, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'w': 28, 'v': 27, 'z': 31, 'j': 15, 'f': 11, 'x': 29}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'l': 17, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'w': 28, 'v': 27, 'z': 31, 'j': 15, 'f': 11, 'x': 29}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 128\n",
      "n_layers: 1\n",
      "Encoder has a total number of 76544 parameters\n",
      "Decoder has a total number of 135204 parameters\n",
      "Total number of all parameters is 211748\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 1 finished in 0m 47.9s (- 79m 1.8s) (1 1.0%). train avg loss: 1.1311, val avg loss: 1.1807\n",
      "Training for epoch 2 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 2 finished in 1m 37.76s (- 79m 50.02s) (2 2.0%). train avg loss: 0.5538, val avg loss: 1.062\n",
      "Training for epoch 3 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 3 finished in 2m 26.94s (- 79m 11.16s) (3 3.0%). train avg loss: 0.4769, val avg loss: 1.1271\n",
      "Training for epoch 4 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 4 finished in 3m 20.91s (- 80m 21.86s) (4 4.0%). train avg loss: 0.5205, val avg loss: 0.9582\n",
      "Training for epoch 5 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 5 finished in 4m 14.58s (- 80m 37.07s) (5 5.0%). train avg loss: 0.4033, val avg loss: 0.9032\n",
      "Training for epoch 6 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 6 finished in 5m 8.59s (- 80m 34.65s) (6 6.0%). train avg loss: 0.3954, val avg loss: 0.9438\n",
      "Training for epoch 7 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 7 finished in 6m 2.85s (- 80m 20.77s) (7 7.0%). train avg loss: 0.376, val avg loss: 1.4312\n",
      "Training for epoch 8 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 8 finished in 6m 56.81s (- 79m 53.36s) (8 8.0%). train avg loss: 0.3969, val avg loss: 0.8653\n",
      "Training for epoch 9 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 9 finished in 7m 50.06s (- 79m 12.85s) (9 9.0%). train avg loss: 0.3516, val avg loss: 0.9269\n",
      "Training for epoch 10 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 10 finished in 8m 44.41s (- 78m 39.71s) (10 10.0%). train avg loss: 0.3478, val avg loss: 0.826\n",
      "Training for epoch 11 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 11 finished in 9m 37.61s (- 77m 53.42s) (11 11.0%). train avg loss: 0.333, val avg loss: 0.8273\n",
      "Training for epoch 12 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 12 finished in 10m 30.42s (- 77m 3.06s) (12 12.0%). train avg loss: 0.3456, val avg loss: 0.8333\n",
      "Training for epoch 13 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 13 finished in 11m 23.01s (- 76m 10.94s) (13 13.0%). train avg loss: 0.3441, val avg loss: 0.9265\n",
      "Training for epoch 14 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 14 finished in 12m 15.16s (- 75m 15.97s) (14 14.0%). train avg loss: 0.3212, val avg loss: 0.8001\n",
      "Training for epoch 15 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 15 finished in 13m 7.76s (- 74m 23.98s) (15 15.0%). train avg loss: 0.3028, val avg loss: 0.7994\n",
      "Training for epoch 16 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 16 finished in 13m 58.99s (- 73m 24.72s) (16 16.0%). train avg loss: 0.3073, val avg loss: 0.8092\n",
      "Training for epoch 17 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 17 finished in 14m 51.41s (- 72m 32.16s) (17 17.0%). train avg loss: 0.3214, val avg loss: 0.7464\n",
      "Training for epoch 18 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 18 finished in 15m 42.93s (- 71m 35.58s) (18 18.0%). train avg loss: 0.2789, val avg loss: 0.7754\n",
      "Training for epoch 19 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 19 finished in 16m 38.39s (- 70m 56.3s) (19 19.0%). train avg loss: 0.2725, val avg loss: 0.7776\n",
      "Training for epoch 20 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 20 finished in 17m 31.29s (- 70m 5.15s) (20 20.0%). train avg loss: 0.2739, val avg loss: 0.8442\n",
      "Training for epoch 21 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 21 finished in 18m 25.79s (- 69m 19.88s) (21 21.0%). train avg loss: 0.305, val avg loss: 0.7258\n",
      "Training for epoch 22 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 22 finished in 19m 19.37s (- 68m 30.48s) (22 22.0%). train avg loss: 0.257, val avg loss: 0.7618\n",
      "Training for epoch 23 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 23 finished in 20m 12.13s (- 67m 38.0s) (23 23.0%). train avg loss: 0.2635, val avg loss: 0.7233\n",
      "Training for epoch 24 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 24 finished in 21m 1.65s (- 66m 35.22s) (24 24.0%). train avg loss: 0.2569, val avg loss: 0.7016\n",
      "Training for epoch 25 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 25 finished in 21m 54.48s (- 65m 43.43s) (25 25.0%). train avg loss: 0.2487, val avg loss: 0.656\n",
      "Training for epoch 26 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 26 finished in 22m 48.65s (- 64m 55.38s) (26 26.0%). train avg loss: 0.237, val avg loss: 0.7363\n",
      "Training for epoch 27 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 27 finished in 23m 39.92s (- 63m 59.03s) (27 27.0%). train avg loss: 0.241, val avg loss: 0.7548\n",
      "Training for epoch 28 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 28 finished in 24m 33.84s (- 63m 9.86s) (28 28.0%). train avg loss: 0.248, val avg loss: 0.6789\n",
      "Training for epoch 29 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 29 finished in 25m 28.34s (- 62m 21.8s) (29 29.0%). train avg loss: 0.2482, val avg loss: 0.6944\n",
      "Training for epoch 30 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 30 finished in 26m 21.52s (- 61m 30.21s) (30 30.0%). train avg loss: 0.236, val avg loss: 0.663\n",
      "Training for epoch 31 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 31 finished in 27m 13.71s (- 60m 36.31s) (31 31.0%). train avg loss: 0.2292, val avg loss: 0.719\n",
      "Training for epoch 32 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 32 finished in 28m 5.58s (- 59m 41.86s) (32 32.0%). train avg loss: 0.2318, val avg loss: 0.6718\n",
      "Training for epoch 33 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 33 finished in 28m 58.42s (- 58m 49.51s) (33 33.0%). train avg loss: 0.22, val avg loss: 0.7383\n",
      "Training for epoch 34 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 34 finished in 29m 51.93s (- 57m 58.45s) (34 34.0%). train avg loss: 0.2267, val avg loss: 0.6628\n",
      "Training for epoch 35 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 35 finished in 30m 47.42s (- 57m 10.93s) (35 35.0%). train avg loss: 0.2148, val avg loss: 0.6937\n",
      "Training for epoch 36 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 36 finished in 31m 41.67s (- 56m 20.74s) (36 36.0%). train avg loss: 0.2099, val avg loss: 0.6515\n",
      "Training for epoch 37 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 37 finished in 32m 34.67s (- 55m 28.22s) (37 37.0%). train avg loss: 0.232, val avg loss: 0.6726\n",
      "Training for epoch 38 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 38 finished in 33m 26.66s (- 54m 34.03s) (38 38.0%). train avg loss: 0.2195, val avg loss: 0.7187\n",
      "Training for epoch 39 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 39 finished in 34m 19.26s (- 53m 40.9s) (39 39.0%). train avg loss: 0.2102, val avg loss: 0.7105\n",
      "Training for epoch 40 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 40 finished in 35m 11.17s (- 52m 46.76s) (40 40.0%). train avg loss: 0.2227, val avg loss: 0.688\n",
      "Training for epoch 41 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 41 finished in 36m 5.0s (- 51m 55.49s) (41 41.0%). train avg loss: 0.2043, val avg loss: 0.615\n",
      "Training for epoch 42 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 42 finished in 36m 59.53s (- 51m 5.06s) (42 42.0%). train avg loss: 0.2086, val avg loss: 0.6532\n",
      "Training for epoch 43 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 43 finished in 37m 52.87s (- 50m 12.88s) (43 43.0%). train avg loss: 0.2136, val avg loss: 0.6909\n",
      "Training for epoch 44 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 44 finished in 38m 47.51s (- 49m 22.29s) (44 44.0%). train avg loss: 0.2048, val avg loss: 0.6404\n",
      "Training for epoch 45 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 45 finished in 39m 41.73s (- 48m 31.0s) (45 45.0%). train avg loss: 0.2084, val avg loss: 0.6345\n",
      "Training for epoch 46 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 46 finished in 40m 35.49s (- 47m 39.06s) (46 46.0%). train avg loss: 0.1921, val avg loss: 0.6387\n",
      "Training for epoch 47 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 47 finished in 41m 31.13s (- 46m 49.15s) (47 47.0%). train avg loss: 0.1834, val avg loss: 0.6536\n",
      "Training for epoch 48 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 48 finished in 42m 26.16s (- 45m 58.34s) (48 48.0%). train avg loss: 0.1987, val avg loss: 0.6652\n",
      "Training for epoch 49 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 49 finished in 43m 22.17s (- 45m 8.38s) (49 49.0%). train avg loss: 0.2112, val avg loss: 0.6878\n",
      "Training for epoch 50 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 50 finished in 44m 15.78s (- 44m 15.78s) (50 50.0%). train avg loss: 0.1981, val avg loss: 0.6719\n",
      "Training for epoch 51 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 51 finished in 45m 9.81s (- 43m 23.55s) (51 51.0%). train avg loss: 0.2051, val avg loss: 0.627\n",
      "Training for epoch 52 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 52 finished in 46m 3.69s (- 42m 31.1s) (52 52.0%). train avg loss: 0.1998, val avg loss: 0.7236\n",
      "Training for epoch 53 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 53 finished in 46m 58.85s (- 41m 39.73s) (53 53.0%). train avg loss: 0.1834, val avg loss: 0.6114\n",
      "Training for epoch 54 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 54 finished in 47m 52.97s (- 40m 47.35s) (54 54.0%). train avg loss: 0.1669, val avg loss: 0.5697\n",
      "Training for epoch 55 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 55 finished in 48m 44.02s (- 39m 52.38s) (55 55.0%). train avg loss: 0.1652, val avg loss: 0.575\n",
      "Training for epoch 56 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 56 finished in 49m 36.82s (- 38m 58.93s) (56 56.0%). train avg loss: 0.1624, val avg loss: 0.5858\n",
      "Training for epoch 57 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 57 finished in 50m 31.17s (- 38m 6.67s) (57 57.0%). train avg loss: 0.156, val avg loss: 0.5706\n",
      "Training for epoch 58 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 58 finished in 51m 25.42s (- 37m 14.27s) (58 58.0%). train avg loss: 0.1535, val avg loss: 0.6221\n",
      "Training for epoch 59 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 59 finished in 52m 18.24s (- 36m 20.81s) (59 59.0%). train avg loss: 0.151, val avg loss: 0.5648\n",
      "Training for epoch 60 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 60 finished in 53m 14.24s (- 35m 29.49s) (60 60.0%). train avg loss: 0.1465, val avg loss: 0.5883\n",
      "Training for epoch 61 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 61 finished in 54m 9.94s (- 34m 37.83s) (61 61.0%). train avg loss: 0.1487, val avg loss: 0.5742\n",
      "Training for epoch 62 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 62 finished in 55m 3.35s (- 33m 44.64s) (62 62.0%). train avg loss: 0.1497, val avg loss: 0.6186\n",
      "Training for epoch 63 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 63 finished in 55m 55.32s (- 32m 50.59s) (63 63.0%). train avg loss: 0.146, val avg loss: 0.5889\n",
      "Training for epoch 64 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 64 finished in 56m 50.66s (- 31m 58.49s) (64 64.0%). train avg loss: 0.142, val avg loss: 0.5774\n",
      "Training for epoch 65 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 65 finished in 57m 43.4s (- 31m 4.91s) (65 65.0%). train avg loss: 0.1461, val avg loss: 0.6013\n",
      "Training for epoch 66 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 66 finished in 58m 35.62s (- 30m 11.08s) (66 66.0%). train avg loss: 0.1435, val avg loss: 0.6239\n",
      "Training for epoch 67 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 67 finished in 59m 31.07s (- 29m 18.88s) (67 67.0%). train avg loss: 0.1387, val avg loss: 0.6079\n",
      "Training for epoch 68 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 68 finished in 60m 28.99s (- 28m 27.76s) (68 68.0%). train avg loss: 0.1411, val avg loss: 0.5933\n",
      "Training for epoch 69 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 69 finished in 61m 25.16s (- 27m 35.65s) (69 69.0%). train avg loss: 0.1386, val avg loss: 0.5899\n",
      "Training for epoch 70 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 70 finished in 62m 20.24s (- 26m 42.96s) (70 70.0%). train avg loss: 0.1363, val avg loss: 0.5976\n",
      "Training for epoch 71 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 71 finished in 63m 13.29s (- 25m 49.37s) (71 71.0%). train avg loss: 0.1293, val avg loss: 0.5786\n",
      "Training for epoch 72 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 72 finished in 64m 8.59s (- 24m 56.68s) (72 72.0%). train avg loss: 0.1229, val avg loss: 0.5798\n",
      "Training for epoch 73 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 73 finished in 65m 3.42s (- 24m 3.73s) (73 73.0%). train avg loss: 0.122, val avg loss: 0.5916\n",
      "Training for epoch 74 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 74 finished in 65m 57.04s (- 23m 10.31s) (74 74.0%). train avg loss: 0.1238, val avg loss: 0.5719\n",
      "Training for epoch 75 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 75 finished in 66m 50.93s (- 22m 16.98s) (75 75.0%). train avg loss: 0.123, val avg loss: 0.5838\n",
      "Training for epoch 76 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 76 finished in 67m 45.23s (- 21m 23.76s) (76 76.0%). train avg loss: 0.1172, val avg loss: 0.5807\n",
      "Training for epoch 77 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 77 finished in 68m 40.32s (- 20m 30.74s) (77 77.0%). train avg loss: 0.1202, val avg loss: 0.5796\n",
      "Training for epoch 78 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 78 finished in 69m 34.07s (- 19m 37.3s) (78 78.0%). train avg loss: 0.1165, val avg loss: 0.5655\n",
      "Training for epoch 79 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 79 finished in 70m 28.68s (- 18m 44.08s) (79 79.0%). train avg loss: 0.1175, val avg loss: 0.5813\n",
      "Training for epoch 80 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 80 finished in 71m 22.13s (- 17m 50.53s) (80 80.0%). train avg loss: 0.1171, val avg loss: 0.6074\n",
      "Training for epoch 81 has started (lr=0.00025). Found 1922 batch(es).\n",
      "Epoch 81 finished in 72m 14.76s (- 16m 56.8s) (81 81.0%). train avg loss: 0.1151, val avg loss: 0.6075\n",
      "Training for epoch 82 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 82 finished in 73m 7.44s (- 16m 3.1s) (82 82.0%). train avg loss: 0.1087, val avg loss: 0.5932\n",
      "Training for epoch 83 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 83 finished in 74m 2.2s (- 15m 9.85s) (83 83.0%). train avg loss: 0.1086, val avg loss: 0.5685\n",
      "Training for epoch 84 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 84 finished in 74m 57.99s (- 14m 16.76s) (84 84.0%). train avg loss: 0.1067, val avg loss: 0.5851\n",
      "Training for epoch 85 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 85 finished in 75m 53.72s (- 13m 23.6s) (85 85.0%). train avg loss: 0.105, val avg loss: 0.6023\n",
      "Training for epoch 86 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 86 finished in 76m 46.97s (- 12m 29.97s) (86 86.0%). train avg loss: 0.1064, val avg loss: 0.5913\n",
      "Training for epoch 87 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 87 finished in 77m 39.77s (- 11m 36.29s) (87 87.0%). train avg loss: 0.1052, val avg loss: 0.5967\n",
      "Training for epoch 88 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 88 finished in 78m 34.46s (- 10m 42.88s) (88 88.0%). train avg loss: 0.1062, val avg loss: 0.596\n",
      "Training for epoch 89 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 89 finished in 79m 23.79s (- 9m 48.78s) (89 89.0%). train avg loss: 0.103, val avg loss: 0.5819\n",
      "Training for epoch 90 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 90 finished in 80m 8.24s (- 8m 54.25s) (90 90.0%). train avg loss: 0.104, val avg loss: 0.5845\n",
      "Training for epoch 91 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 91 finished in 80m 52.25s (- 7m 59.89s) (91 91.0%). train avg loss: 0.105, val avg loss: 0.5995\n",
      "Training for epoch 92 has started (lr=0.000125). Found 1922 batch(es).\n",
      "Epoch 92 finished in 81m 41.99s (- 7m 6.26s) (92 92.0%). train avg loss: 0.1025, val avg loss: 0.6129\n",
      "Training for epoch 93 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 93 finished in 82m 29.97s (- 6m 12.58s) (93 93.0%). train avg loss: 0.1011, val avg loss: 0.5956\n",
      "Training for epoch 94 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 94 finished in 83m 15.11s (- 5m 18.84s) (94 94.0%). train avg loss: 0.0984, val avg loss: 0.5988\n",
      "Training for epoch 95 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 95 finished in 84m 1.48s (- 4m 25.34s) (95 95.0%). train avg loss: 0.0994, val avg loss: 0.5848\n",
      "Training for epoch 96 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 96 finished in 84m 47.42s (- 3m 31.98s) (96 96.0%). train avg loss: 0.0987, val avg loss: 0.5795\n",
      "Training for epoch 97 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 97 finished in 85m 32.0s (- 2m 38.72s) (97 97.0%). train avg loss: 0.098, val avg loss: 0.5795\n",
      "Training for epoch 98 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 98 finished in 86m 25.7s (- 1m 45.83s) (98 98.0%). train avg loss: 0.1006, val avg loss: 0.5845\n",
      "Training for epoch 99 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 99 finished in 87m 14.38s (- 0m 52.87s) (99 99.0%). train avg loss: 0.099, val avg loss: 0.5887\n",
      "Training for epoch 100 has started (lr=6.25e-05). Found 1922 batch(es).\n",
      "Epoch 100 finished in 88m 2.78s (- 0m 0.0s) (100 100.0%). train avg loss: 0.0997, val avg loss: 0.5865\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5, min_value-.1, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 18.20158843350954%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> cermin\n",
      "= ['CH', 'AX', 'R', 'M', 'IY', 'N']\n",
      "< S AX R M IY N ['S', 'AX', 'R', 'M', 'IY', 'N']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc90f246ee0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 411.429x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
