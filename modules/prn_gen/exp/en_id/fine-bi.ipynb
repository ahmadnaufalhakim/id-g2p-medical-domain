{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/exp/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"256\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"128\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6138648959009436\n",
      "ID_WEIGHT: 2.6955844953082524\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[375, 170, 404, 233, 584, 400, 111, 281, 422, 671, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7fc896f51820> ([6, 103, 71, 589, 520, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 103, 71, 589, 520, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 103, 71, 589, 520, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 722 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-f', 31: '-g', 32: '-h', 33: '-i', 34: '-j', 35: '-k', 36: '-l', 37: '-m', 38: '-n', 39: '-o', 40: '-p', 41: '-q', 42: '-r', 43: '-s', 44: '-t', 45: '-u', 46: '-v', 47: '-w', 48: '-y', 49: \"a'\", 50: 'a-', 51: 'aa', 52: 'ab', 53: 'ac', 54: 'ad', 55: 'ae', 56: 'af', 57: 'ag', 58: 'ah', 59: 'ai', 60: 'aj', 61: 'ak', 62: 'al', 63: 'am', 64: 'an', 65: 'ao', 66: 'ap', 67: 'aq', 68: 'ar', 69: 'as', 70: 'at', 71: 'au', 72: 'av', 73: 'aw', 74: 'ax', 75: 'ay', 76: 'az', 77: \"b'\", 78: 'ba', 79: 'bb', 80: 'bc', 81: 'bd', 82: 'be', 83: 'bf', 84: 'bg', 85: 'bh', 86: 'bi', 87: 'bj', 88: 'bk', 89: 'bl', 90: 'bm', 91: 'bn', 92: 'bo', 93: 'bp', 94: 'br', 95: 'bs', 96: 'bt', 97: 'bu', 98: 'bv', 99: 'bw', 100: 'by', 101: 'bz', 102: \"c'\", 103: 'ca', 104: 'cb', 105: 'cc', 106: 'cd', 107: 'ce', 108: 'cf', 109: 'cg', 110: 'ch', 111: 'ci', 112: 'cj', 113: 'ck', 114: 'cl', 115: 'cm', 116: 'cn', 117: 'co', 118: 'cp', 119: 'cq', 120: 'cr', 121: 'cs', 122: 'ct', 123: 'cu', 124: 'cv', 125: 'cw', 126: 'cy', 127: 'cz', 128: \"d'\", 129: 'd-', 130: 'da', 131: 'db', 132: 'dc', 133: 'dd', 134: 'de', 135: 'df', 136: 'dg', 137: 'dh', 138: 'di', 139: 'dj', 140: 'dk', 141: 'dl', 142: 'dm', 143: 'dn', 144: 'do', 145: 'dp', 146: 'dq', 147: 'dr', 148: 'ds', 149: 'dt', 150: 'du', 151: 'dv', 152: 'dw', 153: 'dy', 154: 'dz', 155: \"e'\", 156: 'e-', 157: 'ea', 158: 'eb', 159: 'ec', 160: 'ed', 161: 'ee', 162: 'ef', 163: 'eg', 164: 'eh', 165: 'ei', 166: 'ej', 167: 'ek', 168: 'el', 169: 'em', 170: 'en', 171: 'eo', 172: 'ep', 173: 'eq', 174: 'er', 175: 'es', 176: 'et', 177: 'eu', 178: 'ev', 179: 'ew', 180: 'ex', 181: 'ey', 182: 'ez', 183: \"f'\", 184: 'f-', 185: 'fa', 186: 'fb', 187: 'fc', 188: 'fd', 189: 'fe', 190: 'ff', 191: 'fg', 192: 'fh', 193: 'fi', 194: 'fj', 195: 'fk', 196: 'fl', 197: 'fm', 198: 'fn', 199: 'fo', 200: 'fp', 201: 'fq', 202: 'fr', 203: 'fs', 204: 'ft', 205: 'fu', 206: 'fv', 207: 'fw', 208: 'fx', 209: 'fy', 210: 'fz', 211: \"g'\", 212: 'g-', 213: 'ga', 214: 'gb', 215: 'gc', 216: 'gd', 217: 'ge', 218: 'gf', 219: 'gg', 220: 'gh', 221: 'gi', 222: 'gj', 223: 'gk', 224: 'gl', 225: 'gm', 226: 'gn', 227: 'go', 228: 'gp', 229: 'gq', 230: 'gr', 231: 'gs', 232: 'gt', 233: 'gu', 234: 'gv', 235: 'gw', 236: 'gx', 237: 'gy', 238: 'gz', 239: \"h'\", 240: 'h-', 241: 'ha', 242: 'hb', 243: 'hc', 244: 'hd', 245: 'he', 246: 'hf', 247: 'hg', 248: 'hh', 249: 'hi', 250: 'hj', 251: 'hk', 252: 'hl', 253: 'hm', 254: 'hn', 255: 'ho', 256: 'hp', 257: 'hq', 258: 'hr', 259: 'hs', 260: 'ht', 261: 'hu', 262: 'hv', 263: 'hw', 264: 'hy', 265: 'i', 266: \"i'\", 267: 'i-', 268: 'ia', 269: 'ib', 270: 'ic', 271: 'id', 272: 'ie', 273: 'if', 274: 'ig', 275: 'ih', 276: 'ii', 277: 'ij', 278: 'ik', 279: 'il', 280: 'im', 281: 'in', 282: 'io', 283: 'ip', 284: 'iq', 285: 'ir', 286: 'is', 287: 'it', 288: 'iu', 289: 'iv', 290: 'iw', 291: 'ix', 292: 'iy', 293: 'iz', 294: \"j'\", 295: 'ja', 296: 'jc', 297: 'jd', 298: 'je', 299: 'jf', 300: 'jh', 301: 'ji', 302: 'jj', 303: 'jk', 304: 'jl', 305: 'jm', 306: 'jn', 307: 'jo', 308: 'jr', 309: 'js', 310: 'jt', 311: 'ju', 312: 'jv', 313: 'jw', 314: 'jy', 315: \"k'\", 316: 'k-', 317: 'ka', 318: 'kb', 319: 'kc', 320: 'kd', 321: 'ke', 322: 'kf', 323: 'kg', 324: 'kh', 325: 'ki', 326: 'kj', 327: 'kk', 328: 'kl', 329: 'km', 330: 'kn', 331: 'ko', 332: 'kp', 333: 'kr', 334: 'ks', 335: 'kt', 336: 'ku', 337: 'kv', 338: 'kw', 339: 'ky', 340: 'kz', 341: \"l'\", 342: 'l-', 343: 'la', 344: 'lb', 345: 'lc', 346: 'ld', 347: 'le', 348: 'lf', 349: 'lg', 350: 'lh', 351: 'li', 352: 'lj', 353: 'lk', 354: 'll', 355: 'lm', 356: 'ln', 357: 'lo', 358: 'lp', 359: 'lq', 360: 'lr', 361: 'ls', 362: 'lt', 363: 'lu', 364: 'lv', 365: 'lw', 366: 'lx', 367: 'ly', 368: 'lz', 369: \"m'\", 370: 'm-', 371: 'ma', 372: 'mb', 373: 'mc', 374: 'md', 375: 'me', 376: 'mf', 377: 'mg', 378: 'mh', 379: 'mi', 380: 'mj', 381: 'mk', 382: 'ml', 383: 'mm', 384: 'mn', 385: 'mo', 386: 'mp', 387: 'mq', 388: 'mr', 389: 'ms', 390: 'mt', 391: 'mu', 392: 'mv', 393: 'mw', 394: 'my', 395: 'mz', 396: \"n'\", 397: 'n-', 398: 'na', 399: 'nb', 400: 'nc', 401: 'nd', 402: 'ne', 403: 'nf', 404: 'ng', 405: 'nh', 406: 'ni', 407: 'nj', 408: 'nk', 409: 'nl', 410: 'nm', 411: 'nn', 412: 'no', 413: 'np', 414: 'nq', 415: 'nr', 416: 'ns', 417: 'nt', 418: 'nu', 419: 'nv', 420: 'nw', 421: 'nx', 422: 'ny', 423: 'nz', 424: 'o', 425: \"o'\", 426: 'o-', 427: 'oa', 428: 'ob', 429: 'oc', 430: 'od', 431: 'oe', 432: 'of', 433: 'og', 434: 'oh', 435: 'oi', 436: 'oj', 437: 'ok', 438: 'ol', 439: 'om', 440: 'on', 441: 'oo', 442: 'op', 443: 'oq', 444: 'or', 445: 'os', 446: 'ot', 447: 'ou', 448: 'ov', 449: 'ow', 450: 'ox', 451: 'oy', 452: 'oz', 453: \"p'\", 454: 'p-', 455: 'pa', 456: 'pb', 457: 'pc', 458: 'pd', 459: 'pe', 460: 'pf', 461: 'pg', 462: 'ph', 463: 'pi', 464: 'pj', 465: 'pk', 466: 'pl', 467: 'pm', 468: 'pn', 469: 'po', 470: 'pp', 471: 'pr', 472: 'ps', 473: 'pt', 474: 'pu', 475: 'pw', 476: 'py', 477: 'pz', 478: \"q'\", 479: 'qa', 480: 'qb', 481: 'qg', 482: 'qi', 483: 'qo', 484: 'qu', 485: 'qv', 486: \"r'\", 487: 'r-', 488: 'ra', 489: 'rb', 490: 'rc', 491: 'rd', 492: 're', 493: 'rf', 494: 'rg', 495: 'rh', 496: 'ri', 497: 'rj', 498: 'rk', 499: 'rl', 500: 'rm', 501: 'rn', 502: 'ro', 503: 'rp', 504: 'rq', 505: 'rr', 506: 'rs', 507: 'rt', 508: 'ru', 509: 'rv', 510: 'rw', 511: 'rx', 512: 'ry', 513: 'rz', 514: \"s'\", 515: 's-', 516: 'sa', 517: 'sb', 518: 'sc', 519: 'sd', 520: 'se', 521: 'sf', 522: 'sg', 523: 'sh', 524: 'si', 525: 'sj', 526: 'sk', 527: 'sl', 528: 'sm', 529: 'sn', 530: 'so', 531: 'sp', 532: 'sq', 533: 'sr', 534: 'ss', 535: 'st', 536: 'su', 537: 'sv', 538: 'sw', 539: 'sx', 540: 'sy', 541: 'sz', 542: \"t'\", 543: 't-', 544: 'ta', 545: 'tb', 546: 'tc', 547: 'td', 548: 'te', 549: 'tf', 550: 'tg', 551: 'th', 552: 'ti', 553: 'tj', 554: 'tk', 555: 'tl', 556: 'tm', 557: 'tn', 558: 'to', 559: 'tp', 560: 'tr', 561: 'ts', 562: 'tt', 563: 'tu', 564: 'tv', 565: 'tw', 566: 'tx', 567: 'ty', 568: 'tz', 569: \"u'\", 570: 'u-', 571: 'ua', 572: 'ub', 573: 'uc', 574: 'ud', 575: 'ue', 576: 'uf', 577: 'ug', 578: 'uh', 579: 'ui', 580: 'uj', 581: 'uk', 582: 'ul', 583: 'um', 584: 'un', 585: 'uo', 586: 'up', 587: 'uq', 588: 'ur', 589: 'us', 590: 'ut', 591: 'uu', 592: 'uv', 593: 'uw', 594: 'ux', 595: 'uy', 596: 'uz', 597: \"v'\", 598: 'va', 599: 'vc', 600: 'vd', 601: 've', 602: 'vg', 603: 'vh', 604: 'vi', 605: 'vj', 606: 'vk', 607: 'vl', 608: 'vn', 609: 'vo', 610: 'vr', 611: 'vs', 612: 'vt', 613: 'vu', 614: 'vv', 615: 'vy', 616: 'vz', 617: \"w'\", 618: 'w-', 619: 'wa', 620: 'wb', 621: 'wc', 622: 'wd', 623: 'we', 624: 'wf', 625: 'wg', 626: 'wh', 627: 'wi', 628: 'wk', 629: 'wl', 630: 'wm', 631: 'wn', 632: 'wo', 633: 'wp', 634: 'wq', 635: 'wr', 636: 'ws', 637: 'wt', 638: 'wu', 639: 'wv', 640: 'ww', 641: 'wy', 642: 'wz', 643: \"x'\", 644: 'x-', 645: 'xa', 646: 'xb', 647: 'xc', 648: 'xd', 649: 'xe', 650: 'xf', 651: 'xg', 652: 'xh', 653: 'xi', 654: 'xl', 655: 'xm', 656: 'xn', 657: 'xo', 658: 'xp', 659: 'xq', 660: 'xr', 661: 'xs', 662: 'xt', 663: 'xu', 664: 'xv', 665: 'xw', 666: 'xx', 667: 'xy', 668: 'xz', 669: \"y'\", 670: 'y-', 671: 'ya', 672: 'yb', 673: 'yc', 674: 'yd', 675: 'ye', 676: 'yf', 677: 'yg', 678: 'yh', 679: 'yi', 680: 'yj', 681: 'yk', 682: 'yl', 683: 'ym', 684: 'yn', 685: 'yo', 686: 'yp', 687: 'yq', 688: 'yr', 689: 'ys', 690: 'yt', 691: 'yu', 692: 'yv', 693: 'yw', 694: 'yx', 695: 'yy', 696: 'yz', 697: \"z'\", 698: 'za', 699: 'zb', 700: 'zc', 701: 'zd', 702: 'ze', 703: 'zf', 704: 'zg', 705: 'zh', 706: 'zi', 707: 'zk', 708: 'zl', 709: 'zm', 710: 'zn', 711: 'zo', 712: 'zp', 713: 'zq', 714: 'zr', 715: 'zs', 716: 'zt', 717: 'zu', 718: 'zv', 719: 'zw', 720: 'zy', 721: 'zz'}\n",
      "valid grp 722 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-f', 31: '-g', 32: '-h', 33: '-i', 34: '-j', 35: '-k', 36: '-l', 37: '-m', 38: '-n', 39: '-o', 40: '-p', 41: '-q', 42: '-r', 43: '-s', 44: '-t', 45: '-u', 46: '-v', 47: '-w', 48: '-y', 49: \"a'\", 50: 'a-', 51: 'aa', 52: 'ab', 53: 'ac', 54: 'ad', 55: 'ae', 56: 'af', 57: 'ag', 58: 'ah', 59: 'ai', 60: 'aj', 61: 'ak', 62: 'al', 63: 'am', 64: 'an', 65: 'ao', 66: 'ap', 67: 'aq', 68: 'ar', 69: 'as', 70: 'at', 71: 'au', 72: 'av', 73: 'aw', 74: 'ax', 75: 'ay', 76: 'az', 77: \"b'\", 78: 'ba', 79: 'bb', 80: 'bc', 81: 'bd', 82: 'be', 83: 'bf', 84: 'bg', 85: 'bh', 86: 'bi', 87: 'bj', 88: 'bk', 89: 'bl', 90: 'bm', 91: 'bn', 92: 'bo', 93: 'bp', 94: 'br', 95: 'bs', 96: 'bt', 97: 'bu', 98: 'bv', 99: 'bw', 100: 'by', 101: 'bz', 102: \"c'\", 103: 'ca', 104: 'cb', 105: 'cc', 106: 'cd', 107: 'ce', 108: 'cf', 109: 'cg', 110: 'ch', 111: 'ci', 112: 'cj', 113: 'ck', 114: 'cl', 115: 'cm', 116: 'cn', 117: 'co', 118: 'cp', 119: 'cq', 120: 'cr', 121: 'cs', 122: 'ct', 123: 'cu', 124: 'cv', 125: 'cw', 126: 'cy', 127: 'cz', 128: \"d'\", 129: 'd-', 130: 'da', 131: 'db', 132: 'dc', 133: 'dd', 134: 'de', 135: 'df', 136: 'dg', 137: 'dh', 138: 'di', 139: 'dj', 140: 'dk', 141: 'dl', 142: 'dm', 143: 'dn', 144: 'do', 145: 'dp', 146: 'dq', 147: 'dr', 148: 'ds', 149: 'dt', 150: 'du', 151: 'dv', 152: 'dw', 153: 'dy', 154: 'dz', 155: \"e'\", 156: 'e-', 157: 'ea', 158: 'eb', 159: 'ec', 160: 'ed', 161: 'ee', 162: 'ef', 163: 'eg', 164: 'eh', 165: 'ei', 166: 'ej', 167: 'ek', 168: 'el', 169: 'em', 170: 'en', 171: 'eo', 172: 'ep', 173: 'eq', 174: 'er', 175: 'es', 176: 'et', 177: 'eu', 178: 'ev', 179: 'ew', 180: 'ex', 181: 'ey', 182: 'ez', 183: \"f'\", 184: 'f-', 185: 'fa', 186: 'fb', 187: 'fc', 188: 'fd', 189: 'fe', 190: 'ff', 191: 'fg', 192: 'fh', 193: 'fi', 194: 'fj', 195: 'fk', 196: 'fl', 197: 'fm', 198: 'fn', 199: 'fo', 200: 'fp', 201: 'fq', 202: 'fr', 203: 'fs', 204: 'ft', 205: 'fu', 206: 'fv', 207: 'fw', 208: 'fx', 209: 'fy', 210: 'fz', 211: \"g'\", 212: 'g-', 213: 'ga', 214: 'gb', 215: 'gc', 216: 'gd', 217: 'ge', 218: 'gf', 219: 'gg', 220: 'gh', 221: 'gi', 222: 'gj', 223: 'gk', 224: 'gl', 225: 'gm', 226: 'gn', 227: 'go', 228: 'gp', 229: 'gq', 230: 'gr', 231: 'gs', 232: 'gt', 233: 'gu', 234: 'gv', 235: 'gw', 236: 'gx', 237: 'gy', 238: 'gz', 239: \"h'\", 240: 'h-', 241: 'ha', 242: 'hb', 243: 'hc', 244: 'hd', 245: 'he', 246: 'hf', 247: 'hg', 248: 'hh', 249: 'hi', 250: 'hj', 251: 'hk', 252: 'hl', 253: 'hm', 254: 'hn', 255: 'ho', 256: 'hp', 257: 'hq', 258: 'hr', 259: 'hs', 260: 'ht', 261: 'hu', 262: 'hv', 263: 'hw', 264: 'hy', 265: 'i', 266: \"i'\", 267: 'i-', 268: 'ia', 269: 'ib', 270: 'ic', 271: 'id', 272: 'ie', 273: 'if', 274: 'ig', 275: 'ih', 276: 'ii', 277: 'ij', 278: 'ik', 279: 'il', 280: 'im', 281: 'in', 282: 'io', 283: 'ip', 284: 'iq', 285: 'ir', 286: 'is', 287: 'it', 288: 'iu', 289: 'iv', 290: 'iw', 291: 'ix', 292: 'iy', 293: 'iz', 294: \"j'\", 295: 'ja', 296: 'jc', 297: 'jd', 298: 'je', 299: 'jf', 300: 'jh', 301: 'ji', 302: 'jj', 303: 'jk', 304: 'jl', 305: 'jm', 306: 'jn', 307: 'jo', 308: 'jr', 309: 'js', 310: 'jt', 311: 'ju', 312: 'jv', 313: 'jw', 314: 'jy', 315: \"k'\", 316: 'k-', 317: 'ka', 318: 'kb', 319: 'kc', 320: 'kd', 321: 'ke', 322: 'kf', 323: 'kg', 324: 'kh', 325: 'ki', 326: 'kj', 327: 'kk', 328: 'kl', 329: 'km', 330: 'kn', 331: 'ko', 332: 'kp', 333: 'kr', 334: 'ks', 335: 'kt', 336: 'ku', 337: 'kv', 338: 'kw', 339: 'ky', 340: 'kz', 341: \"l'\", 342: 'l-', 343: 'la', 344: 'lb', 345: 'lc', 346: 'ld', 347: 'le', 348: 'lf', 349: 'lg', 350: 'lh', 351: 'li', 352: 'lj', 353: 'lk', 354: 'll', 355: 'lm', 356: 'ln', 357: 'lo', 358: 'lp', 359: 'lq', 360: 'lr', 361: 'ls', 362: 'lt', 363: 'lu', 364: 'lv', 365: 'lw', 366: 'lx', 367: 'ly', 368: 'lz', 369: \"m'\", 370: 'm-', 371: 'ma', 372: 'mb', 373: 'mc', 374: 'md', 375: 'me', 376: 'mf', 377: 'mg', 378: 'mh', 379: 'mi', 380: 'mj', 381: 'mk', 382: 'ml', 383: 'mm', 384: 'mn', 385: 'mo', 386: 'mp', 387: 'mq', 388: 'mr', 389: 'ms', 390: 'mt', 391: 'mu', 392: 'mv', 393: 'mw', 394: 'my', 395: 'mz', 396: \"n'\", 397: 'n-', 398: 'na', 399: 'nb', 400: 'nc', 401: 'nd', 402: 'ne', 403: 'nf', 404: 'ng', 405: 'nh', 406: 'ni', 407: 'nj', 408: 'nk', 409: 'nl', 410: 'nm', 411: 'nn', 412: 'no', 413: 'np', 414: 'nq', 415: 'nr', 416: 'ns', 417: 'nt', 418: 'nu', 419: 'nv', 420: 'nw', 421: 'nx', 422: 'ny', 423: 'nz', 424: 'o', 425: \"o'\", 426: 'o-', 427: 'oa', 428: 'ob', 429: 'oc', 430: 'od', 431: 'oe', 432: 'of', 433: 'og', 434: 'oh', 435: 'oi', 436: 'oj', 437: 'ok', 438: 'ol', 439: 'om', 440: 'on', 441: 'oo', 442: 'op', 443: 'oq', 444: 'or', 445: 'os', 446: 'ot', 447: 'ou', 448: 'ov', 449: 'ow', 450: 'ox', 451: 'oy', 452: 'oz', 453: \"p'\", 454: 'p-', 455: 'pa', 456: 'pb', 457: 'pc', 458: 'pd', 459: 'pe', 460: 'pf', 461: 'pg', 462: 'ph', 463: 'pi', 464: 'pj', 465: 'pk', 466: 'pl', 467: 'pm', 468: 'pn', 469: 'po', 470: 'pp', 471: 'pr', 472: 'ps', 473: 'pt', 474: 'pu', 475: 'pw', 476: 'py', 477: 'pz', 478: \"q'\", 479: 'qa', 480: 'qb', 481: 'qg', 482: 'qi', 483: 'qo', 484: 'qu', 485: 'qv', 486: \"r'\", 487: 'r-', 488: 'ra', 489: 'rb', 490: 'rc', 491: 'rd', 492: 're', 493: 'rf', 494: 'rg', 495: 'rh', 496: 'ri', 497: 'rj', 498: 'rk', 499: 'rl', 500: 'rm', 501: 'rn', 502: 'ro', 503: 'rp', 504: 'rq', 505: 'rr', 506: 'rs', 507: 'rt', 508: 'ru', 509: 'rv', 510: 'rw', 511: 'rx', 512: 'ry', 513: 'rz', 514: \"s'\", 515: 's-', 516: 'sa', 517: 'sb', 518: 'sc', 519: 'sd', 520: 'se', 521: 'sf', 522: 'sg', 523: 'sh', 524: 'si', 525: 'sj', 526: 'sk', 527: 'sl', 528: 'sm', 529: 'sn', 530: 'so', 531: 'sp', 532: 'sq', 533: 'sr', 534: 'ss', 535: 'st', 536: 'su', 537: 'sv', 538: 'sw', 539: 'sx', 540: 'sy', 541: 'sz', 542: \"t'\", 543: 't-', 544: 'ta', 545: 'tb', 546: 'tc', 547: 'td', 548: 'te', 549: 'tf', 550: 'tg', 551: 'th', 552: 'ti', 553: 'tj', 554: 'tk', 555: 'tl', 556: 'tm', 557: 'tn', 558: 'to', 559: 'tp', 560: 'tr', 561: 'ts', 562: 'tt', 563: 'tu', 564: 'tv', 565: 'tw', 566: 'tx', 567: 'ty', 568: 'tz', 569: \"u'\", 570: 'u-', 571: 'ua', 572: 'ub', 573: 'uc', 574: 'ud', 575: 'ue', 576: 'uf', 577: 'ug', 578: 'uh', 579: 'ui', 580: 'uj', 581: 'uk', 582: 'ul', 583: 'um', 584: 'un', 585: 'uo', 586: 'up', 587: 'uq', 588: 'ur', 589: 'us', 590: 'ut', 591: 'uu', 592: 'uv', 593: 'uw', 594: 'ux', 595: 'uy', 596: 'uz', 597: \"v'\", 598: 'va', 599: 'vc', 600: 'vd', 601: 've', 602: 'vg', 603: 'vh', 604: 'vi', 605: 'vj', 606: 'vk', 607: 'vl', 608: 'vn', 609: 'vo', 610: 'vr', 611: 'vs', 612: 'vt', 613: 'vu', 614: 'vv', 615: 'vy', 616: 'vz', 617: \"w'\", 618: 'w-', 619: 'wa', 620: 'wb', 621: 'wc', 622: 'wd', 623: 'we', 624: 'wf', 625: 'wg', 626: 'wh', 627: 'wi', 628: 'wk', 629: 'wl', 630: 'wm', 631: 'wn', 632: 'wo', 633: 'wp', 634: 'wq', 635: 'wr', 636: 'ws', 637: 'wt', 638: 'wu', 639: 'wv', 640: 'ww', 641: 'wy', 642: 'wz', 643: \"x'\", 644: 'x-', 645: 'xa', 646: 'xb', 647: 'xc', 648: 'xd', 649: 'xe', 650: 'xf', 651: 'xg', 652: 'xh', 653: 'xi', 654: 'xl', 655: 'xm', 656: 'xn', 657: 'xo', 658: 'xp', 659: 'xq', 660: 'xr', 661: 'xs', 662: 'xt', 663: 'xu', 664: 'xv', 665: 'xw', 666: 'xx', 667: 'xy', 668: 'xz', 669: \"y'\", 670: 'y-', 671: 'ya', 672: 'yb', 673: 'yc', 674: 'yd', 675: 'ye', 676: 'yf', 677: 'yg', 678: 'yh', 679: 'yi', 680: 'yj', 681: 'yk', 682: 'yl', 683: 'ym', 684: 'yn', 685: 'yo', 686: 'yp', 687: 'yq', 688: 'yr', 689: 'ys', 690: 'yt', 691: 'yu', 692: 'yv', 693: 'yw', 694: 'yx', 695: 'yy', 696: 'yz', 697: \"z'\", 698: 'za', 699: 'zb', 700: 'zc', 701: 'zd', 702: 'ze', 703: 'zf', 704: 'zg', 705: 'zh', 706: 'zi', 707: 'zk', 708: 'zl', 709: 'zm', 710: 'zn', 711: 'zo', 712: 'zp', 713: 'zq', 714: 'zr', 715: 'zs', 716: 'zt', 717: 'zu', 718: 'zv', 719: 'zw', 720: 'zy', 721: 'zz'}\n",
      "test grp 722 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-f', 31: '-g', 32: '-h', 33: '-i', 34: '-j', 35: '-k', 36: '-l', 37: '-m', 38: '-n', 39: '-o', 40: '-p', 41: '-q', 42: '-r', 43: '-s', 44: '-t', 45: '-u', 46: '-v', 47: '-w', 48: '-y', 49: \"a'\", 50: 'a-', 51: 'aa', 52: 'ab', 53: 'ac', 54: 'ad', 55: 'ae', 56: 'af', 57: 'ag', 58: 'ah', 59: 'ai', 60: 'aj', 61: 'ak', 62: 'al', 63: 'am', 64: 'an', 65: 'ao', 66: 'ap', 67: 'aq', 68: 'ar', 69: 'as', 70: 'at', 71: 'au', 72: 'av', 73: 'aw', 74: 'ax', 75: 'ay', 76: 'az', 77: \"b'\", 78: 'ba', 79: 'bb', 80: 'bc', 81: 'bd', 82: 'be', 83: 'bf', 84: 'bg', 85: 'bh', 86: 'bi', 87: 'bj', 88: 'bk', 89: 'bl', 90: 'bm', 91: 'bn', 92: 'bo', 93: 'bp', 94: 'br', 95: 'bs', 96: 'bt', 97: 'bu', 98: 'bv', 99: 'bw', 100: 'by', 101: 'bz', 102: \"c'\", 103: 'ca', 104: 'cb', 105: 'cc', 106: 'cd', 107: 'ce', 108: 'cf', 109: 'cg', 110: 'ch', 111: 'ci', 112: 'cj', 113: 'ck', 114: 'cl', 115: 'cm', 116: 'cn', 117: 'co', 118: 'cp', 119: 'cq', 120: 'cr', 121: 'cs', 122: 'ct', 123: 'cu', 124: 'cv', 125: 'cw', 126: 'cy', 127: 'cz', 128: \"d'\", 129: 'd-', 130: 'da', 131: 'db', 132: 'dc', 133: 'dd', 134: 'de', 135: 'df', 136: 'dg', 137: 'dh', 138: 'di', 139: 'dj', 140: 'dk', 141: 'dl', 142: 'dm', 143: 'dn', 144: 'do', 145: 'dp', 146: 'dq', 147: 'dr', 148: 'ds', 149: 'dt', 150: 'du', 151: 'dv', 152: 'dw', 153: 'dy', 154: 'dz', 155: \"e'\", 156: 'e-', 157: 'ea', 158: 'eb', 159: 'ec', 160: 'ed', 161: 'ee', 162: 'ef', 163: 'eg', 164: 'eh', 165: 'ei', 166: 'ej', 167: 'ek', 168: 'el', 169: 'em', 170: 'en', 171: 'eo', 172: 'ep', 173: 'eq', 174: 'er', 175: 'es', 176: 'et', 177: 'eu', 178: 'ev', 179: 'ew', 180: 'ex', 181: 'ey', 182: 'ez', 183: \"f'\", 184: 'f-', 185: 'fa', 186: 'fb', 187: 'fc', 188: 'fd', 189: 'fe', 190: 'ff', 191: 'fg', 192: 'fh', 193: 'fi', 194: 'fj', 195: 'fk', 196: 'fl', 197: 'fm', 198: 'fn', 199: 'fo', 200: 'fp', 201: 'fq', 202: 'fr', 203: 'fs', 204: 'ft', 205: 'fu', 206: 'fv', 207: 'fw', 208: 'fx', 209: 'fy', 210: 'fz', 211: \"g'\", 212: 'g-', 213: 'ga', 214: 'gb', 215: 'gc', 216: 'gd', 217: 'ge', 218: 'gf', 219: 'gg', 220: 'gh', 221: 'gi', 222: 'gj', 223: 'gk', 224: 'gl', 225: 'gm', 226: 'gn', 227: 'go', 228: 'gp', 229: 'gq', 230: 'gr', 231: 'gs', 232: 'gt', 233: 'gu', 234: 'gv', 235: 'gw', 236: 'gx', 237: 'gy', 238: 'gz', 239: \"h'\", 240: 'h-', 241: 'ha', 242: 'hb', 243: 'hc', 244: 'hd', 245: 'he', 246: 'hf', 247: 'hg', 248: 'hh', 249: 'hi', 250: 'hj', 251: 'hk', 252: 'hl', 253: 'hm', 254: 'hn', 255: 'ho', 256: 'hp', 257: 'hq', 258: 'hr', 259: 'hs', 260: 'ht', 261: 'hu', 262: 'hv', 263: 'hw', 264: 'hy', 265: 'i', 266: \"i'\", 267: 'i-', 268: 'ia', 269: 'ib', 270: 'ic', 271: 'id', 272: 'ie', 273: 'if', 274: 'ig', 275: 'ih', 276: 'ii', 277: 'ij', 278: 'ik', 279: 'il', 280: 'im', 281: 'in', 282: 'io', 283: 'ip', 284: 'iq', 285: 'ir', 286: 'is', 287: 'it', 288: 'iu', 289: 'iv', 290: 'iw', 291: 'ix', 292: 'iy', 293: 'iz', 294: \"j'\", 295: 'ja', 296: 'jc', 297: 'jd', 298: 'je', 299: 'jf', 300: 'jh', 301: 'ji', 302: 'jj', 303: 'jk', 304: 'jl', 305: 'jm', 306: 'jn', 307: 'jo', 308: 'jr', 309: 'js', 310: 'jt', 311: 'ju', 312: 'jv', 313: 'jw', 314: 'jy', 315: \"k'\", 316: 'k-', 317: 'ka', 318: 'kb', 319: 'kc', 320: 'kd', 321: 'ke', 322: 'kf', 323: 'kg', 324: 'kh', 325: 'ki', 326: 'kj', 327: 'kk', 328: 'kl', 329: 'km', 330: 'kn', 331: 'ko', 332: 'kp', 333: 'kr', 334: 'ks', 335: 'kt', 336: 'ku', 337: 'kv', 338: 'kw', 339: 'ky', 340: 'kz', 341: \"l'\", 342: 'l-', 343: 'la', 344: 'lb', 345: 'lc', 346: 'ld', 347: 'le', 348: 'lf', 349: 'lg', 350: 'lh', 351: 'li', 352: 'lj', 353: 'lk', 354: 'll', 355: 'lm', 356: 'ln', 357: 'lo', 358: 'lp', 359: 'lq', 360: 'lr', 361: 'ls', 362: 'lt', 363: 'lu', 364: 'lv', 365: 'lw', 366: 'lx', 367: 'ly', 368: 'lz', 369: \"m'\", 370: 'm-', 371: 'ma', 372: 'mb', 373: 'mc', 374: 'md', 375: 'me', 376: 'mf', 377: 'mg', 378: 'mh', 379: 'mi', 380: 'mj', 381: 'mk', 382: 'ml', 383: 'mm', 384: 'mn', 385: 'mo', 386: 'mp', 387: 'mq', 388: 'mr', 389: 'ms', 390: 'mt', 391: 'mu', 392: 'mv', 393: 'mw', 394: 'my', 395: 'mz', 396: \"n'\", 397: 'n-', 398: 'na', 399: 'nb', 400: 'nc', 401: 'nd', 402: 'ne', 403: 'nf', 404: 'ng', 405: 'nh', 406: 'ni', 407: 'nj', 408: 'nk', 409: 'nl', 410: 'nm', 411: 'nn', 412: 'no', 413: 'np', 414: 'nq', 415: 'nr', 416: 'ns', 417: 'nt', 418: 'nu', 419: 'nv', 420: 'nw', 421: 'nx', 422: 'ny', 423: 'nz', 424: 'o', 425: \"o'\", 426: 'o-', 427: 'oa', 428: 'ob', 429: 'oc', 430: 'od', 431: 'oe', 432: 'of', 433: 'og', 434: 'oh', 435: 'oi', 436: 'oj', 437: 'ok', 438: 'ol', 439: 'om', 440: 'on', 441: 'oo', 442: 'op', 443: 'oq', 444: 'or', 445: 'os', 446: 'ot', 447: 'ou', 448: 'ov', 449: 'ow', 450: 'ox', 451: 'oy', 452: 'oz', 453: \"p'\", 454: 'p-', 455: 'pa', 456: 'pb', 457: 'pc', 458: 'pd', 459: 'pe', 460: 'pf', 461: 'pg', 462: 'ph', 463: 'pi', 464: 'pj', 465: 'pk', 466: 'pl', 467: 'pm', 468: 'pn', 469: 'po', 470: 'pp', 471: 'pr', 472: 'ps', 473: 'pt', 474: 'pu', 475: 'pw', 476: 'py', 477: 'pz', 478: \"q'\", 479: 'qa', 480: 'qb', 481: 'qg', 482: 'qi', 483: 'qo', 484: 'qu', 485: 'qv', 486: \"r'\", 487: 'r-', 488: 'ra', 489: 'rb', 490: 'rc', 491: 'rd', 492: 're', 493: 'rf', 494: 'rg', 495: 'rh', 496: 'ri', 497: 'rj', 498: 'rk', 499: 'rl', 500: 'rm', 501: 'rn', 502: 'ro', 503: 'rp', 504: 'rq', 505: 'rr', 506: 'rs', 507: 'rt', 508: 'ru', 509: 'rv', 510: 'rw', 511: 'rx', 512: 'ry', 513: 'rz', 514: \"s'\", 515: 's-', 516: 'sa', 517: 'sb', 518: 'sc', 519: 'sd', 520: 'se', 521: 'sf', 522: 'sg', 523: 'sh', 524: 'si', 525: 'sj', 526: 'sk', 527: 'sl', 528: 'sm', 529: 'sn', 530: 'so', 531: 'sp', 532: 'sq', 533: 'sr', 534: 'ss', 535: 'st', 536: 'su', 537: 'sv', 538: 'sw', 539: 'sx', 540: 'sy', 541: 'sz', 542: \"t'\", 543: 't-', 544: 'ta', 545: 'tb', 546: 'tc', 547: 'td', 548: 'te', 549: 'tf', 550: 'tg', 551: 'th', 552: 'ti', 553: 'tj', 554: 'tk', 555: 'tl', 556: 'tm', 557: 'tn', 558: 'to', 559: 'tp', 560: 'tr', 561: 'ts', 562: 'tt', 563: 'tu', 564: 'tv', 565: 'tw', 566: 'tx', 567: 'ty', 568: 'tz', 569: \"u'\", 570: 'u-', 571: 'ua', 572: 'ub', 573: 'uc', 574: 'ud', 575: 'ue', 576: 'uf', 577: 'ug', 578: 'uh', 579: 'ui', 580: 'uj', 581: 'uk', 582: 'ul', 583: 'um', 584: 'un', 585: 'uo', 586: 'up', 587: 'uq', 588: 'ur', 589: 'us', 590: 'ut', 591: 'uu', 592: 'uv', 593: 'uw', 594: 'ux', 595: 'uy', 596: 'uz', 597: \"v'\", 598: 'va', 599: 'vc', 600: 'vd', 601: 've', 602: 'vg', 603: 'vh', 604: 'vi', 605: 'vj', 606: 'vk', 607: 'vl', 608: 'vn', 609: 'vo', 610: 'vr', 611: 'vs', 612: 'vt', 613: 'vu', 614: 'vv', 615: 'vy', 616: 'vz', 617: \"w'\", 618: 'w-', 619: 'wa', 620: 'wb', 621: 'wc', 622: 'wd', 623: 'we', 624: 'wf', 625: 'wg', 626: 'wh', 627: 'wi', 628: 'wk', 629: 'wl', 630: 'wm', 631: 'wn', 632: 'wo', 633: 'wp', 634: 'wq', 635: 'wr', 636: 'ws', 637: 'wt', 638: 'wu', 639: 'wv', 640: 'ww', 641: 'wy', 642: 'wz', 643: \"x'\", 644: 'x-', 645: 'xa', 646: 'xb', 647: 'xc', 648: 'xd', 649: 'xe', 650: 'xf', 651: 'xg', 652: 'xh', 653: 'xi', 654: 'xl', 655: 'xm', 656: 'xn', 657: 'xo', 658: 'xp', 659: 'xq', 660: 'xr', 661: 'xs', 662: 'xt', 663: 'xu', 664: 'xv', 665: 'xw', 666: 'xx', 667: 'xy', 668: 'xz', 669: \"y'\", 670: 'y-', 671: 'ya', 672: 'yb', 673: 'yc', 674: 'yd', 675: 'ye', 676: 'yf', 677: 'yg', 678: 'yh', 679: 'yi', 680: 'yj', 681: 'yk', 682: 'yl', 683: 'ym', 684: 'yn', 685: 'yo', 686: 'yp', 687: 'yq', 688: 'yr', 689: 'ys', 690: 'yt', 691: 'yu', 692: 'yv', 693: 'yw', 694: 'yx', 695: 'yy', 696: 'yz', 697: \"z'\", 698: 'za', 699: 'zb', 700: 'zc', 701: 'zd', 702: 'ze', 703: 'zf', 704: 'zg', 705: 'zh', 706: 'zi', 707: 'zk', 708: 'zl', 709: 'zm', 710: 'zn', 711: 'zo', 712: 'zp', 713: 'zq', 714: 'zr', 715: 'zs', 716: 'zt', 717: 'zu', 718: 'zv', 719: 'zw', 720: 'zy', 721: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "718 {\"'c\": 6, 'ca': 103, 'au': 71, 'us': 589, 'se': 520, 'co': 117, 'ou': 447, 'ur': 588, 'rs': 506, \"'e\": 8, 'em': 169, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 484, 'uo': 585, 'ot': 446, 'te': 548, \"'t\": 21, 'ti': 552, 'is': 286, '--': 24, '-n': 38, 'ny': 422, 'ya': 671, '-a': 25, 'an': 64, 'nd': 401, 'da': 130, '-b': 26, 'be': 82, 'el': 168, 'la': 343, 'as': 69, '-c': 27, 'om': 439, 'mp': 386, 'pe': 459, 'en': 170, 'ng': 404, '-d': 28, 'de': 134, 'ap': 66, 'pa': 455, '-k': 35, 'ka': 317, 'ku': 336, '-l': 36, 'le': 347, 'gg': 219, 'ga': 213, '-m': 37, 'ma': 371, 'ah': 58, 'ha': 241, 'si': 524, 'sw': 538, 'wa': 619, '-s': 43, 'ep': 172, 'pi': 463, 'it': 287, '-t': 44, 'ta': 544, 'ak': 61, '-w': 47, 'at': 70, \"a'\": 49, \"'d\": 7, 'du': 150, \"'h\": 10, 'ad': 54, 'aa': 51, 'ab': 52, 'er': 174, 'rg': 494, 'ac': 53, 'ch': 110, 'he': 245, 'ke': 321, 'al': 62, 'ls': 361, 'et': 176, 'th': 551, 'am': 63, 'mo': 385, 'od': 430, 'dt': 149, 'nc': 400, 'or': 444, 'ar': 68, 'rd': 491, 'dv': 151, 'va': 598, 'rk': 498, 'ro': 502, 'on': 440, \"n'\": 396, \"'s\": 20, 'ns': 416, 'so': 530, 'rt': 507, 'ba': 78, 'ck': 113, 'cu': 123, 'di': 138, 'ia': 268, 'ik': 278, 'ai': 59, 'im': 280, 'na': 398, 'ir': 285, 'lk': 353, 'ki': 325, 'in': 281, 'lo': 357, 'ne': 402, 'os': 445, 'do': 144, 'ni': 406, 'nm': 410, 'me': 375, 'nt': 417, 'ts': 561, 'gn': 226, 'to': 558, 'ra': 488, 'rc': 490, 're': 492, 'sc': 518, 'sh': 523, 'ed': 160, 'es': 175, 'oa': 427, 'bb': 79, 'ie': 272, 'll': 354, 'nh': 405, 'tt': 562, 'ev': 178, 'vi': 604, 'il': 279, 'ey': 181, \"y'\": 669, 'bi': 86, 'bo': 92, \"t'\": 542, 'ud': 574, 'br': 94, 'io': 282, 'bs': 95, 'by': 100, 'bc': 80, 'ek': 167, 'bd': 81, 'ic': 270, 'dn': 143, 'no': 412, 'ol': 438, 'mi': 379, 'uc': 573, 'ct': 122, 'ee': 161, 'ul': 582, 'az': 76, 'zi': 706, 'iz': 293, 'ln': 356, 'ow': 449, 'dr': 147, 'cr': 120, 'mb': 372, 'rl': 499, 'rm': 500, 'rr': 505, 'ce': 107, 'yt': 690, 'bh': 85, 'ho': 255, 'id': 271, 'ig': 274, 'li': 351, 'ty': 567, 'gt': 232, 'ib': 269, 'tu': 563, 'ri': 496, 'tz': 568, 'bj': 87, 'ja': 295, 'je': 298, 'ec': 159, 'bk': 88, 'kh': 324, 'bl': 89, 'ut': 590, 'ze': 702, 'e-': 156, 'st': 535, 'oo': 441, 'ly': 367, 'bn': 91, 'nu': 418, \"o'\": 425, 'oi': 435, 'hi': 249, 'sm': 528, 'gi': 221, 'rn': 501, 'if': 273, 'fa': 185, 'ci': 111, 'iv': 289, 've': 601, 'uh': 578, 'un': 584, 'ds': 148, 'ov': 448, \"e'\": 155, 'bp': 93, 'pl': 466, 'lp': 358, 'ms': 389, 'mc': 373, 'cz': 127, 'zy': 720, 'yk': 681, 'wi': 627, 'ea': 157, 'ks': 334, 'eg': 163, 'go': 227, 'dg': 136, 'ge': 217, 'ko': 331, 'og': 433, 'ru': 508, 'up': 586, 'pt': 473, 'tl': 555, 'tn': 557, 'ss': 534, 'yn': 684, 'uz': 596, 'zz': 721, 'zo': 711, 'sa': 516, 'ei': 165, 'lu': 363, 'lv': 364, 'rb': 489, 'rp': 503, 'ps': 472, 'tr': 560, 'su': 536, 'dl': 141, 'bt': 96, 'bu': 97, 'ua': 571, 'hm': 253, 'uk': 581, 'dz': 154, 'lh': 350, 'ay': 75, 'ys': 689, 'cs': 121, 'my': 394, 'po': 469, 'pu': 474, 'lc': 345, 'cc': 105, 'ry': 512, 't-': 543, '-p': 40, 'pr': 471, 'cl': 114, 'mm': 383, 'yi': 679, \"r'\": 486, 'gl': 224, 'cy': 126, \"s'\": 514, 'ue': 575, 'ui': 579, 'um': 583, 'mu': 391, 'op': 442, 'ph': 462, 'yl': 682, 'eb': 158, 'nb': 399, 'ht': 260, 'hy': 264, 'fi': 193, 'fy': 209, 'kl': 328, 'km': 329, 'kn': 330, 'wl': 629, 'gm': 225, 'kr': 333, 'oy': 451, 'yd': 674, \"d'\": 128, 'cm': 115, 'cn': 116, 'oc': 429, 'of': 432, 'ff': 190, 'cq': 119, 'av': 72, 'ag': 57, 'ob': 428, 'ym': 683, 'tm': 556, 'uf': 576, 'gk': 223, 'sy': 540, 'yc': 673, 'iu': 288, \"m'\": 369, 'za': 698, 'sk': 526, 'wn': 631, 'ex': 180, 'dc': 132, 'dd': 133, 'eo': 171, 'dw': 152, 'we': 623, 'dy': 153, 'eh': 164, 'lb': 344, \"l'\": 341, 'lm': 355, 'nn': 411, 'sb': 517, 'eq': 173, 'rh': 495, 'ld': 346, 'lt': 362, 'dh': 137, 'eu': 177, 'gu': 233, 'kk': 327, 'kt': 335, 'lf': 348, 'ip': 283, 'gh': 220, 'aj': 60, 'ok': 437, 'oh': 434, 'iw': 290, 'gs': 231, 'dj': 139, 'jo': 307, 'ju': 311, 'dk': 140, 'dm': 142, \"f'\": 183, 'fo': 199, 'hs': 259, '-f': 30, 'ae': 55, 'sd': 519, \"g'\": 211, 'vo': 609, 'gr': 230, 'ft': 204, 'oq': 443, \"h'\": 239, 'sp': 531, 'hl': 252, \"p'\": 453, 'af': 56, 'ye': 675, 'fd': 188, 'fe': 189, 'ix': 291, 'xe': 649, 'fl': 196, 'fr': 202, 'ax': 74, 'fg': 191, \"i'\": 266, 'fh': 192, 'ox': 450, 'xi': 653, 'fm': 197, 'fs': 203, 'ef': 162, 'ug': 577, 'rw': 510, 'fw': 207, 'aw': 73, 'mn': 384, 'gy': 237, 'gc': 215, 'gf': 218, 'ih': 275, 'ew': 179, 'xc': 647, 'fu': 205, 'pp': 470, 'hh': 248, 'hk': 251, 'lg': 349, 'lq': 359, 'ub': 572, 'hn': 254, 'hr': 258, 'hu': 261, 'hw': 263, 'ml': 382, 'sl': 527, 'wo': 632, \"c'\": 102, 'rf': 493, 'nk': 408, 'tc': 546, \"x'\": 643, 'ji': 301, 'jn': 306, 'oj': 436, 'uj': 580, 'k-': 316, 'kc': 319, 'hb': 242, 'iy': 292, 'kw': 338, 'ky': 339, 'kz': 340, 'l-': 342, '-q': 41, \"'a\": 4, 'tv': 564, 'uq': 587, 'rq': 504, 'ej': 166, 'xa': 645, 'xo': 657, 'xy': 667, 'nq': 414, 'md': 374, 'nz': 423, 'fb': 186, 'ij': 277, 'iq': 284, 'lj': 352, '-i': 33, 'n-': 397, '-o': 39, 'nw': 420, 'yw': 693, 'lr': 360, 'uv': 592, 'lw': 365, 'ez': 182, 'mq': 387, 'fn': 198, \"k'\": 315, 'tf': 549, 'zh': 705, 'wy': 641, 'lz': 368, 'np': 413, 'xt': 662, 'zc': 700, 'zq': 713, 'mf': 376, 'mg': 377, 'mh': 378, 'yv': 692, 'mk': 381, 'oe': 431, 'pc': 457, 'mr': 388, 'mt': 390, 'mv': 392, 'mw': 393, 'yo': 685, 'yx': 694, 'yz': 696, 'sq': 532, 'nv': 419, 'oz': 452, 'ws': 636, 'cd': 106, 'nf': 403, 'gp': 228, 'nj': 407, 'jl': 304, 'nr': 415, 'sg': 522, 'i-': 267, 'bm': 90, 'tk': 554, \"u'\": 569, 'tw': 565, 'nx': 421, 'yb': 672, 'yh': 678, 'yp': 686, 'wh': 626, 'ao': 65, 'pf': 460, 'pg': 461, 'pk': 465, 'aq': 67, 'qa': 479, 'qi': 482, 'rv': 509, 'ux': 594, 'hd': 244, 'zm': 709, 'rj': 497, 'wr': 635, 'd-': 129, \"w'\": 617, 'zt': 716, 'zu': 717, 'rx': 511, 'rz': 513, 'sn': 529, 'ii': 276, 'sf': 521, 'hc': 243, 'hf': 246, \"v'\": 597, 'pn': 468, 'sr': 533, 'yr': 688, 'uy': 595, 'vd': 600, 'vg': 602, 'vn': 608, 'vr': 610, 'vt': 612, 'wb': 620, 'wf': 624, 'wk': 628, 'wt': 637, 'wu': 638, 'xf': 650, 'xl': 654, 'xs': 661, 'yg': 677, 'yu': 691, 'yy': 695, 'zb': 699, \"b'\": 77, \"'r\": 19, 'kb': 318, 'kd': 320, 'kf': 322, 'kg': 323, 'kp': 332, 'y-': 670, 'tj': 553, 'gb': 214, 'gd': 216, 'gw': 235, \"'i\": 11, 'sz': 541, 'kv': 337, 'xq': 659, 'fk': 195, 'sv': 537, 'vs': 611, 'wm': 630, 'tb': 545, 'td': 547, 'hg': 247, 'uw': 593, 'wd': 622, 'zl': 708, 'cv': 124, 'db': 131, 'df': 135, 'dp': 145, 'vu': 613, \"'o\": 17, 'zr': 714, 'nl': 409, 'jy': 314, \"z'\": 697, 'r-': 487, 'a-': 50, '-g': 31, 'o-': 426, 's-': 515, 'gq': 229, 'jr': 308, 'g-': 212, '-j': 34, 'hp': 256, 'vy': 615, 'zd': 701, 'zn': 710, 'xu': 663, 'xb': 646, 'kj': 326, '-r': 42, '-v': 46, 'bz': 101, 'tg': 550, 'sj': 525, 'gj': 222, 'wc': 621, 'xw': 665, 'xx': 666, 'yf': 676, 'jd': 297, 'zk': 707, 'tp': 559, 'fc': 187, 'uu': 591, '-u': 45, 'py': 476, 'h-': 240, 'zw': 719, 'pb': 456, 'pj': 464, 'pw': 475, \"q'\": 478, \"'v\": 23, 'jk': 303, 'pd': 458, 'pm': 467, 'gx': 236, 'zp': 712, 'bw': 99, '-e': 29, 'wg': 625, 'zf': 703, 'vl': 607, 'cw': 125, 'hv': 262, 'vc': 599, 'zs': 715, 'mj': 380, 'xh': 652, 'vv': 614, 'xv': 664, 'mz': 395, 'bf': 83, 'hq': 257, 'dq': 146, 'lx': 366, '-h': 32, 'vj': 605, 'x-': 644, 'xn': 656, 'xp': 658, 'jv': 312, 'zg': 704, '-y': 48, 'fj': 194, 'jt': 310, 'w-': 618, 'xg': 651, 'xm': 655, 'tx': 566, 'gz': 238, 'gv': 234, 'jj': 302, 'f-': 184, 'wv': 639, \"'l\": 14, 'hj': 250, 'fp': 200, 'js': 309, 'vh': 603, 'wz': 642, 'u-': 570, 'i': 265, 'qb': 480, 'qg': 481, 'zv': 718, 'jf': 299, 'jh': 300, 'jc': 296, 'wp': 633, 'bv': 98, 'pz': 477, 'fq': 201, 'cb': 104, 'cf': 108, 'cg': 109, 'cp': 118, \"j'\": 294, 'xz': 668, 'cj': 112, 'fx': 208, \"'j\": 12, 'fz': 210, 'qv': 485, 'wq': 634, 'ww': 640, 'xr': 660, 'xd': 648, 'o': 424, \"'b\": 5, \"'g\": 9, \"'k\": 13, 'vk': 606, 'qo': 483, 'vz': 616, 'jm': 305, 'yj': 680, 'p-': 454, 'fv': 206, 'bg': 84, \"'u\": 22, 'jw': 313, 'sx': 539, 'm-': 370, 'yq': 687}\n",
      "718 {\"'c\": 6, 'ca': 103, 'au': 71, 'us': 589, 'se': 520, 'co': 117, 'ou': 447, 'ur': 588, 'rs': 506, \"'e\": 8, 'em': 169, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 484, 'uo': 585, 'ot': 446, 'te': 548, \"'t\": 21, 'ti': 552, 'is': 286, '--': 24, '-n': 38, 'ny': 422, 'ya': 671, '-a': 25, 'an': 64, 'nd': 401, 'da': 130, '-b': 26, 'be': 82, 'el': 168, 'la': 343, 'as': 69, '-c': 27, 'om': 439, 'mp': 386, 'pe': 459, 'en': 170, 'ng': 404, '-d': 28, 'de': 134, 'ap': 66, 'pa': 455, '-k': 35, 'ka': 317, 'ku': 336, '-l': 36, 'le': 347, 'gg': 219, 'ga': 213, '-m': 37, 'ma': 371, 'ah': 58, 'ha': 241, 'si': 524, 'sw': 538, 'wa': 619, '-s': 43, 'ep': 172, 'pi': 463, 'it': 287, '-t': 44, 'ta': 544, 'ak': 61, '-w': 47, 'at': 70, \"a'\": 49, \"'d\": 7, 'du': 150, \"'h\": 10, 'ad': 54, 'aa': 51, 'ab': 52, 'er': 174, 'rg': 494, 'ac': 53, 'ch': 110, 'he': 245, 'ke': 321, 'al': 62, 'ls': 361, 'et': 176, 'th': 551, 'am': 63, 'mo': 385, 'od': 430, 'dt': 149, 'nc': 400, 'or': 444, 'ar': 68, 'rd': 491, 'dv': 151, 'va': 598, 'rk': 498, 'ro': 502, 'on': 440, \"n'\": 396, \"'s\": 20, 'ns': 416, 'so': 530, 'rt': 507, 'ba': 78, 'ck': 113, 'cu': 123, 'di': 138, 'ia': 268, 'ik': 278, 'ai': 59, 'im': 280, 'na': 398, 'ir': 285, 'lk': 353, 'ki': 325, 'in': 281, 'lo': 357, 'ne': 402, 'os': 445, 'do': 144, 'ni': 406, 'nm': 410, 'me': 375, 'nt': 417, 'ts': 561, 'gn': 226, 'to': 558, 'ra': 488, 'rc': 490, 're': 492, 'sc': 518, 'sh': 523, 'ed': 160, 'es': 175, 'oa': 427, 'bb': 79, 'ie': 272, 'll': 354, 'nh': 405, 'tt': 562, 'ev': 178, 'vi': 604, 'il': 279, 'ey': 181, \"y'\": 669, 'bi': 86, 'bo': 92, \"t'\": 542, 'ud': 574, 'br': 94, 'io': 282, 'bs': 95, 'by': 100, 'bc': 80, 'ek': 167, 'bd': 81, 'ic': 270, 'dn': 143, 'no': 412, 'ol': 438, 'mi': 379, 'uc': 573, 'ct': 122, 'ee': 161, 'ul': 582, 'az': 76, 'zi': 706, 'iz': 293, 'ln': 356, 'ow': 449, 'dr': 147, 'cr': 120, 'mb': 372, 'rl': 499, 'rm': 500, 'rr': 505, 'ce': 107, 'yt': 690, 'bh': 85, 'ho': 255, 'id': 271, 'ig': 274, 'li': 351, 'ty': 567, 'gt': 232, 'ib': 269, 'tu': 563, 'ri': 496, 'tz': 568, 'bj': 87, 'ja': 295, 'je': 298, 'ec': 159, 'bk': 88, 'kh': 324, 'bl': 89, 'ut': 590, 'ze': 702, 'e-': 156, 'st': 535, 'oo': 441, 'ly': 367, 'bn': 91, 'nu': 418, \"o'\": 425, 'oi': 435, 'hi': 249, 'sm': 528, 'gi': 221, 'rn': 501, 'if': 273, 'fa': 185, 'ci': 111, 'iv': 289, 've': 601, 'uh': 578, 'un': 584, 'ds': 148, 'ov': 448, \"e'\": 155, 'bp': 93, 'pl': 466, 'lp': 358, 'ms': 389, 'mc': 373, 'cz': 127, 'zy': 720, 'yk': 681, 'wi': 627, 'ea': 157, 'ks': 334, 'eg': 163, 'go': 227, 'dg': 136, 'ge': 217, 'ko': 331, 'og': 433, 'ru': 508, 'up': 586, 'pt': 473, 'tl': 555, 'tn': 557, 'ss': 534, 'yn': 684, 'uz': 596, 'zz': 721, 'zo': 711, 'sa': 516, 'ei': 165, 'lu': 363, 'lv': 364, 'rb': 489, 'rp': 503, 'ps': 472, 'tr': 560, 'su': 536, 'dl': 141, 'bt': 96, 'bu': 97, 'ua': 571, 'hm': 253, 'uk': 581, 'dz': 154, 'lh': 350, 'ay': 75, 'ys': 689, 'cs': 121, 'my': 394, 'po': 469, 'pu': 474, 'lc': 345, 'cc': 105, 'ry': 512, 't-': 543, '-p': 40, 'pr': 471, 'cl': 114, 'mm': 383, 'yi': 679, \"r'\": 486, 'gl': 224, 'cy': 126, \"s'\": 514, 'ue': 575, 'ui': 579, 'um': 583, 'mu': 391, 'op': 442, 'ph': 462, 'yl': 682, 'eb': 158, 'nb': 399, 'ht': 260, 'hy': 264, 'fi': 193, 'fy': 209, 'kl': 328, 'km': 329, 'kn': 330, 'wl': 629, 'gm': 225, 'kr': 333, 'oy': 451, 'yd': 674, \"d'\": 128, 'cm': 115, 'cn': 116, 'oc': 429, 'of': 432, 'ff': 190, 'cq': 119, 'av': 72, 'ag': 57, 'ob': 428, 'ym': 683, 'tm': 556, 'uf': 576, 'gk': 223, 'sy': 540, 'yc': 673, 'iu': 288, \"m'\": 369, 'za': 698, 'sk': 526, 'wn': 631, 'ex': 180, 'dc': 132, 'dd': 133, 'eo': 171, 'dw': 152, 'we': 623, 'dy': 153, 'eh': 164, 'lb': 344, \"l'\": 341, 'lm': 355, 'nn': 411, 'sb': 517, 'eq': 173, 'rh': 495, 'ld': 346, 'lt': 362, 'dh': 137, 'eu': 177, 'gu': 233, 'kk': 327, 'kt': 335, 'lf': 348, 'ip': 283, 'gh': 220, 'aj': 60, 'ok': 437, 'oh': 434, 'iw': 290, 'gs': 231, 'dj': 139, 'jo': 307, 'ju': 311, 'dk': 140, 'dm': 142, \"f'\": 183, 'fo': 199, 'hs': 259, '-f': 30, 'ae': 55, 'sd': 519, \"g'\": 211, 'vo': 609, 'gr': 230, 'ft': 204, 'oq': 443, \"h'\": 239, 'sp': 531, 'hl': 252, \"p'\": 453, 'af': 56, 'ye': 675, 'fd': 188, 'fe': 189, 'ix': 291, 'xe': 649, 'fl': 196, 'fr': 202, 'ax': 74, 'fg': 191, \"i'\": 266, 'fh': 192, 'ox': 450, 'xi': 653, 'fm': 197, 'fs': 203, 'ef': 162, 'ug': 577, 'rw': 510, 'fw': 207, 'aw': 73, 'mn': 384, 'gy': 237, 'gc': 215, 'gf': 218, 'ih': 275, 'ew': 179, 'xc': 647, 'fu': 205, 'pp': 470, 'hh': 248, 'hk': 251, 'lg': 349, 'lq': 359, 'ub': 572, 'hn': 254, 'hr': 258, 'hu': 261, 'hw': 263, 'ml': 382, 'sl': 527, 'wo': 632, \"c'\": 102, 'rf': 493, 'nk': 408, 'tc': 546, \"x'\": 643, 'ji': 301, 'jn': 306, 'oj': 436, 'uj': 580, 'k-': 316, 'kc': 319, 'hb': 242, 'iy': 292, 'kw': 338, 'ky': 339, 'kz': 340, 'l-': 342, '-q': 41, \"'a\": 4, 'tv': 564, 'uq': 587, 'rq': 504, 'ej': 166, 'xa': 645, 'xo': 657, 'xy': 667, 'nq': 414, 'md': 374, 'nz': 423, 'fb': 186, 'ij': 277, 'iq': 284, 'lj': 352, '-i': 33, 'n-': 397, '-o': 39, 'nw': 420, 'yw': 693, 'lr': 360, 'uv': 592, 'lw': 365, 'ez': 182, 'mq': 387, 'fn': 198, \"k'\": 315, 'tf': 549, 'zh': 705, 'wy': 641, 'lz': 368, 'np': 413, 'xt': 662, 'zc': 700, 'zq': 713, 'mf': 376, 'mg': 377, 'mh': 378, 'yv': 692, 'mk': 381, 'oe': 431, 'pc': 457, 'mr': 388, 'mt': 390, 'mv': 392, 'mw': 393, 'yo': 685, 'yx': 694, 'yz': 696, 'sq': 532, 'nv': 419, 'oz': 452, 'ws': 636, 'cd': 106, 'nf': 403, 'gp': 228, 'nj': 407, 'jl': 304, 'nr': 415, 'sg': 522, 'i-': 267, 'bm': 90, 'tk': 554, \"u'\": 569, 'tw': 565, 'nx': 421, 'yb': 672, 'yh': 678, 'yp': 686, 'wh': 626, 'ao': 65, 'pf': 460, 'pg': 461, 'pk': 465, 'aq': 67, 'qa': 479, 'qi': 482, 'rv': 509, 'ux': 594, 'hd': 244, 'zm': 709, 'rj': 497, 'wr': 635, 'd-': 129, \"w'\": 617, 'zt': 716, 'zu': 717, 'rx': 511, 'rz': 513, 'sn': 529, 'ii': 276, 'sf': 521, 'hc': 243, 'hf': 246, \"v'\": 597, 'pn': 468, 'sr': 533, 'yr': 688, 'uy': 595, 'vd': 600, 'vg': 602, 'vn': 608, 'vr': 610, 'vt': 612, 'wb': 620, 'wf': 624, 'wk': 628, 'wt': 637, 'wu': 638, 'xf': 650, 'xl': 654, 'xs': 661, 'yg': 677, 'yu': 691, 'yy': 695, 'zb': 699, \"b'\": 77, \"'r\": 19, 'kb': 318, 'kd': 320, 'kf': 322, 'kg': 323, 'kp': 332, 'y-': 670, 'tj': 553, 'gb': 214, 'gd': 216, 'gw': 235, \"'i\": 11, 'sz': 541, 'kv': 337, 'xq': 659, 'fk': 195, 'sv': 537, 'vs': 611, 'wm': 630, 'tb': 545, 'td': 547, 'hg': 247, 'uw': 593, 'wd': 622, 'zl': 708, 'cv': 124, 'db': 131, 'df': 135, 'dp': 145, 'vu': 613, \"'o\": 17, 'zr': 714, 'nl': 409, 'jy': 314, \"z'\": 697, 'r-': 487, 'a-': 50, '-g': 31, 'o-': 426, 's-': 515, 'gq': 229, 'jr': 308, 'g-': 212, '-j': 34, 'hp': 256, 'vy': 615, 'zd': 701, 'zn': 710, 'xu': 663, 'xb': 646, 'kj': 326, '-r': 42, '-v': 46, 'bz': 101, 'tg': 550, 'sj': 525, 'gj': 222, 'wc': 621, 'xw': 665, 'xx': 666, 'yf': 676, 'jd': 297, 'zk': 707, 'tp': 559, 'fc': 187, 'uu': 591, '-u': 45, 'py': 476, 'h-': 240, 'zw': 719, 'pb': 456, 'pj': 464, 'pw': 475, \"q'\": 478, \"'v\": 23, 'jk': 303, 'pd': 458, 'pm': 467, 'gx': 236, 'zp': 712, 'bw': 99, '-e': 29, 'wg': 625, 'zf': 703, 'vl': 607, 'cw': 125, 'hv': 262, 'vc': 599, 'zs': 715, 'mj': 380, 'xh': 652, 'vv': 614, 'xv': 664, 'mz': 395, 'bf': 83, 'hq': 257, 'dq': 146, 'lx': 366, '-h': 32, 'vj': 605, 'x-': 644, 'xn': 656, 'xp': 658, 'jv': 312, 'zg': 704, '-y': 48, 'fj': 194, 'jt': 310, 'w-': 618, 'xg': 651, 'xm': 655, 'tx': 566, 'gz': 238, 'gv': 234, 'jj': 302, 'f-': 184, 'wv': 639, \"'l\": 14, 'hj': 250, 'fp': 200, 'js': 309, 'vh': 603, 'wz': 642, 'u-': 570, 'i': 265, 'qb': 480, 'qg': 481, 'zv': 718, 'jf': 299, 'jh': 300, 'jc': 296, 'wp': 633, 'bv': 98, 'pz': 477, 'fq': 201, 'cb': 104, 'cf': 108, 'cg': 109, 'cp': 118, \"j'\": 294, 'xz': 668, 'cj': 112, 'fx': 208, \"'j\": 12, 'fz': 210, 'qv': 485, 'wq': 634, 'ww': 640, 'xr': 660, 'xd': 648, 'o': 424, \"'b\": 5, \"'g\": 9, \"'k\": 13, 'vk': 606, 'qo': 483, 'vz': 616, 'jm': 305, 'yj': 680, 'p-': 454, 'fv': 206, 'bg': 84, \"'u\": 22, 'jw': 313, 'sx': 539, 'm-': 370, 'yq': 687}\n",
      "718 {\"'c\": 6, 'ca': 103, 'au': 71, 'us': 589, 'se': 520, 'co': 117, 'ou': 447, 'ur': 588, 'rs': 506, \"'e\": 8, 'em': 169, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 484, 'uo': 585, 'ot': 446, 'te': 548, \"'t\": 21, 'ti': 552, 'is': 286, '--': 24, '-n': 38, 'ny': 422, 'ya': 671, '-a': 25, 'an': 64, 'nd': 401, 'da': 130, '-b': 26, 'be': 82, 'el': 168, 'la': 343, 'as': 69, '-c': 27, 'om': 439, 'mp': 386, 'pe': 459, 'en': 170, 'ng': 404, '-d': 28, 'de': 134, 'ap': 66, 'pa': 455, '-k': 35, 'ka': 317, 'ku': 336, '-l': 36, 'le': 347, 'gg': 219, 'ga': 213, '-m': 37, 'ma': 371, 'ah': 58, 'ha': 241, 'si': 524, 'sw': 538, 'wa': 619, '-s': 43, 'ep': 172, 'pi': 463, 'it': 287, '-t': 44, 'ta': 544, 'ak': 61, '-w': 47, 'at': 70, \"a'\": 49, \"'d\": 7, 'du': 150, \"'h\": 10, 'ad': 54, 'aa': 51, 'ab': 52, 'er': 174, 'rg': 494, 'ac': 53, 'ch': 110, 'he': 245, 'ke': 321, 'al': 62, 'ls': 361, 'et': 176, 'th': 551, 'am': 63, 'mo': 385, 'od': 430, 'dt': 149, 'nc': 400, 'or': 444, 'ar': 68, 'rd': 491, 'dv': 151, 'va': 598, 'rk': 498, 'ro': 502, 'on': 440, \"n'\": 396, \"'s\": 20, 'ns': 416, 'so': 530, 'rt': 507, 'ba': 78, 'ck': 113, 'cu': 123, 'di': 138, 'ia': 268, 'ik': 278, 'ai': 59, 'im': 280, 'na': 398, 'ir': 285, 'lk': 353, 'ki': 325, 'in': 281, 'lo': 357, 'ne': 402, 'os': 445, 'do': 144, 'ni': 406, 'nm': 410, 'me': 375, 'nt': 417, 'ts': 561, 'gn': 226, 'to': 558, 'ra': 488, 'rc': 490, 're': 492, 'sc': 518, 'sh': 523, 'ed': 160, 'es': 175, 'oa': 427, 'bb': 79, 'ie': 272, 'll': 354, 'nh': 405, 'tt': 562, 'ev': 178, 'vi': 604, 'il': 279, 'ey': 181, \"y'\": 669, 'bi': 86, 'bo': 92, \"t'\": 542, 'ud': 574, 'br': 94, 'io': 282, 'bs': 95, 'by': 100, 'bc': 80, 'ek': 167, 'bd': 81, 'ic': 270, 'dn': 143, 'no': 412, 'ol': 438, 'mi': 379, 'uc': 573, 'ct': 122, 'ee': 161, 'ul': 582, 'az': 76, 'zi': 706, 'iz': 293, 'ln': 356, 'ow': 449, 'dr': 147, 'cr': 120, 'mb': 372, 'rl': 499, 'rm': 500, 'rr': 505, 'ce': 107, 'yt': 690, 'bh': 85, 'ho': 255, 'id': 271, 'ig': 274, 'li': 351, 'ty': 567, 'gt': 232, 'ib': 269, 'tu': 563, 'ri': 496, 'tz': 568, 'bj': 87, 'ja': 295, 'je': 298, 'ec': 159, 'bk': 88, 'kh': 324, 'bl': 89, 'ut': 590, 'ze': 702, 'e-': 156, 'st': 535, 'oo': 441, 'ly': 367, 'bn': 91, 'nu': 418, \"o'\": 425, 'oi': 435, 'hi': 249, 'sm': 528, 'gi': 221, 'rn': 501, 'if': 273, 'fa': 185, 'ci': 111, 'iv': 289, 've': 601, 'uh': 578, 'un': 584, 'ds': 148, 'ov': 448, \"e'\": 155, 'bp': 93, 'pl': 466, 'lp': 358, 'ms': 389, 'mc': 373, 'cz': 127, 'zy': 720, 'yk': 681, 'wi': 627, 'ea': 157, 'ks': 334, 'eg': 163, 'go': 227, 'dg': 136, 'ge': 217, 'ko': 331, 'og': 433, 'ru': 508, 'up': 586, 'pt': 473, 'tl': 555, 'tn': 557, 'ss': 534, 'yn': 684, 'uz': 596, 'zz': 721, 'zo': 711, 'sa': 516, 'ei': 165, 'lu': 363, 'lv': 364, 'rb': 489, 'rp': 503, 'ps': 472, 'tr': 560, 'su': 536, 'dl': 141, 'bt': 96, 'bu': 97, 'ua': 571, 'hm': 253, 'uk': 581, 'dz': 154, 'lh': 350, 'ay': 75, 'ys': 689, 'cs': 121, 'my': 394, 'po': 469, 'pu': 474, 'lc': 345, 'cc': 105, 'ry': 512, 't-': 543, '-p': 40, 'pr': 471, 'cl': 114, 'mm': 383, 'yi': 679, \"r'\": 486, 'gl': 224, 'cy': 126, \"s'\": 514, 'ue': 575, 'ui': 579, 'um': 583, 'mu': 391, 'op': 442, 'ph': 462, 'yl': 682, 'eb': 158, 'nb': 399, 'ht': 260, 'hy': 264, 'fi': 193, 'fy': 209, 'kl': 328, 'km': 329, 'kn': 330, 'wl': 629, 'gm': 225, 'kr': 333, 'oy': 451, 'yd': 674, \"d'\": 128, 'cm': 115, 'cn': 116, 'oc': 429, 'of': 432, 'ff': 190, 'cq': 119, 'av': 72, 'ag': 57, 'ob': 428, 'ym': 683, 'tm': 556, 'uf': 576, 'gk': 223, 'sy': 540, 'yc': 673, 'iu': 288, \"m'\": 369, 'za': 698, 'sk': 526, 'wn': 631, 'ex': 180, 'dc': 132, 'dd': 133, 'eo': 171, 'dw': 152, 'we': 623, 'dy': 153, 'eh': 164, 'lb': 344, \"l'\": 341, 'lm': 355, 'nn': 411, 'sb': 517, 'eq': 173, 'rh': 495, 'ld': 346, 'lt': 362, 'dh': 137, 'eu': 177, 'gu': 233, 'kk': 327, 'kt': 335, 'lf': 348, 'ip': 283, 'gh': 220, 'aj': 60, 'ok': 437, 'oh': 434, 'iw': 290, 'gs': 231, 'dj': 139, 'jo': 307, 'ju': 311, 'dk': 140, 'dm': 142, \"f'\": 183, 'fo': 199, 'hs': 259, '-f': 30, 'ae': 55, 'sd': 519, \"g'\": 211, 'vo': 609, 'gr': 230, 'ft': 204, 'oq': 443, \"h'\": 239, 'sp': 531, 'hl': 252, \"p'\": 453, 'af': 56, 'ye': 675, 'fd': 188, 'fe': 189, 'ix': 291, 'xe': 649, 'fl': 196, 'fr': 202, 'ax': 74, 'fg': 191, \"i'\": 266, 'fh': 192, 'ox': 450, 'xi': 653, 'fm': 197, 'fs': 203, 'ef': 162, 'ug': 577, 'rw': 510, 'fw': 207, 'aw': 73, 'mn': 384, 'gy': 237, 'gc': 215, 'gf': 218, 'ih': 275, 'ew': 179, 'xc': 647, 'fu': 205, 'pp': 470, 'hh': 248, 'hk': 251, 'lg': 349, 'lq': 359, 'ub': 572, 'hn': 254, 'hr': 258, 'hu': 261, 'hw': 263, 'ml': 382, 'sl': 527, 'wo': 632, \"c'\": 102, 'rf': 493, 'nk': 408, 'tc': 546, \"x'\": 643, 'ji': 301, 'jn': 306, 'oj': 436, 'uj': 580, 'k-': 316, 'kc': 319, 'hb': 242, 'iy': 292, 'kw': 338, 'ky': 339, 'kz': 340, 'l-': 342, '-q': 41, \"'a\": 4, 'tv': 564, 'uq': 587, 'rq': 504, 'ej': 166, 'xa': 645, 'xo': 657, 'xy': 667, 'nq': 414, 'md': 374, 'nz': 423, 'fb': 186, 'ij': 277, 'iq': 284, 'lj': 352, '-i': 33, 'n-': 397, '-o': 39, 'nw': 420, 'yw': 693, 'lr': 360, 'uv': 592, 'lw': 365, 'ez': 182, 'mq': 387, 'fn': 198, \"k'\": 315, 'tf': 549, 'zh': 705, 'wy': 641, 'lz': 368, 'np': 413, 'xt': 662, 'zc': 700, 'zq': 713, 'mf': 376, 'mg': 377, 'mh': 378, 'yv': 692, 'mk': 381, 'oe': 431, 'pc': 457, 'mr': 388, 'mt': 390, 'mv': 392, 'mw': 393, 'yo': 685, 'yx': 694, 'yz': 696, 'sq': 532, 'nv': 419, 'oz': 452, 'ws': 636, 'cd': 106, 'nf': 403, 'gp': 228, 'nj': 407, 'jl': 304, 'nr': 415, 'sg': 522, 'i-': 267, 'bm': 90, 'tk': 554, \"u'\": 569, 'tw': 565, 'nx': 421, 'yb': 672, 'yh': 678, 'yp': 686, 'wh': 626, 'ao': 65, 'pf': 460, 'pg': 461, 'pk': 465, 'aq': 67, 'qa': 479, 'qi': 482, 'rv': 509, 'ux': 594, 'hd': 244, 'zm': 709, 'rj': 497, 'wr': 635, 'd-': 129, \"w'\": 617, 'zt': 716, 'zu': 717, 'rx': 511, 'rz': 513, 'sn': 529, 'ii': 276, 'sf': 521, 'hc': 243, 'hf': 246, \"v'\": 597, 'pn': 468, 'sr': 533, 'yr': 688, 'uy': 595, 'vd': 600, 'vg': 602, 'vn': 608, 'vr': 610, 'vt': 612, 'wb': 620, 'wf': 624, 'wk': 628, 'wt': 637, 'wu': 638, 'xf': 650, 'xl': 654, 'xs': 661, 'yg': 677, 'yu': 691, 'yy': 695, 'zb': 699, \"b'\": 77, \"'r\": 19, 'kb': 318, 'kd': 320, 'kf': 322, 'kg': 323, 'kp': 332, 'y-': 670, 'tj': 553, 'gb': 214, 'gd': 216, 'gw': 235, \"'i\": 11, 'sz': 541, 'kv': 337, 'xq': 659, 'fk': 195, 'sv': 537, 'vs': 611, 'wm': 630, 'tb': 545, 'td': 547, 'hg': 247, 'uw': 593, 'wd': 622, 'zl': 708, 'cv': 124, 'db': 131, 'df': 135, 'dp': 145, 'vu': 613, \"'o\": 17, 'zr': 714, 'nl': 409, 'jy': 314, \"z'\": 697, 'r-': 487, 'a-': 50, '-g': 31, 'o-': 426, 's-': 515, 'gq': 229, 'jr': 308, 'g-': 212, '-j': 34, 'hp': 256, 'vy': 615, 'zd': 701, 'zn': 710, 'xu': 663, 'xb': 646, 'kj': 326, '-r': 42, '-v': 46, 'bz': 101, 'tg': 550, 'sj': 525, 'gj': 222, 'wc': 621, 'xw': 665, 'xx': 666, 'yf': 676, 'jd': 297, 'zk': 707, 'tp': 559, 'fc': 187, 'uu': 591, '-u': 45, 'py': 476, 'h-': 240, 'zw': 719, 'pb': 456, 'pj': 464, 'pw': 475, \"q'\": 478, \"'v\": 23, 'jk': 303, 'pd': 458, 'pm': 467, 'gx': 236, 'zp': 712, 'bw': 99, '-e': 29, 'wg': 625, 'zf': 703, 'vl': 607, 'cw': 125, 'hv': 262, 'vc': 599, 'zs': 715, 'mj': 380, 'xh': 652, 'vv': 614, 'xv': 664, 'mz': 395, 'bf': 83, 'hq': 257, 'dq': 146, 'lx': 366, '-h': 32, 'vj': 605, 'x-': 644, 'xn': 656, 'xp': 658, 'jv': 312, 'zg': 704, '-y': 48, 'fj': 194, 'jt': 310, 'w-': 618, 'xg': 651, 'xm': 655, 'tx': 566, 'gz': 238, 'gv': 234, 'jj': 302, 'f-': 184, 'wv': 639, \"'l\": 14, 'hj': 250, 'fp': 200, 'js': 309, 'vh': 603, 'wz': 642, 'u-': 570, 'i': 265, 'qb': 480, 'qg': 481, 'zv': 718, 'jf': 299, 'jh': 300, 'jc': 296, 'wp': 633, 'bv': 98, 'pz': 477, 'fq': 201, 'cb': 104, 'cf': 108, 'cg': 109, 'cp': 118, \"j'\": 294, 'xz': 668, 'cj': 112, 'fx': 208, \"'j\": 12, 'fz': 210, 'qv': 485, 'wq': 634, 'ww': 640, 'xr': 660, 'xd': 648, 'o': 424, \"'b\": 5, \"'g\": 9, \"'k\": 13, 'vk': 606, 'qo': 483, 'vz': 616, 'jm': 305, 'yj': 680, 'p-': 454, 'fv': 206, 'bg': 84, \"'u\": 22, 'jw': 313, 'sx': 539, 'm-': 370, 'yq': 687}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'L': 19, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'JH': 17, 'Y': 34, 'OY': 24, 'F': 13}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 256\n",
      "hidden_size: 128\n",
      "n_layers: 1\n",
      "Encoder has a total number of 333056 parameters\n",
      "Decoder has a total number of 215844 parameters\n",
      "Total number of all parameters is 548900\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 1 finished in 0m 47.49s (- 78m 21.46s) (1 1.0%). train avg loss: 1.1831, val avg loss: 1.1663\n",
      "Training for epoch 2 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 2 finished in 1m 33.12s (- 76m 2.76s) (2 2.0%). train avg loss: 0.5943, val avg loss: 1.0726\n",
      "Training for epoch 3 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 3 finished in 2m 19.28s (- 75m 3.37s) (3 3.0%). train avg loss: 0.5113, val avg loss: 1.0814\n",
      "Training for epoch 4 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 4 finished in 3m 5.19s (- 74m 4.6s) (4 4.0%). train avg loss: 0.499, val avg loss: 0.9472\n",
      "Training for epoch 5 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 5 finished in 3m 50.55s (- 73m 0.53s) (5 5.0%). train avg loss: 0.4501, val avg loss: 0.9217\n",
      "Training for epoch 6 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 6 finished in 4m 37.92s (- 72m 34.09s) (6 6.0%). train avg loss: 0.3994, val avg loss: 0.9005\n",
      "Training for epoch 7 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 7 finished in 5m 23.77s (- 71m 41.55s) (7 7.0%). train avg loss: 0.4053, val avg loss: 0.9175\n",
      "Training for epoch 8 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 8 finished in 6m 10.7s (- 71m 3.04s) (8 8.0%). train avg loss: 0.4038, val avg loss: 0.9046\n",
      "Training for epoch 9 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 9 finished in 6m 57.33s (- 70m 19.65s) (9 9.0%). train avg loss: 0.3829, val avg loss: 0.9187\n",
      "Training for epoch 10 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 10 finished in 7m 43.78s (- 69m 34.05s) (10 10.0%). train avg loss: 0.3734, val avg loss: 0.8506\n",
      "Training for epoch 11 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 11 finished in 8m 28.64s (- 68m 35.33s) (11 11.0%). train avg loss: 0.3677, val avg loss: 0.8156\n",
      "Training for epoch 12 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 12 finished in 9m 13.93s (- 67m 42.12s) (12 12.0%). train avg loss: 0.3525, val avg loss: 0.8883\n",
      "Training for epoch 13 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 13 finished in 10m 0.59s (- 66m 59.34s) (13 13.0%). train avg loss: 0.3735, val avg loss: 0.8547\n",
      "Training for epoch 14 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 14 finished in 10m 47.02s (- 66m 14.58s) (14 14.0%). train avg loss: 0.3387, val avg loss: 0.8003\n",
      "Training for epoch 15 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 15 finished in 11m 32.62s (- 65m 24.82s) (15 15.0%). train avg loss: 0.3265, val avg loss: 0.7838\n",
      "Training for epoch 16 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 16 finished in 12m 17.91s (- 64m 34.02s) (16 16.0%). train avg loss: 0.3279, val avg loss: 0.848\n",
      "Training for epoch 17 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 17 finished in 13m 3.59s (- 63m 45.78s) (17 17.0%). train avg loss: 0.3601, val avg loss: 0.8529\n",
      "Training for epoch 18 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 18 finished in 13m 48.58s (- 62m 54.65s) (18 18.0%). train avg loss: 0.3397, val avg loss: 0.8245\n",
      "Training for epoch 19 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 19 finished in 14m 34.71s (- 62m 9.05s) (19 19.0%). train avg loss: 0.3314, val avg loss: 0.8666\n",
      "Training for epoch 20 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 20 finished in 15m 22.24s (- 61m 28.94s) (20 20.0%). train avg loss: 0.3417, val avg loss: 0.8261\n",
      "Training for epoch 21 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 21 finished in 16m 8.63s (- 60m 43.89s) (21 21.0%). train avg loss: 0.3078, val avg loss: 0.7628\n",
      "Training for epoch 22 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 22 finished in 16m 54.23s (- 59m 55.91s) (22 22.0%). train avg loss: 0.3352, val avg loss: 0.8282\n",
      "Training for epoch 23 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 23 finished in 17m 39.46s (- 59m 6.89s) (23 23.0%). train avg loss: 0.3013, val avg loss: 0.767\n",
      "Training for epoch 24 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 24 finished in 18m 25.54s (- 58m 20.86s) (24 24.0%). train avg loss: 0.3029, val avg loss: 0.7451\n",
      "Training for epoch 25 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 25 finished in 19m 10.2s (- 57m 30.61s) (25 25.0%). train avg loss: 0.2973, val avg loss: 0.8276\n",
      "Training for epoch 26 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 26 finished in 19m 55.54s (- 56m 42.68s) (26 26.0%). train avg loss: 0.3018, val avg loss: 0.703\n",
      "Training for epoch 27 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 27 finished in 20m 40.82s (- 55m 54.81s) (27 27.0%). train avg loss: 0.2612, val avg loss: 0.7398\n",
      "Training for epoch 28 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 28 finished in 21m 25.97s (- 55m 6.78s) (28 28.0%). train avg loss: 0.2871, val avg loss: 0.7085\n",
      "Training for epoch 29 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 29 finished in 22m 10.5s (- 54m 17.44s) (29 29.0%). train avg loss: 0.2991, val avg loss: 0.7777\n",
      "Training for epoch 30 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 30 finished in 22m 54.86s (- 53m 28.0s) (30 30.0%). train avg loss: 0.2945, val avg loss: 0.7677\n",
      "Training for epoch 31 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 31 finished in 23m 39.1s (- 52m 38.64s) (31 31.0%). train avg loss: 0.2908, val avg loss: 0.7094\n",
      "Training for epoch 32 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 32 finished in 24m 23.69s (- 51m 50.35s) (32 32.0%). train avg loss: 0.2962, val avg loss: 0.7918\n",
      "Training for epoch 33 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 33 finished in 25m 9.04s (- 51m 3.81s) (33 33.0%). train avg loss: 0.2883, val avg loss: 0.7271\n",
      "Training for epoch 34 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 34 finished in 25m 54.09s (- 50m 16.75s) (34 34.0%). train avg loss: 0.2708, val avg loss: 0.7085\n",
      "Training for epoch 35 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 35 finished in 26m 40.0s (- 49m 31.44s) (35 35.0%). train avg loss: 0.2633, val avg loss: 0.7041\n",
      "Training for epoch 36 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 36 finished in 27m 25.8s (- 48m 45.86s) (36 36.0%). train avg loss: 0.2526, val avg loss: 0.7024\n",
      "Training for epoch 37 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 37 finished in 28m 10.57s (- 47m 58.54s) (37 37.0%). train avg loss: 0.2746, val avg loss: 0.769\n",
      "Training for epoch 38 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 38 finished in 28m 56.37s (- 47m 13.02s) (38 38.0%). train avg loss: 0.2929, val avg loss: 0.7217\n",
      "Training for epoch 39 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 39 finished in 29m 42.14s (- 46m 27.44s) (39 39.0%). train avg loss: 0.2633, val avg loss: 0.7466\n",
      "Training for epoch 40 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 40 finished in 30m 29.03s (- 45m 43.54s) (40 40.0%). train avg loss: 0.266, val avg loss: 0.6998\n",
      "Training for epoch 41 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 41 finished in 31m 21.56s (- 45m 7.62s) (41 41.0%). train avg loss: 0.3152, val avg loss: 0.8119\n",
      "Training for epoch 42 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 42 finished in 32m 13.99s (- 44m 30.75s) (42 42.0%). train avg loss: 0.2864, val avg loss: 0.8385\n",
      "Training for epoch 43 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 43 finished in 32m 59.37s (- 43m 43.81s) (43 43.0%). train avg loss: 0.285, val avg loss: 0.7416\n",
      "Training for epoch 44 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 44 finished in 33m 48.5s (- 43m 1.73s) (44 44.0%). train avg loss: 0.2415, val avg loss: 0.701\n",
      "Training for epoch 45 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 45 finished in 34m 36.41s (- 42m 17.83s) (45 45.0%). train avg loss: 0.2503, val avg loss: 0.7565\n",
      "Training for epoch 46 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 46 finished in 35m 21.56s (- 41m 30.53s) (46 46.0%). train avg loss: 0.2557, val avg loss: 0.6816\n",
      "Training for epoch 47 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 47 finished in 36m 8.54s (- 40m 45.37s) (47 47.0%). train avg loss: 0.2371, val avg loss: 0.6722\n",
      "Training for epoch 48 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 48 finished in 36m 55.69s (- 40m 0.33s) (48 48.0%). train avg loss: 0.2453, val avg loss: 0.684\n",
      "Training for epoch 49 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 49 finished in 37m 42.77s (- 39m 15.13s) (49 49.0%). train avg loss: 0.2557, val avg loss: 0.8311\n",
      "Training for epoch 50 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 50 finished in 38m 29.73s (- 38m 29.73s) (50 50.0%). train avg loss: 0.275, val avg loss: 0.7239\n",
      "Training for epoch 51 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 51 finished in 39m 16.48s (- 37m 44.06s) (51 51.0%). train avg loss: 0.2707, val avg loss: 0.7053\n",
      "Training for epoch 52 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 52 finished in 40m 2.9s (- 36m 58.07s) (52 52.0%). train avg loss: 0.2771, val avg loss: 0.727\n",
      "Training for epoch 53 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 53 finished in 40m 49.48s (- 36m 12.18s) (53 53.0%). train avg loss: 0.2447, val avg loss: 0.7429\n",
      "Training for epoch 54 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 54 finished in 41m 36.59s (- 35m 26.73s) (54 54.0%). train avg loss: 0.2412, val avg loss: 0.6534\n",
      "Training for epoch 55 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 55 finished in 42m 23.94s (- 34m 41.4s) (55 55.0%). train avg loss: 0.2353, val avg loss: 0.6561\n",
      "Training for epoch 56 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 56 finished in 43m 11.1s (- 33m 55.86s) (56 56.0%). train avg loss: 0.2615, val avg loss: 0.6863\n",
      "Training for epoch 57 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 57 finished in 43m 57.4s (- 33m 9.62s) (57 57.0%). train avg loss: 0.2276, val avg loss: 0.6449\n",
      "Training for epoch 58 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 58 finished in 44m 43.76s (- 32m 23.41s) (58 58.0%). train avg loss: 0.2384, val avg loss: 0.6652\n",
      "Training for epoch 59 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 59 finished in 45m 30.4s (- 31m 37.39s) (59 59.0%). train avg loss: 0.2442, val avg loss: 0.637\n",
      "Training for epoch 60 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 60 finished in 46m 17.79s (- 30m 51.86s) (60 60.0%). train avg loss: 0.244, val avg loss: 0.6319\n",
      "Training for epoch 61 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 61 finished in 47m 4.97s (- 30m 6.13s) (61 61.0%). train avg loss: 0.2354, val avg loss: 0.6586\n",
      "Training for epoch 62 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 62 finished in 47m 50.93s (- 29m 19.6s) (62 62.0%). train avg loss: 0.2281, val avg loss: 0.6259\n",
      "Training for epoch 63 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 63 finished in 48m 37.53s (- 28m 33.47s) (63 63.0%). train avg loss: 0.2216, val avg loss: 0.6437\n",
      "Training for epoch 64 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 64 finished in 49m 28.41s (- 27m 49.73s) (64 64.0%). train avg loss: 0.2269, val avg loss: 0.6906\n",
      "Training for epoch 65 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 65 finished in 50m 22.75s (- 27m 7.64s) (65 65.0%). train avg loss: 0.2154, val avg loss: 0.664\n",
      "Training for epoch 66 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 66 finished in 51m 15.23s (- 26m 24.21s) (66 66.0%). train avg loss: 0.2397, val avg loss: 0.6617\n",
      "Training for epoch 67 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 67 finished in 52m 7.12s (- 25m 40.22s) (67 67.0%). train avg loss: 0.2265, val avg loss: 0.6733\n",
      "Training for epoch 68 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 68 finished in 53m 0.54s (- 24m 56.72s) (68 68.0%). train avg loss: 0.2392, val avg loss: 0.6366\n",
      "Training for epoch 69 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 69 finished in 53m 45.85s (- 24m 9.29s) (69 69.0%). train avg loss: 0.2297, val avg loss: 0.6537\n",
      "Training for epoch 70 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 70 finished in 54m 31.1s (- 23m 21.9s) (70 70.0%). train avg loss: 0.2215, val avg loss: 0.5974\n",
      "Training for epoch 71 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 71 finished in 55m 16.14s (- 22m 34.48s) (71 71.0%). train avg loss: 0.202, val avg loss: 0.5867\n",
      "Training for epoch 72 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 72 finished in 56m 2.01s (- 21m 47.45s) (72 72.0%). train avg loss: 0.2052, val avg loss: 0.597\n",
      "Training for epoch 73 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 73 finished in 56m 47.23s (- 21m 0.21s) (73 73.0%). train avg loss: 0.2166, val avg loss: 0.6239\n",
      "Training for epoch 74 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 74 finished in 57m 31.25s (- 20m 12.6s) (74 74.0%). train avg loss: 0.1987, val avg loss: 0.5791\n",
      "Training for epoch 75 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 75 finished in 58m 15.17s (- 19m 25.06s) (75 75.0%). train avg loss: 0.229, val avg loss: 0.6045\n",
      "Training for epoch 76 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 76 finished in 59m 0.21s (- 18m 37.96s) (76 76.0%). train avg loss: 0.2199, val avg loss: 0.6743\n",
      "Training for epoch 77 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 77 finished in 59m 44.96s (- 17m 50.83s) (77 77.0%). train avg loss: 0.2017, val avg loss: 0.6016\n",
      "Training for epoch 78 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 78 finished in 60m 29.61s (- 17m 3.74s) (78 78.0%). train avg loss: 0.1957, val avg loss: 0.6582\n",
      "Training for epoch 79 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 79 finished in 61m 15.32s (- 16m 16.98s) (79 79.0%). train avg loss: 0.2064, val avg loss: 0.6381\n",
      "Training for epoch 80 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 80 finished in 62m 0.44s (- 15m 30.11s) (80 80.0%). train avg loss: 0.1947, val avg loss: 0.5623\n",
      "Training for epoch 81 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 81 finished in 62m 44.79s (- 14m 43.1s) (81 81.0%). train avg loss: 0.2027, val avg loss: 0.6114\n",
      "Training for epoch 82 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 82 finished in 63m 28.35s (- 13m 55.98s) (82 82.0%). train avg loss: 0.2011, val avg loss: 0.6095\n",
      "Training for epoch 83 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 83 finished in 64m 12.43s (- 13m 9.05s) (83 83.0%). train avg loss: 0.1994, val avg loss: 0.6046\n",
      "Training for epoch 84 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 84 finished in 64m 56.81s (- 12m 22.25s) (84 84.0%). train avg loss: 0.1948, val avg loss: 0.5709\n",
      "Training for epoch 85 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 85 finished in 65m 42.25s (- 11m 35.69s) (85 85.0%). train avg loss: 0.1973, val avg loss: 0.6533\n",
      "Training for epoch 86 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 86 finished in 66m 26.96s (- 10m 49.04s) (86 86.0%). train avg loss: 0.1957, val avg loss: 0.6141\n",
      "Training for epoch 87 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 87 finished in 67m 10.97s (- 10m 2.33s) (87 87.0%). train avg loss: 0.1878, val avg loss: 0.6235\n",
      "Training for epoch 88 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 88 finished in 67m 55.32s (- 9m 15.73s) (88 88.0%). train avg loss: 0.2013, val avg loss: 0.6051\n",
      "Training for epoch 89 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 89 finished in 68m 39.74s (- 8m 29.18s) (89 89.0%). train avg loss: 0.1964, val avg loss: 0.5625\n",
      "Training for epoch 90 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 90 finished in 69m 27.5s (- 7m 43.06s) (90 90.0%). train avg loss: 0.1838, val avg loss: 0.5787\n",
      "Training for epoch 91 has started (lr=0.001). Found 1922 batch(es).\n",
      "Epoch 91 finished in 70m 18.1s (- 6m 57.17s) (91 91.0%). train avg loss: 0.1862, val avg loss: 0.5775\n",
      "Training for epoch 92 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 92 finished in 71m 2.46s (- 6m 10.65s) (92 92.0%). train avg loss: 0.1676, val avg loss: 0.5654\n",
      "Training for epoch 93 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 93 finished in 71m 46.38s (- 5m 24.14s) (93 93.0%). train avg loss: 0.1516, val avg loss: 0.533\n",
      "Training for epoch 94 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 94 finished in 72m 30.41s (- 4m 37.69s) (94 94.0%). train avg loss: 0.149, val avg loss: 0.5292\n",
      "Training for epoch 95 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 95 finished in 73m 15.24s (- 3m 51.33s) (95 95.0%). train avg loss: 0.1432, val avg loss: 0.523\n",
      "Training for epoch 96 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 96 finished in 73m 59.64s (- 3m 4.99s) (96 96.0%). train avg loss: 0.1467, val avg loss: 0.5279\n",
      "Training for epoch 97 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 97 finished in 74m 44.21s (- 2m 18.69s) (97 97.0%). train avg loss: 0.143, val avg loss: 0.5072\n",
      "Training for epoch 98 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 98 finished in 75m 29.07s (- 1m 32.43s) (98 98.0%). train avg loss: 0.1381, val avg loss: 0.5406\n",
      "Training for epoch 99 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 99 finished in 76m 13.61s (- 0m 46.2s) (99 99.0%). train avg loss: 0.1369, val avg loss: 0.5087\n",
      "Training for epoch 100 has started (lr=0.0005). Found 1922 batch(es).\n",
      "Epoch 100 finished in 76m 57.84s (- 0m 0.0s) (100 100.0%). train avg loss: 0.1376, val avg loss: 0.5179\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5, min_value-.075, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 15.087769780208045%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> decennial\n",
      "= ['D', 'AX', 'S', 'EH', 'N', 'IY', 'AX', 'L']\n",
      "< D AX S EH N IY AX L ['D', 'AX', 'S', 'EH', 'N', 'IY', 'AX', 'L']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc90e63e9d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 426.667x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
