{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/exp/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"unigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"64\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"128\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 10, 19, 12, 26, 19, 8, 14, 19, 30, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f1825435340> ([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "([4, 8, 6, 26, 24, 10, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "valid grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "test grp 32 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'\", 5: '-', 6: 'a', 7: 'b', 8: 'c', 9: 'd', 10: 'e', 11: 'f', 12: 'g', 13: 'h', 14: 'i', 15: 'j', 16: 'k', 17: 'l', 18: 'm', 19: 'n', 20: 'o', 21: 'p', 22: 'q', 23: 'r', 24: 's', 25: 't', 26: 'u', 27: 'v', 28: 'w', 29: 'x', 30: 'y', 31: 'z'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "28 {\"'\": 4, 'c': 8, 'a': 6, 'u': 26, 's': 24, 'e': 10, 'o': 20, 'r': 23, 'm': 18, 'n': 19, 'q': 22, 't': 25, 'i': 14, 'l': 17, 'w': 28, '-': 5, 'y': 30, 'd': 9, 'b': 7, 'p': 21, 'g': 12, 'k': 16, 'h': 13, 'v': 27, 'z': 31, 'f': 11, 'x': 29, 'j': 15}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 64\n",
      "hidden_size: 128\n",
      "n_layers: 1\n",
      "Encoder has a total number of 76544 parameters\n",
      "Decoder has a total number of 135204 parameters\n",
      "Total number of all parameters is 211748\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 42.44s (- 70m 1.82s) (1 1.0%). train avg loss: 1.0452, val avg loss: 1.3512\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 23.52s (- 68m 12.29s) (2 2.0%). train avg loss: 0.5172, val avg loss: 1.0111\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 3.9s (- 66m 46.09s) (3 3.0%). train avg loss: 0.4681, val avg loss: 1.0832\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 2m 44.71s (- 65m 53.13s) (4 4.0%). train avg loss: 0.4602, val avg loss: 0.9366\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 3m 25.53s (- 65m 5.12s) (5 5.0%). train avg loss: 0.3961, val avg loss: 0.889\n",
      "Training for epoch 6 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 6 finished in 4m 6.11s (- 64m 15.74s) (6 6.0%). train avg loss: 0.4099, val avg loss: 0.8827\n",
      "Training for epoch 7 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 7 finished in 4m 47.56s (- 63m 40.49s) (7 7.0%). train avg loss: 0.3695, val avg loss: 1.0151\n",
      "Training for epoch 8 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 8 finished in 5m 28.62s (- 62m 59.16s) (8 8.0%). train avg loss: 0.4108, val avg loss: 0.9132\n",
      "Training for epoch 9 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 9 finished in 6m 9.45s (- 62m 15.55s) (9 9.0%). train avg loss: 0.3408, val avg loss: 0.8755\n",
      "Training for epoch 10 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 10 finished in 6m 58.29s (- 62m 44.62s) (10 10.0%). train avg loss: 0.3596, val avg loss: 0.8612\n",
      "Training for epoch 11 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 11 finished in 7m 42.31s (- 62m 20.51s) (11 11.0%). train avg loss: 0.3327, val avg loss: 0.8593\n",
      "Training for epoch 12 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 12 finished in 8m 25.23s (- 61m 45.0s) (12 12.0%). train avg loss: 0.3331, val avg loss: 0.9052\n",
      "Training for epoch 13 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 13 finished in 9m 8.25s (- 61m 9.05s) (13 13.0%). train avg loss: 0.3268, val avg loss: 0.8816\n",
      "Training for epoch 14 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 14 finished in 9m 51.13s (- 60m 31.2s) (14 14.0%). train avg loss: 0.3341, val avg loss: 0.859\n",
      "Training for epoch 15 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 15 finished in 10m 34.75s (- 59m 56.89s) (15 15.0%). train avg loss: 0.3188, val avg loss: 0.8063\n",
      "Training for epoch 16 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 16 finished in 11m 18.4s (- 59m 21.61s) (16 16.0%). train avg loss: 0.3085, val avg loss: 0.8056\n",
      "Training for epoch 17 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 17 finished in 12m 1.72s (- 58m 43.7s) (17 17.0%). train avg loss: 0.3026, val avg loss: 0.8272\n",
      "Training for epoch 18 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 18 finished in 12m 44.83s (- 58m 4.24s) (18 18.0%). train avg loss: 0.2896, val avg loss: 0.7836\n",
      "Training for epoch 19 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 19 finished in 13m 27.72s (- 57m 23.45s) (19 19.0%). train avg loss: 0.2967, val avg loss: 0.7968\n",
      "Training for epoch 20 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 20 finished in 14m 11.17s (- 56m 44.67s) (20 20.0%). train avg loss: 0.2723, val avg loss: 0.8041\n",
      "Training for epoch 21 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 21 finished in 14m 54.34s (- 56m 4.43s) (21 21.0%). train avg loss: 0.2713, val avg loss: 1.0081\n",
      "Training for epoch 22 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 22 finished in 15m 36.75s (- 55m 21.2s) (22 22.0%). train avg loss: 0.2665, val avg loss: 0.6923\n",
      "Training for epoch 23 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 23 finished in 16m 16.86s (- 54m 30.36s) (23 23.0%). train avg loss: 0.274, val avg loss: 0.8413\n",
      "Training for epoch 24 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 24 finished in 16m 57.13s (- 53m 40.91s) (24 24.0%). train avg loss: 0.2789, val avg loss: 0.8684\n",
      "Training for epoch 25 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 25 finished in 17m 37.05s (- 52m 51.14s) (25 25.0%). train avg loss: 0.2641, val avg loss: 0.7429\n",
      "Training for epoch 26 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 26 finished in 18m 16.67s (- 52m 1.28s) (26 26.0%). train avg loss: 0.2673, val avg loss: 0.7442\n",
      "Training for epoch 27 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 27 finished in 18m 56.72s (- 51m 13.34s) (27 27.0%). train avg loss: 0.2941, val avg loss: 1.7311\n",
      "Training for epoch 28 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 28 finished in 19m 36.93s (- 50m 26.38s) (28 28.0%). train avg loss: 0.3043, val avg loss: 0.7366\n",
      "Training for epoch 29 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 29 finished in 20m 16.76s (- 49m 38.97s) (29 29.0%). train avg loss: 0.2529, val avg loss: 0.7685\n",
      "Training for epoch 30 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 30 finished in 20m 57.25s (- 48m 53.59s) (30 30.0%). train avg loss: 0.249, val avg loss: 0.7094\n",
      "Training for epoch 31 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 31 finished in 21m 37.65s (- 48m 8.32s) (31 31.0%). train avg loss: 0.2549, val avg loss: 0.7262\n",
      "Training for epoch 32 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 32 finished in 22m 16.79s (- 47m 20.68s) (32 32.0%). train avg loss: 0.2493, val avg loss: 0.7486\n",
      "Training for epoch 33 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 33 finished in 22m 56.07s (- 46m 33.83s) (33 33.0%). train avg loss: 0.2392, val avg loss: 0.703\n",
      "Training for epoch 34 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 34 finished in 23m 35.4s (- 45m 47.54s) (34 34.0%). train avg loss: 0.2093, val avg loss: 0.6791\n",
      "Training for epoch 35 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 35 finished in 24m 15.08s (- 45m 2.29s) (35 35.0%). train avg loss: 0.2014, val avg loss: 0.6207\n",
      "Training for epoch 36 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 36 finished in 24m 54.62s (- 44m 17.1s) (36 36.0%). train avg loss: 0.1937, val avg loss: 0.6171\n",
      "Training for epoch 37 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 37 finished in 25m 33.92s (- 43m 31.82s) (37 37.0%). train avg loss: 0.1883, val avg loss: 0.6363\n",
      "Training for epoch 38 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 38 finished in 26m 13.7s (- 42m 47.62s) (38 38.0%). train avg loss: 0.1943, val avg loss: 0.6317\n",
      "Training for epoch 39 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 39 finished in 26m 53.15s (- 42m 3.14s) (39 39.0%). train avg loss: 0.1881, val avg loss: 0.6513\n",
      "Training for epoch 40 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 40 finished in 27m 32.96s (- 41m 19.43s) (40 40.0%). train avg loss: 0.1814, val avg loss: 0.6469\n",
      "Training for epoch 41 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 41 finished in 28m 14.57s (- 40m 38.52s) (41 41.0%). train avg loss: 0.1861, val avg loss: 0.6316\n",
      "Training for epoch 42 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 42 finished in 28m 54.53s (- 39m 55.31s) (42 42.0%). train avg loss: 0.1755, val avg loss: 0.6468\n",
      "Training for epoch 43 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 43 finished in 29m 41.02s (- 39m 20.89s) (43 43.0%). train avg loss: 0.1769, val avg loss: 0.6362\n",
      "Training for epoch 44 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 44 finished in 30m 24.35s (- 38m 41.91s) (44 44.0%). train avg loss: 0.1748, val avg loss: 0.623\n",
      "Training for epoch 45 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 45 finished in 31m 8.43s (- 38m 3.64s) (45 45.0%). train avg loss: 0.1684, val avg loss: 0.6155\n",
      "Training for epoch 46 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 46 finished in 31m 58.23s (- 37m 31.84s) (46 46.0%). train avg loss: 0.1683, val avg loss: 0.6221\n",
      "Training for epoch 47 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 47 finished in 32m 42.92s (- 36m 53.51s) (47 47.0%). train avg loss: 0.1689, val avg loss: 0.6188\n",
      "Training for epoch 48 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 48 finished in 33m 27.63s (- 36m 14.93s) (48 48.0%). train avg loss: 0.1626, val avg loss: 0.6248\n",
      "Training for epoch 49 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 49 finished in 34m 14.86s (- 35m 38.74s) (49 49.0%). train avg loss: 0.1718, val avg loss: 0.6125\n",
      "Training for epoch 50 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 50 finished in 35m 3.46s (- 35m 3.46s) (50 50.0%). train avg loss: 0.1639, val avg loss: 0.6225\n",
      "Training for epoch 51 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 51 finished in 35m 49.77s (- 34m 25.47s) (51 51.0%). train avg loss: 0.1656, val avg loss: 0.6176\n",
      "Training for epoch 52 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 52 finished in 36m 32.19s (- 33m 43.56s) (52 52.0%). train avg loss: 0.1587, val avg loss: 0.6489\n",
      "Training for epoch 53 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 53 finished in 37m 14.26s (- 33m 1.33s) (53 53.0%). train avg loss: 0.1762, val avg loss: 0.5921\n",
      "Training for epoch 54 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 54 finished in 37m 59.84s (- 32m 22.09s) (54 54.0%). train avg loss: 0.1576, val avg loss: 0.6486\n",
      "Training for epoch 55 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 55 finished in 38m 44.32s (- 31m 41.72s) (55 55.0%). train avg loss: 0.1556, val avg loss: 0.6218\n",
      "Training for epoch 56 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 56 finished in 39m 27.36s (- 31m 0.07s) (56 56.0%). train avg loss: 0.1599, val avg loss: 0.5862\n",
      "Training for epoch 57 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 57 finished in 40m 13.11s (- 30m 20.42s) (57 57.0%). train avg loss: 0.154, val avg loss: 0.5997\n",
      "Training for epoch 58 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 58 finished in 40m 57.08s (- 29m 39.26s) (58 58.0%). train avg loss: 0.1528, val avg loss: 0.6446\n",
      "Training for epoch 59 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 59 finished in 41m 40.24s (- 28m 57.45s) (59 59.0%). train avg loss: 0.1531, val avg loss: 0.619\n",
      "Training for epoch 60 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 60 finished in 42m 26.43s (- 28m 17.62s) (60 60.0%). train avg loss: 0.1513, val avg loss: 0.626\n",
      "Training for epoch 61 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 61 finished in 43m 11.14s (- 27m 36.63s) (61 61.0%). train avg loss: 0.1676, val avg loss: 0.6804\n",
      "Training for epoch 62 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 62 finished in 43m 54.25s (- 26m 54.54s) (62 62.0%). train avg loss: 0.1607, val avg loss: 0.6163\n",
      "Training for epoch 63 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 63 finished in 44m 42.92s (- 26m 15.68s) (63 63.0%). train avg loss: 0.1457, val avg loss: 0.6201\n",
      "Training for epoch 64 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 64 finished in 45m 28.57s (- 25m 34.82s) (64 64.0%). train avg loss: 0.1491, val avg loss: 0.6199\n",
      "Training for epoch 65 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 65 finished in 46m 15.93s (- 24m 54.73s) (65 65.0%). train avg loss: 0.1474, val avg loss: 0.6262\n",
      "Training for epoch 66 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 66 finished in 47m 3.05s (- 24m 14.3s) (66 66.0%). train avg loss: 0.1459, val avg loss: 0.6262\n",
      "Training for epoch 67 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 67 finished in 47m 51.59s (- 23m 34.36s) (67 67.0%). train avg loss: 0.1467, val avg loss: 0.6073\n",
      "Training for epoch 68 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 68 finished in 48m 35.73s (- 22m 52.11s) (68 68.0%). train avg loss: 0.1377, val avg loss: 0.5907\n",
      "Training for epoch 69 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 69 finished in 49m 21.56s (- 22m 10.56s) (69 69.0%). train avg loss: 0.1292, val avg loss: 0.5822\n",
      "Training for epoch 70 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 70 finished in 50m 10.82s (- 21m 30.35s) (70 70.0%). train avg loss: 0.1294, val avg loss: 0.5874\n",
      "Training for epoch 71 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 71 finished in 50m 56.24s (- 20m 48.32s) (71 71.0%). train avg loss: 0.1287, val avg loss: 0.5933\n",
      "Training for epoch 72 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 72 finished in 51m 45.42s (- 20m 7.66s) (72 72.0%). train avg loss: 0.1269, val avg loss: 0.5531\n",
      "Training for epoch 73 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 73 finished in 52m 31.89s (- 19m 25.77s) (73 73.0%). train avg loss: 0.1265, val avg loss: 0.5884\n",
      "Training for epoch 74 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 74 finished in 53m 13.91s (- 18m 42.18s) (74 74.0%). train avg loss: 0.1249, val avg loss: 0.5686\n",
      "Training for epoch 75 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 75 finished in 53m 57.82s (- 17m 59.27s) (75 75.0%). train avg loss: 0.1241, val avg loss: 0.5747\n",
      "Training for epoch 76 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 76 finished in 54m 42.9s (- 17m 16.71s) (76 76.0%). train avg loss: 0.1266, val avg loss: 0.5907\n",
      "Training for epoch 77 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 77 finished in 55m 24.8s (- 16m 33.12s) (77 77.0%). train avg loss: 0.1215, val avg loss: 0.5839\n",
      "Training for epoch 78 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 78 finished in 56m 7.31s (- 15m 49.75s) (78 78.0%). train avg loss: 0.1239, val avg loss: 0.577\n",
      "Training for epoch 79 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 79 finished in 56m 48.5s (- 15m 6.06s) (79 79.0%). train avg loss: 0.1239, val avg loss: 0.5891\n",
      "Training for epoch 80 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 80 finished in 57m 29.74s (- 14m 22.44s) (80 80.0%). train avg loss: 0.1195, val avg loss: 0.5816\n",
      "Training for epoch 81 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 81 finished in 58m 12.27s (- 13m 39.17s) (81 81.0%). train avg loss: 0.1212, val avg loss: 0.5841\n",
      "Training for epoch 82 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 82 finished in 58m 53.75s (- 12m 55.7s) (82 82.0%). train avg loss: 0.1191, val avg loss: 0.6115\n",
      "Training for epoch 83 has started (lr=0.00025). Found 1916 batch(es).\n",
      "Epoch 83 finished in 59m 38.67s (- 12m 12.98s) (83 83.0%). train avg loss: 0.1214, val avg loss: 0.6076\n",
      "Training for epoch 84 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 84 finished in 60m 27.39s (- 11m 30.93s) (84 84.0%). train avg loss: 0.114, val avg loss: 0.5881\n",
      "Training for epoch 85 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 85 finished in 61m 15.76s (- 10m 48.66s) (85 85.0%). train avg loss: 0.1136, val avg loss: 0.5832\n",
      "Training for epoch 86 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 86 finished in 62m 5.46s (- 10m 6.47s) (86 86.0%). train avg loss: 0.11, val avg loss: 0.6017\n",
      "Training for epoch 87 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 87 finished in 62m 46.84s (- 9m 22.86s) (87 87.0%). train avg loss: 0.1099, val avg loss: 0.5769\n",
      "Training for epoch 88 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 88 finished in 63m 30.44s (- 8m 39.6s) (88 88.0%). train avg loss: 0.1088, val avg loss: 0.6274\n",
      "Training for epoch 89 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 89 finished in 64m 10.57s (- 7m 55.91s) (89 89.0%). train avg loss: 0.11, val avg loss: 0.5885\n",
      "Training for epoch 90 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 90 finished in 64m 50.82s (- 7m 12.31s) (90 90.0%). train avg loss: 0.1085, val avg loss: 0.601\n",
      "Training for epoch 91 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 91 finished in 65m 32.85s (- 6m 28.96s) (91 91.0%). train avg loss: 0.1089, val avg loss: 0.5999\n",
      "Training for epoch 92 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 92 finished in 66m 13.83s (- 5m 45.55s) (92 92.0%). train avg loss: 0.1089, val avg loss: 0.6072\n",
      "Training for epoch 93 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 93 finished in 66m 56.08s (- 5m 2.29s) (93 93.0%). train avg loss: 0.1067, val avg loss: 0.6128\n",
      "Training for epoch 94 has started (lr=0.000125). Found 1916 batch(es).\n",
      "Epoch 94 finished in 67m 39.65s (- 4m 19.13s) (94 94.0%). train avg loss: 0.1091, val avg loss: 0.6088\n",
      "Training for epoch 95 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 95 finished in 68m 21.69s (- 3m 35.88s) (95 95.0%). train avg loss: 0.1048, val avg loss: 0.5953\n",
      "Training for epoch 96 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 96 finished in 69m 3.16s (- 2m 52.63s) (96 96.0%). train avg loss: 0.1038, val avg loss: 0.5825\n",
      "Training for epoch 97 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 97 finished in 69m 45.31s (- 2m 9.44s) (97 97.0%). train avg loss: 0.1034, val avg loss: 0.5788\n",
      "Training for epoch 98 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 98 finished in 70m 27.97s (- 1m 26.29s) (98 98.0%). train avg loss: 0.1033, val avg loss: 0.5961\n",
      "Training for epoch 99 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 99 finished in 71m 7.45s (- 0m 43.11s) (99 99.0%). train avg loss: 0.1042, val avg loss: 0.6046\n",
      "Training for epoch 100 has started (lr=6.25e-05). Found 1916 batch(es).\n",
      "Epoch 100 finished in 71m 46.88s (- 0m 0.0s) (100 100.0%). train avg loss: 0.1027, val avg loss: 0.5805\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5, min_value-.125, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 22.579850100640154%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> broder's\n",
      "= ['B', 'R', 'AO', 'W', 'D', 'AX', 'R', 'Z']\n",
      "< B R AO W D AX R Z ['B', 'R', 'AO', 'W', 'D', 'AX', 'R', 'Z']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f189f2e5490>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAGkCAYAAADHdBU4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAaAUlEQVR4nO3dfXBUhf3v8c+SkAUxWQEJJpcFUkRBAkgTagOo+JTfpMjF2ylVR2mUOtO0QcGMHY32gdrC4h91tLWmDWVSuAzCdCpI7wgYWgkySE0iqRQdwMKQqNAMjOyG2N9iknP/6HW9Sx5P2M3JV9+vmTN1d3Z7PqMzb46buMfnOI4jAIBJQ7weAADoPyIOAIYRcQAwjIgDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcAwIg4AhpmP+IsvvqicnBwNGzZMeXl5euONN7yeFGfv3r1auHChsrOz5fP5tG3bNq8nxQmFQpo9e7bS09OVmZmpu+66S0eOHPF6VicVFRWaMWOGMjIylJGRoYKCAu3YscPrWT0KhULy+XxasWKF11PirFy5Uj6fL+646qqrvJ7VyYcffqj7779fo0eP1mWXXabrr79e9fX1Xs+KmThxYqe/jz6fT6WlpQO6w3TEt2zZohUrVuipp57SwYMHdeONN6qoqEiNjY1eT4tpbW3VzJkz9cILL3g9pUs1NTUqLS3VgQMHVF1drba2NhUWFqq1tdXraXHGjRunNWvWqK6uTnV1dbr11lu1aNEiHT582OtpXaqtrVVlZaVmzJjh9ZQuTZs2TadOnYodhw4d8npSnI8//lhz587V0KFDtWPHDr377rv65S9/qSuuuMLraTG1tbVxfw+rq6slSYsXLx7YIY5hX/va15ySkpK456ZMmeI88cQTHi3qmSRn69atXs/oUXNzsyPJqamp8XpKr0aOHOn8/ve/93pGJy0tLc7kyZOd6upq5+abb3aWL1/u9aQ4P/3pT52ZM2d6PaNHjz/+uDNv3jyvZ7iyfPlyZ9KkSU5HR8eAntfslfiFCxdUX1+vwsLCuOcLCwu1f/9+j1bZFw6HJUmjRo3yeEn32tvbtXnzZrW2tqqgoMDrOZ2UlpZqwYIFuv32272e0q1jx44pOztbOTk5uueee3T8+HGvJ8XZvn278vPztXjxYmVmZmrWrFlau3at17O6deHCBW3cuFFLly6Vz+cb0HObjfiZM2fU3t6usWPHxj0/duxYnT592qNVtjmOo7KyMs2bN0+5ublez+nk0KFDuvzyy+X3+1VSUqKtW7fquuuu83pWnM2bN+vtt99WKBTyekq3brjhBm3YsEG7du3S2rVrdfr0ac2ZM0dnz571elrM8ePHVVFRocmTJ2vXrl0qKSnRI488og0bNng9rUvbtm3TuXPn9MADDwz4uVMH/IwJdvGfeo7jDPifhF8Uy5Yt0zvvvKN9+/Z5PaVL1157rRoaGnTu3Dn96U9/UnFxsWpqagZNyJuamrR8+XK99tprGjZsmNdzulVUVBT76+nTp6ugoECTJk3S+vXrVVZW5uGyz3V0dCg/P1+rV6+WJM2aNUuHDx9WRUWFvvOd73i8rrN169apqKhI2dnZA35us1fiV155pVJSUjpddTc3N3e6OkfvHn74YW3fvl2vv/66xo0b5/WcLqWlpenqq69Wfn6+QqGQZs6cqeeff97rWTH19fVqbm5WXl6eUlNTlZqaqpqaGv3qV79Samqq2tvbvZ7YpREjRmj69Ok6duyY11NisrKyOv3hPHXq1EH1SwufOXnypHbv3q2HHnrIk/ObjXhaWpry8vJiPxH+THV1tebMmePRKnscx9GyZcv08ssv669//atycnK8ntRnjuMoGo16PSPmtttu06FDh9TQ0BA78vPzdd9996mhoUEpKSleT+xSNBrVe++9p6ysLK+nxMydO7fTr7oePXpUEyZM8GhR96qqqpSZmakFCxZ4cn7TH6eUlZVpyZIlys/PV0FBgSorK9XY2KiSkhKvp8WcP39e77//fuzxiRMn1NDQoFGjRmn8+PEeLvuP0tJSbdq0Sa+88orS09Nj/2YTCAQ0fPhwj9d97sknn1RRUZGCwaBaWlq0efNm7dmzRzt37vR6Wkx6enqnnyWMGDFCo0ePHlQ/Y3jssce0cOFCjR8/Xs3NzfrFL36hSCSi4uJir6fFPProo5ozZ45Wr16tb3/723rrrbdUWVmpyspKr6fF6ejoUFVVlYqLi5Wa6lFOB/R3YZLgN7/5jTNhwgQnLS3N+epXvzrofjXu9ddfdyR1OoqLi72e5jiO0+U2SU5VVZXX0+IsXbo09s95zJgxzm233ea89tprXs/q1WD8FcO7777bycrKcoYOHepkZ2c73/zmN53Dhw97PauTP//5z05ubq7j9/udKVOmOJWVlV5P6mTXrl2OJOfIkSOebfA5DjdKBgCrzH4mDgAg4gBgGhEHAMOIOAAYRsQBwDAiDgCGmY94NBrVypUrB9V/udcVCzvZmDgWdrIxcbzcaf73xCORiAKBgMLhsDIyMrye0y0LO9mYOBZ2sjFxvNxp/kocAL7MiDgAGDbg39jS0dGhjz76SOnp6Qn53u9IJBL3v4OVhZ1sTBwLO9mYOIne6TiOWlpalJ2drSFDer7WHvDPxD/44AMFg8GBPCUAmNTU1NTr9/sP+JV4enq6JOnG1LuU6hs60KfvM+fTC15PAPAl1aZPtU+vxnrZkwGP+GcfoaT6hg7uiPtM/9IOAMv+X3768pEzP9gEAMOIOAAYRsQBwDAiDgCGEXEAMIyIA4BhRBwADCPiAGAYEQcAw4g4ABhGxAHAMCIOAIYRcQAwjIgDgGH9iviLL76onJwcDRs2THl5eXrjjTcSvQsA0AeuI75lyxatWLFCTz31lA4ePKgbb7xRRUVFamxsTMY+AEAPXEf82Wef1Xe/+1099NBDmjp1qp577jkFg0FVVFQkYx8AoAeuIn7hwgXV19ersLAw7vnCwkLt37+/y/dEo1FFIpG4AwCQGK4ifubMGbW3t2vs2LFxz48dO1anT5/u8j2hUEiBQCB2cJNkAEicfv1g8+L7vjmO0+294MrLyxUOh2NHU1NTf04JAOiCqxslX3nllUpJSel01d3c3Nzp6vwzfr9ffr+//wsBAN1ydSWelpamvLw8VVdXxz1fXV2tOXPmJHQYAKB3rq7EJamsrExLlixRfn6+CgoKVFlZqcbGRpWUlCRjHwCgB64jfvfdd+vs2bN6+umnderUKeXm5urVV1/VhAkTkrEPANADn+M4zkCeMBKJKBAI6Jahi5XqGzqQp3bF+fSC1xMAfEm1OZ9qj15ROBxWRkZGj6/lu1MAwDAiDgCGEXEAMIyIA4BhRBwADCPiAGAYEQcAw4g4ABhGxAHAMCIOAIYRcQAwjIgDgGFEHAAMc/1VtIny8uF6ZaQP3j9D/ut/zPJ6Qt8M7JdQAhhkBm9FAQC9IuIAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCGEXEAMIyIA4BhriO+d+9eLVy4UNnZ2fL5fNq2bVsydgEA+sB1xFtbWzVz5ky98MILydgDAHDB9e3ZioqKVFRUlIwtAACXkn6PzWg0qmg0GnsciUSSfUoA+NJI+g82Q6GQAoFA7AgGg8k+JQB8aSQ94uXl5QqHw7Gjqakp2acEgC+NpH+c4vf75ff7k30aAPhS4vfEAcAw11fi58+f1/vvvx97fOLECTU0NGjUqFEaP358QscBAHrmOuJ1dXW65ZZbYo/LysokScXFxfrDH/6QsGEAgN65jvj8+fPlOE4ytgAAXOIzcQAwjIgDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYlvTbs3Xnf10zXam+oV6dvldH1+V5PaFP/B+keT2hVxN+ut/rCcAXFlfiAGAYEQcAw4g4ABhGxAHAMCIOAIYRcQAwjIgDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAxzFfFQKKTZs2crPT1dmZmZuuuuu3TkyJFkbQMA9MJVxGtqalRaWqoDBw6ourpabW1tKiwsVGtra7L2AQB64Or2bDt37ox7XFVVpczMTNXX1+umm27q8j3RaFTRaDT2OBKJ9GMmAKArl/SZeDgcliSNGjWq29eEQiEFAoHYEQwGL+WUAID/T78j7jiOysrKNG/ePOXm5nb7uvLycoXD4djR1NTU31MCAC7S77vdL1u2TO+884727dvX4+v8fr/8fn9/TwMA6EG/Iv7www9r+/bt2rt3r8aNG5foTQCAPnIVccdx9PDDD2vr1q3as2ePcnJykrULANAHriJeWlqqTZs26ZVXXlF6erpOnz4tSQoEAho+fHhSBgIAuufqB5sVFRUKh8OaP3++srKyYseWLVuStQ8A0APXH6cAAAYPvjsFAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCGEXEAMIyIA4BhRBwADCPiAGAYEQcAw4g4ABhGxAHAsH7fnu2L7prv1nk9oU/+z4f1Xk/o1cKf3+D1hF45bW1eTwD6hStxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCGEXEAMIyIA4BhRBwADCPiAGAYEQcAw4g4ABhGxAHAMCIOAIa5inhFRYVmzJihjIwMZWRkqKCgQDt27EjWNgBAL1xFfNy4cVqzZo3q6upUV1enW2+9VYsWLdLhw4eTtQ8A0ANXt2dbuHBh3ONVq1apoqJCBw4c0LRp0xI6DADQu37fY7O9vV1//OMf1draqoKCgm5fF41GFY1GY48jkUh/TwkAuIjrH2weOnRIl19+ufx+v0pKSrR161Zdd9113b4+FAopEAjEjmAweEmDAQCfcx3xa6+9Vg0NDTpw4IC+//3vq7i4WO+++263ry8vL1c4HI4dTU1NlzQYAPA51x+npKWl6eqrr5Yk5efnq7a2Vs8//7x+97vfdfl6v98vv99/aSsBAF265N8Tdxwn7jNvAMDAcXUl/uSTT6qoqEjBYFAtLS3avHmz9uzZo507dyZrHwCgB64i/q9//UtLlizRqVOnFAgENGPGDO3cuVN33HFHsvYBAHrgKuLr1q1L1g4AQD/w3SkAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCG9ftGyRgc/uc1N3k9oVerjr3u9YRe/Xjm7V5P6JOOTz7xekKvnLY2ryd8qXAlDgCGEXEAMIyIA4BhRBwADCPiAGAYEQcAw4g4ABhGxAHAMCIOAIYRcQAwjIgDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcCwS4p4KBSSz+fTihUrErUHAOBCvyNeW1uryspKzZgxI5F7AAAu9Cvi58+f13333ae1a9dq5MiRid4EAOijfkW8tLRUCxYs0O23935fwmg0qkgkEncAABLD9Y2SN2/erLffflu1tbV9en0oFNLPfvYz18MAAL1zdSXe1NSk5cuXa+PGjRo2bFif3lNeXq5wOBw7mpqa+jUUANCZqyvx+vp6NTc3Ky8vL/Zce3u79u7dqxdeeEHRaFQpKSlx7/H7/fL7/YlZCwCI4yrit912mw4dOhT33IMPPqgpU6bo8ccf7xRwAEByuYp4enq6cnNz454bMWKERo8e3el5AEDy8V9sAoBhrn875WJ79uxJwAwAQH9wJQ4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCGEXEAMIyIA4BhRBwADCPiAGAYEQcAwy75q2jhrY7WVq8n9OrJnK95PaFX/7tph9cT+qR4SqHXE3rltLV5PaFvhgziO5E5HVJH317KlTgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCGEXEAMIyIA4BhRBwADCPiAGAYEQcAw1xFfOXKlfL5fHHHVVddlaxtAIBeuL6zz7Rp07R79+7Y45SUQXx3DAD4gnMd8dTUVFdX39FoVNFoNPY4Eom4PSUAoBuuPxM/duyYsrOzlZOTo3vuuUfHjx/v8fWhUEiBQCB2BIPBfo8FAMRzFfEbbrhBGzZs0K5du7R27VqdPn1ac+bM0dmzZ7t9T3l5ucLhcOxoamq65NEAgP9w9XFKUVFR7K+nT5+ugoICTZo0SevXr1dZWVmX7/H7/fL7/Ze2EgDQpUv6FcMRI0Zo+vTpOnbsWKL2AABcuKSIR6NRvffee8rKykrUHgCAC64i/thjj6mmpkYnTpzQ3/72N33rW99SJBJRcXFxsvYBAHrg6jPxDz74QPfee6/OnDmjMWPG6Otf/7oOHDigCRMmJGsfAKAHriK+efPmZO0AAPQD350CAIYRcQAwjIgDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgmOu73QNfREsm3OT1hD5Jrxnu9YRe/bt4jNcT+uTTsQGvJ3TLaftv6a2+vZYrcQAwjIgDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCGuY74hx9+qPvvv1+jR4/WZZddpuuvv1719fXJ2AYA6IWrO/t8/PHHmjt3rm655Rbt2LFDmZmZ+uc//6krrrgiWfsAAD1wFfFnnnlGwWBQVVVVsecmTpyY6E0AgD5y9XHK9u3blZ+fr8WLFyszM1OzZs3S2rVre3xPNBpVJBKJOwAAieEq4sePH1dFRYUmT56sXbt2qaSkRI888og2bNjQ7XtCoZACgUDsCAaDlzwaAPAfPsdxnL6+OC0tTfn5+dq/f3/suUceeUS1tbV68803u3xPNBpVNBqNPY5EIgoGg5qvRUr1Db2E6UACDUnxekGfpNeM9HpCr/5dfLnXE/pkMN/tvq3tv1Xz1iqFw2FlZGT0+FpXV+JZWVm67rrr4p6bOnWqGhsbu32P3+9XRkZG3AEASAxXEZ87d66OHDkS99zRo0c1YcKEhI4CAPSNq4g/+uijOnDggFavXq33339fmzZtUmVlpUpLS5O1DwDQA1cRnz17trZu3aqXXnpJubm5+vnPf67nnntO9913X7L2AQB64Or3xCXpzjvv1J133pmMLQAAl/juFAAwjIgDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMNcf4sh8IXU0e71gj4595PxXk/o1YXpNrLSdNfg/Wfe8e9U6a2+vZYrcQAwjIgDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCGuYr4xIkT5fP5Oh2lpaXJ2gcA6IGrW3DU1taqvf3zu2H84x//0B133KHFixcnfBgAoHeuIj5mzJi4x2vWrNGkSZN08803J3QUAKBv+n0zvAsXLmjjxo0qKyuTz+fr9nXRaFTRaDT2OBKJ9PeUAICL9PsHm9u2bdO5c+f0wAMP9Pi6UCikQCAQO4LBYH9PCQC4SL8jvm7dOhUVFSk7O7vH15WXlyscDseOpqam/p4SAHCRfn2ccvLkSe3evVsvv/xyr6/1+/3y+/39OQ0AoBf9uhKvqqpSZmamFixYkOg9AAAXXEe8o6NDVVVVKi4uVmpqv38uCgBIANcR3717txobG7V06dJk7AEAuOD6UrqwsFCO4yRjCwDAJb47BQAMI+IAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCGcVcHwJCU19/2ekKvhvt8Xk/okxMVB72e0K1IS4dG9vG1XIkDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAwj4gBgGBEHAMOIOAAYRsQBwDAiDgCGEXEAMMxVxNva2vSjH/1IOTk5Gj58uL7yla/o6aefVkdHR7L2AQB64OrOPs8884x++9vfav369Zo2bZrq6ur04IMPKhAIaPny5cnaCADohquIv/nmm1q0aJEWLFggSZo4caJeeukl1dXVdfueaDSqaDQaexyJRPo5FQBwMVcfp8ybN09/+ctfdPToUUnS3//+d+3bt0/f+MY3un1PKBRSIBCIHcFg8NIWAwBiXF2JP/744wqHw5oyZYpSUlLU3t6uVatW6d577+32PeXl5SorK4s9jkQihBwAEsRVxLds2aKNGzdq06ZNmjZtmhoaGrRixQplZ2eruLi4y/f4/X75/f6EjAUAxHMV8R/+8Id64okndM8990iSpk+frpMnTyoUCnUbcQBA8rj6TPyTTz7RkCHxb0lJSeFXDAHAI66uxBcuXKhVq1Zp/PjxmjZtmg4ePKhnn31WS5cuTdY+AEAPXEX817/+tX784x/rBz/4gZqbm5Wdna3vfe97+slPfpKsfQCAHvgcx3EG8oSRSESBQEDztUipvqEDeWoAA8Hn83pBn+z68KDXE7oVaenQyGuOKxwOKyMjo8fX8t0pAGAYEQcAw4g4ABhGxAHAMCIOAIYRcQAwjIgDgGFEHAAMI+IAYBgRBwDDiDgAGEbEAcAwV99imAiffd9Wmz6VBvSrtwAMDBtfgBVpGbz3QYic/8+2vnw/4YBHvKWlRZK0T68O9KkBDAQjF2cjr/F6Qe9aWloUCAR6fM2AfxVtR0eHPvroI6Wnp8uXgK+s/OzGy01NTb1+ZaOXLOxkY+JY2MnGxEn0Tsdx1NLSouzs7E53U7vYgF+JDxkyROPGjUv4/29GRsag/of8GQs72Zg4FnayMXESubO3K/DP8INNADCMiAOAYSkrV65c6fWIS5WSkqL58+crNXXAPx1yxcJONiaOhZ1sTByvdg74DzYBAInDxykAYBgRBwDDiDgAGEbEAcAwIg4AhhFxADCMiAOAYUQcAAz7v6tHmvhed7UuAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 426.667x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
