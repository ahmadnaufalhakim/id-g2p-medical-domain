{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec9kWAWVD9UU"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1740675351638,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "JT1nFx9SK5UF",
    "outputId": "a0a0765a-0875-4b33-ecc4-bd79a983e1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Hakims/college/S2/tesis/id-g2p-medical-domain/modules/prn-gen/exp/en_id\n"
     ]
    }
   ],
   "source": [
    "print(globals()[\"_dh\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5402,
     "status": "ok",
     "timestamp": 1740675357038,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "LdstRSwX86t4",
    "outputId": "a2a9c1c8-0899-4fd8-ba03-79f0c447b594"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8519,
     "status": "ok",
     "timestamp": 1740675365559,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FlovFUQYJj6-",
    "outputId": "7e8d72e5-7442-46de-cd60-a0b8d7a078e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jiwer in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/ahmadnaufalhakim/.local/lib/python3.8/site-packages (from jiwer) (3.9.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cudnn-cu12 (/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1740675365597,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "tIgQTZ7ZJsfT"
   },
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xGGFh-68xYx"
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1740675365637,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "R2DTSa3h8zgf"
   },
   "outputs": [],
   "source": [
    "GRP_TYPE = \"bigram\" # @param [\"unigram\", \"bigram\", \"trigram\"]\n",
    "ATTN_MODEL = \"dot\"\n",
    "EMB_DIM = \"256\" # @param [16, 32, 64, 128, 256, 512]\n",
    "HIDDEN_SIZE = \"128\" # @param [64, 128, 256, 512, 1024]\n",
    "N_LAYERS = \"1\" # @param [1, 2]\n",
    "DROPOUT_PROBA = .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiSP-GxlIvUG"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1740675365872,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "FfAkZ0ErIxOW",
    "outputId": "6b054b91-1e7f-4738-c254-2f9c73138c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "USE_CUDA = (DEVICE.type == \"cuda\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Data preprocessing\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "PAD_TOKEN = 2\n",
    "UNK_TOKEN = 3\n",
    "\n",
    "# Directories\n",
    "CURR_DIR = globals()[\"_dh\"][0]\n",
    "DATA_SOURCE_DIR = os.path.join(CURR_DIR, \"../../data/en_ma\")\n",
    "DATA_DIR = os.path.join(CURR_DIR, \"data\")\n",
    "if not os.path.exists(DATA_DIR) :\n",
    "  os.mkdir(DATA_DIR)\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"train_converted.csv\"), os.path.join(DATA_DIR, \"train.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"val_converted.csv\"), os.path.join(DATA_DIR, \"val.csv\"))\n",
    "shutil.copy(os.path.join(DATA_SOURCE_DIR, \"test_converted.csv\"), os.path.join(DATA_DIR, \"test.csv\"))\n",
    "MODELS_DIR = os.path.join(CURR_DIR, \"models\")\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "MODELS_DIR = os.path.join(MODELS_DIR, GRP_TYPE)\n",
    "if not os.path.exists(MODELS_DIR) :\n",
    "  os.mkdir(MODELS_DIR)\n",
    "\n",
    "# Dataset preparation\n",
    "PHONEME_REGEX_PATTERNS = {\n",
    "  'C': [\n",
    "    \"((tʃ)|(dʒ)|(ŋ)|(ɲ)|(sj))\",\n",
    "    \"((ʔ)|(b)|(d)|(f)|(g)|(h)|(k)|(l)|(m)|(n)|(p)|(r)|(s)|(t)|(v)|(w)|(j)|(z))\"\n",
    "  ],\n",
    "  'V': [\n",
    "    \"((ai)|(au)|(oi)|(ei))\",\n",
    "    \"(a|i|u|e|ə|o)\"\n",
    "  ]\n",
    "}\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = '|'.join(\n",
    "  pattern for patterns in PHONEME_REGEX_PATTERNS.values() for pattern in patterns\n",
    ")\n",
    "COMBINED_PHONEME_REGEX_PATTERNS = f\"(?:{COMBINED_PHONEME_REGEX_PATTERNS})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd1ak2wwJJem"
   },
   "source": [
    "## Preparing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFIgZGRJ9qLV"
   },
   "source": [
    "### `G2PDataset` torch dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1740675365908,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "qaOrjh2JJLLO"
   },
   "outputs": [],
   "source": [
    "class G2PDataset(Dataset) :\n",
    "  def __init__(self, graphemes_list, phonemes_list, lang_list) -> None :\n",
    "    assert len(graphemes_list) == len(phonemes_list) == len(lang_list)\n",
    "    # Handle graphemes\n",
    "    self.graphemes_list = graphemes_list\n",
    "    self.grapheme2index = {}\n",
    "    self.index2grapheme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\", 3:\"<UNK>\"}\n",
    "    self.n_graphemes = 4\n",
    "    for graphemes in graphemes_list :\n",
    "      for grapheme in graphemes :\n",
    "        self.add_grapheme(grapheme)\n",
    "    i = 4\n",
    "    for grapheme in sorted(self.grapheme2index) :\n",
    "      self.grapheme2index[grapheme] = i\n",
    "      self.index2grapheme[i] = grapheme\n",
    "      i += 1\n",
    "    # Handle phonemes\n",
    "    self.phonemes_list = phonemes_list\n",
    "    self.phoneme2index = {}\n",
    "    self.index2phoneme = {0: \"<SOS>\", 1: \"<EOS>\", 2:\"<PAD>\"}\n",
    "    self.n_phonemes = 3\n",
    "    for phonemes in phonemes_list :\n",
    "      for phoneme in phonemes :\n",
    "        self.add_phoneme(phoneme)\n",
    "    i = 3\n",
    "    for phoneme in sorted(self.phoneme2index) :\n",
    "      self.phoneme2index[phoneme] = i\n",
    "      self.index2phoneme[i] = phoneme\n",
    "      i += 1\n",
    "    # Handle lang\n",
    "    self.lang_list = lang_list\n",
    "\n",
    "  def add_grapheme(self, grapheme) :\n",
    "    if grapheme not in self.grapheme2index :\n",
    "      self.grapheme2index[grapheme] = self.n_graphemes\n",
    "      self.index2grapheme[self.n_graphemes] = grapheme\n",
    "      self.n_graphemes += 1\n",
    "\n",
    "  def add_phoneme(self, phoneme) :\n",
    "    if phoneme not in self.phoneme2index :\n",
    "      self.phoneme2index[phoneme] = self.n_phonemes\n",
    "      self.index2phoneme[self.n_phonemes] = phoneme\n",
    "      self.n_phonemes += 1\n",
    "\n",
    "  def __len__(self) :\n",
    "    return len(self.graphemes_list)\n",
    "\n",
    "  def __getitem__(self, index) -> str :\n",
    "    graphemes = [self.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in self.graphemes_list[index]] + [EOS_TOKEN]\n",
    "    phonemes = [self.phoneme2index[phoneme] for phoneme in self.phonemes_list[index]] + [EOS_TOKEN]\n",
    "    lang = self.lang_list[index]\n",
    "    return graphemes, phonemes, lang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8IB7SapA7MM"
   },
   "source": [
    "### Helper functions to prepare `train/val/test` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1740675365912,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vU52JF_pBru4"
   },
   "outputs": [],
   "source": [
    "def extract_graphemes(grapheme_syllable_sequence:str, grp_type:str) :\n",
    "  word = ''.join(grapheme_syllable_sequence.split('.'))\n",
    "  # Unigram\n",
    "  if grp_type == \"unigram\" :\n",
    "    return [*word]\n",
    "  # Bigram\n",
    "  elif grp_type == \"bigram\" :\n",
    "    if len(word) < 2 :\n",
    "      return [word]\n",
    "    return [word[i:i+2] for i in range(len(word)-1)]\n",
    "  # Trigram\n",
    "  elif grp_type == \"trigram\" :\n",
    "    if len(word) < 3 :\n",
    "      return [word]\n",
    "    return [word[i:i+3] for i in range(len(word)-2)]\n",
    "\n",
    "def extract_arpabet_phonemes(arpabet_phoneme_sequence:str) :\n",
    "  return arpabet_phoneme_sequence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675365919,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "59DHgoYQ-R6T"
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(split_name:str, grp_type:str) :\n",
    "  assert split_name in [\"train\", \"val\", \"test\"]\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  print(f\"Reading {split_name} entries ..\")\n",
    "  # Read the file and split into lines\n",
    "  with open(os.path.join(DATA_DIR, f\"{split_name}.csv\"), encoding=\"utf-8\") as f_csv :\n",
    "    next(f_csv, None)\n",
    "    # Split every row into pairs\n",
    "    pairs = [[s.strip('\\n') for s in row.split(',')] for row in f_csv]\n",
    "    # Accumulate all lines into two graphemes and phonemes lists\n",
    "    graphemes_list = [extract_graphemes(pair[0], grp_type) for pair in pairs] # Split grapheme as desired\n",
    "    phonemes_list = [extract_arpabet_phonemes(pair[1]) for pair in pairs]\n",
    "    lang_list = [pair[2] for pair in pairs]\n",
    "    # Create the G2PDataset object\n",
    "    g2p_dataset = G2PDataset(graphemes_list, phonemes_list, lang_list)\n",
    "  return g2p_dataset, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1740675367697,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "DwbB20gZDJOe",
    "outputId": "ec8792bd-f2b2-4246-9f0b-b93dbb078385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading train entries ..\n",
      "Reading val entries ..\n",
      "Reading test entries ..\n",
      "EN_WEIGHT: 0.6142802737996211\n",
      "ID_WEIGHT: 2.6876041392615977\n"
     ]
    }
   ],
   "source": [
    "train_g2p_dataset, train_pairs = prepare_dataset(\"train\", grp_type=GRP_TYPE)\n",
    "val_g2p_dataset, val_pairs = prepare_dataset(\"val\", grp_type=GRP_TYPE)\n",
    "test_g2p_dataset, test_pairs = prepare_dataset(\"test\", grp_type=GRP_TYPE)\n",
    "pairs = train_pairs + val_pairs + test_pairs\n",
    "\n",
    "# Equalize grapheme and phoneme mappings for val and test set\n",
    "## Valid set\n",
    "val_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "val_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "val_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "val_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "val_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "val_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "## Test set\n",
    "test_g2p_dataset.grapheme2index = train_g2p_dataset.grapheme2index\n",
    "test_g2p_dataset.index2grapheme = train_g2p_dataset.index2grapheme\n",
    "test_g2p_dataset.n_graphemes = train_g2p_dataset.n_graphemes\n",
    "test_g2p_dataset.phoneme2index = train_g2p_dataset.phoneme2index\n",
    "test_g2p_dataset.index2phoneme = train_g2p_dataset.index2phoneme\n",
    "test_g2p_dataset.n_phonemes = train_g2p_dataset.n_phonemes\n",
    "\n",
    "# Find the maximum output sequence length among graphemes and phonemes across all datasets\n",
    "MAX_LENGTH = -999\n",
    "for graphemes in train_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in train_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in val_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in val_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "for graphemes in test_g2p_dataset.graphemes_list :\n",
    "  if MAX_LENGTH < len(graphemes) :\n",
    "    MAX_LENGTH = len(graphemes)\n",
    "for phonemes in test_g2p_dataset.phonemes_list :\n",
    "  if MAX_LENGTH < len(phonemes) :\n",
    "    MAX_LENGTH = len(phonemes)\n",
    "\n",
    "# Initialize weight loss for en and id\n",
    "N = len(train_pairs)\n",
    "K = 2\n",
    "EN_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"en\"))\n",
    "ID_WEIGHT = N/(K * sum(1 for pair in train_pairs if pair[2]==\"ma\"))\n",
    "print(f\"EN_WEIGHT: {EN_WEIGHT}\")\n",
    "print(f\"ID_WEIGHT: {ID_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1740675367742,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "O9fmoIa9M6RG"
   },
   "outputs": [],
   "source": [
    "torch.save(train_g2p_dataset.index2grapheme, os.path.join(MODELS_DIR, \"id2grp.pth\"))\n",
    "torch.save(train_g2p_dataset.index2phoneme, os.path.join(MODELS_DIR, \"id2phn.pth\"))\n",
    "\n",
    "# Custom Collate function (for padding)\n",
    "def collate_fn(batch) :\n",
    "  # batch: [(input_seq, target_seq, lang), ...]\n",
    "  graphemes, phonemes, langs = zip(*batch)\n",
    "  # Pad sequences\n",
    "  graphemes_padded = pad_sequence([torch.tensor(x) for x in graphemes], padding_value=PAD_TOKEN)\n",
    "  phonemes_padded = pad_sequence([torch.tensor(y) for y in phonemes], padding_value=PAD_TOKEN)\n",
    "  return graphemes_padded, phonemes_padded, langs\n",
    "\n",
    "train_dataloader = DataLoader(train_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "valid_dataloader = DataLoader(val_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_g2p_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9rbldUGJN7b"
   },
   "source": [
    "## Turning grapheme and phoneme data to Tensors/Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1740675367879,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "I2cem0ghY0mA"
   },
   "outputs": [],
   "source": [
    "def indexes_from_word(dataset, word, grp_type) :\n",
    "  assert grp_type in [\"unigram\", \"bigram\", \"trigram\"]\n",
    "  if grp_type == \"unigram\" :\n",
    "    graphemes = [*word]\n",
    "  elif grp_type == \"bigram\" :\n",
    "    graphemes = [word[i:i+2] for i in range(len(word)-1)] if len(word)>=2 else [word]\n",
    "  elif grp_type == \"trigram\" :\n",
    "    graphemes = [word[i:i+3] for i in range(len(word)-2)] if len(word)>=3 else [word]\n",
    "  return [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in graphemes] + [EOS_TOKEN]\n",
    "\n",
    "def variable_from_word(dataset, word, grp_type) :\n",
    "  indexes = indexes_from_word(dataset, word, grp_type)\n",
    "  var = torch.LongTensor(indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    var = var.cuda()\n",
    "  return var\n",
    "\n",
    "### BOTH NOT USED until further observation\n",
    "def indexes_from_pair(dataset, pair) :\n",
    "  \"\"\"\n",
    "  pair: [graphemes, phonemes]\n",
    "  \"\"\"\n",
    "  graphemes_indexes = [dataset.grapheme2index.get(grapheme, UNK_TOKEN) for grapheme in pair[0].split()] + [EOS_TOKEN]\n",
    "  phonemes_indexes = [dataset.phoneme2index[phoneme] for phoneme in pair[1].split()] + [EOS_TOKEN]\n",
    "  return graphemes_indexes, phonemes_indexes\n",
    "\n",
    "def variables_from_pair(dataset, pair) :\n",
    "  graphemes_indexes, phonemes_indexes = indexes_from_pair(dataset, pair)\n",
    "  graphemes_var = torch.LongTensor(graphemes_indexes).view(-1, 1)\n",
    "  phonemes_var = torch.LongTensor(phonemes_indexes).view(-1, 1)\n",
    "  if USE_CUDA :\n",
    "    graphemes_var = graphemes_var.cuda()\n",
    "    phonemes_var = phonemes_var.cuda()\n",
    "  return graphemes_var, phonemes_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1740675367882,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "433SrerOr4_Y",
    "outputId": "b03a688e-d182-44ca-a5ee-f8e0b5679352"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[369, 165, 398, 227, 577, 394, 107, 275, 416, 659, 1]\n"
     ]
    }
   ],
   "source": [
    "print(indexes_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))\n",
    "# print(variable_from_word(train_g2p_dataset, \"menguncinya\", grp_type=GRP_TYPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1740675367884,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "vBgWZU1zZIEp",
    "outputId": "801ff8d1-c0c5-49fc-a337-df5484e9e424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.G2PDataset object at 0x7f19c8343be0> ([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "([6, 99, 67, 582, 513, 1], [18, 6, 35, 1], 'en')\n",
      "train grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "valid grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "test grp 710 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>', 4: \"'a\", 5: \"'b\", 6: \"'c\", 7: \"'d\", 8: \"'e\", 9: \"'g\", 10: \"'h\", 11: \"'i\", 12: \"'j\", 13: \"'k\", 14: \"'l\", 15: \"'m\", 16: \"'n\", 17: \"'o\", 18: \"'q\", 19: \"'r\", 20: \"'s\", 21: \"'t\", 22: \"'u\", 23: \"'v\", 24: '--', 25: '-a', 26: '-b', 27: '-c', 28: '-d', 29: '-e', 30: '-g', 31: '-h', 32: '-i', 33: '-j', 34: '-k', 35: '-l', 36: '-m', 37: '-n', 38: '-p', 39: '-q', 40: '-r', 41: '-s', 42: '-t', 43: '-u', 44: '-w', 45: \"a'\", 46: 'a-', 47: 'aa', 48: 'ab', 49: 'ac', 50: 'ad', 51: 'ae', 52: 'af', 53: 'ag', 54: 'ah', 55: 'ai', 56: 'aj', 57: 'ak', 58: 'al', 59: 'am', 60: 'an', 61: 'ao', 62: 'ap', 63: 'aq', 64: 'ar', 65: 'as', 66: 'at', 67: 'au', 68: 'av', 69: 'aw', 70: 'ax', 71: 'ay', 72: 'az', 73: \"b'\", 74: 'ba', 75: 'bb', 76: 'bc', 77: 'bd', 78: 'be', 79: 'bf', 80: 'bg', 81: 'bh', 82: 'bi', 83: 'bj', 84: 'bk', 85: 'bl', 86: 'bm', 87: 'bn', 88: 'bo', 89: 'bp', 90: 'br', 91: 'bs', 92: 'bt', 93: 'bu', 94: 'bv', 95: 'bw', 96: 'by', 97: 'bz', 98: \"c'\", 99: 'ca', 100: 'cb', 101: 'cc', 102: 'cd', 103: 'ce', 104: 'cf', 105: 'cg', 106: 'ch', 107: 'ci', 108: 'cj', 109: 'ck', 110: 'cl', 111: 'cm', 112: 'cn', 113: 'co', 114: 'cp', 115: 'cq', 116: 'cr', 117: 'cs', 118: 'ct', 119: 'cu', 120: 'cv', 121: 'cw', 122: 'cy', 123: 'cz', 124: \"d'\", 125: 'da', 126: 'db', 127: 'dc', 128: 'dd', 129: 'de', 130: 'df', 131: 'dg', 132: 'dh', 133: 'di', 134: 'dj', 135: 'dk', 136: 'dl', 137: 'dm', 138: 'dn', 139: 'do', 140: 'dp', 141: 'dq', 142: 'dr', 143: 'ds', 144: 'dt', 145: 'du', 146: 'dv', 147: 'dw', 148: 'dy', 149: 'dz', 150: \"e'\", 151: 'e-', 152: 'ea', 153: 'eb', 154: 'ec', 155: 'ed', 156: 'ee', 157: 'ef', 158: 'eg', 159: 'eh', 160: 'ei', 161: 'ej', 162: 'ek', 163: 'el', 164: 'em', 165: 'en', 166: 'eo', 167: 'ep', 168: 'eq', 169: 'er', 170: 'es', 171: 'et', 172: 'eu', 173: 'ev', 174: 'ew', 175: 'ex', 176: 'ey', 177: 'ez', 178: \"f'\", 179: 'fa', 180: 'fb', 181: 'fc', 182: 'fd', 183: 'fe', 184: 'ff', 185: 'fg', 186: 'fh', 187: 'fi', 188: 'fj', 189: 'fk', 190: 'fl', 191: 'fm', 192: 'fn', 193: 'fo', 194: 'fp', 195: 'fq', 196: 'fr', 197: 'fs', 198: 'ft', 199: 'fu', 200: 'fv', 201: 'fw', 202: 'fx', 203: 'fy', 204: 'fz', 205: \"g'\", 206: 'g-', 207: 'ga', 208: 'gb', 209: 'gc', 210: 'gd', 211: 'ge', 212: 'gf', 213: 'gg', 214: 'gh', 215: 'gi', 216: 'gj', 217: 'gk', 218: 'gl', 219: 'gm', 220: 'gn', 221: 'go', 222: 'gp', 223: 'gq', 224: 'gr', 225: 'gs', 226: 'gt', 227: 'gu', 228: 'gv', 229: 'gw', 230: 'gx', 231: 'gy', 232: 'gz', 233: \"h'\", 234: 'h-', 235: 'ha', 236: 'hb', 237: 'hc', 238: 'hd', 239: 'he', 240: 'hf', 241: 'hg', 242: 'hh', 243: 'hi', 244: 'hj', 245: 'hk', 246: 'hl', 247: 'hm', 248: 'hn', 249: 'ho', 250: 'hp', 251: 'hq', 252: 'hr', 253: 'hs', 254: 'ht', 255: 'hu', 256: 'hv', 257: 'hw', 258: 'hy', 259: 'i', 260: \"i'\", 261: 'i-', 262: 'ia', 263: 'ib', 264: 'ic', 265: 'id', 266: 'ie', 267: 'if', 268: 'ig', 269: 'ih', 270: 'ii', 271: 'ij', 272: 'ik', 273: 'il', 274: 'im', 275: 'in', 276: 'io', 277: 'ip', 278: 'iq', 279: 'ir', 280: 'is', 281: 'it', 282: 'iu', 283: 'iv', 284: 'iw', 285: 'ix', 286: 'iy', 287: 'iz', 288: \"j'\", 289: 'ja', 290: 'jc', 291: 'jd', 292: 'je', 293: 'jf', 294: 'jh', 295: 'ji', 296: 'jj', 297: 'jk', 298: 'jl', 299: 'jm', 300: 'jn', 301: 'jo', 302: 'jr', 303: 'js', 304: 'jt', 305: 'ju', 306: 'jv', 307: 'jw', 308: 'jy', 309: 'jz', 310: \"k'\", 311: 'k-', 312: 'ka', 313: 'kb', 314: 'kc', 315: 'kd', 316: 'ke', 317: 'kf', 318: 'kg', 319: 'kh', 320: 'ki', 321: 'kj', 322: 'kk', 323: 'kl', 324: 'km', 325: 'kn', 326: 'ko', 327: 'kp', 328: 'kr', 329: 'ks', 330: 'kt', 331: 'ku', 332: 'kv', 333: 'kw', 334: 'ky', 335: 'kz', 336: \"l'\", 337: 'l-', 338: 'la', 339: 'lb', 340: 'lc', 341: 'ld', 342: 'le', 343: 'lf', 344: 'lg', 345: 'lh', 346: 'li', 347: 'lj', 348: 'lk', 349: 'll', 350: 'lm', 351: 'ln', 352: 'lo', 353: 'lp', 354: 'lq', 355: 'lr', 356: 'ls', 357: 'lt', 358: 'lu', 359: 'lv', 360: 'lw', 361: 'lx', 362: 'ly', 363: 'lz', 364: \"m'\", 365: 'ma', 366: 'mb', 367: 'mc', 368: 'md', 369: 'me', 370: 'mf', 371: 'mg', 372: 'mh', 373: 'mi', 374: 'mj', 375: 'mk', 376: 'ml', 377: 'mm', 378: 'mn', 379: 'mo', 380: 'mp', 381: 'mq', 382: 'mr', 383: 'ms', 384: 'mt', 385: 'mu', 386: 'mv', 387: 'mw', 388: 'my', 389: 'mz', 390: \"n'\", 391: 'n-', 392: 'na', 393: 'nb', 394: 'nc', 395: 'nd', 396: 'ne', 397: 'nf', 398: 'ng', 399: 'nh', 400: 'ni', 401: 'nj', 402: 'nk', 403: 'nl', 404: 'nm', 405: 'nn', 406: 'no', 407: 'np', 408: 'nq', 409: 'nr', 410: 'ns', 411: 'nt', 412: 'nu', 413: 'nv', 414: 'nw', 415: 'nx', 416: 'ny', 417: 'nz', 418: 'o', 419: \"o'\", 420: 'o-', 421: 'oa', 422: 'ob', 423: 'oc', 424: 'od', 425: 'oe', 426: 'of', 427: 'og', 428: 'oh', 429: 'oi', 430: 'oj', 431: 'ok', 432: 'ol', 433: 'om', 434: 'on', 435: 'oo', 436: 'op', 437: 'oq', 438: 'or', 439: 'os', 440: 'ot', 441: 'ou', 442: 'ov', 443: 'ow', 444: 'ox', 445: 'oy', 446: 'oz', 447: \"p'\", 448: 'pa', 449: 'pb', 450: 'pc', 451: 'pd', 452: 'pe', 453: 'pf', 454: 'pg', 455: 'ph', 456: 'pi', 457: 'pj', 458: 'pk', 459: 'pl', 460: 'pm', 461: 'pn', 462: 'po', 463: 'pp', 464: 'pr', 465: 'ps', 466: 'pt', 467: 'pu', 468: 'pw', 469: 'py', 470: 'pz', 471: \"q'\", 472: 'qa', 473: 'qb', 474: 'qg', 475: 'qi', 476: 'qo', 477: 'qu', 478: 'qv', 479: \"r'\", 480: 'r-', 481: 'ra', 482: 'rb', 483: 'rc', 484: 'rd', 485: 're', 486: 'rf', 487: 'rg', 488: 'rh', 489: 'ri', 490: 'rj', 491: 'rk', 492: 'rl', 493: 'rm', 494: 'rn', 495: 'ro', 496: 'rp', 497: 'rq', 498: 'rr', 499: 'rs', 500: 'rt', 501: 'ru', 502: 'rv', 503: 'rw', 504: 'rx', 505: 'ry', 506: 'rz', 507: \"s'\", 508: 's-', 509: 'sa', 510: 'sb', 511: 'sc', 512: 'sd', 513: 'se', 514: 'sf', 515: 'sg', 516: 'sh', 517: 'si', 518: 'sj', 519: 'sk', 520: 'sl', 521: 'sm', 522: 'sn', 523: 'so', 524: 'sp', 525: 'sq', 526: 'sr', 527: 'ss', 528: 'st', 529: 'su', 530: 'sv', 531: 'sw', 532: 'sx', 533: 'sy', 534: 'sz', 535: \"t'\", 536: 't-', 537: 'ta', 538: 'tb', 539: 'tc', 540: 'td', 541: 'te', 542: 'tf', 543: 'tg', 544: 'th', 545: 'ti', 546: 'tj', 547: 'tk', 548: 'tl', 549: 'tm', 550: 'tn', 551: 'to', 552: 'tp', 553: 'tr', 554: 'ts', 555: 'tt', 556: 'tu', 557: 'tv', 558: 'tw', 559: 'tx', 560: 'ty', 561: 'tz', 562: \"u'\", 563: 'u-', 564: 'ua', 565: 'ub', 566: 'uc', 567: 'ud', 568: 'ue', 569: 'uf', 570: 'ug', 571: 'uh', 572: 'ui', 573: 'uj', 574: 'uk', 575: 'ul', 576: 'um', 577: 'un', 578: 'uo', 579: 'up', 580: 'uq', 581: 'ur', 582: 'us', 583: 'ut', 584: 'uu', 585: 'uv', 586: 'uw', 587: 'ux', 588: 'uy', 589: 'uz', 590: \"v'\", 591: 'va', 592: 'vc', 593: 'vd', 594: 've', 595: 'vg', 596: 'vh', 597: 'vi', 598: 'vj', 599: 'vk', 600: 'vl', 601: 'vm', 602: 'vn', 603: 'vo', 604: 'vr', 605: 'vs', 606: 'vt', 607: 'vu', 608: 'vv', 609: 'vy', 610: \"w'\", 611: 'wa', 612: 'wb', 613: 'wc', 614: 'wd', 615: 'we', 616: 'wf', 617: 'wg', 618: 'wh', 619: 'wi', 620: 'wk', 621: 'wl', 622: 'wm', 623: 'wn', 624: 'wo', 625: 'wp', 626: 'wr', 627: 'ws', 628: 'wt', 629: 'wu', 630: 'wv', 631: 'ww', 632: 'wy', 633: 'wz', 634: \"x'\", 635: 'xa', 636: 'xb', 637: 'xc', 638: 'xd', 639: 'xe', 640: 'xf', 641: 'xg', 642: 'xh', 643: 'xi', 644: 'xl', 645: 'xm', 646: 'xn', 647: 'xo', 648: 'xp', 649: 'xq', 650: 'xr', 651: 'xs', 652: 'xt', 653: 'xu', 654: 'xv', 655: 'xw', 656: 'xx', 657: 'xy', 658: \"y'\", 659: 'ya', 660: 'yb', 661: 'yc', 662: 'yd', 663: 'ye', 664: 'yf', 665: 'yg', 666: 'yh', 667: 'yi', 668: 'yj', 669: 'yk', 670: 'yl', 671: 'ym', 672: 'yn', 673: 'yo', 674: 'yp', 675: 'yq', 676: 'yr', 677: 'ys', 678: 'yt', 679: 'yu', 680: 'yv', 681: 'yw', 682: 'yx', 683: 'yy', 684: 'yz', 685: \"z'\", 686: 'za', 687: 'zb', 688: 'zc', 689: 'zd', 690: 'ze', 691: 'zf', 692: 'zg', 693: 'zh', 694: 'zi', 695: 'zk', 696: 'zl', 697: 'zm', 698: 'zn', 699: 'zo', 700: 'zp', 701: 'zq', 702: 'zr', 703: 'zs', 704: 'zt', 705: 'zu', 706: 'zv', 707: 'zw', 708: 'zy', 709: 'zz'}\n",
      "train phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "valid phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "test phn 36 {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: 'AA', 4: 'AO', 5: 'AW', 6: 'AX', 7: 'AY', 8: 'B', 9: 'CH', 10: 'D', 11: 'EH', 12: 'EY', 13: 'F', 14: 'G', 15: 'HH', 16: 'IY', 17: 'JH', 18: 'K', 19: 'L', 20: 'M', 21: 'N', 22: 'NG', 23: 'NY', 24: 'OY', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'SH', 30: 'T', 31: 'UW', 32: 'V', 33: 'W', 34: 'Y', 35: 'Z'}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "706 {\"'c\": 6, 'ca': 99, 'au': 67, 'us': 582, 'se': 513, 'co': 113, 'ou': 441, 'ur': 581, 'rs': 499, \"'e\": 8, 'em': 164, \"'m\": 15, \"'n\": 16, \"'q\": 18, 'qu': 477, 'uo': 578, 'ot': 440, 'te': 541, \"'t\": 21, 'ti': 545, 'il': 273, 'is': 280, 'tw': 558, 'wa': 611, 'as': 65, '--': 24, '-n': 37, 'ny': 416, 'ya': 659, '-a': 25, 'an': 60, 'nd': 395, 'da': 125, '-b': 26, 'be': 78, 'el': 163, 'la': 338, '-c': 27, 'om': 433, 'mp': 380, 'pe': 452, 'en': 165, 'ng': 398, '-d': 28, 'de': 129, 'ap': 62, 'pa': 448, '-k': 34, 'ka': 312, 'ku': 331, '-l': 35, 'le': 342, 'gg': 213, 'ga': 207, '-m': 36, 'ma': 365, 'ah': 54, 'ha': 235, 'si': 517, 'sw': 531, '-s': 41, 'ep': 167, 'pi': 456, 'it': 281, '-t': 42, 'ta': 537, 'ak': 57, '-w': 44, 'at': 66, \"a'\": 45, \"'d\": 7, 'du': 145, \"'h\": 10, 'ad': 50, 'aa': 47, 'ab': 48, 'er': 169, 'rg': 487, 'ac': 49, 'ch': 106, 'he': 239, 'ke': 316, 'al': 58, 'ls': 356, 'et': 171, 'th': 544, 'am': 59, 'mo': 379, 'od': 424, 'dt': 144, 'ar': 64, 'rd': 484, 'dv': 146, 'va': 591, 'rk': 491, 'ro': 495, 'on': 434, \"n'\": 390, \"'s\": 20, 'ns': 410, 'so': 523, 'rt': 500, 'ba': 74, 'ck': 109, 'cu': 119, 'di': 133, 'ia': 262, 'ie': 266, 'ik': 272, 'ai': 55, 'im': 274, 'na': 392, 'ir': 279, 'lk': 348, 'ki': 320, 'in': 275, 'lo': 352, 'os': 439, 'do': 139, 'ne': 396, 'ed': 155, 'ni': 400, 'nm': 404, 'me': 369, 'nt': 411, 'gn': 220, 'to': 551, 'ra': 481, 'rc': 483, 're': 485, 'sc': 511, 'sh': 516, 'ts': 554, 'es': 170, 'oa': 421, 'bb': 75, 'll': 349, 'tt': 555, 'ev': 173, 'vi': 597, 'ey': 176, \"y'\": 658, 'bi': 82, 'bo': 88, 'ud': 567, 'br': 90, 'io': 276, 'ru': 501, 'uz': 589, 'zz': 709, 'ze': 690, 'bs': 91, 'by': 96, 'bc': 76, 'ek': 162, 'bd': 77, 'ic': 264, 'dn': 138, 'no': 406, 'or': 438, 'mi': 373, 'uc': 566, 'ct': 118, 'ee': 156, 'ul': 575, 'az': 72, 'zi': 694, 'iz': 287, 'ow': 443, 'dr': 142, 'rf': 486, 'fo': 193, 'rl': 492, 'rm': 493, 'rn': 494, 'hy': 258, 'rr': 498, 'ex': 175, 'nc': 394, 'ce': 103, 'bh': 81, 'ho': 249, 'id': 265, 'li': 346, 'ty': 560, 'gt': 226, 'ol': 432, 'ib': 263, 'tu': 556, 'ri': 489, 'bj': 83, 'ja': 289, 'je': 292, 'ec': 154, 'bk': 84, 'kh': 319, 'bl': 85, 'ut': 583, 'st': 528, 'ly': 362, 'bn': 87, 'nu': 412, \"o'\": 419, 'oi': 429, 'hi': 243, 'sm': 521, 'oo': 435, 'ig': 268, 'gi': 215, 'iv': 283, 've': 594, 'uh': 571, 'un': 577, 'ds': 143, \"t'\": 535, 'ov': 442, \"e'\": 150, 'eb': 153, 'ms': 383, 'mc': 367, 'cz': 123, 'zy': 708, 'yk': 669, 'tz': 561, 'wi': 619, 'ea': 152, 'ks': 329, 'eg': 158, 'go': 221, 'dg': 131, 'ge': 211, 'ko': 326, 'og': 427, 'up': 579, 'pt': 466, 'tl': 548, 'tn': 550, 'ss': 527, 'yn': 672, 'zo': 699, 'sa': 509, 'ei': 160, 'lu': 358, 'lv': 359, 'rb': 482, 'rp': 496, 'ps': 465, 'tr': 553, 'su': 529, 'bt': 92, 'bu': 93, 'ua': 564, 'hm': 247, 'uk': 574, 'dz': 149, 'lh': 345, 'ay': 71, 'ys': 677, 'bz': 97, 'zu': 705, 'ug': 570, 'ci': 107, 'my': 388, 'po': 462, 'pu': 467, 'lc': 340, 'cc': 101, 'ry': 505, 'cl': 110, 'mm': 377, 'yi': 667, 'pl': 459, 'cy': 122, \"s'\": 507, 'cr': 116, 'ue': 568, 'ui': 572, 'um': 576, 'mu': 385, 'gl': 218, 'op': 436, 'ph': 455, 'yl': 670, 'nb': 393, 'ht': 254, 'if': 267, 'fi': 187, 'fy': 203, 'dl': 136, \"r'\": 479, 'nn': 405, 'kl': 323, 'kn': 325, 'wl': 621, 'gm': 219, 'kr': 328, 'oy': 445, 'yd': 662, \"d'\": 124, 'cm': 111, 'cn': 112, 'of': 426, 'ff': 184, 'yt': 678, 'cs': 117, 'cq': 115, 'av': 68, 'ag': 53, 'ob': 422, 'ym': 671, 'uf': 569, 'gk': 217, 'sy': 533, 'yc': 661, 'iu': 282, \"m'\": 364, 'mk': 375, 'sk': 519, 'wn': 623, 'za': 686, 'dc': 127, 'oc': 423, 'dd': 128, 'eo': 166, 'dw': 147, 'we': 615, 'dy': 148, 'eh': 159, 'lb': 339, \"l'\": 336, 'lm': 350, 'lp': 353, 'sb': 510, 'eq': 168, 'rh': 488, 'ld': 341, 'lt': 357, 'dh': 132, 'gu': 227, 'kk': 322, 'kt': 330, 'ip': 277, 'gh': 214, 'aj': 56, 'ok': 431, 'oh': 428, 'iw': 284, 'gs': 225, 'dj': 134, 'jo': 301, 'ju': 305, 'dk': 135, 'dm': 137, 'lf': 343, \"f'\": 178, 'hs': 253, 'ft': 198, 'ae': 51, 'sd': 512, 'vo': 603, 'fl': 190, 'gr': 224, 'xi': 643, \"h'\": 233, 'sp': 524, \"p'\": 447, 'af': 52, 'fa': 179, 'ye': 663, 'fd': 182, 'fe': 183, 'ix': 285, 'xe': 639, 'fr': 196, 'ax': 70, 'fg': 185, \"i'\": 260, 'fh': 186, 'fm': 191, 'ef': 157, 'fu': 199, 'fw': 201, 'aw': 69, 'mn': 378, 'gy': 231, 'gf': 212, 'rw': 503, 'ih': 269, 'ew': 174, 'xc': 637, 'pp': 463, 'yo': 673, 'hh': 242, 'hk': 245, 'hl': 246, 'lg': 344, 'lq': 354, 'ub': 565, 'hn': 248, 'hr': 252, 'hu': 255, 'hw': 257, 'km': 324, 'ml': 376, 'sl': 520, 'nl': 403, 'wo': 624, \"c'\": 98, 'nk': 402, 'tc': 539, 'tk': 547, \"j'\": 288, 'ji': 295, 'jn': 300, 'oj': 430, 'uj': 573, 'k-': 311, 'ln': 351, 'kc': 314, 'hb': 236, 'mb': 366, 'kw': 333, 'ky': 334, 'kz': 335, 'l-': 337, '-q': 39, \"'a\": 4, 'tv': 557, 'uq': 580, 'rq': 497, 'wy': 632, 'ej': 161, 'eu': 172, \"x'\": 634, 'xa': 635, 'xy': 657, 'nq': 408, 'tm': 549, 'md': 368, 'nz': 417, 'fb': 180, 'ij': 271, 'iq': 278, 'lj': 347, 'nh': 399, 'nw': 414, 'yw': 681, 'ws': 627, 'lr': 355, 'uv': 585, 'lw': 360, 'ez': 177, 'mg': 371, 'mq': 381, 'oe': 425, 'fn': 192, \"k'\": 310, 'tf': 542, 'zh': 693, 'lz': 363, 'np': 407, 'xt': 652, 'zc': 688, 'zq': 701, 'mf': 370, 'mh': 372, 'yv': 680, \"g'\": 205, 'pc': 450, 'pr': 464, 'mr': 382, 'mt': 384, 'mv': 386, 'mw': 387, 'yz': 684, 'sq': 525, 'nv': 413, 'oz': 446, 'rz': 506, 'cd': 102, 'nf': 397, 'gp': 222, 'nj': 401, 'jl': 298, 'nr': 409, 'sg': 515, 'i-': 261, 'ox': 444, 'bm': 86, \"u'\": 562, 'nx': 415, 'yb': 660, 'yh': 666, 'yp': 674, 'wh': 618, 'ao': 61, 'pf': 453, 'pg': 454, 'pk': 458, 'pn': 461, 'aq': 63, 'qa': 472, 'qi': 475, 'rv': 502, 'ux': 587, 'hd': 238, 'yr': 676, 'zm': 697, 'rj': 490, \"w'\": 610, 'wr': 626, 'rx': 504, 'zt': 704, 'sn': 522, 'ii': 270, 'sf': 514, 'hc': 237, 'hf': 240, 'hv': 256, \"v'\": 590, 'sr': 526, 'uy': 588, 'vc': 592, 'vd': 593, 'vg': 595, 'vm': 601, 'vn': 602, 'vr': 604, 'vt': 606, 'wb': 612, 'wf': 616, 'wk': 620, 'wt': 628, 'wu': 629, 'xf': 640, 'xl': 644, 'xo': 647, 'xs': 651, 'yg': 665, 'yu': 679, 'yy': 683, 'zb': 687, 'zp': 700, \"b'\": 73, \"'r\": 19, 'kb': 313, 'kd': 315, 'kf': 317, 'kg': 318, 'kp': 327, 'tj': 546, 'gb': 208, 'gd': 210, 'gw': 229, \"'i\": 11, 'zs': 703, 'sz': 534, 'kv': 332, 'xq': 649, 'fs': 197, 'sv': 530, 'vs': 605, 'wm': 622, 'tb': 538, 'td': 540, 'uw': 586, 'wd': 614, 'zl': 696, 'cv': 120, 'db': 126, 'df': 130, 'dp': 140, 'vu': 607, \"'o\": 17, 'zr': 702, 'jy': 308, \"z'\": 685, 'r-': 480, 'a-': 46, '-g': 30, 'o-': 420, 's-': 508, 'gq': 223, 'jr': 302, 'fk': 189, 'g-': 206, '-j': 33, 'hp': 250, 'vy': 609, 'zd': 689, 'zn': 698, 'xu': 653, 'xb': 636, 'kj': 321, 'zk': 695, 'xh': 642, 'tg': 543, 'sj': 518, 'gj': 216, 'oq': 437, 'wc': 613, 'xw': 655, 'xx': 656, 'yf': 664, 'jd': 291, 'tp': 552, 'fc': 181, 'py': 469, 'h-': 234, 'uu': 584, 'zw': 707, 'yx': 682, 'pb': 449, 'gc': 209, 'pj': 457, 'pw': 468, \"q'\": 471, \"'v\": 23, 'jk': 297, 'pd': 451, 'pm': 460, 'gx': 230, 'iy': 286, 'hg': 241, 'bw': 95, 'wg': 617, 'wp': 625, 'zf': 691, 'vl': 600, 'cw': 121, 'mj': 374, 'vv': 608, 'xv': 654, 'bf': 79, 'hq': 251, 'dq': 141, 'lx': 361, 'vj': 598, 'xp': 648, 'wv': 630, 'jv': 306, 'zg': 692, 'fj': 188, 'xm': 645, 'xn': 646, 'jt': 304, 'xg': 641, 'tx': 559, 'vh': 596, 'mz': 389, 'fp': 194, 'gv': 228, 'jj': 296, '-e': 29, 'hj': 244, 'bg': 80, 'wz': 633, 'u-': 563, '-h': 31, 'i': 259, \"'l\": 14, 'qb': 473, 'qg': 474, 'jf': 293, 'jh': 294, '-r': 40, '-p': 38, 'js': 303, 'jc': 290, 'bv': 94, 'pz': 470, 'fq': 195, \"'b\": 5, 'cb': 100, 'cf': 104, 'cg': 105, 'cp': 114, 'cj': 108, 't-': 536, '-i': 32, 'zv': 706, 'fx': 202, \"'j\": 12, 'jz': 309, 'fz': 204, 'qv': 478, 'ww': 631, 'xr': 650, 'xd': 638, 'o': 418, \"'g\": 9, \"'k\": 13, 'e-': 151, 'n-': 391, 'vk': 599, 'qo': 476, 'jm': 299, 'yj': 668, 'fv': 200, 'bp': 89, \"'u\": 22, 'jw': 307, '-u': 43, 'sx': 532, 'yq': 675, 'gz': 232}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n",
      "33 {'K': 18, 'AX': 6, 'Z': 35, 'AO': 4, 'R': 27, 'S': 28, 'M': 20, 'N': 21, 'W': 33, 'T': 30, 'IY': 16, 'L': 19, 'NY': 23, 'AA': 3, 'D': 10, 'B': 8, 'CH': 9, 'P': 25, 'EH': 11, 'NG': 22, 'UW': 31, 'G': 14, 'HH': 15, 'Q': 26, 'V': 32, 'AY': 7, 'SH': 29, 'EY': 12, 'AW': 5, 'F': 13, 'JH': 17, 'Y': 34, 'OY': 24}\n"
     ]
    }
   ],
   "source": [
    "print(train_g2p_dataset, train_dataloader.dataset[0])\n",
    "print(train_g2p_dataset[0])\n",
    "print(train_dataloader.dataset[0])\n",
    "print(\"train grp\", len(train_g2p_dataset.index2grapheme), train_g2p_dataset.index2grapheme)\n",
    "print(\"valid grp\", len(val_g2p_dataset.index2grapheme), val_g2p_dataset.index2grapheme)\n",
    "print(\"test grp\", len(test_g2p_dataset.index2grapheme), test_g2p_dataset.index2grapheme)\n",
    "print(\"train phn\", len(train_g2p_dataset.index2phoneme), train_g2p_dataset.index2phoneme)\n",
    "print(\"valid phn\", len(val_g2p_dataset.index2phoneme), val_g2p_dataset.index2phoneme)\n",
    "print(\"test phn\", len(test_g2p_dataset.index2phoneme), test_g2p_dataset.index2phoneme)\n",
    "print(len(train_g2p_dataset.grapheme2index), train_g2p_dataset.grapheme2index)\n",
    "print(len(val_g2p_dataset.grapheme2index), val_g2p_dataset.grapheme2index)\n",
    "print(len(test_g2p_dataset.grapheme2index), test_g2p_dataset.grapheme2index)\n",
    "print(len(train_g2p_dataset.phoneme2index), train_g2p_dataset.phoneme2index)\n",
    "print(len(val_g2p_dataset.phoneme2index), val_g2p_dataset.phoneme2index)\n",
    "print(len(test_g2p_dataset.phoneme2index), test_g2p_dataset.phoneme2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rR7MN5jhZKF_"
   },
   "source": [
    "## Model definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygVvCWJJZOeP"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367889,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "Q3UD4r8FZP5U"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module) :\n",
    "  def __init__(self, input_size, emb_dim, hidden_size, n_layers=1) -> None :\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(input_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim, hidden_size, n_layers, batch_first=False)\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "\n",
    "  def forward(self, token_inputs, hidden) :\n",
    "    embedded = self.embedding(token_inputs) # [seq_len, batch_size, emb_dim]\n",
    "    output, hidden = self.gru(embedded, hidden)\n",
    "    return output, hidden # output: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "  def init_hidden(self, batch_size=1) :\n",
    "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "    # hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    if USE_CUDA :\n",
    "      hidden = hidden.cuda()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVCH2fcZZRgY"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367890,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "P_7WmJUSZSgT"
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module) :\n",
    "  def __init__(self, method, hidden_size) -> None :\n",
    "    super(Attn, self).__init__()\n",
    "    self.method = method\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    if self.method == \"general\" :\n",
    "      self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "    elif self.method == \"concat\" :\n",
    "      self.attn = nn.Linear(self.hidden_size*2, hidden_size)\n",
    "      self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "      if USE_CUDA :\n",
    "        self.attn = self.attn.cuda()\n",
    "        self.v = self.v.cuda()\n",
    "\n",
    "  def forward(self, hidden, encoder_outputs) :\n",
    "    # hidden shape: [1, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden_size]\n",
    "\n",
    "    if self.method == \"dot\" :\n",
    "      # Vectorized dot product for all positions in the sequence\n",
    "      attn_energies = torch.sum(hidden * encoder_outputs, dim=2) # [seq_len, batch_size]\n",
    "    elif self.method == \"general\" :\n",
    "      energy = self.attn(encoder_outputs) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(hidden * energy, dim=2)\n",
    "    elif self.method == \"concat\" :\n",
    "      hidden_expanded = hidden.expand(encoder_outputs.size(0), -1, -1) # [seq_len, batch_size, hidden_size]\n",
    "      energy = self.attn(torch.cat((hidden_expanded, encoder_outputs), 2)) # [seq_len, batch_size, hidden_size]\n",
    "      attn_energies = torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    # Normalize energies to weights\n",
    "    attn_weights = F.softmax(attn_energies, dim=0) # [seq_len, batch_size]\n",
    "    return attn_weights.transpose(0, 1).unsqueeze(1) # [batch_size, 1, seq_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLkvrRCoZWf3"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740675367895,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "oVtYeNNIZa8U"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module) :\n",
    "  def __init__(self, attn_model, emb_dim, hidden_size, output_size, n_layers=1, dropout_proba=.1) -> None :\n",
    "    super(Decoder, self).__init__()\n",
    "    self.attn_model = attn_model\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.n_layers = n_layers\n",
    "    self.dropout_proba = dropout_proba\n",
    "\n",
    "    # Define layers\n",
    "    self.embedding = nn.Embedding(output_size, emb_dim)\n",
    "    self.gru = nn.GRU(emb_dim + hidden_size, hidden_size, n_layers, dropout=dropout_proba, batch_first=False)\n",
    "    self.out = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    # Choose attention model\n",
    "    if attn_model != \"none\" :\n",
    "      self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    if USE_CUDA :\n",
    "      self.embedding = self.embedding.cuda()\n",
    "      self.gru = self.gru.cuda()\n",
    "      self.out = self.out.cuda()\n",
    "      self.attn = self.attn.cuda()\n",
    "\n",
    "  def forward(self, token_input, last_context, last_hidden, encoder_outputs) :\n",
    "    # token_input shape: [1, batch_size]\n",
    "    # last_context shape: [batch_size, hidden_size]\n",
    "    # last_hidden shape: [n_layers, batch_size, hidden_size]\n",
    "    # encoder_outputs shape: [seq_len, batch_size, hidden]\n",
    "    # Get the embedding of the current input token (last output token)\n",
    "\n",
    "    embedded = self.embedding(token_input) # [1, batch_size, emb_dim]\n",
    "    # Combine embedded input token and last context, run through RNN\n",
    "    rnn_input = torch.cat((embedded, last_context.unsqueeze(0)), dim=2) # [1, batch_size, emb_dim + hidden_size]\n",
    "    # GRU forward\n",
    "    rnn_output, hidden = self.gru(rnn_input, last_hidden) # rnn_output: [1, batch_size, hidden_size]\n",
    "\n",
    "    # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "    attn_weights = self.attn(rnn_output, encoder_outputs) # [batch_size, 1, seq_len]\n",
    "    context = torch.bmm(attn_weights, encoder_outputs.transpose(0, 1)) # [batch_size, 1, hidden_size]\n",
    "    context = context.transpose(0, 1) # [1, batch_size, hidden_size]\n",
    "\n",
    "    # Final output layer (next token prediction) using the RNN hidden state and context vector\n",
    "    rnn_output = rnn_output.squeeze(0)  # [batch_size, hidden_size]\n",
    "    context = context.squeeze(0)        # [batch_size, hidden_size]\n",
    "    output = torch.cat((rnn_output, context), dim=1) # [batch_size, hidden_size * 2]\n",
    "    output = F.log_softmax(self.out(output), dim=1) # [batch_size, output_size]\n",
    "\n",
    "    # Return final output, hidden state, and attention weights (for visualization)\n",
    "    return output, context, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8mDO6QlJZpUZ"
   },
   "source": [
    "## Technical check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1740675367910,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "kC8lx7n7Zr1Z",
    "outputId": "7c50e3bf-898c-41ed-9c99-f73b0bcbb1f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Input batch shape: torch.Size([4, 3])\n",
      "Encoder outputs shape: torch.Size([4, 3, 15])\n",
      "Encoder hidden shape: torch.Size([1, 3, 15])\n",
      "Step 0:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n",
      "Step 1:\n",
      "Decoder output shape: torch.Size([3, 100])\n",
      "Decoder context shape: torch.Size([3, 15])\n",
      "Decoder hidden shape: torch.Size([1, 3, 15])\n",
      "Attention shape: torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "# testing params\n",
    "batch_size = 3\n",
    "input_size = 100\n",
    "emb_dim = 25\n",
    "hidden_size = 15\n",
    "n_layers = 1\n",
    "\n",
    "# Init models\n",
    "encoder_test = Encoder(input_size, emb_dim, hidden_size, n_layers)\n",
    "decoder_test = Decoder(\"dot\", emb_dim, hidden_size, output_size=input_size, n_layers=n_layers)\n",
    "# Test encoder\n",
    "encoder_hidden = encoder_test.init_hidden(batch_size=batch_size)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "# Test input (seq_len=4, batch_size=3)\n",
    "input_batch = torch.LongTensor([[1,4,7], [2,5,8], [3,6,9], [4,7,10]])\n",
    "if USE_CUDA :\n",
    "  input_batch = input_batch.cuda()\n",
    "print(\"Input batch shape:\", input_batch.shape)\n",
    "\n",
    "encoder_outputs, encoder_hidden = encoder_test(input_batch, encoder_hidden)\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Encoder hidden shape:\", encoder_hidden.shape)\n",
    "\n",
    "# Test decoder\n",
    "decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size]) # (1, batch_size)\n",
    "decoder_context = torch.zeros(batch_size, hidden_size) # (batch_size, hidden_size)\n",
    "decoder_hidden = encoder_hidden\n",
    "if USE_CUDA :\n",
    "  decoder_input = decoder_input.cuda()\n",
    "  decoder_context = decoder_context.cuda()\n",
    "for di in range(2) :\n",
    "  decoder_output, decoder_context, decoder_hidden, attn = decoder_test(\n",
    "      decoder_input,\n",
    "      decoder_context,\n",
    "      decoder_hidden,\n",
    "      encoder_outputs\n",
    "  )\n",
    "  print(f\"Step {di}:\")\n",
    "  print(\"Decoder output shape:\", decoder_output.shape) # [batch_size, output_size]\n",
    "  print(\"Decoder context shape:\", decoder_context.shape)\n",
    "  print(\"Decoder hidden shape:\", decoder_hidden.shape) # [n_layers, batch_size, hidden_size]\n",
    "  print(\"Attention shape:\", attn.shape) # [batch_size, 1, seq_len]\n",
    "  decoder_input = torch.argmax(decoder_output, dim=1).unsqueeze(0) # Greedy decoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8eP0I6rZtvY"
   },
   "source": [
    "## Helper functions (for training log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1740675367916,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "V-3uJwDCZvqz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "def as_minutes(seconds) :\n",
    "  minutes = math.floor(seconds/60)\n",
    "  seconds -= minutes*60\n",
    "  return f\"{minutes}m {round(seconds, 2)}s\"\n",
    "\n",
    "def time_since(since, percent) :\n",
    "  now = time.time()\n",
    "  seconds = now - since\n",
    "  eta_seconds = seconds/(percent)\n",
    "  remaining_seconds = eta_seconds - seconds\n",
    "  return f\"{as_minutes(seconds)} (- {as_minutes(remaining_seconds)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbvh639cZxlA"
   },
   "source": [
    "## Train and validation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1740675368009,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "8l3KSCAaZy5K"
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "\n",
    "def train_batch(input_batch, target_batch, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion) :\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # # Debug: check for nan\n",
    "    # if torch.isnan(criterion(decoder_output, target_batch[di])).any() :\n",
    "    #   print(\"nan detected in decoder_output at step\", di)\n",
    "    #   print(f\"step {di}/{target_batch.size(0)-1}\")\n",
    "    #   print(f\"{di-1}:\\t{target_batch[di-1]}\")\n",
    "    #   for i in range(di, target_batch.size(0)) :\n",
    "    #     print(f\"{i}:\\t{target_batch[i]}\")\n",
    "    #   print(target_batch)\n",
    "    #   print(decoder_output.shape, target_batch[di].shape, criterion(decoder_output, target_batch[di]))\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Teacher forcing\n",
    "    decoder_input = target_batch[di].unsqueeze(0) if random.random() < teacher_forcing_ratio else decoder_output.argmax(1).unsqueeze(0)\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  # Apply language weights\n",
    "  weights = torch.tensor([EN_WEIGHT if lang==\"en\" else ID_WEIGHT for lang in langs])\n",
    "  if USE_CUDA :\n",
    "    weights = weights.cuda()\n",
    "  weighted_loss = (loss * weights).mean()\n",
    "\n",
    "  # Backpropagate weighted loss\n",
    "  weighted_loss.backward()\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item(), weighted_loss.item()\n",
    "\n",
    "def infer_batch(input_batch, target_batch, encoder, decoder, criterion) :\n",
    "  if USE_CUDA :\n",
    "    input_batch = input_batch.cuda()\n",
    "    target_batch = target_batch.cuda()\n",
    "\n",
    "  batch_size = input_batch.size(1)\n",
    "  # Forward through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_batch, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN] * batch_size])\n",
    "  decoder_context = torch.zeros(batch_size, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  loss = 0\n",
    "  for di in range(target_batch.size(0)) :\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Calculate loss for this step (sum over batch)\n",
    "    loss += criterion(decoder_output, target_batch[di])\n",
    "    # Greedy decoding without teacher forcing\n",
    "    topi = decoder_output.argmax(1)\n",
    "    decoder_input = topi.unsqueeze(0).detach() # detach from history\n",
    "\n",
    "  # Normalize loss\n",
    "  loss /= target_batch.size(0)\n",
    "\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9nserDfZ4Xo"
   },
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1857,
     "status": "ok",
     "timestamp": 1740675369864,
     "user": {
      "displayName": "23522026 Ahmad Naufal Hakim",
      "userId": "16621933245327613442"
     },
     "user_tz": -420
    },
    "id": "s1JHP18oZ6zy",
    "outputId": "95183643-e690-43c7-c973-86c6d9cce6ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmadnaufalhakim/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_model: dot\n",
      "emb_dim: 256\n",
      "hidden_size: 128\n",
      "n_layers: 1\n",
      "Encoder has a total number of 329984 parameters\n",
      "Decoder has a total number of 215844 parameters\n",
      "Total number of all parameters is 545828\n"
     ]
    }
   ],
   "source": [
    "attn_model = ATTN_MODEL\n",
    "emb_dim = EMB_DIM\n",
    "hidden_size = HIDDEN_SIZE\n",
    "n_layers = N_LAYERS\n",
    "dropout_proba = DROPOUT_PROBA\n",
    "\n",
    "# Initialize models\n",
    "encoder = Encoder(train_g2p_dataset.n_graphemes, int(emb_dim), int(hidden_size), int(n_layers))\n",
    "decoder = Decoder(attn_model, int(emb_dim), int(hidden_size), train_g2p_dataset.n_phonemes, int(n_layers), dropout_proba=dropout_proba)\n",
    "n_encoder_parameters = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "n_decoder_parameters = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"attn_model: {attn_model}\")\n",
    "print(f\"emb_dim: {emb_dim}\")\n",
    "print(f\"hidden_size: {hidden_size}\")\n",
    "print(f\"n_layers: {n_layers}\")\n",
    "print(f\"Encoder has a total number of {n_encoder_parameters} parameters\")\n",
    "print(f\"Decoder has a total number of {n_decoder_parameters} parameters\")\n",
    "print(f\"Total number of all parameters is {n_encoder_parameters+n_decoder_parameters}\")\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA :\n",
    "  encoder.cuda()\n",
    "  decoder.cuda()\n",
    "\n",
    "# Learning rate and weight decay parameters\n",
    "learning_rate = .001\n",
    "weight_decay = 1e-5\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 999\n",
    "epochs_without_improvement = 0\n",
    "# Learning rate scheduling\n",
    "encoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(encoder_optimizer, factor=.5)\n",
    "decoder_scheduler = optim.lr_scheduler.ReduceLROnPlateau(decoder_optimizer, factor=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OI03JU0Z9dw"
   },
   "source": [
    "## Run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gP6Zh2q4aBvd",
    "outputId": "200a4116-04e5-447b-885d-c22f6ad6642f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch 1 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 1 finished in 0m 40.41s (- 66m 40.17s) (1 1.0%). train avg loss: 1.1433, val avg loss: 1.1965\n",
      "Training for epoch 2 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 2 finished in 1m 22.5s (- 67m 22.53s) (2 2.0%). train avg loss: 0.5719, val avg loss: 0.9944\n",
      "Training for epoch 3 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 3 finished in 2m 13.82s (- 72m 6.86s) (3 3.0%). train avg loss: 0.4667, val avg loss: 0.9952\n",
      "Training for epoch 4 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 4 finished in 3m 4.65s (- 73m 51.7s) (4 4.0%). train avg loss: 0.4346, val avg loss: 1.0482\n",
      "Training for epoch 5 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 5 finished in 3m 48.4s (- 72m 19.6s) (5 5.0%). train avg loss: 0.4122, val avg loss: 0.9634\n",
      "Training for epoch 6 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 6 finished in 4m 30.91s (- 70m 44.27s) (6 6.0%). train avg loss: 0.3934, val avg loss: 0.9523\n",
      "Training for epoch 7 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 7 finished in 5m 14.91s (- 69m 43.75s) (7 7.0%). train avg loss: 0.4146, val avg loss: 0.9594\n",
      "Training for epoch 8 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 8 finished in 5m 58.05s (- 68m 37.57s) (8 8.0%). train avg loss: 0.3851, val avg loss: 0.9347\n",
      "Training for epoch 9 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 9 finished in 6m 38.02s (- 67m 4.42s) (9 9.0%). train avg loss: 0.3833, val avg loss: 0.8785\n",
      "Training for epoch 10 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 10 finished in 7m 21.45s (- 66m 13.09s) (10 10.0%). train avg loss: 0.3408, val avg loss: 0.8685\n",
      "Training for epoch 11 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 11 finished in 8m 3.44s (- 65m 11.45s) (11 11.0%). train avg loss: 0.3329, val avg loss: 0.8656\n",
      "Training for epoch 12 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 12 finished in 8m 48.82s (- 64m 38.01s) (12 12.0%). train avg loss: 0.3266, val avg loss: 0.8459\n",
      "Training for epoch 13 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 13 finished in 9m 32.3s (- 63m 50.03s) (13 13.0%). train avg loss: 0.3243, val avg loss: 0.8603\n",
      "Training for epoch 14 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 14 finished in 10m 18.4s (- 63m 18.72s) (14 14.0%). train avg loss: 0.3285, val avg loss: 0.7957\n",
      "Training for epoch 15 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 15 finished in 11m 3.65s (- 62m 40.67s) (15 15.0%). train avg loss: 0.3441, val avg loss: 0.8281\n",
      "Training for epoch 16 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 16 finished in 11m 49.66s (- 62m 5.73s) (16 16.0%). train avg loss: 0.329, val avg loss: 0.8019\n",
      "Training for epoch 17 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 17 finished in 12m 34.04s (- 61m 21.5s) (17 17.0%). train avg loss: 0.32, val avg loss: 0.8756\n",
      "Training for epoch 18 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 18 finished in 13m 18.17s (- 60m 36.11s) (18 18.0%). train avg loss: 0.3295, val avg loss: 0.7713\n",
      "Training for epoch 19 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 19 finished in 13m 59.1s (- 59m 37.23s) (19 19.0%). train avg loss: 0.2964, val avg loss: 0.8181\n",
      "Training for epoch 20 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 20 finished in 14m 39.95s (- 58m 39.82s) (20 20.0%). train avg loss: 0.3183, val avg loss: 0.855\n",
      "Training for epoch 21 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 21 finished in 15m 25.83s (- 58m 2.87s) (21 21.0%). train avg loss: 0.3065, val avg loss: 0.8225\n",
      "Training for epoch 22 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 22 finished in 16m 14.38s (- 57m 34.61s) (22 22.0%). train avg loss: 0.288, val avg loss: 0.7678\n",
      "Training for epoch 23 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 23 finished in 17m 5.27s (- 57m 12.41s) (23 23.0%). train avg loss: 0.2901, val avg loss: 0.8111\n",
      "Training for epoch 24 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 24 finished in 17m 48.15s (- 56m 22.47s) (24 24.0%). train avg loss: 0.2917, val avg loss: 0.7627\n",
      "Training for epoch 25 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 25 finished in 18m 40.18s (- 56m 0.55s) (25 25.0%). train avg loss: 0.2645, val avg loss: 0.7312\n",
      "Training for epoch 26 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 26 finished in 19m 30.19s (- 55m 30.55s) (26 26.0%). train avg loss: 0.2886, val avg loss: 0.827\n",
      "Training for epoch 27 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 27 finished in 20m 16.44s (- 54m 48.89s) (27 27.0%). train avg loss: 0.2625, val avg loss: 0.7208\n",
      "Training for epoch 28 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 28 finished in 20m 58.32s (- 53m 55.69s) (28 28.0%). train avg loss: 0.261, val avg loss: 0.736\n",
      "Training for epoch 29 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 29 finished in 21m 40.04s (- 53m 2.85s) (29 29.0%). train avg loss: 0.3053, val avg loss: 0.7857\n",
      "Training for epoch 30 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 30 finished in 22m 20.48s (- 52m 7.78s) (30 30.0%). train avg loss: 0.2622, val avg loss: 0.8073\n",
      "Training for epoch 31 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 31 finished in 23m 4.51s (- 51m 21.65s) (31 31.0%). train avg loss: 0.2811, val avg loss: 0.7437\n",
      "Training for epoch 32 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 32 finished in 23m 50.82s (- 50m 40.49s) (32 32.0%). train avg loss: 0.2472, val avg loss: 0.7222\n",
      "Training for epoch 33 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 33 finished in 24m 31.27s (- 49m 47.12s) (33 33.0%). train avg loss: 0.2459, val avg loss: 0.7303\n",
      "Training for epoch 34 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 34 finished in 25m 10.48s (- 48m 52.1s) (34 34.0%). train avg loss: 0.2371, val avg loss: 0.6847\n",
      "Training for epoch 35 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 35 finished in 25m 51.02s (- 48m 0.47s) (35 35.0%). train avg loss: 0.2537, val avg loss: 0.7539\n",
      "Training for epoch 36 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 36 finished in 26m 37.0s (- 47m 19.1s) (36 36.0%). train avg loss: 0.2522, val avg loss: 0.6887\n",
      "Training for epoch 37 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 37 finished in 27m 20.01s (- 46m 32.44s) (37 37.0%). train avg loss: 0.2301, val avg loss: 0.701\n",
      "Training for epoch 38 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 38 finished in 28m 0.54s (- 45m 41.93s) (38 38.0%). train avg loss: 0.2207, val avg loss: 0.7379\n",
      "Training for epoch 39 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 39 finished in 28m 39.08s (- 44m 48.82s) (39 39.0%). train avg loss: 0.2434, val avg loss: 0.6786\n",
      "Training for epoch 40 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 40 finished in 29m 19.24s (- 43m 58.86s) (40 40.0%). train avg loss: 0.2319, val avg loss: 0.7555\n",
      "Training for epoch 41 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 41 finished in 30m 1.72s (- 43m 12.72s) (41 41.0%). train avg loss: 0.2719, val avg loss: 0.765\n",
      "Training for epoch 42 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 42 finished in 30m 41.77s (- 42m 23.4s) (42 42.0%). train avg loss: 0.2295, val avg loss: 0.7681\n",
      "Training for epoch 43 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 43 finished in 31m 24.49s (- 41m 38.05s) (43 43.0%). train avg loss: 0.2606, val avg loss: 0.6935\n",
      "Training for epoch 44 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 44 finished in 32m 2.6s (- 40m 46.95s) (44 44.0%). train avg loss: 0.2266, val avg loss: 0.7388\n",
      "Training for epoch 45 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 45 finished in 32m 40.74s (- 39m 56.46s) (45 45.0%). train avg loss: 0.2456, val avg loss: 0.7208\n",
      "Training for epoch 46 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 46 finished in 33m 18.67s (- 39m 6.27s) (46 46.0%). train avg loss: 0.2259, val avg loss: 0.6554\n",
      "Training for epoch 47 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 47 finished in 33m 56.89s (- 38m 16.92s) (47 47.0%). train avg loss: 0.2075, val avg loss: 0.6994\n",
      "Training for epoch 48 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 48 finished in 34m 44.53s (- 37m 38.24s) (48 48.0%). train avg loss: 0.2363, val avg loss: 0.644\n",
      "Training for epoch 49 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 49 finished in 35m 31.16s (- 36m 58.14s) (49 49.0%). train avg loss: 0.2395, val avg loss: 0.7061\n",
      "Training for epoch 50 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 50 finished in 36m 14.5s (- 36m 14.5s) (50 50.0%). train avg loss: 0.2166, val avg loss: 0.6379\n",
      "Training for epoch 51 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 51 finished in 36m 52.15s (- 35m 25.4s) (51 51.0%). train avg loss: 0.2285, val avg loss: 0.6703\n",
      "Training for epoch 52 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 52 finished in 37m 30.23s (- 34m 37.13s) (52 52.0%). train avg loss: 0.2172, val avg loss: 0.6434\n",
      "Training for epoch 53 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 53 finished in 38m 8.27s (- 33m 49.22s) (53 53.0%). train avg loss: 0.2809, val avg loss: 0.6991\n",
      "Training for epoch 54 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 54 finished in 38m 46.64s (- 33m 1.95s) (54 54.0%). train avg loss: 0.3856, val avg loss: 0.8111\n",
      "Training for epoch 55 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 55 finished in 39m 24.22s (- 32m 14.36s) (55 55.0%). train avg loss: 0.2717, val avg loss: 0.6791\n",
      "Training for epoch 56 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 56 finished in 40m 2.04s (- 31m 27.32s) (56 56.0%). train avg loss: 0.2492, val avg loss: 0.6762\n",
      "Training for epoch 57 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 57 finished in 40m 39.98s (- 30m 40.69s) (57 57.0%). train avg loss: 0.2167, val avg loss: 0.6509\n",
      "Training for epoch 58 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 58 finished in 41m 18.09s (- 29m 54.48s) (58 58.0%). train avg loss: 0.2064, val avg loss: 0.6213\n",
      "Training for epoch 59 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 59 finished in 41m 56.02s (- 29m 8.42s) (59 59.0%). train avg loss: 0.2158, val avg loss: 0.6791\n",
      "Training for epoch 60 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 60 finished in 42m 33.6s (- 28m 22.4s) (60 60.0%). train avg loss: 0.2257, val avg loss: 0.6863\n",
      "Training for epoch 61 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 61 finished in 43m 12.88s (- 27m 37.74s) (61 61.0%). train avg loss: 0.2173, val avg loss: 0.6605\n",
      "Training for epoch 62 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 62 finished in 43m 53.53s (- 26m 54.1s) (62 62.0%). train avg loss: 0.2238, val avg loss: 0.7004\n",
      "Training for epoch 63 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 63 finished in 44m 31.01s (- 26m 8.69s) (63 63.0%). train avg loss: 0.226, val avg loss: 0.6571\n",
      "Training for epoch 64 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 64 finished in 45m 8.61s (- 25m 23.6s) (64 64.0%). train avg loss: 0.2015, val avg loss: 0.6056\n",
      "Training for epoch 65 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 65 finished in 45m 46.25s (- 24m 38.75s) (65 65.0%). train avg loss: 0.1986, val avg loss: 0.5989\n",
      "Training for epoch 66 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 66 finished in 46m 23.63s (- 23m 53.99s) (66 66.0%). train avg loss: 0.2007, val avg loss: 0.6358\n",
      "Training for epoch 67 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 67 finished in 47m 1.67s (- 23m 9.78s) (67 67.0%). train avg loss: 0.2051, val avg loss: 0.6614\n",
      "Training for epoch 68 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 68 finished in 47m 39.28s (- 22m 25.54s) (68 68.0%). train avg loss: 0.2169, val avg loss: 0.6738\n",
      "Training for epoch 69 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 69 finished in 48m 17.38s (- 21m 41.72s) (69 69.0%). train avg loss: 0.1982, val avg loss: 0.6059\n",
      "Training for epoch 70 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 70 finished in 48m 54.93s (- 20m 57.83s) (70 70.0%). train avg loss: 0.2101, val avg loss: 0.5887\n",
      "Training for epoch 71 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 71 finished in 49m 34.94s (- 20m 15.12s) (71 71.0%). train avg loss: 0.1864, val avg loss: 0.597\n",
      "Training for epoch 72 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 72 finished in 50m 15.06s (- 19m 32.52s) (72 72.0%). train avg loss: 0.1942, val avg loss: 0.6352\n",
      "Training for epoch 73 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 73 finished in 50m 55.0s (- 18m 49.93s) (73 73.0%). train avg loss: 0.1796, val avg loss: 0.6035\n",
      "Training for epoch 74 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 74 finished in 51m 35.25s (- 18m 7.52s) (74 74.0%). train avg loss: 0.197, val avg loss: 0.6523\n",
      "Training for epoch 75 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 75 finished in 52m 14.95s (- 17m 24.98s) (75 75.0%). train avg loss: 0.1976, val avg loss: 0.6438\n",
      "Training for epoch 76 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 76 finished in 52m 52.29s (- 16m 41.78s) (76 76.0%). train avg loss: 0.191, val avg loss: 0.6306\n",
      "Training for epoch 77 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 77 finished in 53m 30.06s (- 15m 58.85s) (77 77.0%). train avg loss: 0.1743, val avg loss: 0.6055\n",
      "Training for epoch 78 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 78 finished in 54m 9.18s (- 15m 16.44s) (78 78.0%). train avg loss: 0.1954, val avg loss: 0.6428\n",
      "Training for epoch 79 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 79 finished in 54m 47.91s (- 14m 34.0s) (79 79.0%). train avg loss: 0.199, val avg loss: 0.621\n",
      "Training for epoch 80 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 80 finished in 55m 27.38s (- 13m 51.84s) (80 80.0%). train avg loss: 0.1757, val avg loss: 0.7001\n",
      "Training for epoch 81 has started (lr=0.001). Found 1916 batch(es).\n",
      "Epoch 81 finished in 56m 7.47s (- 13m 9.9s) (81 81.0%). train avg loss: 0.1921, val avg loss: 0.7022\n",
      "Training for epoch 82 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 82 finished in 56m 45.42s (- 12m 27.53s) (82 82.0%). train avg loss: 0.1797, val avg loss: 0.5707\n",
      "Training for epoch 83 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 83 finished in 57m 24.06s (- 11m 45.41s) (83 83.0%). train avg loss: 0.1546, val avg loss: 0.5605\n",
      "Training for epoch 84 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 84 finished in 58m 3.11s (- 11m 3.45s) (84 84.0%). train avg loss: 0.1489, val avg loss: 0.5629\n",
      "Training for epoch 85 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 85 finished in 58m 41.28s (- 10m 21.4s) (85 85.0%). train avg loss: 0.1506, val avg loss: 0.5709\n",
      "Training for epoch 86 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 86 finished in 59m 18.76s (- 9m 39.33s) (86 86.0%). train avg loss: 0.1434, val avg loss: 0.5655\n",
      "Training for epoch 87 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 87 finished in 59m 56.55s (- 8m 57.42s) (87 87.0%). train avg loss: 0.1391, val avg loss: 0.5835\n",
      "Training for epoch 88 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 88 finished in 60m 35.54s (- 8m 15.76s) (88 88.0%). train avg loss: 0.1367, val avg loss: 0.5655\n",
      "Training for epoch 89 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 89 finished in 61m 14.73s (- 7m 34.18s) (89 89.0%). train avg loss: 0.1394, val avg loss: 0.5609\n",
      "Training for epoch 90 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 90 finished in 61m 55.52s (- 6m 52.84s) (90 90.0%). train avg loss: 0.1336, val avg loss: 0.5604\n",
      "Training for epoch 91 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 91 finished in 62m 35.98s (- 6m 11.47s) (91 91.0%). train avg loss: 0.1324, val avg loss: 0.5423\n",
      "Training for epoch 92 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 92 finished in 63m 17.0s (- 5m 30.17s) (92 92.0%). train avg loss: 0.1325, val avg loss: 0.5895\n",
      "Training for epoch 93 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 93 finished in 63m 57.29s (- 4m 48.83s) (93 93.0%). train avg loss: 0.131, val avg loss: 0.5616\n",
      "Training for epoch 94 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 94 finished in 64m 37.02s (- 4m 7.47s) (94 94.0%). train avg loss: 0.1292, val avg loss: 0.5723\n",
      "Training for epoch 95 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 95 finished in 65m 17.53s (- 3m 26.19s) (95 95.0%). train avg loss: 0.1339, val avg loss: 0.5389\n",
      "Training for epoch 96 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 96 finished in 65m 57.98s (- 2m 44.92s) (96 96.0%). train avg loss: 0.125, val avg loss: 0.6075\n",
      "Training for epoch 97 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 97 finished in 66m 38.47s (- 2m 3.66s) (97 97.0%). train avg loss: 0.1255, val avg loss: 0.5509\n",
      "Training for epoch 98 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 98 finished in 67m 18.24s (- 1m 22.41s) (98 98.0%). train avg loss: 0.1249, val avg loss: 0.5669\n",
      "Training for epoch 99 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 99 finished in 67m 59.36s (- 0m 41.21s) (99 99.0%). train avg loss: 0.1287, val avg loss: 0.5538\n",
      "Training for epoch 100 has started (lr=0.0005). Found 1916 batch(es).\n",
      "Epoch 100 finished in 68m 40.95s (- 0m 0.0s) (100 100.0%). train avg loss: 0.1231, val avg loss: 0.5697\n"
     ]
    }
   ],
   "source": [
    "# Training configurations\n",
    "n_epochs = 100\n",
    "# Keep track of time elapsed and running averages\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Keep track of the best validation set loss\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(1, n_epochs+1) :\n",
    "  # Set both encoder and decoder to training mode\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  print(f\"Training for epoch {epoch} has started (lr={encoder_optimizer.param_groups[0]['lr']}). Found {len(train_dataloader)} batch(es).\")\n",
    "  # Training\n",
    "  total_train_loss = 0\n",
    "  total_train_tokens = 0\n",
    "  for batch, (grps, phns, langs) in enumerate(train_dataloader) :\n",
    "    # Count tokens (excluding padding)\n",
    "    total_train_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "    # Train and get WEIGHTED loss\n",
    "    unweighted_train_loss, weighted_train_loss = train_batch(grps, phns, langs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    # Track UNWEIGHTED train loss for logging\n",
    "    total_train_loss += unweighted_train_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Set both encoder and decoder to evaluation mode\n",
    "  encoder.eval()\n",
    "  decoder.eval()\n",
    "  # Validation\n",
    "  with torch.no_grad() :\n",
    "    total_val_loss = 0\n",
    "    total_val_tokens = 0\n",
    "    for grps, phns, langs in valid_dataloader :\n",
    "      # Count tokens (excluding padding)\n",
    "      total_val_tokens += (phns!=PAD_TOKEN).sum().item()\n",
    "      # Infer and get loss\n",
    "      val_loss = infer_batch(grps, phns, encoder, decoder, criterion)\n",
    "      # Track val loss for logging\n",
    "      total_val_loss += val_loss * (phns!=PAD_TOKEN).sum().item()\n",
    "\n",
    "  # Calculate epoch metrics\n",
    "  avg_train_loss = total_train_loss/total_train_tokens\n",
    "  avg_val_loss = total_val_loss/total_val_tokens\n",
    "  print(f\"Epoch {epoch} finished in {time_since(start, epoch/n_epochs)} ({epoch} {epoch*100/n_epochs}%). train avg loss: {round(avg_train_loss, 4)}, val avg loss: {round(avg_val_loss, 4)}\")\n",
    "\n",
    "  # Update schedulers based on validation loss\n",
    "  encoder_scheduler.step(avg_val_loss)\n",
    "  decoder_scheduler.step(avg_val_loss)\n",
    "\n",
    "  # Save the losses for visualization\n",
    "  train_losses.append(avg_train_loss)\n",
    "  val_losses.append(avg_val_loss)\n",
    "\n",
    "  # Save the model if the validation loss is better than the previous iterations' validation loss\n",
    "  if avg_val_loss < best_val_loss :\n",
    "    epochs_without_improvement = 0\n",
    "    if epoch > 1 :\n",
    "      previous_best_encoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      previous_best_decoder = [f for f in os.listdir(MODELS_DIR) if f.startswith(f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}\")][0]\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_encoder))\n",
    "      os.remove(os.path.join(MODELS_DIR, previous_best_decoder))\n",
    "    torch.save(encoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-encoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    torch.save(decoder.state_dict(), os.path.join(MODELS_DIR, f\"FIN-decoder-wdecay_1e_5-attn_{attn_model}-emb_{emb_dim}-hddn_{hidden_size}-layers_{n_layers}-epoch_{epoch}.pth\"))\n",
    "    best_val_loss = avg_val_loss\n",
    "  else :\n",
    "    epochs_without_improvement += 1\n",
    "    if epochs_without_improvement >= patience :\n",
    "      print(f\"Early stopping after {epoch} epochs\")\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvWFlA69aJXJ"
   },
   "source": [
    "## Visualize training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-498emHUaNzb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_plot(**kwargs) :\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # This locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=.2)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.title(\"Loss\")\n",
    "  legends = []\n",
    "  for k, v in kwargs.items() :\n",
    "    plt.plot(v)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    legends.append(k)\n",
    "    if k == \"val\" :\n",
    "      # Find the minimum value and its index\n",
    "      min_value = min(v)\n",
    "      min_index = v.index(min_value)\n",
    "      # Plot a red dot at the minimum value\n",
    "      plt.plot(min_index, min_value, \"ro\")\n",
    "      # Add text box in the middle of the plot showing the minimum value\n",
    "      plt.text(min_index-5, min_value-.08, f\"{min_value:.4f}\", bbox=dict(facecolor=\"white\", alpha=.5))\n",
    "  plt.legend(legends)\n",
    "\n",
    "show_plot(train=train_losses, val=val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8q3Ubs5aUWa"
   },
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgFQoUrINk_L"
   },
   "source": [
    "### Helper functions to evaluate encoder-decoder attention model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "wHJTHcbONu7W"
   },
   "outputs": [],
   "source": [
    "def evaluate(word, max_length=MAX_LENGTH) :\n",
    "  # Convert word to tensor with batch dimension\n",
    "  input_variable = variable_from_word(test_g2p_dataset, word, grp_type=GRP_TYPE) # Already in [seq_len, 1]\n",
    "\n",
    "  # Run through encoder\n",
    "  encoder_hidden = encoder.init_hidden(batch_size=1)\n",
    "  encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "  # Prepare decoder inputs\n",
    "  decoder_input = torch.LongTensor([[SOS_TOKEN]]) # [1, 1]\n",
    "  decoder_context = torch.zeros(1, decoder.hidden_size)\n",
    "  decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "  if USE_CUDA :\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "  decoded_phonemes = []\n",
    "  attentions = torch.zeros(max_length, max_length)\n",
    "  for di in range(max_length) :\n",
    "    decoder_output, decoder_context, decoder_hidden, attn_weights = decoder(\n",
    "      decoder_input, decoder_context, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    # Store attention\n",
    "    attentions[di, :attn_weights.size(2)] += attn_weights.squeeze(0).squeeze(0).cpu().data\n",
    "    # Get most likely token\n",
    "    topv, topi = decoder_output.data.topk(1)\n",
    "    ni = topi[0][0]\n",
    "    if ni.item() == EOS_TOKEN :\n",
    "      decoded_phonemes.append(\"<EOS>\")\n",
    "      break\n",
    "    else :\n",
    "      decoded_phonemes.append(test_g2p_dataset.index2phoneme[ni.item()])\n",
    "    # Next input is predicted token\n",
    "    decoder_input = torch.LongTensor([[ni.item()]])\n",
    "    if USE_CUDA :\n",
    "      decoder_input = decoder_input.cuda()\n",
    "\n",
    "  return decoded_phonemes, attentions[:di+1, 1:len(encoder_outputs)]\n",
    "\n",
    "def evaluate_randomly() :\n",
    "  pair = random.choice(pairs)\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  output_phoneme_sequence = ' '.join(output_phonemes)\n",
    "  label_phonemes = arpabet_phoneme_sequence.split()\n",
    "\n",
    "  print('>', word)\n",
    "  print('=', label_phonemes)\n",
    "  print('<', output_phoneme_sequence, output_phonemes)\n",
    "  print('')\n",
    "  return pair, output_phonemes, decoder_attns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hedzsfDAM_LC"
   },
   "source": [
    "### Using `val` set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MlSPdqo3QDyr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone error rate (PER) on val set: 21.60611259446093%\n"
     ]
    }
   ],
   "source": [
    "# Total Phone Error Rate (PER)\n",
    "total_per = .0\n",
    "for pair in val_pairs :\n",
    "  word, arpabet_phoneme_sequence, lang = pair\n",
    "  output_phonemes, decoder_attns = evaluate(word)\n",
    "  try :\n",
    "    output_phonemes.remove(\"<EOS>\")\n",
    "  except ValueError as e :\n",
    "    pass\n",
    "  total_per += wer(\n",
    "    arpabet_phoneme_sequence,\n",
    "    ' '.join(output_phonemes)\n",
    "  )\n",
    "average_per = total_per / len(val_pairs)\n",
    "print(f\"Phone error rate (PER) on val set: {average_per*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yQDWj6lNzOZ"
   },
   "source": [
    "### Using randomly chosen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HSHGOjSmc3Vi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> corse\n",
      "= ['K', 'AO', 'R', 'S']\n",
      "< S ['S']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1822c60ca0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 960x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair, output_phonemes, decoder_attns = evaluate_randomly()\n",
    "plt.matshow(decoder_attns.numpy())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMRh9GumEBVlz3ZAFeGMpGk",
   "collapsed_sections": [
    "T8eP0I6rZtvY"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
